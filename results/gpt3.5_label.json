{
    "response1": "Descriptive Statistics: [\"table 1 ( upper part ) shows the results of our basic semantic parser ( with glove embeddings ) on all six graphbanks\", \"our results are competitive across the board, and set a new state of the art for eds smatch scores ( cai and knight , 2013 ) among eds parsers which are not trained on gold syntax information\", \"our edm score ( dridan and oepen , 2011 ) on eds is lower\", \"the results on the test dataset are shown in table 1 ( bottom )\", \"with glove, multi - task learning led to substantial improvements; with bert the improvements are smaller but still noticeable\"]\nComparison and Contrast: [\"the use of bert embeddings is highly effective across the board\", \"we set a new state of the art ( without gold syntax ) on all graphbanks except amr 2017; the improvement is particularly pronounced in the out - of - domain evaluations, illustrating bert ' s ability to transfer across domains\"]\nTrend and Pattern Analysis: [\"the improvements are smaller but still noticeable\"]\nProportional and Percentage Calculations: [\"our results are competitive across the board, and set a new state of the art for eds smatch scores ( cai and knight , 2013 ) among eds parsers which are not trained on gold syntax information\"]\nData Manipulation and Calculation: [\"the use of bert embeddings is highly effective across the board\", \"we set a new state of the art ( without gold syntax ) on all graphbanks except amr 2017; the improvement is particularly pronounced in the out - of - domain evaluations, illustrating bert ' s ability to transfer across domains\"]\nCausal Relationships and Evaluation: [\"with glove, multi - task learning led to substantial improvements; with bert the improvements are smaller but still noticeable\"]",
    "response2": "Descriptive Statistics: [\"we conduct transfer learning on four different combinations of mednli , snli , and mnli as it shown in the table 4 ( line 4 to 7 ) and also add the results of general domain tasks ( mnli , snli ) for comparison .\", \"bert performs better on tasks in the general domain while biobert performs better on mednli which is in the clinical domain .\", \"positive transfer occurs on mednli .\", \"even though biobert is finetuned on general domain tasks before mednli , transfer learning shows better results than that fine - tuned on mednli directly .\", \"the domain specific language representations from biobert are maintained while fine - tuning on general domain tasks by showing that the transfer learning results of mednli on biobert have better performance than the results on bert ( line 4 to 7 ) .\", \"the accuracy of mnli and snli on biobert is lower than the accuracy on bert .\", \"the best combination is snli → mnli → mednli on biobert .\", \"mednli ( expanded ) shows better performance than mednli on biobert while mednli works better on bert ( see table 4 ) .\", \"the performance of mednli ( expanded ) with transfer learning is higher on bert and lower on biobert than the performance of mednli with transfer learning .\"]\nComparison and Contrast: [\"bert performs better on tasks in the general domain while biobert performs better on mednli which is in the clinical domain .\", \"even though biobert is finetuned on general domain tasks before mednli , transfer learning shows better results than that fine - tuned on mednli directly .\", \"the accuracy of mnli and snli on biobert is lower than the accuracy on bert .\", \"mednli ( expanded ) shows better performance than mednli on biobert while mednli works better on bert ( see table 4 ) .\"]\nTrend and Pattern Analysis: [\"the domain specific language representations from biobert are maintained while fine - tuning on general domain tasks by showing that the transfer learning results of mednli on biobert have better performance than the results on bert ( line 4 to 7 ) .\"]\nProportional and Percentage Calculations: [\"we conduct transfer learning on four different combinations of mednli , snli , and mnli as it shown in the table 4 ( line 4 to 7 ) and also add the results of general domain tasks ( mnli , snli ) for comparison .\"]\nData Manipulation and Calculation: [\"even though biobert is finetuned on general domain tasks before mednli , transfer learning shows better results than that fine - tuned on mednli directly .\", \"the domain specific language representations from biobert are maintained while fine - tuning on general domain tasks by showing that the transfer learning results of mednli on biobert have better performance than the results on bert ( line 4 to 7 ) .\"]\nCausal Relationships and Evaluation: [\"positive transfer occurs on mednli .\", \"the best combination is snli → mnli → mednli on biobert .\", \"the performance of mednli ( expanded ) with transfer learning is higher on bert and lower on biobert than the performance of mednli with transfer learning .\"]",
    "response3": "Descriptive Statistics: [\"switching to elmo word embeddings improves performance by 2.9 percentage points on an average, corresponding to about 53 test sentences.\", \"of these, about 32 sentences (60% of the improvement) correspond to a-but-b and negation style sentences.\", \"the non-elmo baseline model (no-project, no-distill) gets 255 sentences wrong in the test corpus on average, only 89 (34.8%) of which are a-but-b style or negations.\"]\nComparison and Contrast: [\"as further evidence that elmo helps on these specific constructions.\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"about 32 sentences (60% of the improvement)\", \"only 89 (34.8%) of which are a-but-b style or negations\"]\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response4": "Descriptive Statistics: [\"we present statistics of our dataset10 in table 3 .\", \"according to landis and koch ( 1977 ) , κ ∈ ( 0 . 2 , 0 . 4 ] corresponds to ' fair agreement ' , whereas κ ∈ ( 0 . 4 , 0 . 6 ] corresponds to ' moderate agreement ' .\", \"table 3 shows that elmo ' s performance gains in table 2 extends across all thresholds .\"]\nComparison and Contrast: [\"as expected , inter - annotator agreement is higher for higher thresholds ( less ambiguous sentences ) .\"]\nTrend and Pattern Analysis: [\"higher thresholds correspond to sets of less ambiguous sentences .\"]\nProportional and Percentage Calculations: [\"we next compute the accuracy of our model for each threshold by removing the corresponding neutral sentences .\"]\nData Manipulation and Calculation: [\"we next compute the accuracy of our model for each threshold by removing the corresponding neutral sentences .\"]\nCausal Relationships and Evaluation: []",
    "response5": "Descriptive Statistics: [\"the top tuning data scores for avg cos sim ( table 1 ) show that the google embeddings with tf * idf weighting yield the top f score for all three concept input types ( . 881 - . 945 ) .\", \"somewhat expectedly , the best overall f score ( . 945 ) is produced in the setting both , which provides the most information .\", \"actually , this is true for all four weighting schemes for both glove and google , while fasttext consistently yields its top f scores ( . 840 - . 911 ) in the label setting , which provides the least information .\"]\nComparison and Contrast: [\"somewhat expectedly , the best overall f score ( . 945 ) is produced in the setting both , which provides the most information .\", \"actually , this is true for all four weighting schemes for both glove and google , while fasttext consistently yields its top f scores ( . 840 - . 911 ) in the label setting , which provides the least information .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"the top tuning data scores for avg cos sim ( table 1 ) show that the google embeddings with tf * idf weighting yield the top f score for all three concept input types ( . 881 - . 945 ) .\"]\nData Manipulation and Calculation: [\"the top tuning data scores for avg cos sim ( table 1 ) show that the google embeddings with tf * idf weighting yield the top f score for all three concept input types ( . 881 - . 945 ) .\", \"actually , this is true for all four weighting schemes for both glove and google , while fasttext consistently yields its top f scores ( . 840 - . 911 ) in the label setting , which provides the least information .\"]\nCausal Relationships and Evaluation: []",
    "response6": "Descriptive Statistics: [\"for top n cos sim avg , the tuning data results ( table 2 ) are somewhat more varied : first , there is no single best performing set of embeddings : google yields the best f score for the label setting ( . 953 ) , while glove ( though only barely ) leads in the description setting ( . 912 ) . this time , it is fasttext which produces the best f score in the both setting , which is also the best overall tuning data f score for top n cos sim avg ( . 954 ) .\"]\nComparison and Contrast: [\"google yields the best f score for the label setting ( . 953 ) , while glove ( though only barely ) leads in the description setting ( . 912 ) .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"google yields the best f score for the label setting ( . 953 ) .\"]\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: [\"this time , it is fasttext which produces the best f score in the both setting , which is also the best overall tuning data f score for top n cos sim avg ( . 954 ) .\"]",
    "response7": "Descriptive Statistics: [\"the results can be found in table 3 .\", \"the first interesting finding is that the avg cos sim measure again performs very well : in all three settings , it beats both the system based on general - purpose embeddings ( topic wiki ) and the one that is adapted to the science domain ( topic science ) , with again the both setting yielding the best overall result ( . 926 ) .\", \"note that our both setting is probably the one most similar to the concept input used by gong et al . ( 2018 ) .\"]\nComparison and Contrast: [\"for comparison , the two top rows provide the best results of gong et al . ( 2018 ) .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"the first interesting finding is that the avg cos sim measure again performs very well : in all three settings , it beats both the system based on general - purpose embeddings ( topic wiki ) and the one that is adapted to the science domain ( topic science ) , with again the both setting yielding the best overall result ( . 926 ) .\"]\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response8": "Descriptive Statistics: [\"Table 1 presents four different accuracy metrics for this case: 1) full columns correspond to the accuracy on the whole dataset.\", \"Dict columns correspond to the accuracy over the cases where the target word is among the 90k words of the CNN dictionary.\", \"List columns report the accuracy over the cases where the right word was among the k-best produced by the baseline.\", \"MRR mean reciprocal rank (MRR), we compare the results of our encoder with several state-of-the-art sentence encoders.\"]\nComparison and Contrast: [\"Our model FDCLSTM without attention achieves a better result in the case of the second baseline LSTM that is full of false-positives and short words.\", \"We also compare our result with current state-of-the-art word embeddings trained on a large general text using Glove and FastText.\", \"Our proposed models achieve better performance than our two previous models (Sabir et al., 2018) that trained a word embedding from scratch on the same task.\"]\nTrend and Pattern Analysis: [\"As seen in Table 1, the introduction of this unigram lexicon produces the best results.\"]\nProportional and Percentage Calculations: [\"The target word is among the 90k words of the CNN dictionary, which correspond to 43.3% of the whole dataset.\"]\nData Manipulation and Calculation: [\"We experimented extracting k-best hypotheses for k = 1...10.\"]\nCausal Relationships and Evaluation: [\"Our model FDCLSTM without attention achieves a better result in the case of the second baseline LSTM that is full of false-positives and short words.\"]",
    "response9": "Descriptive Statistics: [\"table 1 contains the results on all four datasets.\"]\nComparison and Contrast: [\"synst achieves speedups of ∼ 4 − 5 × that of the vanilla transformer, which is larger than nearly all of the sat configurations.\", \"quality-wise, synst again significantly outperforms the sat configurations at comparable speedups on all datasets.\", \"on wmt en - de, synst improves by 1 bleu over lt ( 20 . 74 vs lt ' s 19 . 8 without reranking ).\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"synst achieves speedups of ∼ 4 − 5 × that of the vanilla transformer.\"]\nData Manipulation and Calculation: [\"synst achieves speedups of ∼ 4 − 5 × that of the vanilla transformer, which is larger than nearly all of the sat configurations.\"]\nCausal Relationships and Evaluation: [\"quality-wise, synst again significantly outperforms the sat configurations at comparable speedups on all datasets.\"]",
    "response10": "Descriptive Statistics: [\"table 2 shows the development results of various s - lstm settings\", \"we fix the hidden size to 300 accordingly\"]\nComparison and Contrast: [\"adding one additional sentence - level node does not lead to accuracy improvements, although the number of parameters and decoding time increase accordingly\", \"the accuracies of s - lstm increases as the hidden layer size for each node increases from 100 to 300, but does not further increase when the size increases beyond 300\", \"without using ⟨s⟩, ⟨/s⟩, the performance of s - lstm drops from 82.64 % to 82.36 %, showing the effectiveness of having these additional nodes\"]\nTrend and Pattern Analysis: [\"the accuracies of s - lstm increases as the hidden layer size for each node increases from 100 to 300, but does not further increase when the size increases beyond 300\"]\nProportional and Percentage Calculations: [\"the performance of s - lstm drops from 82.64 % to 82.36 %\"]\nData Manipulation and Calculation: [\"adding one additional sentence - level node does not lead to accuracy improvements, although the number of parameters and decoding time increase accordingly\"]\nCausal Relationships and Evaluation: [\"without using ⟨s⟩, ⟨/s⟩, the performance of s - lstm drops from 82.64 % to 82.36 %, showing the effectiveness of having these additional nodes\"]",
    "response11": "Descriptive Statistics: [\"Table 3: Movie review development results\", \"LSTM, 67, 80.72, 5,977K\", \"BiLSTM, 106, 81.73, 7,059K\", \"2 stacked BiLSTM, 207, 81.97, 9,221K\", \"3 stacked BiLSTM, 310, 81.53, 11,383K\", \"4 stacked BiLSTM, 411, 81.37, 13,546K\", \"S-LSTM, 65, 82.64*, 8,768K\", \"CNN, 34, 80.35, 5,637K\", \"2 stacked CNN, 40, 80.97, 5,717K\", \"3 stacked CNN, 47, 81.46, 5,808K\", \"4 stacked CNN, 51, 81.39, 5,855K\", \"Transformer (N=6), 138, 81.03, 7,234K\", \"Transformer (N=8), 174, 81.86, 7,615K\", \"Transformer (N=10), 214, 81.63, 8,004K\", \"BiLSTM+Attention, 126, 82.37, 7,419K\", \"S-LSTM+Attention, 87, 83.07*, 8,858K\"]\nComparison and Contrast: [\"bilstm gives significantly better accuracies compared to uni-directional lstm2, with the training time per epoch growing from 67 seconds to 106 seconds.\", \"stacking 2 layers of bilstm gives further improvements to development results, with a larger time of 207 seconds.\", \"3 layers of stacked bilstm does not further improve the results.\", \"in contrast, s-lstm gives a development result of 82.64%, which is significantly better compared to 2-layer stacked bilstm, with a smaller number of model parameters and a shorter time of 65 seconds.\", \"cnn is the most efficient among all models compared, with the smallest model size.\", \"on the other hand, a 3-layer stacked cnn gives an accuracy of 81.46%, which is also the lowest compared with bilstm, hierarchical attention and s-lstm.\", \"the best performance of hierarchical attention is between single-layer and two-layer bilstms in terms of both accuracy and efficiency.\", \"s-lstm gives significantly better accuracies compared with both cnn and hierarchical attention.\", \"attention leads to improved accuracies for both bilstm and s-lstm in classification, with s-lstm still outperforming bilstm significantly.\"]\nTrend and Pattern Analysis: [\"stacking 2 layers of bilstm gives further improvements to development results, with a larger time of 207 seconds.\", \"3 layers of stacked bilstm does not further improve the results.\"]\nProportional and Percentage Calculations: [\"bilstm gives significantly better accuracies compared to uni-directional lstm2, with the training time per epoch growing from 67 seconds to 106 seconds.\", \"in contrast, s-lstm gives a development result of 82.64%, which is significantly better compared to 2-layer stacked bilstm, with a smaller number of model parameters and a shorter time of 65 seconds.\"]\nData Manipulation and Calculation: [\"stacking 2 layers of bilstm gives further improvements to development results, with a larger time of 207 seconds.\"]\nCausal Relationships and Evaluation: [\"attention leads to improved accuracies for both bilstm and s-lstm in classification, with s-lstm still outperforming bilstm significantly.\"]",
    "response12": "Descriptive Statistics: [\"the final results on the movie review and rich text classification datasets are shown in tables 4 and 5 , respectively .\"]\nComparison and Contrast: [\"as shown in table 4 , s - lstm outperforms bilstm significantly , with a faster speed .\", \"s - lstm also gives highly competitive results when compared with existing methods in the literature .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response13": "Descriptive Statistics: [\"the final results on the movie review and rich text classification datasets are shown in tables 4 and 5 , respectively .\", \"the average accuracy of s - lstm is 85 . 6 % , significantly higher compared with 84 . 9 % by 2 - layer stacked bilstm .\", \"3 - layer stacked bilstm gives an average accuracy of 84 . 57 % , which is lower compared to a 2 - layer stacked bilstm , with a training time per epoch of 423 . 6 seconds .\"]\nComparison and Contrast: [\"as shown in table 5 , among the 16 datasets s - lstm gives the best results on 12 , compared with bilstm and 2 layered bilstm models .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"the average accuracy of s - lstm is 85 . 6 % , significantly higher compared with 84 . 9 % by 2 - layer stacked bilstm .\"]\nData Manipulation and Calculation: [\"as shown in table 5 , among the 16 datasets s - lstm gives the best results on 12 , compared with bilstm and 2 layered bilstm models .\"]\nCausal Relationships and Evaluation: []",
    "response14": "Descriptive Statistics: [\"as can be seen in table 6, s-LSTM gives significantly better results compared with BiLSTM on the WSJ dataset.\", \"stacking two layers of BiLSTMs leads to improved results compared to one-layer BiLSTM, but the accuracy does not further improve with three layers of stacked LSTMs.\"]\nComparison and Contrast: [\"s-LSTM gives significantly better results compared with BiLSTM on the WSJ dataset.\", \"it also gives competitive accuracies as compared with existing methods in the literature.\", \"stacking two layers of BiLSTMs leads to improved results compared to one-layer BiLSTM, but the accuracy does not further improve with three layers of stacked LSTMs.\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"s-LSTM gives significantly better results compared with BiLSTM on the WSJ dataset.\"]\nData Manipulation and Calculation: [\"stacking two layers of BiLSTMs leads to improved results compared to one-layer BiLSTM, but the accuracy does not further improve with three layers of stacked LSTMs.\"]\nCausal Relationships and Evaluation: []",
    "response15": "Descriptive Statistics: [\"s - lstm gives an f1 - score of 91 . 57 % on the conll test set\", \"our bilstm results are comparable to the results reported by ma and hovy ( 2016 ) and lample et al . ( 2016 )\", \"in the second section of table 7 , yang et al . ( 2017 ) obtain an fscore of 91 . 26 %\"]\nComparison and Contrast: [\"which is significantly better compared with bilstms\", \"stacking more layers of bilstms leads to slightly better f1 - scores compared with a single - layer bilstm\", \"in contrast , s - lstm gives the best reported results under the same settings\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"s - lstm gives an f1 - score of 91 . 57 % on the conll test set\", \"in the second section of table 7 , yang et al . ( 2017 ) obtain an fscore of 91 . 26 %\"]\nData Manipulation and Calculation: [\"stacking more layers of bilstms leads to slightly better f1 - scores compared with a single - layer bilstm\"]\nCausal Relationships and Evaluation: [\"which is significantly better compared with bilstms\", \"our bilstm results are comparable to the results reported by ma and hovy ( 2016 ) and lample et al . ( 2016 )\", \"in contrast , s - lstm gives the best reported results under the same settings\"]",
    "response16": "Descriptive Statistics: [\"table 2 and 3 display the results on the e2e and webnlg test sets for models of the respective challenges and our own models on the e2e test set\", \"our single best word - and character - based models reach comparable results to the best challenge submissions\", \"the word - based models achieve significantly higher bleu and rouge - l scores than the character - based models\"]\nComparison and Contrast: [\"our single best word - and character - based models reach comparable results to the best challenge submissions\", \"the word - based models achieve significantly higher bleu and rouge - l scores than the character - based models\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response17": "Descriptive Statistics: [\"own, own, own\"]\nComparison and Contrast: [\"the bleu score of our best word-based model outperforms the best challenge submission by a small margin.\", \"the character-based model achieves a significantly higher rouge-l score than the word-based model, whereas the bleu score difference is not significant.\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"the bleu score of our best word-based model outperforms the best challenge submission by a small margin.\"]\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response18": "Descriptive Statistics: [\"table 4 shows that increasing the number of layers from 1 to 5 results in a bleu increase of only 0.5, while the speedup drops from 3.8× to 1.4×.\", \"the final row of table 4 shows that exposing the parse decoder to multiple possible chunkings of the same sentence during training allows it to choose a sequence of chunks that has a higher likelihood at test time, improving bleu by 1.5 while decreasing the speedup from 3.8× to 3.1×;\"]\nComparison and Contrast: [\"increasing the number of layers from 1 to 5 results in a bleu increase of only 0.5, while the speedup drops from 3.8× to 1.4×.\", \"the final row of table 4 shows that exposing the parse decoder to multiple possible chunkings of the same sentence during training allows it to choose a sequence of chunks that has a higher likelihood at test time, improving bleu by 1.5 while decreasing the speedup from 3.8× to 3.1×;\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"increasing the number of layers from 1 to 5 results in a bleu increase of only 0.5, while the speedup drops from 3.8× to 1.4×.\", \"the final row of table 4 shows that exposing the parse decoder to multiple possible chunkings of the same sentence during training allows it to choose a sequence of chunks that has a higher likelihood at test time, improving bleu by 1.5 while decreasing the speedup from 3.8× to 3.1×;\"]\nData Manipulation and Calculation: [\"the final row of table 4 shows that exposing the parse decoder to multiple possible chunkings of the same sentence during training allows it to choose a sequence of chunks that has a higher likelihood at test time, improving bleu by 1.5 while decreasing the speedup from 3.8× to 3.1×;\"]\nCausal Relationships and Evaluation: [\"the final row of table 4 shows that exposing the parse decoder to multiple possible chunkings of the same sentence during training allows it to choose a sequence of chunks that has a higher likelihood at test time, improving bleu by 1.5 while decreasing the speedup from 3.8× to 3.1×;\"]",
    "response19": "Descriptive Statistics: [\"table 4 shows the bleu and rouge - l development set scores when treating each human reference as prediction once and evaluating it against the remaining references\"]\nComparison and Contrast: [\"compared to the scores of the word - based and character - based models strikingly , on the e2e development set , both model variants significantly outperform human texts by far with respect to both automatic evaluation measures .\", \"while the human bleu score is significantly higher than those of both systems on the webnlg development set , there is no statistical difference between human and system rouge - l scores .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response20": "Descriptive Statistics: [\"one annotator ( one of the authors of this paper ) manually assessed the outputs of the models that obtained the best development set bleu score as summarized in table 56 .\", \"spelling mistakes only appeared in webnlg texts , mainly concerning omissions of accents or umlauts .\"]\nComparison and Contrast: [\"as we can see from the bottom part of the table , all models struggle more with getting the content right than with producing linguistically correct texts ; 70 - 80 % of the texts generated by all models are completely correct linguistically .\", \"comparing the two datasets , we again observe that the webnlg dataset is much more challenging than the e2e dataset , especially with respect to correctly verbalizing the content .\", \"in comparison , character - based models reproduce the content more faithfully on the e2e dataset while offering the same level of linguistic quality as word - based models , leading to more correct outputs overall .\", \"on the webnlg dataset , the word - based model is more faithful to the inputs , probably because of the effective delexicalization strategy , whereas the character - based model errs less on the linguistic side .\", \"overall , the word - based model yields more correct texts , stressing the importance of delexicalization and data normalization in low resource settings .\"]\nTrend and Pattern Analysis: [\"the most frequent content error in both datasets concerns omission of information .\", \"information addition and repetition only occur in the webnlg dataset .\", \"the latter is an especially frequent problem of the character - based model , affecting more than a quarter of all texts .\"]\nProportional and Percentage Calculations: [\"70 - 80 % of the texts generated by all models are completely correct linguistically .\"]\nData Manipulation and Calculation: [\"the most frequent content error in both datasets concerns omission of information .\"]\nCausal Relationships and Evaluation: [\"overall , the word - based model yields more correct texts , stressing the importance of delexicalization and data normalization in low resource settings .\"]",
    "response21": "Descriptive Statistics: [\"table 6 shows automatically computed statistics on the diversity of the generated texts of both models and human texts and on the overlap of the ( generated ) texts with the training set.\"]\nComparison and Contrast: [\"on both datasets , our systems produce significantly less varied outputs and reproduce more texts and sentences from the training data than the human texts .\", \"interestingly , however , the characterbased models generate significantly more unique sentences and copy significantly less from the training data than the word - based models , which copy about 40 % of their generated sentences from the training data .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"% new texts, 99.7±0.2, 98.2±0.3, 98.8±0.2, 91.1±0.3, 69.8±4.8, 87.5±0.6\", \"% new sents., 85.1±1.1, 61.8±6.4, 71.4±4.7, 87.4±0.4, 57.2±5.8, 82.1±1.2\"]\nData Manipulation and Calculation: [\"the characterbased models generate significantly more unique sentences and copy significantly less from the training data than the word - based models , which copy about 40 % of their generated sentences from the training data .\"]\nCausal Relationships and Evaluation: []",
    "response22": "Descriptive Statistics: [\"table 7 shows our manual evaluation of the top 30 hypotheses for 10 random e2e test inputs generated by models trained with data synthesized from the two templates .\"]\nComparison and Contrast: [\"as is evident from the first two rows , all models learned to generalize from the training data to produce correct texts for novel inputs consisting of unseen combinations of input attributes .\", \"yet , the picture is a bit different for the model trained on data generated by both templates .\", \"while the top two hypotheses are equally distributed between adhering to template 1 and template 2 , more than 5 % among the lower - ranked hypotheses constitute a template combination such as the example shown in the bottom part of figure 2 .\"]\nTrend and Pattern Analysis: [\"as can be seen in the final row of table 7 , this simple reranker successfully places correct hypotheses higher up in the ranking , improving the practical usability of the generation model by now offering almost three correct variants for each input among the top five hypotheses on average .\"]\nProportional and Percentage Calculations: [\"more than 5 % among the lower - ranked hypotheses constitute a template combination such as the example shown in the bottom part of figure 2 .\"]\nData Manipulation and Calculation: [\"this simple reranker successfully places correct hypotheses higher up in the ranking\"]\nCausal Relationships and Evaluation: [\"improving the practical usability of the generation model by now offering almost three correct variants for each input among the top five hypotheses on average .\"]",
    "response23": "Descriptive Statistics: [\"the automatic evaluation scores are presented in table 1 and table 2\", \"our system achieves comparable results to Wang and Lee (2018) a system based on both GANs and reinforcement training\", \"in table 1, we also list scores of the state-of-the-art supervised model, an attention-based seq-to-seq model of our own implementation, as well as the oracle scores of our method obtained by choosing the best summary among all finished hypothesis from beam search\", \"the oracle scores are much higher, indicating that our unsupervised method does allow summaries of better quality\"]\nComparison and Contrast: [\"our method outperforms commonly used prefix baselines for this task which take the first 75 characters or 8 words of the source as a summary\", \"our system achieves comparable results to Wang and Lee (2018) a system based on both GANs and reinforcement training\", \"for another unsupervised work Fevry and Phang (2018), we attempted to replicate on our test set, but were unable to obtain results better than the baselines\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"our method outperforms commonly used prefix baselines for this task which take the first 75 characters or 8 words of the source as a summary\"]\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: [\"the oracle scores are much higher, indicating that our unsupervised method does allow summaries of better quality\"]",
    "response24": "Descriptive Statistics: [\"our method achieves good compression rate and significantly raises a previous unsupervised baseline on token level f1 score\"]\nComparison and Contrast: []\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: [\"our method achieves good compression rate and significantly raises a previous unsupervised baseline on token level f1 score\"]",
    "response25": "Descriptive Statistics: [\"table 3 considers analysis of different aspects of the model .\", \"second , we vary the 3 - layer representation out of elmo forward language model to do contextual matching ( bot / mid / top : bottom / middle / top layer only , avg : average of 3 layers , cat : concatenation of all layers ) .\"]\nComparison and Contrast: [\"first , we look at the fluency model and compare the cluster smoothing ( cs ) approach with softmax temperature ( tempx with x being the temperature ) commonly used for generation in lm - integrated models ( chorowski and jaitly , 2016 ) as well as no adjustment ( na ) .\"]\nTrend and Pattern Analysis: [\"results show the effectiveness of our cluster smoothing method for the vocabulary adaptive language model pfm , although temperature smoothing is an option for abstractive datasets .\"]\nProportional and Percentage Calculations: [\"results show the effectiveness of our cluster smoothing method for the vocabulary adaptive language model pfm , although temperature smoothing is an option for abstractive datasets .\"]\nData Manipulation and Calculation: [\"results show the effectiveness of our cluster smoothing method for the vocabulary adaptive language model pfm , although temperature smoothing is an option for abstractive datasets .\"]\nCausal Relationships and Evaluation: [\"when using word embeddings ( bottom layer only from elmo language model ) in our contextual matching model pcm , the summarization performance drops significantly to below simple baselines as demonstrated by score decrease .\"]",
    "response26": "Descriptive Statistics: [\"the final results are shown in table 3 .\"]\nComparison and Contrast: [\"we select the 1st and 2nd best performing models on the development datasets as well as the majority vote ( mv ) of 5 models for the final submission .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response27": "Descriptive Statistics: [\"we evaluate two configurations of the parse decoder , one in which it is trained separately from the token decoder ( first column of table 3 ) , and the other where both decoders are trained jointly ( second column of ta ble 3 ) .\", \"in section 5 . 3 ( see the final row of table 3 ) we consider the effect of randomly sampling the max chunk size k during training .\"]\n\nComparison and Contrast: [\"we observe that joint training boosts the chunk f1 from 65 . 4 to 69 . 6 , although , in both cases the f1 scores are relatively low , which matches our intuition as most source sentences can be translated into multiple target syntactic forms .\", \"the resulting f1 is indeed almost 10 points higher ( third column of table 3 ) , indicating that the token decoder does have the ability to correct mistakes .\"]\n\nTrend and Pattern Analysis: [\"to measure how often the token decoder follows the predicted chunk sequence , we parse the generated translation and compute the f1 between the resulting chunk sequence and the parse decoder ' s prediction ( fourth column of table 3 ) .\", \"this provides a considerable boost to bleu with a minimal impact to speedup .\"]\n\nProportional and Percentage Calculations: [\"Exact match, 4.23%, 5.24%, 5.94%, 43.10%\"]\n\nData Manipulation and Calculation: [\"we observe that joint training boosts the chunk f1 from 65 . 4 to 69 . 6 , although , in both cases the f1 scores are relatively low , which matches our intuition as most source sentences can be translated into multiple target syntactic forms .\", \"the resulting f1 is indeed almost 10 points higher ( third column of table 3 ) , indicating that the token decoder does have the ability to correct mistakes .\", \"in section 5 . 3 ( see the final row of table 3 ) we consider the effect of randomly sampling the max chunk size k during training .\"]\n\nCausal Relationships and Evaluation: [\"strong results of 89 . 9 f1 and 43 . 1 % exact match indicate that the token decoder is heavily reliant on the generated chunk sequences .\"]",
    "response28": "Descriptive Statistics: [\"the results are presented in table 1 ( top two subparts ) .\", \"the results are also presented in table 1 ( 3rd and 4th subparts ) .\", \"the performance of the predict - allwords rnn decoder does not significantly differ from that of any one of the autoregressive rnn de coders , and the same situation can be also observed in cnn decoders .\", \"in our proposed rnn - cnn model , we empirically show that the mean + max pooling provides stronger transferability than the max pooling alone does , and the results are presented in the last two sections of table 1 .\"]\nComparison and Contrast: [\"the three decoding settings do not differ significantly in terms of the performance on selected downstream tasks , with rnn or cnn as the decoder .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: [\"the performance of the predict - allwords rnn decoder does not significantly differ from that of any one of the autoregressive rnn de coders , and the same situation can be also observed in cnn decoders .\"]\nCausal Relationships and Evaluation: [\"in our proposed rnn - cnn model , we empirically show that the mean + max pooling provides stronger transferability than the max pooling alone does , and the results are presented in the last two sections of table 1 .\"]",
    "response29": "Descriptive Statistics: [\"we present the table 4 in the supplementary material and we summarise it as follows\", \"more words for decoding didn't give us a significant performance gain, and took longer to train\", \"we report results from both smallest and largest models in table 2\"]\nComparison and Contrast: [\"our designed asymmetric RNN-CNN model works better than other asymmetric models (CNN-LSTM, row 11), and models with symmetric structure (RNN-RNN, row 5 and 10)\", \"as the transferability of the models trained in both cases perform similarly on the evaluation tasks (see rows 1 and 2 in table 4), we focus on the simpler predictall-words CNN decoder that learns to reconstruct the next window of contiguous words\", \"compared to our designed CNN-CNN model, their CNN-LSTM model contains more parameters than our model does, but they have similar performance on the evaluation\"]\nTrend and Pattern Analysis: [\"decoding the subsequent 30 words, which was adopted from the skip-thought training code, gave reasonably good performance\", \"adding more layers into the decoder and enlarging the dimension of the convolutional layers indeed slightly improved the performance on the three downstream tasks, but as training efficiency is one of our main concerns, it wasn't worth sacrificing training efficiency for the minor performance gain\", \"increasing the dimensionality of the RNN encoder improved the model performance, and the additional training time required was less than needed for increasing the complexity in the CNN decoder\"]\nProportional and Percentage Calculations: [\"decoding short target sequences results in a slightly lower Pearson score on SICK, and decoding longer target sequences lead to a longer training time\"]\nData Manipulation and Calculation: [\"decoding the next sentence performed similarly to decoding the subsequent contiguous words\", \"we tweaked the CNN encoder, including different kernel size and activation function, and we report the best results of CNN-CNN model at row 6 in table 4\"]\nCausal Relationships and Evaluation: [\"the future predictor in (Gan et al., 2017) also applies a CNN as the encoder, but the decoder is still an RNN, listed at row 11 in table 4\"]",
    "response30": "Descriptive Statistics: [\"systems built on larger data generally benefit from larger vocabularies while smaller systems perform better with smaller vocabularies .\"]\nComparison and Contrast: [\"sub - word systems outperform full - word systems across the board , despite having fewer total parameters .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response31": "Descriptive Statistics: [\"the results of the pre - selection are reported in table 1 .\", \"all models have ≈5M parameters.\"]\nComparison and Contrast: [\"all syllable - aware models comfortably outperform the char - cnn when the budget is limited to 5m parameters .\", \"a pure word - level model , 6 lstm - word , also beats the character - aware one under such budget .\"]\nTrend and Pattern Analysis: [\"the three best configurations are syl - concat , syl - sum , and syl - cnn -\"]\nProportional and Percentage Calculations: [\"all models have ≈5M parameters.\"]\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response32": "Descriptive Statistics: [\"the results of evaluating these three models on small ( 1m tokens ) and medium - sized ( 17m – 57m tokens ) data sets against char - cnn for different languages are provided in table 3 .\"]\nComparison and Contrast: [\"the models demonstrate similar performance on small data , but char - cnn scales significantly better on medium - sized data .\", \"from the three syllable - aware models , syl - concat looks the most advantageous as it demonstrates stable results and has the least number of parameters .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"In each case the smallest model, Syl-Concat, has 18%–33% less parameters than Char-CNN and is trained 1.2–2.2 times faster (Appendix C).\"]\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: [\"therefore in what follows we will make a more detailed comparison of sylconcat with char - cnn .\"]",
    "response33": "Descriptive Statistics: [\"RHN-Char-CNN, 8, 650, 20M, 67.6\", \"RHN-Syl-Concat, 8, 439, 13M, 72.0\", \"RHN-Syl-Concat, 8, 650, 20M, 69.4\"]\nComparison and Contrast: [\"to find out whether this was the case we replaced the lstm by a variational rhn ( zilly et al . , 2017 ) , and that resulted in a significant reduction of perplexities on ptb for both char - cnn and syl - concat ( table 5 ) .\"]\nTrend and Pattern Analysis: [\"increasing dlm from 439 to 650 did result in better performance for syl - concat .\"]\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response34": "Descriptive Statistics: []\nComparison and Contrast: [\"cohen ' s kappa is 68 % higher than that for esim .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"on a support basis , we find a 52 % increase in kappa by adding the titles .\"]\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response35": "Descriptive Statistics: []\nComparison and Contrast: [\"makes the fever title five oracle performance better than fever title five .\", \"the transformer model is accurate enough that oracle guessing does not help .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response36": "Descriptive Statistics: [\"the named entity retrieval strategy boosts the evidence retrieval rate to 80.8%, the film retrievals raise evidence retrieval to 81.2%.\"]\nProportional and Percentage Calculations: [\"the named entity retrieval strategy boosts the evidence retrieval rate to 80.8%, the film retrievals raise evidence retrieval to 81.2%.\"]\nData Manipulation and Calculation: [\"the named entity retrieval strategy boosts the evidence retrieval rate to 80.8%, the film retrievals raise evidence retrieval to 81.2%.\"]\nCausal Relationships and Evaluation: [\"the named entity retrieval strategy boosts the evidence retrieval rate to 80.8%, the film retrievals raise evidence retrieval to 81.2%.\"]",
    "response37": "Descriptive Statistics: [\"limiting evidence in this way when only five statements are retrieved ( 'narrow evidence' in table 4) pushes fever score down very little, to .5550 from .5617 on the development set.\"]\nComparison and Contrast: [\"indeed, when the system reviews the extra evidence, fever score goes up to .5844 on the development set.\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response38": "Descriptive Statistics: [\"Table 2 demonstrates the resulting performance ( reflecting how close the predicted vectors are to the actual armed groups active in this or that location ) .\", \"Note that out of 38 pairs from 2001 , 31 were already present in the previous data set ( ongoing conflicts ) .\"]\nComparison and Contrast: [\"However , even for the new conflicts , the projection performance is encouraging .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"All (38), 44.7, 73.7, 84.2\", \"New (7), 14.3, 28.6, 42.9\"]\nData Manipulation and Calculation: [\"Then , this projection is applied to the second model embeddings of the 47 locations , which are subject to armed conflicts in the year 2001 ( 38 after skipping pairs with outof - vocabulary elements ) .\"]\nCausal Relationships and Evaluation: []",
    "response39": "Descriptive Statistics: [\"table 3 presents the results for these experiments , as well as baselines ( averaged across 15 years ) .\"]\nComparison and Contrast: [\"for the proposed incr . dynamic approach , the performance of the previous projections is comparable to that of the up - to - now projections on the accuracies @ 5 and @ 10 , and is even higher on the accuracy @ 1 ( statistically significant with t - test , p < 0 . 01 ) .\", \"the results of the separate baseline look more like random jitter .\", \"the cumulative baseline results are slightly better , probably simply because they are trained on more data . however , they still perform much worse than the models trained using incremental updates .\", \"the results for the incr . static baseline , when tested only on the words present in the test model vocabulary ( the left part of the table ) , seem better than those of the proposed incr . dynamic approach .\"]\nTrend and Pattern Analysis: [\"the fact that our models were incrementally updated , not trained from scratch , is crucial .\"]\nProportional and Percentage Calculations: [\"the results for the incr . static baseline , when tested only on the words present in the test model vocabulary ( the left part of the table ) , seem better than those of the proposed incr . dynamic approach .\", \"about 62 % in average\"]\nData Manipulation and Calculation: [\"the fact that our models were incrementally updated , not trained from scratch , is crucial .\"]\nCausal Relationships and Evaluation: [\"this stems from the fact that incremental updating with static vocabulary means that we never add new words to the models ; thus , they contain only the vocabulary learned from the 1994 texts .\", \"of course , skipping large parts of the data would be a major drawback for any realistic application , so the incr . static baseline is not really plausible .\"]",
    "response40": "Descriptive Statistics: [\"we compared our three proposed models for the three loss functions lτ , lτ ce , and lτ h , and their linear ( unweighted ) combination l ∗ , on te3 ‡ and td ‡ , for which the results are shown in table 4 .\"]\nComparison and Contrast: [\"a trend that can be observed is that overall performance on td ‡ is higher than that of te3 ‡ if we compare loss functions lτ , lτ ce , and lτh , and combination l ∗ , it can be noticed that , although all loss functions seem to give fairly similar performance , lτ gives the most robust results ( never lowest ) , especially noticeable for the smaller dataset td ‡ .\", \"the combination of losses l ∗ shows mixed results , and has lower performance for s - tlm and c - tlm , but better performance for tl2rtl .\", \"moreover , we can clearly see that on te3 ‡ , ctlm performs better than the indirect models , across all loss functions .\", \"on td ‡ , the indirect models seem to perform slightly better .\", \"the difference between c - tlm and s - tlm is small on the smaller td ‡ on te3 ‡ , the larger dataset , c - tlm clearly outperforms s - tlm across all loss functions ,\"]\nTrend and Pattern Analysis: [\"a trend that can be observed is that overall performance on td ‡ is higher than that of te3 ‡ if we compare loss functions lτ , lτ ce , and lτh , and combination l ∗ , it can be noticed that , although all loss functions seem to give fairly similar performance , lτ gives the most robust results ( never lowest ) , especially noticeable for the smaller dataset td ‡ .\"]\nProportional and Percentage Calculations: [\"we compared our three proposed models for the three loss functions lτ , lτ ce , and lτ h , and their linear ( unweighted ) combination l ∗ , on te3 ‡ and td ‡ , for which the results are shown in table 4 .\"]\nData Manipulation and Calculation: [\"the combination of losses l ∗ shows mixed results , and has lower performance for s - tlm and c - tlm , but better performance for tl2rtl .\"]\nCausal Relationships and Evaluation: [\"moreover , we can clearly see that on te3 ‡ , ctlm performs better than the indirect models , across all loss functions .\", \"on td ‡ , the indirect models seem to perform slightly better .\", \"the difference between c - tlm and s - tlm is small on the smaller td ‡ on te3 ‡ , the larger dataset , c - tlm clearly outperforms s - tlm across all loss functions ,\"]",
    "response41": "Descriptive Statistics: [\"table 1 presents our results .\"]\nComparison and Contrast: [\"the self - distance f the ebay , legal onion and illegal onion corpora lies between 0 . 40 to 0 . 45 by the jensenshannon divergence , but the distance between each pair is 0 . 60 to 0 . 65 , with the three approximately forming an equilateral triangle in the space of word distributions .\"]\nTrend and Pattern Analysis: [\"similar results are obtained using variational distance , and are omitted for brevity .\"]\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response42": "Descriptive Statistics: [\"each average .\"]\nComparison and Contrast: [\"according to our results ( table 2 ) , the wikification success ratios of ebay and illegal onion named entities is comparable and relatively low .\", \"sites selling legal drugs on onion have a much higher wikification percentage .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"according to our results ( table 2 ) , the wikification success ratios of ebay and illegal onion named entities is comparable and relatively low .\", \"sites selling legal drugs on onion have a much higher wikification percentage .\"]\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response43": "Descriptive Statistics: [\"the two top rows in table 1 show conflicting results for ukb .\", \"the table also reports the best performing knowledge - based systems on this dataset .\", \"the table shows that ukb yields the best overall result .\"]\nComparison and Contrast: [\"as the results show , that paper reports a suboptimal use of ukb .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"the table shows that ukb yields the best overall result .\"]\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: [\"also report we ( chaplot and sakajhutdinov , 2018 ) , the latest work on this area , as well as the most frequent sense as given by wordnet counts\"]",
    "response44": "Descriptive Statistics: [\"table 2 reports the results of supervised systems on the same dataset\"]\nComparison and Contrast: [\"as expected, supervised systems outperform knowledge-based systems, by a small margin in some of the cases\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response45": "Descriptive Statistics: [\"the table shows that the key factor is the use of sense frequencies\", \"systems that do not use them (those with a nf subscript) suffer a loss between 7 and 8 percentage points in f1\", \"the table also shows that extending the context is mildly effective\", \"regarding the algorithm, the table confirms that the best method is ppr w2w, followed by the subgraph approach (dfs) and ppr\"]\nComparison and Contrast: [\"systems that do not use sense frequencies (those with a nf subscript) suffer a loss between 7 and 8 percentage points in f1\"]\nTrend and Pattern Analysis: [\"extending the context is mildly effective\"]\nProportional and Percentage Calculations: [\"systems that do not use them (those with a nf subscript) suffer a loss between 7 and 8 percentage points in f1\"]\nData Manipulation and Calculation: [\"the table shows that the key factor is the use of sense frequencies\"]\nCausal Relationships and Evaluation: [\"the table confirms that the best method is ppr w2w, followed by the subgraph approach (dfs) and ppr\"]",
    "response46": "Descriptive Statistics: [\"we compare the performance of our model ( table 2 ) with traditional bag of words ( bow ) , tf - idf , and n - grams features based classifiers .\", \"we also compare against averaged skip - gram ( mikolov et al . , 2013 ) , doc2vec ( le and mikolov , 2014 ) , cnn ( kim , 2014 ) , hierarchical attention ( hn - att ) ( yang et al . , 2016 ) and hierarchical network ( hn ) models .\"]\nComparison and Contrast: [\"we compare the performance of our model ( table 2 ) with traditional bag of words ( bow ) , tf - idf , and n - grams features based classifiers .\", \"we also compare against averaged skip - gram ( mikolov et al . , 2013 ) , doc2vec ( le and mikolov , 2014 ) , cnn ( kim , 2014 ) , hierarchical attention ( hn - att ) ( yang et al . , 2016 ) and hierarchical network ( hn ) models .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"we compare the performance of our model ( table 2 ) with traditional bag of words ( bow ) , tf - idf , and n - grams features based classifiers .\"]\nData Manipulation and Calculation: [\"we also compare against averaged skip - gram ( mikolov et al . , 2013 ) , doc2vec ( le and mikolov , 2014 ) , cnn ( kim , 2014 ) , hierarchical attention ( hn - att ) ( yang et al . , 2016 ) and hierarchical network ( hn ) models .\"]\nCausal Relationships and Evaluation: []",
    "response47": "Descriptive Statistics: [\"table 1 presents the exact matching accuracy of irnet and various baselines on the development set and the test set .\", \"irnet clearly outperforms all the baselines by a substantial margin .\", \"it obtains 27 . 0 % absolute improvement over syntaxsqlit also obtains 19 . 5 % absolute net on test set .\", \"improvement over syntaxsqlnet ( augment ) that performs large - scale data augmentation .\", \"when incorporating bert , the performance of both syntaxsqlnet and irnet is substantially improved and the accuracy gap between them on both the development set and the test set is widened .\"]\nComparison and Contrast: [\"irnet clearly outperforms all the baselines by a substantial margin .\", \"it obtains 27 . 0 % absolute improvement over syntaxsqlit also obtains 19 . 5 % absolute net on test set .\", \"improvement over syntaxsqlnet ( augment ) that performs large - scale data augmentation .\", \"when incorporating bert , the performance of both syntaxsqlnet and irnet is substantially improved and the accuracy gap between them on both the development set and the test set is widened .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"it obtains 27 . 0 % absolute improvement over syntaxsqlit also obtains 19 . 5 % absolute net on test set .\"]\nData Manipulation and Calculation: [\"it obtains 27 . 0 % absolute improvement over syntaxsqlit also obtains 19 . 5 % absolute net on test set .\"]\nCausal Relationships and Evaluation: [\"when incorporating bert , the performance of both syntaxsqlnet and irnet is substantially improved and the accuracy gap between them on both the development set and the test set is widened .\"]",
    "response48": "Descriptive Statistics: [\"as shown in table 2 , irnet significantly outperforms syntaxsqlnet in all four hardness levels with or without bert .\"]\nComparison and Contrast: [\"for example , compared with syntaxsqlnet , irnet obtains 23 . 3 % absolute improvement in hard level .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"SyntaxSQLNet, 38.6%, 17.6%, 16.3%, 4.9%\", \"SyntaxSQLNet, 42.9%, 24.9%, 21.9%, 8.6%\", \"(BERT), 42.9%, 24.9%, 21.9%, 8.6%\", \"IRNet, 70.1%, 49.2%, 39.5%, 19.1%\", \"IRNet(BERT), 77.2%, 58.7%, 48.1%, 25.3%\"]\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response49": "Descriptive Statistics: [\"as shown in table 3 , there are at least 6 . 6 % and up to 14 . 4 % absolute improvements on accuracy of exact matching on the development set .\", \"table 4 presents the ablation study results .\"]\nComparison and Contrast: [\"for example , when syntaxsqlnet is learned to generate semql queries instead of sql queries , it registers 8 . 6 % absolute improvement and even outperforms syntaxsqlnet ( augment ) which performs largescale data augmentation .\", \"it is clear that our base model significantly outperforms syntaxsqlnet , syntaxsqlnet ( augment ) and syntaxsqlnet ( bert ) .\"]\nTrend and Pattern Analysis: [\"the f1 score on the where clause increases by 12 . 5 % when irnet performs schema linking .\", \"the number of examples suffering from this problem decreases by 70 % , when using the memory augmented pointer network .\"]\nProportional and Percentage Calculations: [\"as shown in table 3 , there are at least 6 . 6 % and up to 14 . 4 % absolute improvements on accuracy of exact matching on the development set .\", \"for example , when syntaxsqlnet is learned to generate semql queries instead of sql queries , it registers 8 . 6 % absolute improvement and even outperforms syntaxsqlnet ( augment ) which performs largescale data augmentation .\"]\nData Manipulation and Calculation: [\"performing schema linking ( ' + sl ' ) brings about 8 . 5 % and 6 . 4 % absolute improvement on irnet and irnet ( bert ) .\"]\nCausal Relationships and Evaluation: [\"the number of examples suffering from this problem decreases by 70 % , when using the memory augmented pointer network .\"]",
    "response50": "Descriptive Statistics: [\"table 3 summarizes the performance of the three models on the supports and refutes pairs from the fever dev set and on the created symmetric test set pairs .\", \"all models perform relatively well on fever dev but achieve less than 60 % accuracy on the synthetic ones .\"]\nComparison and Contrast: [\"the re - weighting method increases the accuracy of the esim and bert models by an absolute 3 . 4 % and 3 . 3 % respectively .\", \"one can notice that this improvement comes at a cost in the accuracy over the fever dev pairs .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"the re - weighting method increases the accuracy of the esim and bert models by an absolute 3 . 4 % and 3 . 3 % respectively .\"]\nData Manipulation and Calculation: [\"the re - weighting method increases the accuracy of the esim and bert models by an absolute 3 . 4 % and 3 . 3 % respectively .\"]\nCausal Relationships and Evaluation: [\"applying the regularization method , using the same training data , helps to train a more robust model that performs better on our test set , where verification in context is a key requirement .\"]",
    "response51": "Descriptive Statistics: [\"table 7 and table 8 summarize the top 10 bigrams for support and not enough info .\", \"the correlation between the biased phrases in the two dataset splits is not as strong as in the refute label , presented in the paper .\", \"however , one can notice that some of the biased bigrams in the training set , such as ' least one ' and ' starred movie ' , translate to cues that can help in predictions over the development set .\"]\n\nComparison and Contrast: [\"the correlation between the biased phrases in the two dataset splits is not as strong as in the refute label , presented in the paper .\"]\n\nTrend and Pattern Analysis: []\n\nProportional and Percentage Calculations: []\n\nData Manipulation and Calculation: []\n\nCausal Relationships and Evaluation: [\"one can notice that some of the biased bigrams in the training set , such as ' least one ' and ' starred movie ' , translate to cues that can help in predictions over the development set .\"]",
    "response52": "Descriptive Statistics: [\"report brief results on squad ( table 3 ) , we also evaluate the synnet on the newsqato - squad direction .\"]\nComparison and Contrast: [\"we directly apply the best setting from the other direction and report the result in table 3 .\", \"the synnet improves over the baseline by 1 . 6 % in em and 0 . 7 % in f1 .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"the synnet improves over the baseline by 1 . 6 % in em and 0 . 7 % in f1 .\"]\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: [\"limited by space , we leave out ablation studies in this direction .\"]",
    "response53": "Descriptive Statistics: [\"table 3 shows the performance of baselines against user simulator and human on the two datasets .\"]\nComparison and Contrast: [\"equation ( 11 ) ) is also proven effective in deliver ing more achievement , which can be seen from the second and last rows of table 3 .\"]\nTrend and Pattern Analysis: [\"the results are consistent with those in table 3 .\"]\nProportional and Percentage Calculations: [\"equation ( 11 ) ) is also proven effective in deliver ing more achievement , which can be seen from the second and last rows of table 3 .\"]\nData Manipulation and Calculation: [\"equation ( 11 ) ) is also proven effective in deliver ing more achievement , which can be seen from the second and last rows of table 3 .\"]\nCausal Relationships and Evaluation: []",
    "response54": "Descriptive Statistics: [\"table 2 : correlation between attribution word importance with pos tags , fertility , and syntactic depth .\", \"fertility can be categorized into 4 types : one - to - many ( ' ≥ 2 ' ) , one - to - one ( ' 1 ' ) , many - to - one ( ' ( 0 , 1 ) ' ) , and null - aligned ( ' 0 ' ) .\", \"syntactic depth shows the depth of a word in the dependency tree .\", \"a lower tree depth indicates closer to the root node in the dependency tree , which might indicate a more important word .\"]\nComparison and Contrast: []\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"fertility can be categorized into 4 types : one - to - many ( ' ≥ 2 ' ) , one - to - one ( ' 1 ' ) , many - to - one ( ' ( 0 , 1 ) ' ) , and null - aligned ( ' 0 ' ) .\"]\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: [\"a lower tree depth indicates closer to the root node in the dependency tree , which might indicate a more important word .\"]",
    "response55": "Descriptive Statistics: [\"table 1 : average accuracies and macro - f1 scores over five runs with random initialization along with their standard deviations .\", \"bold : best results or within std of them .\", \"the data annotations s , and a indicate training with sentence - level , noisy sentence - level and aspect - level data respectively .\"]\nComparison and Contrast: [\"∗ indicates that the method ' s result is significantly better than all baseline methods ,\", \"† indicates that the method ' s result is significantly better than all baselines methods that use the aspect - based data only , with p < 0 . 05 according to a one - tailed unpaired t - test .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"N, BiLSTM-XR-Dev Estimation, [BOLD] 83.31∗± 0.62, 62.24 ± 0.66, [BOLD] 87.68∗± 0.47, 63.23 ± 1.81\", \"N, BiLSTM-XR, [BOLD] 83.31∗± 0.77, 64.42 ± 2.78, [BOLD] 88.12∗± 0.24, 68.60 ± 1.79\", \"N+A, BiLSTM-XR →Aspect Based Finetuning, [BOLD] 83.44∗± 0.74, [BOLD] 67.23 ± 1.42, [BOLD] 87.66∗± 0.28, [BOLD] 71.19†± 1.40\"]\nData Manipulation and Calculation: [\"numbers for tdlstm + att , atae - lstm , mm , ram and lstm + synatt + tarrep are from ( he et al . , 2018a ) .\", \"numbers for semisupervised are from ( he et al . , 2018b ) .\"]\nCausal Relationships and Evaluation: [\"∗ indicates that the method ' s result is significantly better than all baseline methods ,\", \"† indicates that the method ' s result is significantly better than all baselines methods that use the aspect - based data only , with p < 0 . 05 according to a one - tailed unpaired t - test .\"]",
    "response56": "Descriptive Statistics: [\"conduct ablation studies ( table 4 ) , to better understand how various components in our training procedure and model impact overall performance we conduct several ablation studies , as summarized in table 4 .\", \"results in table 4 ( a ) show that using human - annotated answers to generate questions leads to a significant performance boost over using answers from an answer generation module .\", \"this supports the hypothesis that the answers humans choose to generate questions for provide important linguistic cues for finetuning the machine comprehension model .\", \"to see how copying impacts performance , we explore using the entire paragraph to generate the question vs . only the two sentences before and one sentence after the answer span and report results in table 4 ( b ) .\"]\nComparison and Contrast: [\"results in table 4 ( a ) show that using human - annotated answers to generate questions leads to a significant performance boost over using answers from an answer generation module .\", \"to see how copying impacts performance , we explore using the entire paragraph to generate the question vs . only the two sentences before and one sentence after the answer span and report results in table 4 ( b ) .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: [\"this supports the hypothesis that the answers humans choose to generate questions for provide important linguistic cues for finetuning the machine comprehension model .\"]",
    "response57": "Descriptive Statistics: [\"table 2 gives single model validation scores for es2en and en2es models with standard and iterative transfer learning.\"]\nComparison and Contrast: [\"we find that the all - biomed domain gains 1 - 2 bleu points from transfer learning.\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response58": "Descriptive Statistics: [\"tables 3 and 4 give validation and test scores.\"]\nComparison and Contrast: [\"for de2en and es2en, uniform ensembling performs similarly to the oracles, and performs similarly to bi.\", \"we see small gains from using bi with ensembles on most validation sets, but only on en2es test.\", \"for en2es uniform ensembling underperforms the health and bio oracle models on their validation sets, and the uniform ensemble slightly underperforms bi on the test data.\"]\nTrend and Pattern Analysis: [\"we submitted three runs to the wmt19 biomedical task for each language pair: the best single all-biomed model, a uniform ensemble of models on two en-de and three es-en domains, and an ensemble with bayesian interpolation.\", \"that a uniform multi-domain ensemble performs well, giving 0.5-1.2 bleu improvement on the test set over strong single models.\"]\nProportional and Percentage Calculations: [\"we could predict bi (α = 0.5) performance by comparing the uniform ensemble with the oracle model performing best on each validation domain.\"]\nData Manipulation and Calculation: [\"we submitted three runs to the wmt19 biomedical task for each language pair: the best single all-biomed model, a uniform ensemble of models on two en-de and three es-en domains, and an ensemble with bayesian interpolation.\"]\nCausal Relationships and Evaluation: [\"we see small gains from using bi with ensembles on most validation sets, but only on en2es test.\", \"we noted that, in general, we could predict bi (α = 0.5) performance by comparing the uniform ensemble with the oracle model performing best on each validation domain.\"]",
    "response59": "Descriptive Statistics: [\"we see small gains from using bi with ensembles on most validation sets\", \"only on en2es test\", \"for en2de, by contrast, uniform ensembling is consistently better than oracles on the dev sets\", \"outperforms bi on the test data\"]\nComparison and Contrast: [\"only on en2es test\", \"for en2de, by contrast, uniform ensembling is consistently better than oracles on the dev sets\", \"outperforms bi on the test data\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"uniform ensemble, 45.3, 48.4, 28.6, 32.6, 42.9, 27.2\", \"BI ensemble (α=0.5), 45.4, 48.8, 28.5, 32.4, 43.1, 26.4\"]\nData Manipulation and Calculation: [\"uniform ensemble, 45.3, 48.4, 28.6, 32.6, 42.9, 27.2\", \"BI ensemble (α=0.5), 45.4, 48.8, 28.5, 32.4, 43.1, 26.4\"]\nCausal Relationships and Evaluation: [\"for en2de, by contrast, uniform ensembling is consistently better than oracles on the dev sets\", \"outperforms bi on the test data\"]",
    "response60": "Descriptive Statistics: [\"consequently in table 5 we experiment with bi ( α = 0 . 1 ) .\"]\nComparison and Contrast: [\"in this case bi matches or out - performs the uniform ensemble .\", \"notably , for en2es , where bi ( α = 0 . 5 ) performed well , taking α = 0 . 1 does not harm performance .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response61": "Descriptive Statistics: [\"Table 1 shows that there is a significant gap between the synthetic eScape data set (Negri et al., 2018) and the real-life data sets (the development set and the original training set from posteditors).\"]\nComparison and Contrast: [\"Table 1 shows that there is a significant gap between the synthetic eScape data set (Negri et al., 2018) and the real-life data sets (the development set and the original training set from posteditors).\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response62": "Descriptive Statistics: []\n\nComparison and Contrast: [\"even the ensemble of 5 models did not result in significant differences especially in bleu scores .\"]\n\nTrend and Pattern Analysis: []\n\nProportional and Percentage Calculations: []\n\nData Manipulation and Calculation: []\n\nCausal Relationships and Evaluation: []",
    "response63": "Descriptive Statistics: [\"Table 2 shows that the performance got slightly hurt ( comparing 'processed MT' with 'MT as PE') with pre-processing and post-processing procedures which are normally applied in training seq2seq models for reducing vocabulary size.\", \"The multi-source transformer (base) model achieved the highest single model BLEU score without joint training with the de-noising encoder task.\"]\nComparison and Contrast: [\"Table 2 shows that the performance got slightly hurt ( comparing 'processed MT' with 'MT as PE') with pre-processing and post-processing procedures which are normally applied in training seq2seq models for reducing vocabulary size.\", \"Even with the ensembled model, our APE approach does not significantly improve machine translation outputs measured in BLEU (+0.46).\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"Even with the ensembled model, our APE approach does not significantly improve machine translation outputs measured in BLEU (+0.46).\"]\nData Manipulation and Calculation: [\"Table 2 shows that the performance got slightly hurt ( comparing 'processed MT' with 'MT as PE') with pre-processing and post-processing procedures which are normally applied in training seq2seq models for reducing vocabulary size.\", \"The multi-source transformer (base) model achieved the highest single model BLEU score without joint training with the de-noising encoder task.\"]\nCausal Relationships and Evaluation: [\"Even with the ensembled model, our APE approach does not significantly improve machine translation outputs measured in BLEU (+0.46).\"]",
    "response64": "Descriptive Statistics: [\"we consider variations in instructions , incentives , data domains , and workflows .\", \"we manually analyzed paraphrases for correctness , grammaticality , and linguistic diversity .\", \"there was relatively little variation in grammaticality or time across the conditions .\"]\nComparison and Contrast: [\"our observations provide new insight into the trade - offs between accuracy and diversity in crowd responses that arise as a result of task design ,\", \"our analysis shows that the most important factor is how workers are primed for a task , with the choice of examples and the prompt sentence affecting diversity and correctness significantly .\", \"priming had a major impact , with the shift to lexical examples leading to a significant improvement in correctness , but much lower diversity .\", \"the surprising increase in correctness when providing no examples changing the incentives by providing either a bonus for novelty , or no bonus at all , did not substantially impact any of the metrics .\", \"changing the number of paraphrases written by each worker did not significantly impact diversity ( we worried that collecting more than one may lead to a decrease ) .\", \"the one paraphrase condition did have lower grammaticality ,\", \"changing the source of the prompt sentence to create a chain of paraphrases led to a significant increase in diversity .\", \"showing the answer to the question being para phrased did not significantly affect correctness or diversity ,\"]\nTrend and Pattern Analysis: [\"our observations provide new insight into the trade - offs between accuracy and diversity in crowd responses that arise as a result of task design ,\", \"our analysis shows that the most important factor is how workers are primed for a task , with the choice of examples and the prompt sentence affecting diversity and correctness significantly .\", \"priming had a major impact , with the shift to lexical examples leading to a significant improvement in correctness , but much lower diversity .\", \"the one paraphrase condition did have lower grammaticality ,\", \"changing the source of the prompt sentence to create a chain of paraphrases led to a significant increase in diversity .\"]\nProportional and Percentage Calculations: [\"our observations provide new insight into the trade - offs between accuracy and diversity in crowd responses that arise as a result of task design ,\"]\nData Manipulation and Calculation: [\"changing the number of paraphrases written by each worker did not significantly impact diversity ( we worried that collecting more than one may lead to a decrease ) .\", \"changing the source of the prompt sentence to create a chain of paraphrases led to a significant increase in diversity .\"]\nCausal Relationships and Evaluation: [\"our analysis shows that the most important factor is how workers are primed for a task , with the choice of examples and the prompt sentence affecting diversity and correctness significantly .\", \"the surprising increase in correctness when providing no examples changing the incentives by providing either a bonus for novelty , or no bonus at all , did not substantially impact any of the metrics .\", \"changing the number of paraphrases written by each worker did not significantly impact diversity ( we worried that collecting more than one may lead to a decrease ) .\", \"the one paraphrase condition did have lower grammaticality ,\", \"showing the answer to the question being para phrased did not significantly affect correctness or diversity ,\"]",
    "response65": "Descriptive Statistics: [\"in table 3 and figure 1 , the results of the experiment are presented .\", \"triframes based on watset clustering outperformed the other methods on both verb f1 and overall frame f1 .\", \"the use of the watset fuzzy clustering algorithm that splits the hubs by disambiguating them leads to the best results ( see table 3 ) .\"]\nComparison and Contrast: [\"triframes based on watset clustering outperformed the other methods on both verb f1 and overall frame f1 .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"triframes based on watset clustering outperformed the other methods on both verb f1 and overall frame f1 .\"]\nData Manipulation and Calculation: [\"triframes based on watset clustering outperformed the other methods on both verb f1 and overall frame f1 .\"]\nCausal Relationships and Evaluation: [\"the use of the watset fuzzy clustering algorithm that splits the hubs by disambiguating them leads to the best results ( see table 3 ) .\"]",
    "response66": "Descriptive Statistics: [\"table 4 presents results on the second dataset for the best models identified on the first dataset .\"]\nComparison and Contrast: [\"the lda - frames yielded the best results with our approach performing comparably in terms of the f1 - score .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response67": "Descriptive Statistics: [\"Table 1: French-English Performance.\", \"Baseline indicates current state of the art performance.\"]\nComparison and Contrast: [\"using global structure greatly improves upon the state of the art baseline performance.\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response68": "Descriptive Statistics: [\"Table 4: French-English Performance (large data). Baseline indicates state of the art performance.\"]\nComparison and Contrast: [\"we can see that the reverse rank and forward rank methods of taking into account the global structure of interactions among predictions is still helpful , providing large improvements in performance even in this challenging large data condition over strong state of the art baselines\"]\nTrend and Pattern Analysis: [\"tables 4 through 6 show the summary metrics for the three language pairs for the large data experiments .\"]\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response69": "Descriptive Statistics: [\"five source and six target ner data sets , each selected to provide a range of fields ( i . e . , biology , computer science , medications , local business ) and tenors ( i . e . , encyclopedia articles , journal articles , experimental protocols , online reviews ) .\", \"we use five data sets as source data , covering a range of fields ( i . e . , clinical , biomedical , local business and wiki with diverse fields ) and tenors ( i . e . , popular reporting , notes , scholarly publications , online reviews and encyclopedia ) .\", \"details of these target data are listed in table 2 .\"]\n\nComparison and Contrast: []\n\nTrend and Pattern Analysis: []\n\nProportional and Percentage Calculations: []\n\nData Manipulation and Calculation: []\n\nCausal Relationships and Evaluation: []",
    "response70": "Descriptive Statistics: [\"the results in table 4 show that our proposed similarity measures are predictive of the effectiveness of the pretraining data .\", \"vccr is the most informative factor in predicting the effectiveness of pretrained word vectors given a target data set .\"]\nComparison and Contrast: []\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response71": "Descriptive Statistics: [\"we find that word vectors and lms pretrained on small similar sources can achieve competitive or even better performance than the ones pretrained on larger sources ( table 5 ) .\"]\nComparison and Contrast: [\"on jnlpba , scienceie and wetlab , lms pretrained on the small similar source perform better , while word vectors pretrained on the small similar source perform better on craft , jnlpba , and scienceie .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"we find that word vectors and lms pretrained on small similar sources can achieve competitive or even better performance than the ones pretrained on larger sources ( table 5 ) .\"]\nData Manipulation and Calculation: [\"on jnlpba , scienceie and wetlab , lms pretrained on the small similar source perform better , while word vectors pretrained on the small similar source perform better on craft , jnlpba , and scienceie .\"]\nCausal Relationships and Evaluation: []",
    "response72": "Descriptive Statistics: [\"our results suggest that this hyper - parameter setting can overall ( except wiki - scienceie and mimic - wetlab pairs ) produce better performance compare to the default setting ( table 6 ) .\"]\nComparison and Contrast: []\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response73": "Descriptive Statistics: [\"most errors in our annotated corpus are related to person deixis , specifically gender marking in the russian translation , and the t - v distinction between informal and formal you ( latin ' tu ' and ' vos ' ) .\", \"from table 3 , we see that the most frequent error category related to deixis in our annotated corpus is the inconsistency of t - v forms when translating second person pronouns .\"]\nProportional and Percentage Calculations: [\"T-V distinction, 67%\", \"speaker/addressee gender:, [EMPTY]\", \"same speaker, 22%\", \"different speaker, 09%\", \"other, 02%\"]",
    "response74": "Descriptive Statistics: [\"The subsets of Event×Event (EE) and Timex3×Event (TE) relation pairs are of sizes 3.3k and 2.7k respectively.\", \"The intervals 0-100, 100-500 and 500+ are subsets reflecting average argument token frequency in the training data (of sizes 2.2k, 2.2k and 1.8k respectively).\"]\nComparison and Contrast: [\"ee relations are harder to recognize than the te relations, as all models achieve higher scores for te compared to ee relations.\", \"when training with the combined loss (sg or sglr) we obtain a clear improvement on the more difficult ee relations, and perform slightly worse on te relations compared to using pre-trained embeddings (the three upper settings).\", \"the rc + sg model performs best for low-frequency words, and rc + sglr performs best for the higher frequency ranges.\"]\nTrend and Pattern Analysis: [\"both combined loss settings outperform the baselines consistently.\"]\nProportional and Percentage Calculations: [\"The subsets of Event×Event (EE) and Timex3×Event (TE) relation pairs are of sizes 3.3k and 2.7k respectively.\", \"The intervals 0-100, 100-500 and 500+ are subsets reflecting average argument token frequency in the training data (of sizes 2.2k, 2.2k and 1.8k respectively).\"]\nData Manipulation and Calculation: [\"when training with the combined loss (sg or sglr) we obtain a clear improvement on the more difficult ee relations, and perform slightly worse on te relations compared to using pre-trained embeddings (the three upper settings).\"]\nCausal Relationships and Evaluation: [\"when evaluating on the full dev set, both combined loss settings outperform the baselines consistently.\"]",
    "response75": "Descriptive Statistics: [\"table 2 shows that initializing the model with the pre - trained embeddings gives a significant 4 1 . 1 point increase in f - measure compared to random initialization , due to an increase in precision .\", \"fixing the embeddings gives slightly better performance than using them as initialization , an increase of 0 . 9 point in f - measure , mostly due to higher recall .\", \"when extending the loss with the sglr loss , we gain6 1 . 6 in f - measure compared to fixing the word embeddings , and also surpass the state of the art by 0 . 4 even without specialized resources .\", \"if we train our model using the sg loss extension we obtain the best results , and gain6 1 . 9 points in f - measure compared to using pre - trained fixed word embeddings .\", \"this setting also exceeds the state of the art ( lin et al . , 2017 ) by 0 . 7 points in f - measure , due to a gain of 1 . 2 points in recall , again without using any specialized clinical nlp tools for feature engineering , in contrast to all state - of - the - art baselines .\"]\n\nComparison and Contrast: [\"table 2 shows that initializing the model with the pre - trained embeddings gives a significant 4 1 . 1 point increase in f - measure compared to random initialization , due to an increase in precision .\", \"fixing the embeddings gives slightly better performance than using them as initialization , an increase of 0 . 9 point in f - measure , mostly due to higher recall .\", \"this setting also exceeds the state of the art ( lin et al . , 2017 ) by 0 . 7 points in f - measure , due to a gain of 1 . 2 points in recall , again without using any specialized clinical nlp tools for feature engineering , in contrast to all state - of - the - art baselines .\"]\n\nTrend and Pattern Analysis: [\"if we train our model using the sg loss extension we obtain the best results , and gain6 1 . 9 points in f - measure compared to using pre - trained fixed word embeddings .\"]\n\nProportional and Percentage Calculations: [\"table 2 shows that initializing the model with the pre - trained embeddings gives a significant 4 1 . 1 point increase in f - measure compared to random initialization , due to an increase in precision .\", \"fixing the embeddings gives slightly better performance than using them as initialization , an increase of 0 . 9 point in f - measure , mostly due to higher recall .\", \"when extending the loss with the sglr loss , we gain6 1 . 6 in f - measure compared to fixing the word embeddings , and also surpass the state of the art by 0 . 4 even without specialized resources .\", \"if we train our model using the sg loss extension we obtain the best results , and gain6 1 . 9 points in f - measure compared to using pre - trained fixed word embeddings .\"]\n\nData Manipulation and Calculation: [\"fixing the embeddings gives slightly better performance than using them as initialization , an increase of 0 . 9 point in f - measure , mostly due to higher recall .\", \"when extending the loss with the sglr loss , we gain6 1 . 6 in f - measure compared to fixing the word embeddings , and also surpass the state of the art by 0 . 4 even without specialized resources .\", \"if we train our model using the sg loss extension we obtain the best results , and gain6 1 . 9 points in f - measure compared to using pre - trained fixed word embeddings .\"]\n\nCausal Relationships and Evaluation: [\"this setting also exceeds the state of the art ( lin et al . , 2017 ) by 0 . 7 points in f - measure , due to a gain of 1 . 2 points in recall , again without using any specialized clinical nlp tools for feature engineering , in contrast to all state - of - the - art baselines .\"]",
    "response76": "Descriptive Statistics: [\"from table 3 we can see that all models have difficulties with distant relations that cross sentence or clause boundaries ( ccr ) .\", \"furthermore it can be noticed that rc + sg has less errors for infrequent arguments ( < 10 ) in the supervised data .\"]\nComparison and Contrast: []\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: [\"furthermore , arguments that are frequent in the supervised data ( > 250 ) are a dominant error category .\"]",
    "response77": "Descriptive Statistics: [\"in table 2 we report the results , that we compute as the average of ten runs with random parameter initialization .\"]\nComparison and Contrast: [\"the results show that social information helps improve the performance on stance and hate speech detection , while it has no effect for sentiment analysis .\", \"ling + random never improves over ling : we find that both pv and n2v user representations lead to an improvement over ling .\", \"where ling + n2v outperforms ling + pv , while for hate speech the performance of the two models is comparable ( the difference between ling + pv and ling + n2v is not statistically significative due to the high variance of the ling + pv results - see extended results table in the supplementary material ) .\"]\nTrend and Pattern Analysis: [\"our model outperforms any other model on both stance and hate speech detection .\"]\nProportional and Percentage Calculations: [\"9 we use the unpaired welch ' s t test to check for statistically significant difference between models .\"]\nData Manipulation and Calculation: [\"9 we use the unpaired welch ' s t test to check for statistically significant difference between models .\"]\nCausal Relationships and Evaluation: [\"our model outperforms any other model on both stance and hate speech detection .\"]",
    "response78": "Descriptive Statistics: [\"an overview of the language pairs as well as the amount of annotated parallel sentences per language pair is given in table 1 .\", \"overview of annotated parallel sentences per language pair\"]\nComparison and Contrast: []\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response79": "Descriptive Statistics: [\"we classified ellipsis examples which lead to errors in sentence - level translations by the type of error they cause .\", \"results are provided in table 4 .\"]\nComparison and Contrast: [\"from table 4 , we see that the two most frequent types of ambiguity caused by the presence of an elliptical structure have different nature , hence we construct individual test sets for each of them .\"]\nProportional and Percentage Calculations: [\"wrong morphological form, 66%\", \"wrong verb (VP-ellipsis), 20%\", \"other error, 14%\"]",
    "response80": "Descriptive Statistics: [\"the main results of our experiments are shown in table 1 .\"]\nComparison and Contrast: [\"we notice that m . 1 ( bag - of - words + logistic regression ) and m . 2 ( bilstm ) show a statistically significant difference between the two genders , with higher predicted positive class probabilities for sentences with female nouns .\", \"on the contrary , m . 3 ( bert ) shows that sentences with male nouns have a statistically significant higher predicted positive class probability than sentences with female nouns .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response81": "Descriptive Statistics: [\"because of the small size of the dataset, we report the average performance over 5 runs with different random seeds.\", \"table 1 shows the results.\"]\nComparison and Contrast: [\"compared with wm18, our rgb model outperforms under all conditions.\", \"according to the cosine similarity, the hsv model is superior for most test conditions (confirming our hypothesis about simpler modifier behaviour in this space).\", \"however for delta-e, the rgb model and ensemble perform better.\", \"unlike cosine, delta-e is sensitive to differences in vector length, and we would argue it is the most appropriate metric because lengths are critical to measuring the extent of lightness and darkness of colors.\", \"accordingly the hsv model does worse under this metric, as it more directly models the direction of color modifiers, but as a consequence this leads to errors in its length predictions.\"]\nTrend and Pattern Analysis: [\"over-all the ensemble does well according to both metrics, and has the best performance for several test conditions with delta-e.\"]\nProportional and Percentage Calculations: [\"compared with wm18, our rgb model outperforms under all conditions.\"]\nData Manipulation and Calculation: [\"unlike cosine, delta-e is sensitive to differences in vector length, and we would argue it is the most appropriate metric because lengths are critical to measuring the extent of lightness and darkness of colors.\"]\nCausal Relationships and Evaluation: [\"according to the cosine similarity, the hsv model is superior for most test conditions (confirming our hypothesis about simpler modifier behaviour in this space).\"]",
    "response82": "Descriptive Statistics: [\"bleu scores for our model and the baselines are given in table 6 .\", \"CADec ' s performance remains the same independently from the number of context sentences ( 1 , 2 or 3 ) as measured with bleu .\", \"s - hier - to - 2 . tied performs worst in terms of bleu , but note that this is a shallow recurrent model , while others are transformer - based .\"]\nComparison and Contrast: [\"we observe that our model is no worse in bleu than the baseline despite the second - pass model being trained only on a fraction of the data .\", \"in contrast , the concatenation baseline , trained on a mixture of data with and without context is about 1 bleu below the context - agnostic baseline and our model when using all 3 context sentences .\", \"CADec ' s performance remains the same independently from the number of context sentences ( 1 , 2 or 3 ) as measured with bleu .\", \"s - hier - to - 2 . tied performs worst in terms of bleu , but note that this is a shallow recurrent model , while others are transformer - based .\", \"it also suffers from the asymmetric data setting , like the concatenation baseline .\"]\nTrend and Pattern Analysis: [\"5 for context - aware models , all sentences in a group were translated , and then only the current sentence is evaluated .\"]\nProportional and Percentage Calculations: [\"we also report bleu for the context - agnostic baseline trained only on 1 . 5m dataset to show how the performance is influenced by the amount of data .\"]\nData Manipulation and Calculation: [\"in contrast , the concatenation baseline , trained on a mixture of data with and without context is about 1 bleu below the context - agnostic baseline and our model when using all 3 context sentences .\"]\nCausal Relationships and Evaluation: [\"our model is no worse in bleu than the baseline despite the second - pass model being trained only on a fraction of the data .\"]",
    "response83": "Descriptive Statistics: [\"we reorganized the train / dev / test sets , forming new splits , which we refer to as no - leak f & c .\", \"the new split sizes can be found in table 2 .\"]\nComparison and Contrast: [\"we re - ran the current models on no - leak f & c and , as expected , we observe a drop of 5 - 6 % in accuracy : from the original 76 % accuracy on the dev / test sets , to 70 % and 71 % accuracy , respectively .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"we re - ran the current models on no - leak f & c and , as expected , we observe a drop of 5 - 6 % in accuracy : from the original 76 % accuracy on the dev / test sets , to 70 % and 71 % accuracy , respectively .\"]\nData Manipulation and Calculation: [\"we reorganized the train / dev / test sets , forming new splits , which we refer to as no - leak f & c .\", \"the new split sizes can be found in table 2 .\"]\nCausal Relationships and Evaluation: []",
    "response84": "Descriptive Statistics: [\"the left column of table 4 presents results for the cleaned version of the forbes and choi ( 2017 ) dataset .\", \"results on the new objects comparison dataset we created are shown in the rightmost column of table 4 .\"]\nComparison and Contrast: [\"we get better results than previous methods on this dataset : 63 % and 61 % accuracy on the dev / test sets compared to 60 % and 57 % .\", \"these relatively low results on this new dataset indicate that it is more challenging .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"we get better results than previous methods on this dataset : 63 % and 61 % accuracy on the dev / test sets compared to 60 % and 57 % .\"]\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: [\"these relatively low results on this new dataset indicate that it is more challenging .\"]",
    "response85": "Descriptive Statistics: [\"noun comparatives is on relative ( bagherinezhad et al . , 2016 ) , presented in table 5 .\", \"we report the results of the original work , where the best score used a combination of visual and textual signals , achieving 83 . 5 % accuracy .\", \"the accuracy achieved by this method is 85 . 8 % , surpassing the previous method by more than 2 points .\", \"we evaluated our method on this dataset , achieving a new state - of - the - art result of 87 . 7 % accuracy with k = 10 as a filter method .\"]\nComparison and Contrast: [\"we also tested the method by yang et al . ( 2018 ) on this dataset .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"where the best score used a combination of visual and textual signals , achieving 83 . 5 % accuracy .\", \"the accuracy achieved by this method is 85 . 8 % , surpassing the previous method by more than 2 points .\", \"we evaluated our method on this dataset , achieving a new state - of - the - art result of 87 . 7 % accuracy with k = 10 as a filter method .\"]\nData Manipulation and Calculation: [\"the accuracy achieved by this method is 85 . 8 % , surpassing the previous method by more than 2 points .\", \"we evaluated our method on this dataset , achieving a new state - of - the - art result of 87 . 7 % accuracy with k = 10 as a filter method .\"]\nCausal Relationships and Evaluation: [\"the accuracy achieved by this method is 85 . 8 % , surpassing the previous method by more than 2 points .\", \"we evaluated our method on this dataset , achieving a new state - of - the - art result of 87 . 7 % accuracy with k = 10 as a filter method .\"]",
    "response86": "Descriptive Statistics: [\"the results of the intrinsic evaluation on a sample of doq are shown in table 7 .\", \"the total agreement is 69 % , while the specific agreements for mass , length , speed and currency are 61 % , 79 % , 77 % and 58 % respectively .\", \"we re - annotated the samples in the currency category with annotators from the u . s . and found a much higher agreement score : 76 % .\"]\nComparison and Contrast: []\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"the total agreement is 69 % , while the specific agreements for mass , length , speed and currency are 61 % , 79 % , 77 % and 58 % respectively .\", \"we re - annotated the samples in the currency category with annotators from the u . s . and found a much higher agreement score : 76 % .\"]\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response87": "Descriptive Statistics: [\"significant features of the logistic regression model are shown in table 1 with the respective significance levels\", \"two indices related to the text easability and readability\"]\nComparison and Contrast: [\"those components that are also significant in the step-wise model appear in bold\", \"satire articles are more sophisticated, or less easy to read, than fake news articles\"]\nTrend and Pattern Analysis: [\"a combination of surface level related features, such as sentence length and average word frequency, as well as semantic features including LSA (latent semantic analysis) overlaps between verbs and between adjacent sentences\"]\nProportional and Percentage Calculations: [\"semantic features which are associated with the gist representation of content are particularly interesting to see among the predictors\"]\nData Manipulation and Calculation: [\"we also run a step-wise backward elimination regression\"]\nCausal Relationships and Evaluation: [\"gist representation of content drives individual's decision to spread misinformation online\", \"causal connectives are proven to be important in text comprehension\"]",
    "response88": "Descriptive Statistics: [\"results are shown in table 2.\"]\nComparison and Contrast: [\"the models based on the headline and text body give a similar f1 score.\", \"while the headline model performs poorly on precision, perhaps due to the short text, the model based on the text body performs poorly on recall.\", \"the model based on the full text of headline and body gives the best performance.\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response89": "Descriptive Statistics: [\"for all tasks, we observe a large improvement from using context.\"]\nComparison and Contrast: [\"for deixis, the concatenation model (concat) and CADec improve over the baseline by 33.5 and 31.6 percentage points, respectively.\", \"on the lexical cohesion test set, CADec shows a large improvement over the context-agnostic baseline (12.2 percentage points), while concat performs similarly to the baseline.\", \"when looking only at the scores where the latest relevant context is in the model's context window (column 2 in Table 7), s-hier-to-2.tied outperforms the concatenation baseline for lexical cohesion, but remains behind the performance of CADec.\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"for deixis, the concatenation model (concat) and CADec improve over the baseline by 33.5 and 31.6 percentage points, respectively.\", \"on the lexical cohesion test set, CADec shows a large improvement over the context-agnostic baseline (12.2 percentage points), while concat performs similarly to the baseline.\"]\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response90": "Descriptive Statistics: [\"table 3 provides a summary of the results .\"]\nComparison and Contrast: [\"we compare the results of our methods of the pre - trained bert , using both the headline and text body , and the coh - mertix approach , to the language - based baseline with multinomial naive bayes from ( golbeck et al . , 2018 ) 2 .\", \"both the semantic cues with bert and the linguistic cues with coh - metrix significantly outperform the baseline on the f1 score .\", \"the best result is given by the bert model .\"]\nTrend and Pattern Analysis: [\"overall , these results provide an answer to research question rq1 regarding the existence of semantic and linguistic difference between fake news and satire .\"]\nProportional and Percentage Calculations: [\"both the semantic cues with bert and the linguistic cues with coh - metrix significantly outperform the baseline on the f1 score .\"]\nData Manipulation and Calculation: [\"the two - tailed paired t - test with a 0 . 05 significance level was used for testing statistical significance of performance differences .\"]\nCausal Relationships and Evaluation: [\"both the semantic cues with bert and the linguistic cues with coh - metrix significantly outperform the baseline on the f1 score .\"]",
    "response91": "Descriptive Statistics: [\"table 1 shows micro f1 scores on aida - b of the sota methods and ours , which all use wikipedia and yago mention - entity index .\", \"all four our models outperform any previous method , with ment - norm achieving the best results , 0 . 85 % higher than that of ganea and hofmann ( 2017 ) .\", \"the experimental results show that ment - norm outperforms rel - norm , and that mention padding plays an important role .\"]\nComparison and Contrast: [\"the others use only one relation , coreference , which is given by simple heuristics or supervised third - party resolvers .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"all four our models outperform any previous method , with ment - norm achieving the best results , 0 . 85 % higher than that of ganea and hofmann ( 2017 ) .\"]\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: [\"to our knowledge , ours are the only ( unsupervisedly ) inducing and employing more than one relations on this dataset .\"]",
    "response92": "Descriptive Statistics: [\"table 2 shows micro f1 scores on 5 out - domain test sets .\", \"on average , ment - norm ' s f1 score is 0 . 3 % higher than that of ganea and hofmann ( 2017 ) , but 0 . 2 % lower than guo and barbosa ( 2016 ) ' s .\"]\nComparison and Contrast: [\"besides ours , only cheng and roth ( 2013 ) employs several mention relations .\", \"it is worth noting that guo and barbosa ( 2016 ) performs exceptionally well on wiki , but substantially worse than ment - norm on all other datasets .\", \"our other three models , however , have lower average f1 scores compared to the best previous model .\"]\nTrend and Pattern Analysis: [\"mentnorm achieves the highest f1 scores on msnbc and ace2004 .\"]\nProportional and Percentage Calculations: [\"on average , ment - norm ' s f1 score is 0 . 3 % higher than that of ganea and hofmann ( 2017 ) , but 0 . 2 % lower than guo and barbosa ( 2016 ) ' s .\"]\nData Manipulation and Calculation: [\"our other three models , however , have lower average f1 scores compared to the best previous model .\"]\nCausal Relationships and Evaluation: [\"the experimental results show that ment - norm outperforms rel - norm , and that mention padding plays an important role .\"]",
    "response93": "Descriptive Statistics: [\"the results are presented in table 2 .\"]\nComparison and Contrast: [\"the results in table 2 show very high performance , which is likely to further increase with ongoing training .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response94": "Descriptive Statistics: [\"both models improve substantially over the baseline ( by 19 - 51 percentage points )\"]\nComparison and Contrast: [\"concat stronger for inflection tasks and cadec stronger for vpellipsis\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"both models improve substantially over the baseline ( by 19 - 51 percentage points )\"]\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response95": "Descriptive Statistics: [\"all results of these experiments are presented in table 3 with the absolute improvement of the two main measures uar and ea over the svm - based approach\"]\nComparison and Contrast: []\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response96": "Descriptive Statistics: [\"the results are shown in table 1 .\"]\nComparison and Contrast: [\"we see that the mean ner performance increases in joint models .\", \"as one can see from the table , it achieved the best results compared to our joint models .\", \"however , we cannot confirm the difference between ext m feat and j multi models as the calculated p is well above . 05 .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: [\"the results are shown in table 1 .\"]\nData Manipulation and Calculation: [\"we see that the mean ner performance increases in joint models .\", \"as one can see from the table , it achieved the best results compared to our joint models .\", \"however , we cannot confirm the difference between ext m feat and j multi models as the calculated p is well above . 05 .\"]\nCausal Relationships and Evaluation: []",
    "response97": "Descriptive Statistics: [\"as can be seen from table 2 , we are very close to the state of the art md performance even if we only trained with a low number of parameters as stated in the beginning of this section .\"]\nComparison and Contrast: [\"we have to also note that in contrast with the ner task , the md task did not enjoy a performance increase from joint learning .\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response98": "Descriptive Statistics: [\"we report accuracy as the performance metric.\", \"all results are summarized in table 1.\"]\nComparison and Contrast: [\"table 1 represents the performance comparison of our proposed models and the baselines, which shows that incorporation of knowledge graph embeddings helps to improve the model performance.\"]\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []",
    "response99": "Descriptive Statistics: [\"results for different values of p are given in table 9 .\", \"all models have about the same bleu , not statistically significantly different from the baseline .\"]\nComparison and Contrast: [\"they are quite different in terms of incorporating context .\"]\nTrend and Pattern Analysis: [\"the denoising positively influences almost all tasks except for deixis , yielding the largest improvement on lexical cohesion .\"]\nProportional and Percentage Calculations: [\"all models have about the same bleu , not statistically significantly different from the baseline .\"]\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: [\"the denoising positively influences almost all tasks except for deixis , yielding the largest improvement on lexical cohesion .\"]",
    "response100": "Descriptive Statistics: [\"the purpose of table 5 is to show that the traditional cnn classifier in phase 2 was highly accurate\"]\nComparison and Contrast: []\nTrend and Pattern Analysis: []\nProportional and Percentage Calculations: []\nData Manipulation and Calculation: []\nCausal Relationships and Evaluation: []"
}