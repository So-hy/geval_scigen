[
    {
        "table_id": "0",
        "table_info": {
            "table_caption": "Table 1: Semantic parsing accuracies (id = in domain test set; ood = out of domain test set).",
            "table_column_names": [
                "[EMPTY]",
                "[BOLD] DM id F",
                "[BOLD] DM ood F",
                "[BOLD] PAS id F",
                "[BOLD] PAS ood F",
                "[BOLD] PSD id F",
                "[BOLD] PSD ood F",
                "[BOLD] EDS Smatch F",
                "[BOLD] EDS EDM",
                "[BOLD] AMR 2015 Smatch F",
                "[BOLD] AMR 2017 Smatch F"
            ],
            "table_content_values": [
                [
                    "Groschwitz et al. ( 2018 )",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "70.2",
                    "71.0"
                ],
                [
                    "Lyu and Titov ( 2018 )",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "73.7",
                    "74.4 ±0.16"
                ],
                [
                    "Zhang et al. ( 2019 )",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "[BOLD] 76.3 ±0.1"
                ],
                [
                    "Peng et al. ( 2017 ) Basic",
                    "89.4",
                    "84.5",
                    "92.2",
                    "88.3",
                    "77.6",
                    "75.3",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "Dozat and Manning ( 2018 )",
                    "93.7",
                    "88.9",
                    "94.0",
                    "90.8",
                    "81.0",
                    "79.4",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "Buys and Blunsom ( 2017 )",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "85.5",
                    "85.9",
                    "60.1",
                    "-"
                ],
                [
                    "Chen et al. ( 2018 )",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "[BOLD] 90.9,",
                    "[BOLD] 90.41",
                    "-",
                    "-"
                ],
                [
                    "This paper (GloVe)",
                    "90.4 ±0.2",
                    "84.3 ±0.2",
                    "91.4 ±0.1",
                    "86.6 ±0.1",
                    "78.1 ±0.2",
                    "74.5 ±0.2",
                    "87.6 ±0.1",
                    "82.5 ±0.1",
                    "69.2 ±0.4",
                    "70.7 ±0.2"
                ],
                [
                    "This paper (BERT)",
                    "[BOLD] 93.9 ±0.1",
                    "[BOLD] 90.3 ±0.1",
                    "[BOLD] 94.5 ±0.1",
                    "[BOLD] 92.5 ±0.1",
                    "[BOLD] 82.0 ±0.1",
                    "[BOLD] 81.5 ±0.3",
                    "90.1 ±0.1",
                    "84.9 ±0.1",
                    "[BOLD] 74.3 ±0.2",
                    "75.3 ±0.2"
                ],
                [
                    "Peng et al. ( 2017 ) Freda1",
                    "90.0",
                    "84.9",
                    "92.3",
                    "88.3",
                    "78.1",
                    "75.8",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "Peng et al. ( 2017 ) Freda3",
                    "90.4",
                    "85.3",
                    "92.7",
                    "89.0",
                    "78.5",
                    "76.4",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "This paper, MTL (GloVe)",
                    "91.2 ±0.1",
                    "85.7 ±0.0",
                    "92.2 ±0.2",
                    "88.0 ±0.3",
                    "78.9 ±0.3",
                    "76.2 ±0.4",
                    "88.2 ±0.1",
                    "83.3 ±0.1",
                    "(70.4) ±0.2",
                    "71.2 ±0.2"
                ],
                [
                    "This paper, MTL (BERT)",
                    "[BOLD] 94.1 ±0.1",
                    "[BOLD] 90.5 ±0.1",
                    "[BOLD] 94.7 ±0.1",
                    "[BOLD] 92.8 ±0.1",
                    "[BOLD] 82.1 ±0.2",
                    "[BOLD] 81.6 ±0.1",
                    "90.4 ±0.1",
                    "85.2 ±0.1",
                    "(74.5)3 ±0.1",
                    "75.3 ±0.1"
                ]
            ]
        },
        "gold_description": "table 1 ( upper part ) shows the results of our basic semantic parser ( with glove embeddings ) on all six graphbanks our results are competitive across the board , and set a new state of the art for eds smatch scores ( cai and knight , 2013 ) among eds parsers which are not trained on gold syntax information . our edm score ( dridan and oepen , 2011 ) on eds is lower , the use of bert embeddings is highly effective across the board . we set a new state of the art ( without gold syntax ) on all graphbanks except amr 2017 ; the improvement is particularly pronounced in the out - of - domain evaluations , illustrating bert ' s ability to transfer across domains . the results on the test dataset are shown in table 1 ( bottom ) . with glove , multi - task learning led to substantial improvements ; with bert the improvements are smaller but still noticeable .",
        "generated_description": "dm , pas , psd , eds and amr . we achieve state - of - the - art performance on three of the four datasets in terms of f1 score , and a very competitive performance on the eds test set with a score of 76 . 3 ± 0 . 1 and 77 . 6 smatch f . note that the performance of peng et al . on amr 2017 test set is lower than that of their basic model , which we believe is due to the fact that the test set of amr 2015 and 2016 was not drawn from the same training set as that of the other two datasets ."
    },
    {
        "table_id": "1",
        "table_info": {
            "table_caption": "Table 4: All experiment results of transfer learning and abbreviation expansion (top-2 scores marked as bold). MedNLI (expanded) denotes MedNLI with abbreviation expansion.",
            "table_column_names": [
                "[BOLD] Dataset",
                "[BOLD] BERT dev",
                "[BOLD] BERT test",
                "[BOLD] BioBERT dev",
                "[BOLD] BioBERT test"
            ],
            "table_content_values": [
                [
                    "MedNLI",
                    "79.56",
                    "77.49",
                    "82.15",
                    "79.04"
                ],
                [
                    "MNLI (M)",
                    "83.52",
                    "-",
                    "81.23",
                    "-"
                ],
                [
                    "SNLI (S)",
                    "90.39",
                    "-",
                    "89.10",
                    "-"
                ],
                [
                    "M → MedNLI",
                    "80.14",
                    "[BOLD] 78.62",
                    "82.72",
                    "80.80"
                ],
                [
                    "S → MedNLI",
                    "80.28",
                    "78.19",
                    "83.29",
                    "81.29"
                ],
                [
                    "M → S → MedNLI",
                    "80.43",
                    "78.12",
                    "83.29",
                    "80.30"
                ],
                [
                    "S → M → MedNLI",
                    "[BOLD] 81.72",
                    "77.98",
                    "[BOLD] 83.51",
                    "[BOLD] 82.63"
                ],
                [
                    "MedNLI (expanded)",
                    "79.13",
                    "77.07",
                    "[BOLD] 83.87",
                    "79.95"
                ],
                [
                    "S → M → MedNLI (expanded)",
                    "[BOLD] 82.15",
                    "[BOLD] 79.95",
                    "83.08",
                    "[BOLD] 81.85"
                ]
            ]
        },
        "gold_description": "we conduct transfer learning on four different combinations of mednli , snli , and mnli as it shown in the table 4 ( line 4 to 7 ) and also add the results of general domain tasks ( mnli , snli ) for comparison . bert performs better on tasks in the general domain while biobert performs better on mednli which is in the clinical domain . positive transfer occurs on mednli . even though biobert is finetuned on general domain tasks before mednli , transfer learning shows better results than that fine - tuned on mednli directly . the domain specific language representations from biobert are maintained while fine - tuning on general domain tasks by showing that the transfer learning results of mednli on biobert have better performance than the results on bert ( line 4 to 7 ) . the accuracy of mnli and snli on biobert is lower than the accuracy on bert . the best combination is snli → mnli → mednli on biobert . mednli ( expanded ) shows better performance than mednli on biobert while mednli works better on bert ( see table 4 ) . the performance of mednli ( expanded ) with transfer learning is higher on bert and lower on biobert than the performance of mednli with transfer learning .",
        "generated_description": "s5ss1sss0px4 results we first compare the performance of biobert and bert on the mednli datasets . the results show that both models outperform the snli ( s ) and mnli ( m ) baselines in the cross - validation setting , which confirms the effectiveness of the proposed approach . further , we find that the extended versions of the mednli datasets perform better than the original datasets , which shows the benefit of continuous learning of entailment and nli across multiple tasks . in addition , we note that there is no significant difference between the models trained on the original and the extended datasets ."
    },
    {
        "table_id": "2",
        "table_info": {
            "table_caption": "Table 2: Average performance (across 100 seeds) of ELMo on the SST2 task. We show performance on A-but-B sentences (“but”), negations (“neg”).",
            "table_column_names": [
                "Model no-distill",
                "Model no-project",
                "Test 85.98",
                "[ITALIC] but 78.69",
                "[ITALIC] but or  [ITALIC] neg 80.13"
            ],
            "table_content_values": [
                [
                    "no-distill",
                    "project",
                    "86.54",
                    "83.40",
                    "-"
                ],
                [
                    "distill",
                    "no-project",
                    "86.11",
                    "79.04",
                    "-"
                ],
                [
                    "distill",
                    "project",
                    "86.62",
                    "83.32",
                    "-"
                ],
                [
                    "ELMo",
                    "no-project",
                    "88.89",
                    "86.51",
                    "87.24"
                ],
                [
                    "ELMo",
                    "project",
                    "88.96",
                    "87.20",
                    "-"
                ]
            ]
        },
        "gold_description": "switching to elmo word embeddings improves performance by 2 . 9 percentage points on an average , corresponding to about 53 test sentences . of these , about 32 sentences ( 60 % of the improvement ) correspond to a - but - b and negation style sentences , as further evidence that elmo helps on these specific constructions , the non - elmo baseline model ( no - project , no - distill ) gets 255 sentences wrong in the test corpus on average , only 89 ( 34 . 8 % ) of which are a - but - b style or negations .",
        "generated_description": "we evaluate elmo on the sst2 task in table 2 . we show the model performance for the “ no - project ” and “ but ” groups , and compare the model ’ s performance for negated and but - but - b sentences . the distillation model performs better than the no - project model for both groups , suggesting that the distillation process encourages elmo to focus more on the correct label of the target label ."
    },
    {
        "table_id": "3",
        "table_info": {
            "table_caption": "Table 3: Number of sentences in the crowdsourced study (447 sentences) which got marked as neutral and which got the opposite of their labels in the SST2 dataset, using various thresholds. Inter-annotator agreement is computed using Fleiss’ Kappa. Average accuracies of the baseline and ELMo (over 100 seeds) on non-neutral sentences are also shown.",
            "table_column_names": [
                "Threshold",
                "0.50",
                "0.66",
                "0.75",
                "0.90"
            ],
            "table_content_values": [
                [
                    "Neutral Sentiment",
                    "10",
                    "70",
                    "95",
                    "234"
                ],
                [
                    "Flipped Sentiment",
                    "15",
                    "4",
                    "2",
                    "0"
                ],
                [
                    "Fleiss’ Kappa ( [ITALIC] κ)",
                    "0.38",
                    "0.42",
                    "0.44",
                    "0.58"
                ],
                [
                    "no-distill, no-project",
                    "81.32",
                    "83.54",
                    "84.54",
                    "87.55"
                ],
                [
                    "ELMo, no-project",
                    "87.56",
                    "90.00",
                    "91.31",
                    "93.14"
                ]
            ]
        },
        "gold_description": "we present statistics of our dataset10 in table 3 . as expected , inter - annotator agreement is higher for higher thresholds ( less ambiguous sentences ) . according to landis and koch ( 1977 ) , κ ∈ ( 0 . 2 , 0 . 4 ] corresponds to \" fair agreement \" , whereas κ ∈ ( 0 . 4 , 0 . 6 ] corresponds to \" moderate agreement \" . we next compute the accuracy of our model for each threshold by removing the corresponding neutral sentences . higher thresholds correspond to sets of less ambiguous sentences . table 3 shows that elmo ' s performance gains in table 2 extends across all thresholds .",
        "generated_description": "the task was done using fleiss ’ kappa to measure inter - annotator agreement with the labels assigned by the evaluators . the cohen ’ s kappa is used to measure the degree of agreement between the two annotators , which is normalized using pearson ’ s r . we used a threshold of 0 . 5 to reject sentences which get tagged as neutral and those which got tagged as non - neutral . the thresholds range from 0 . 50 to 0 . 75 . when the thresholds are above a threshold ( 0 . 5 ) , a sentence is considered to be labeled as neutral if its label is the majority label in the sst2 dataset . when thresholds are below these thresholds , a random sample of 1 , 000 sentences is used as the final label . for the no - project and no - distill settings , the elmo classifier performs better than the baseline on the neutral sentences , but worse on the flipped - noun ones . this shows that the task is difficult , and that elmo is better at classifying neutral sentences when presented with examples of the same label as the majority class ."
    },
    {
        "table_id": "4",
        "table_info": {
            "table_caption": "Table 1: Tuning Data Results AVG_COS_SIM. Top F per Concept Input Type in Bold.",
            "table_column_names": [
                "[BOLD] Concept Input →  [BOLD] Embeddings",
                "[BOLD] Concept Input →  [BOLD] TF",
                "[BOLD] Concept Input →  [BOLD] IDF",
                "[BOLD] Label  [BOLD] T",
                "[BOLD] Label  [BOLD] P",
                "[BOLD] Label  [BOLD] R",
                "[BOLD] Label  [BOLD] F",
                "[BOLD] Description  [BOLD] T",
                "[BOLD] Description  [BOLD] P",
                "[BOLD] Description  [BOLD] R",
                "[BOLD] Description  [BOLD] F",
                "[BOLD] Both  [BOLD] T",
                "[BOLD] Both  [BOLD] P",
                "[BOLD] Both  [BOLD] R",
                "[BOLD] Both  [BOLD] F"
            ],
            "table_content_values": [
                [
                    "[BOLD] GloVe",
                    "[BOLD] -",
                    "[BOLD] -",
                    ".635",
                    ".750",
                    ".818",
                    ".783",
                    ".720",
                    ".754",
                    ".891",
                    ".817",
                    ".735",
                    ".765",
                    ".945",
                    ".846"
                ],
                [
                    "[BOLD] GloVe",
                    "[BOLD] +",
                    "[BOLD] -",
                    ".640",
                    ".891",
                    ".745",
                    ".812",
                    ".700",
                    ".831",
                    ".891",
                    ".860",
                    ".690",
                    ".813",
                    ".945",
                    ".874"
                ],
                [
                    "[BOLD] GloVe",
                    "[BOLD] -",
                    "[BOLD] +",
                    ".600",
                    ".738",
                    ".873",
                    ".800",
                    ".670",
                    ".746",
                    ".909",
                    ".820",
                    ".755",
                    ".865",
                    ".818",
                    ".841"
                ],
                [
                    "[BOLD] GloVe",
                    "[BOLD] +",
                    "[BOLD] +",
                    ".605",
                    ".904",
                    ".855",
                    ".879",
                    ".665",
                    ".857",
                    ".873",
                    ".865",
                    ".715",
                    ".923",
                    ".873",
                    ".897"
                ],
                [
                    "[BOLD] Google",
                    "[BOLD] -",
                    "[BOLD] -",
                    ".440",
                    ".813",
                    ".945",
                    ".874",
                    ".515",
                    ".701",
                    ".982",
                    ".818",
                    ".635",
                    ".920",
                    ".836",
                    ".876"
                ],
                [
                    "[BOLD] Google",
                    "[BOLD] +",
                    "[BOLD] -",
                    ".445",
                    ".943",
                    ".909",
                    "[BOLD] .926",
                    ".540",
                    ".873",
                    ".873",
                    ".873",
                    ".565",
                    ".927",
                    ".927",
                    ".927"
                ],
                [
                    "[BOLD] Google",
                    "[BOLD] -",
                    "[BOLD] +",
                    ".435",
                    ".839",
                    ".945",
                    ".889",
                    ".520",
                    ".732",
                    ".945",
                    ".825",
                    ".590",
                    ".877",
                    ".909",
                    ".893"
                ],
                [
                    "[BOLD] Google",
                    "[BOLD] +",
                    "[BOLD] +",
                    ".430",
                    ".943",
                    ".909",
                    "[BOLD] .926",
                    ".530",
                    ".889",
                    ".873",
                    "[BOLD] .881",
                    ".545",
                    ".945",
                    ".945",
                    "[BOLD] .945"
                ],
                [
                    "[BOLD] fastText",
                    "[BOLD] -",
                    "[BOLD] -",
                    ".440",
                    ".781",
                    ".909",
                    ".840",
                    ".555",
                    ".708",
                    ".927",
                    ".803",
                    ".615",
                    ".778",
                    ".891",
                    ".831"
                ],
                [
                    "[BOLD] fastText",
                    "[BOLD] +",
                    "[BOLD] -",
                    ".435",
                    ".850",
                    ".927",
                    ".887",
                    ".520",
                    ".781",
                    ".909",
                    ".840",
                    ".530",
                    ".803",
                    ".964",
                    ".876"
                ],
                [
                    "[BOLD] fastText",
                    "[BOLD] -",
                    "[BOLD] +",
                    ".435",
                    ".850",
                    ".927",
                    ".887",
                    ".525",
                    ".722",
                    ".945",
                    ".819",
                    ".600",
                    ".820",
                    ".909",
                    ".862"
                ],
                [
                    "[BOLD] fastText",
                    "[BOLD] +",
                    "[BOLD] +",
                    ".420",
                    ".895",
                    ".927",
                    ".911",
                    ".505",
                    ".803",
                    ".891",
                    ".845",
                    ".520",
                    ".833",
                    ".909",
                    ".870"
                ]
            ]
        },
        "gold_description": "the top tuning data scores for avg cos sim ( table 1 ) show that the google embeddings with tf * idf weighting yield the top f score for all three concept input types ( . 881 - . 945 ) . somewhat expectedly , the best overall f score ( . 945 ) is produced in the setting both , which provides the most information . actually , this is true for all four weighting schemes for both glove and google , while fasttext consistently yields its top f scores ( . 840 - . 911 ) in the label setting , which provides the least information .",
        "generated_description": "from the results , we can see that : ( 1 ) using both tf and idf information improves the f - score of the proposed embeddings significantly . this is due to the fact that the idf information plays an important role in separating the positive and negative classes : the higher the score is , the more positive the category label is . ( 2 ) using only one of the embedding dimensionality of the given concept input outperforms the other one , which demonstrates the advantage of using a larger dimensionality in our embedding layer . ( 3 ) using the description information does not improve the idf score . the reason may be related to the lack of enough training data to train a description embedding for this task ."
    },
    {
        "table_id": "5",
        "table_info": {
            "table_caption": "Table 2: Tuning Data Results TOP_n_COS_SIM_AVG. Top F per Concept Input Type in Bold.",
            "table_column_names": [
                "[BOLD] Concept Input →  [BOLD] Embeddings",
                "[BOLD] Concept Input →  [BOLD] TF",
                "[BOLD] Concept Input →  [BOLD] IDF",
                "[BOLD] Label  [BOLD] T/n",
                "[BOLD] Label  [BOLD] P",
                "[BOLD] Label  [BOLD] R",
                "[BOLD] Label  [BOLD] F",
                "[BOLD] Description  [BOLD] T/n",
                "[BOLD] Description  [BOLD] P",
                "[BOLD] Description  [BOLD] R",
                "[BOLD] Description  [BOLD] F",
                "[BOLD] Both  [BOLD] T/n",
                "[BOLD] Both  [BOLD] P",
                "[BOLD] Both  [BOLD] R",
                "[BOLD] Both  [BOLD] F"
            ],
            "table_content_values": [
                [
                    "[BOLD] GloVe",
                    "[BOLD] +",
                    "[BOLD] -",
                    ".365/6",
                    ".797",
                    ".927",
                    ".857",
                    ".690/14",
                    ".915",
                    ".782",
                    ".843",
                    ".675/16",
                    ".836",
                    ".927",
                    ".879"
                ],
                [
                    "[BOLD] GloVe",
                    "[BOLD] -",
                    "[BOLD] +",
                    ".300/30",
                    ".929",
                    ".236",
                    ".377",
                    ".300/30",
                    ".806",
                    ".455",
                    ".581",
                    ".300/30",
                    ".778",
                    ".636",
                    ".700"
                ],
                [
                    "[BOLD] GloVe",
                    "[BOLD] +",
                    "[BOLD] +",
                    ".330/6",
                    ".879",
                    ".927",
                    ".903",
                    ".345/6",
                    ".881",
                    ".945",
                    "[BOLD] .912",
                    ".345/6",
                    ".895",
                    ".927",
                    ".911"
                ],
                [
                    "[BOLD] Google",
                    "[BOLD] +",
                    "[BOLD] -",
                    ".345/22",
                    ".981",
                    ".927",
                    "[BOLD] .953",
                    ".480/16",
                    ".895",
                    ".927",
                    ".911",
                    ".520/16",
                    ".912",
                    ".945",
                    ".929"
                ],
                [
                    "[BOLD] Google",
                    "[BOLD] -",
                    "[BOLD] +",
                    ".300/30",
                    "1.00",
                    ".345",
                    ".514",
                    ".300/8",
                    "1.00",
                    ".345",
                    ".514",
                    ".300/30",
                    "1.00",
                    ".600",
                    ".750"
                ],
                [
                    "[BOLD] Google",
                    "[BOLD] +",
                    "[BOLD] +",
                    ".300/10",
                    "1.00",
                    ".509",
                    ".675",
                    ".300/14",
                    ".972",
                    ".636",
                    ".769",
                    ".350/22",
                    "1.00",
                    ".836",
                    ".911"
                ],
                [
                    "[BOLD] fastText",
                    "[BOLD] +",
                    "[BOLD] -",
                    ".415/22",
                    ".980",
                    ".873",
                    ".923",
                    ".525/14",
                    ".887",
                    ".855",
                    ".870",
                    ".535/20",
                    ".869",
                    ".964",
                    ".914"
                ],
                [
                    "[BOLD] fastText",
                    "[BOLD] -",
                    "[BOLD] +",
                    ".350/24",
                    "1.00",
                    ".309",
                    ".472",
                    ".300/30",
                    "1.00",
                    ".382",
                    ".553",
                    ".300/28",
                    "1.00",
                    ".673",
                    ".804"
                ],
                [
                    "[BOLD] fastText",
                    "[BOLD] +",
                    "[BOLD] +",
                    ".300/20",
                    "1.00",
                    ".800",
                    ".889",
                    ".300/10",
                    ".953",
                    ".745",
                    ".837",
                    ".310/14",
                    ".963",
                    ".945",
                    "[BOLD] .954"
                ]
            ]
        },
        "gold_description": "for top n cos sim avg , the tuning data results ( table 2 ) are somewhat more varied : first , there is no single best performing set of embeddings : google yields the best f score for the label setting ( . 953 ) , while glove ( though only barely ) leads in the description setting ( . 912 ) . this time , it is fasttext which produces the best f score in the both setting , which is also the best overall tuning data f score for top n cos sim avg ( . 954 ) .",
        "generated_description": "the idf score is used to measure the precision , recall , and f - score of each of the three parts of the test set , i . e . , the percentage of items that are labelled as t / n and both , and the recall percentage of the items classified as both . we can see that the performance of the embeddings based on semantic concepts is significantly better than the ones based on textual concepts : the pearson ’ s r - squared value is . 929 and . 927 for idf and tf - idf , respectively , and . 898 and . 916 for both ."
    },
    {
        "table_id": "6",
        "table_info": {
            "table_caption": "Table 3: Test Data Results",
            "table_column_names": [
                "Gong et al.  [BOLD] ( 2018  [BOLD] )",
                "topic_science",
                "topic_science",
                "topic_science",
                "topic_science",
                "topic_science",
                "[BOLD] P .758",
                "[BOLD] P ±.012",
                "[BOLD] R .885",
                "[BOLD] R ±.071",
                "[BOLD] F .818",
                "[BOLD] F ±.028"
            ],
            "table_content_values": [
                [
                    "Gong et al.  [BOLD] ( 2018  [BOLD] )",
                    "topic_wiki",
                    "topic_wiki",
                    "topic_wiki",
                    "topic_wiki",
                    "topic_wiki",
                    ".750",
                    "±.009",
                    ".842",
                    "±.010",
                    ".791",
                    "±.007"
                ],
                [
                    "[BOLD] Method",
                    "[BOLD] Embeddings",
                    "[BOLD] Settings",
                    "[BOLD] Settings",
                    "[BOLD] T/n",
                    "[BOLD] Conc. Input",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[BOLD] AVG_COS_SIM",
                    "Google",
                    "+TF",
                    "+IDF",
                    ".515",
                    "Label",
                    ".939",
                    "±.043",
                    ".839",
                    "±.067",
                    ".884",
                    "±.038"
                ],
                [
                    "[BOLD] AVG_COS_SIM",
                    "Google",
                    "+TF",
                    "+IDF",
                    ".520",
                    "Description",
                    ".870",
                    "±.068",
                    ".834",
                    "±.048",
                    ".849",
                    "±.038"
                ],
                [
                    "[BOLD] AVG_COS_SIM",
                    "Google",
                    "+TF",
                    "+IDF",
                    ".545",
                    "Both",
                    ".915",
                    "±.040",
                    ".938",
                    "±.047",
                    "[BOLD] .926",
                    "±.038"
                ],
                [
                    "[BOLD] TOP_n_COS_SIM_AVG",
                    "Google",
                    "+TF",
                    "-IDF",
                    ".345/22",
                    "Label",
                    ".854",
                    "±.077",
                    ".861",
                    "±.044",
                    ".856",
                    "±.054"
                ],
                [
                    "[BOLD] TOP_n_COS_SIM_AVG",
                    "GloVe",
                    "+TF",
                    "+IDF",
                    ".345/6",
                    "Description",
                    ".799",
                    "±.063",
                    ".766",
                    "±.094",
                    ".780",
                    "±.068"
                ],
                [
                    "[BOLD] TOP_n_COS_SIM_AVG",
                    "fastText",
                    "+TF",
                    "+IDF",
                    ".310/14",
                    "Both",
                    ".850",
                    "±.059",
                    ".918",
                    "±.049",
                    "[BOLD] .881",
                    "±.037"
                ]
            ]
        },
        "gold_description": "the results can be found in table 3 . for comparison , the two top rows provide the best results of gong et al . ( 2018 ) . the first interesting finding is that the avg cos sim measure again performs very well : in all three settings , it beats both the system based on general - purpose embeddings ( topic wiki ) and the one that is adapted to the science domain ( topic science ) , with again the both setting yielding the best overall result ( . 926 ) . note that our both setting is probably the one most similar to the concept input used by gong et al . ( 2018 ) .",
        "generated_description": "avg_cos_sim shows the average precision , recall , and f - measure of the three different embedding sets obtained from the different methods . the label embeddings show a statistically significant difference ( p < 0 . 001 ) between the results reported in gong et al . ( tagged ) and our results ( unigram , bigram , and trigram ) . regarding the second research topic , topic_science was chosen as the main topic due to its larger size than topic_wiki , which was also the source of most of the noise . here , the results show that our method outperforms both baselines in terms of both precision and recall in both cases ."
    },
    {
        "table_id": "7",
        "table_info": {
            "table_caption": "Table 1: Best results after re-ranking using different re-ranker, and different values for k-best hypotheses extracted from the baseline output (%). In addition, to evaluate our re-ranker with MRR we fixed k CNNk=8 LSTMk=4",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] CNN  [ITALIC] full",
                "[BOLD] CNN  [ITALIC] dict",
                "[BOLD] CNN  [ITALIC] list",
                "[BOLD] CNN  [ITALIC] k",
                "[BOLD] CNN MRR",
                "[BOLD] LSTM  [ITALIC] full",
                "[BOLD] LSTM  [ITALIC] list",
                "[BOLD] LSTM k",
                "[BOLD] LSTM MRR"
            ],
            "table_content_values": [
                [
                    "Baseline (BL)",
                    "[BOLD] full: 19.7 dict: 56.0",
                    "[BOLD] full: 19.7 dict: 56.0",
                    "[BOLD] full: 19.7 dict: 56.0",
                    "[BOLD] full: 19.7 dict: 56.0",
                    "[BOLD] full: 19.7 dict: 56.0",
                    "[BOLD] full: 17.9",
                    "[BOLD] full: 17.9",
                    "[BOLD] full: 17.9",
                    "[BOLD] full: 17.9"
                ],
                [
                    "BL+ Glove (Jeffrey:14)",
                    "22.0",
                    "62.5",
                    "75.8",
                    "7",
                    "44.5",
                    "19.1",
                    "75.3",
                    "4",
                    "78.8"
                ],
                [
                    "BL+C-LSTM (Chunting:15)",
                    "21.4",
                    "61.0",
                    "71.3",
                    "8",
                    "45.6",
                    "18.9",
                    "74.7",
                    "4",
                    "80.7"
                ],
                [
                    "BL+CNN-RNN (Xingyou:16)",
                    "21.7",
                    "61.8",
                    "73.3",
                    "8",
                    "44.5",
                    "19.5",
                    "77.1",
                    "4",
                    "80.9"
                ],
                [
                    "BL+MVCNN (Wenpeng:16)",
                    "21.3",
                    "60.6",
                    "71.9",
                    "8",
                    "44.2",
                    "19.2",
                    "75.8",
                    "4",
                    "78.8"
                ],
                [
                    "BL+Attentive LSTM (Ming:16)",
                    "21.9",
                    "62.4",
                    "74.0",
                    "8",
                    "45.7",
                    "19.1",
                    "71.4",
                    "5",
                    "80.2"
                ],
                [
                    "BL+fasttext (Armand:17)",
                    "21.9",
                    "62.2",
                    "75.4",
                    "7",
                    "44.6",
                    "19.4",
                    "76.1",
                    "4",
                    "80.3"
                ],
                [
                    "BL+InferSent (Alexis:17)",
                    "22.0",
                    "62.5",
                    "75.8",
                    "7",
                    "44.5",
                    "19.4",
                    "76.7",
                    "4",
                    "79.7"
                ],
                [
                    "BL+USE-T (Daniel:18)",
                    "22.0",
                    "62.5",
                    "[BOLD] 78.3",
                    "6",
                    "44.7",
                    "19.2",
                    "75.8",
                    "4",
                    "79.5"
                ],
                [
                    "BL+TWE (Ahmed:18)",
                    "22.2",
                    "63.0",
                    "76.3",
                    "7",
                    "44.7",
                    "19.5",
                    "76.7",
                    "4",
                    "80.2"
                ],
                [
                    "BL+FDCLSTM (ours)",
                    "22.3",
                    "63.3",
                    "75.1",
                    "8",
                    "45.0",
                    "[BOLD] 20.2",
                    "67.9",
                    "9",
                    "79.8"
                ],
                [
                    "BL+FDCLSTM [ITALIC] AT (ours)",
                    "[BOLD] 22.4",
                    "63.7",
                    "75.5",
                    "8",
                    "[BOLD] 45.9",
                    "20.1",
                    "67.6",
                    "9",
                    "[BOLD] 81.8"
                ],
                [
                    "BL+FDCLSTM [ITALIC] lexicon (ours)",
                    "22.6",
                    "64.3",
                    "76.3",
                    "8",
                    "45.1",
                    "19.4",
                    "76.4",
                    "4",
                    "78.8"
                ],
                [
                    "BL+FDCLSTM [ITALIC] AT+ [ITALIC] lexicon (ours)",
                    "22.6",
                    "64.3",
                    "76.3",
                    "8",
                    "45.1",
                    "19.7",
                    "[BOLD] 77.8",
                    "4",
                    "80.4"
                ]
            ]
        },
        "gold_description": "we use two pre - trained deep models : a cnn ( jaderberg et al . , 2016 ) and an lstm ( ghosh et al . , 2017 ) as baselines ( bl ) to extract the initial list of word hypotheses . we experimented extracting kbest hypotheses for k = 1 . . . 10 . table 1 presents four different accuracy metrics for this case : 1 ) full columns correspond to the accuracy on the whole dataset . 2 ) dict columns correspond to the accuracy over the cases where the target word is among the 90k words of the cnn dictionary ( which correspond to 43 . 3 % of the whole dataset . 3 ) list columns report the accuracy over the cases where the right word was among the k - best produced by the baseline . 4 ) mrr mean reciprocal rank ( mrr ) , we compare the results of our encoder with several stateof - the - art sentence encoders , tuned or trained on the same dataset . table 1 are trained in the same conditions that our model with glove initialization with dual - channel overlapping non - static pre - trained embedding on the same dataset . our model fdclstm without attention achieves a better result in the case of the second baseline lstm that full of false - positives and short words . we also compare our result with current state - of - the - art word embeddings trained on a large general text using glove and fasttext . the word model used only object and place information , and ignored the caption . our proposed models achieve better performance than our twe previous model ( sabir et al . , 2018 ) , that trained a word embedding ( mikolov et al . , 2013 ) from scratch on the same task . as seen in table 1 , the introduction of this unigram lexicon produces the best results .",
        "generated_description": "the results show that the cnn and lstm models outperform the baseline by a large margin in terms of both dict and mrr when using the full cnn and list cnns , but the results are slightly worse when using a list cnn . we also see that the c - lstm model performs better than the bl + c - cnn model , which is a strong bidirectional language model ."
    },
    {
        "table_id": "8",
        "table_info": {
            "table_caption": "Table 1: Controlled experiments comparing SynST to a baseline Transformer, SAT, and LT on four different datasets (two language pairs) demonstrate speed and BLEU improvements. Wall-clock speedup is measured on a single Nvidia TitanX Pascal by computing the average time taken to decode a single sentence in the dev/test set, averaged over five runs. When beam width b is not specified, we perform greedy decoding (i.e., b=1). Note that the LT results are reported by latentTransformer and not from our own implementation; as such, they are not directly comparable to the other results.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] WMT En-De BLEU",
                "[BOLD] WMT En-De Speedup",
                "[BOLD] WMT De-En BLEU",
                "[BOLD] WMT De-En Speedup",
                "[BOLD] IWSLT En-De BLEU",
                "[BOLD] IWSLT En-De Speedup",
                "[BOLD] WMT En-Fr BLEU",
                "[BOLD] WMT En-Fr Speedup"
            ],
            "table_content_values": [
                [
                    "Baseline ( [ITALIC] b=1)",
                    "25.82",
                    "1.15×",
                    "29.83",
                    "1.14×",
                    "28.66",
                    "1.16×",
                    "39.41",
                    "1.18×"
                ],
                [
                    "Baseline ( [ITALIC] b=4)",
                    "26.87",
                    "1.00×",
                    "30.73",
                    "1.00×",
                    "30.00",
                    "1.00×",
                    "40.22",
                    "1.00×"
                ],
                [
                    "SAT ( [ITALIC] k=2)",
                    "22.81",
                    "2.05×",
                    "26.78",
                    "2.04×",
                    "25.48",
                    "2.03×",
                    "36.62",
                    "2.14×"
                ],
                [
                    "SAT ( [ITALIC] k=4)",
                    "16.44",
                    "3.61×",
                    "21.27",
                    "3.58×",
                    "20.25",
                    "3.45×",
                    "28.07",
                    "3.34×"
                ],
                [
                    "SAT ( [ITALIC] k=6)",
                    "12.55",
                    "4.86×",
                    "15.23",
                    "4.27×",
                    "14.02",
                    "4.39×",
                    "24.63",
                    "4.77×"
                ],
                [
                    "LT*",
                    "19.8",
                    "3.89×",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "SynST( [ITALIC] k=6)",
                    "20.74",
                    "4.86×",
                    "25.50",
                    "5.06×",
                    "23.82",
                    "3.78×",
                    "33.47",
                    "5.32×"
                ]
            ]
        },
        "gold_description": "table 1 contains the results on all four datasets . synst achieves speedups of ∼ 4 − 5 × that of the vanilla transformer , which is larger than nearly all of the sat configurations . quality - wise , synst again significantly outperforms the sat configurations at comparable speedups on all datasets . on wmt en - de , synst improves by 1 bleu over lt ( 20 . 74 vs lt ' s 19 . 8 without reranking ) .",
        "generated_description": "as can be seen from the table , sat consistently outperforms the baselines with a speedup ranging from 1 . 2 × to 3 . 6 × on both wmt and iwslt tasks . moreover , the speedup is even greater when the number of attention heads is increased from 1 to 4 , indicating the importance of having multiple heads in the attention layer ."
    },
    {
        "table_id": "9",
        "table_info": {
            "table_caption": "Table 2: Movie review Dev results of S-LSTM",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Time (s)",
                "[BOLD] Acc",
                "[BOLD] # Param"
            ],
            "table_content_values": [
                [
                    "+0 dummy node",
                    "56",
                    "81.76",
                    "7,216K"
                ],
                [
                    "+1 dummy node",
                    "65",
                    "82.64",
                    "8,768K"
                ],
                [
                    "+2 dummy node",
                    "76",
                    "82.24",
                    "10,321K"
                ],
                [
                    "Hidden size 100",
                    "42",
                    "81.75",
                    "4,891K"
                ],
                [
                    "Hidden size 200",
                    "54",
                    "82.04",
                    "6,002K"
                ],
                [
                    "Hidden size 300",
                    "65",
                    "82.64",
                    "8,768K"
                ],
                [
                    "Hidden size 600",
                    "175",
                    "81.84",
                    "17,648K"
                ],
                [
                    "Hidden size 900",
                    "235",
                    "81.66",
                    "33,942K"
                ],
                [
                    "Without ⟨s⟩, ⟨/s⟩",
                    "63",
                    "82.36",
                    "8,768K"
                ],
                [
                    "With ⟨s⟩, ⟨/s⟩",
                    "65",
                    "82.64",
                    "8,768K"
                ]
            ]
        },
        "gold_description": "table 2 shows the development results of various s - lstm settings , adding one additional sentence - level node does not lead to accuracy improvements , although the number of parameters and decoding time increase accordingly . the accuracies of s - lstm increases as the hidden layer size for each node increases from 100 to 300 , but does not further increase when the size increases beyond 300 . we fix the hidden size to 300 accordingly . without using ( cid : 104 ) s ( cid : 105 ) and ( cid : 104 ) / s ( cid : 105 ) , the performance of s - lstm drops from 82 . 64 % to 82 . 36 % , showing the effectiveness of having these additional nodes .",
        "generated_description": "s - lstm with a dummy node has the same number of hidden units as in the movie review model , but the size of its hidden layers is reduced from 10 , 321k to 4 , 891k to reduce the model size to 8 , 768k . the model size is also reduced from 7 , 216k to 6 , 002k to save the computational cost . with the smaller hidden size , the model is able to achieve an accuracy of 82 . 64 % with a small number of parameters , which is slightly better than the model with a hidden size of 900 ."
    },
    {
        "table_id": "10",
        "table_info": {
            "table_caption": "Table 3: Movie review development results",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Time (s)",
                "[BOLD] Acc",
                "[BOLD] # Param"
            ],
            "table_content_values": [
                [
                    "LSTM",
                    "67",
                    "80.72",
                    "5,977K"
                ],
                [
                    "BiLSTM",
                    "106",
                    "81.73",
                    "7,059K"
                ],
                [
                    "2 stacked BiLSTM",
                    "207",
                    "81.97",
                    "9,221K"
                ],
                [
                    "3 stacked BiLSTM",
                    "310",
                    "81.53",
                    "11,383K"
                ],
                [
                    "4 stacked BiLSTM",
                    "411",
                    "81.37",
                    "13,546K"
                ],
                [
                    "S-LSTM",
                    "65",
                    "82.64*",
                    "8,768K"
                ],
                [
                    "CNN",
                    "34",
                    "80.35",
                    "5,637K"
                ],
                [
                    "2 stacked CNN",
                    "40",
                    "80.97",
                    "5,717K"
                ],
                [
                    "3 stacked CNN",
                    "47",
                    "81.46",
                    "5,808K"
                ],
                [
                    "4 stacked CNN",
                    "51",
                    "81.39",
                    "5,855K"
                ],
                [
                    "Transformer (N=6)",
                    "138",
                    "81.03",
                    "7,234K"
                ],
                [
                    "Transformer (N=8)",
                    "174",
                    "81.86",
                    "7,615K"
                ],
                [
                    "Transformer (N=10)",
                    "214",
                    "81.63",
                    "8,004K"
                ],
                [
                    "BiLSTM+Attention",
                    "126",
                    "82.37",
                    "7,419K"
                ],
                [
                    "S-LSTM+Attention",
                    "87",
                    "83.07*",
                    "8,858K"
                ]
            ]
        },
        "gold_description": "as shown in table 3 , bilstm gives significantly better accuracies compared to uni - directional lstm2 , with the training time per epoch growing from 67 seconds to 106 seconds . stacking 2 layers of bilstm gives further improvements to development results , with a larger time of 207 seconds . 3 layers of stacked bilstm does not further improve the results . in contrast , s - lstm gives a development result of 82 . 64 % , which is significantly better compared to 2 - layer stacked bilstm , with a smaller number of model parameters and a shorter time of 65 seconds . we additionally make comparisons with stacked cnns and hierarchical attention ( vaswani et al . , 2017 ) , shown in table 3 ( the cnn and transformer rows ) , cnn is the most efficient among all models compared , with the smallest model size . on the other hand , a 3 - layer stacked cnn gives an accuracy of 81 . 46 % , which is also the lowest compared with bilstm , hierarchical attention and s - lstm . the best performance of hierarchical attention is between single - layer and two - layer bilstms in terms of both accuracy and efficiency . s - lstm gives significantly better accuracies compared with both cnn and hierarchical attention . table 3 additionally shows the results of bilstm and s - lstm when external attention is used attention leads to improved accuracies for both bilstm and s - lstm in classification , with s - lstm still outperforming bilstm significantly .",
        "generated_description": "for a fair comparison , we use the same number of stacked cnn layers and number of transformer layers for all our experiments . we can see that the time complexity of stacked bilstm is similar to that of lstm and cnn , while the number of parameters of all transformer models is about 1 / 8 of that of them . for the stacked cnn models , the accuracy is slightly higher than that of the stacked lstms , but the time to do so is much smaller . for instance , for a stacked cnn model with n = 6 , the time taken to do the prediction is only 1 / 4 of that required by the standard transformer model with 8 layers . the accuracy achieved by our model is also 1 / 3 of the time cost of the standard transformer model with 6 layers , which makes our model approximately 3 times faster ."
    },
    {
        "table_id": "11",
        "table_info": {
            "table_caption": "Table 4: Test set results on movie review dataset (* denotes significance in all tables).",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Accuracy",
                "[BOLD] Train (s)",
                "[BOLD] Test (s)"
            ],
            "table_content_values": [
                [
                    "socher2011semi",
                    "77.70",
                    "–",
                    "–"
                ],
                [
                    "socher2012semantic",
                    "79.00",
                    "–",
                    "–"
                ],
                [
                    "kim2014convolutional",
                    "81.50",
                    "–",
                    "–"
                ],
                [
                    "qian2016linguistically",
                    "81.50",
                    "–",
                    "–"
                ],
                [
                    "BiLSTM",
                    "81.61",
                    "51",
                    "1.62"
                ],
                [
                    "2 stacked BiLSTM",
                    "81.94",
                    "98",
                    "3.18"
                ],
                [
                    "3 stacked BiLSTM",
                    "81.71",
                    "137",
                    "4.67"
                ],
                [
                    "3 stacked CNN",
                    "81.59",
                    "31",
                    "1.04"
                ],
                [
                    "Transformer (N=8)",
                    "81.97",
                    "89",
                    "2.75"
                ],
                [
                    "S-LSTM",
                    "[BOLD] 82.45*",
                    "41",
                    "1.53"
                ]
            ]
        },
        "gold_description": "the final results on the movie review and rich text classification datasets are shown in tables 4 and 5 , respectively . as shown in table 4 , s - lstm outperforms bilstm significantly , with a faster speed . s - lstm also gives highly competitive results when compared with existing methods in the literature .",
        "generated_description": "our s - lstm model outperforms the previous state - of - the - art models by 1 . 53 % in terms of accuracy on the test set . note that our model is much simpler than the ones used in \\ newcitesocher2011semi and \\ newcitewang2012semantic , and does not require any heavy feature engineering effort . moreover , it is also much faster than the transformer , which requires 8 - layer transformer to be trained and tested on the same dataset ."
    },
    {
        "table_id": "12",
        "table_info": {
            "table_caption": "Table 5: Results on the 16 datasets of Liu et al. (2017). Time format: train (test)",
            "table_column_names": [
                "[BOLD] Dataset Camera",
                "[BOLD] SLSTM  [BOLD] 90.02*",
                "[BOLD] Time (s) 50 (2.85)",
                "[BOLD] BiLSTM 87.05",
                "[BOLD] Time (s) 115 (8.37)",
                "[BOLD] 2 BiLSTM 88.07",
                "[BOLD] Time (s) 221 (16.1)"
            ],
            "table_content_values": [
                [
                    "Video",
                    "[BOLD] 86.75*",
                    "55 (3.95)",
                    "84.73",
                    "140 (12.59)",
                    "85.23",
                    "268 (25.86)"
                ],
                [
                    "Health",
                    "[BOLD] 86.5",
                    "37 (2.17)",
                    "85.52",
                    "118 (6.38)",
                    "85.89",
                    "227 (11.16)"
                ],
                [
                    "Music",
                    "[BOLD] 82.04*",
                    "52 (3.44)",
                    "78.74",
                    "185 (12.27)",
                    "80.45",
                    "268 (23.46)"
                ],
                [
                    "Kitchen",
                    "[BOLD] 84.54*",
                    "40 (2.50)",
                    "82.22",
                    "118 (10.18)",
                    "83.77",
                    "225 (19.77)"
                ],
                [
                    "DVD",
                    "[BOLD] 85.52*",
                    "63 (5.29)",
                    "83.71",
                    "166 (15.42)",
                    "84.77",
                    "217 (28.31)"
                ],
                [
                    "Toys",
                    "85.25",
                    "39 (2.42)",
                    "85.72",
                    "119 (7.58)",
                    "[BOLD] 85.82",
                    "231 (14.83)"
                ],
                [
                    "Baby",
                    "[BOLD] 86.25*",
                    "40 (2.63)",
                    "84.51",
                    "125 (8.50)",
                    "85.45",
                    "238 (17.73)"
                ],
                [
                    "Books",
                    "[BOLD] 83.44*",
                    "64 (3.64)",
                    "82.12",
                    "240 (13.59)",
                    "82.77",
                    "458 (28.82)"
                ],
                [
                    "IMDB",
                    "[BOLD] 87.15*",
                    "67 (3.69)",
                    "86.02",
                    "248 (13.33)",
                    "86.55",
                    "486 (26.22)"
                ],
                [
                    "MR",
                    "[BOLD] 76.2",
                    "27 (1.25)",
                    "75.73",
                    "39 (2.27)",
                    "75.98",
                    "72 (4.63)"
                ],
                [
                    "Appeal",
                    "85.75",
                    "35 (2.83)",
                    "86.05",
                    "119 (11.98)",
                    "[BOLD] 86.35*",
                    "229 (22.76)"
                ],
                [
                    "Magazines",
                    "[BOLD] 93.75*",
                    "51 (2.93)",
                    "92.52",
                    "214 (11.06)",
                    "92.89",
                    "417 (22.77)"
                ],
                [
                    "Electronics",
                    "[BOLD] 83.25*",
                    "47 (2.55)",
                    "82.51",
                    "195 (10.14)",
                    "82.33",
                    "356 (19.77)"
                ],
                [
                    "Sports",
                    "[BOLD] 85.75*",
                    "44 (2.64)",
                    "84.04",
                    "172 (8.64)",
                    "84.78",
                    "328 (16.34)"
                ],
                [
                    "Software",
                    "[BOLD] 87.75*",
                    "54 (2.98)",
                    "86.73",
                    "245 (12.38)",
                    "86.97",
                    "459 (24.68)"
                ],
                [
                    "[BOLD] Average",
                    "[BOLD] 85.38*",
                    "47.30 (2.98)",
                    "84.01",
                    "153.48 (10.29)",
                    "84.64",
                    "282.24 (20.2)"
                ]
            ]
        },
        "gold_description": "the final results on the movie review and rich text classification datasets are shown in tables 4 and 5 , respectively . as shown in table 5 , among the 16 datasets s - lstm gives the best results on 12 , compared with bilstm and 2 layered bilstm models . the average accuracy of s - lstm is 85 . 6 % , significantly higher compared with 84 . 9 % by 2 - layer stacked bilstm . 3 - layer stacked bilstm gives an average accuracy of 84 . 57 % , which is lower compared to a 2 - layer stacked bilstm , with a training time per epoch of 423 . 6 seconds .",
        "generated_description": "the time complexity of slstm and bilstm models is measured by the average number of frames per second ( ms ) , while the time taken by the 2 - layers models is proportional to the number of videos . we can observe that slstms are able to achieve the best performance in terms of time complexity , whereas bi - lstms and cnns are the slowest ones . however , the time complexity is not negatively correlated with the performance of these models . for instance , a 2 - layer cnn can achieve accuracies of 88 . 07 % and 88 . 05 % on the dvd and kitchen datasets , respectively , while a single layer cnn achieves accuracy of 87 . 05 % . in terms of training time , we can see that a 2 layer cnn is more than three times faster than a 1 layer cnn . moreover , the training time of a 1 - layer bidirectional lstm is only 1 time slower than that of a 2 layered lstms ."
    },
    {
        "table_id": "13",
        "table_info": {
            "table_caption": "Table 6: Results on PTB (POS tagging)",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Accuracy",
                "[BOLD] Train (s)",
                "[BOLD] Test (s)"
            ],
            "table_content_values": [
                [
                    "manning2011part",
                    "97.28",
                    "–",
                    "–"
                ],
                [
                    "collobert2011natural",
                    "97.29",
                    "–",
                    "–"
                ],
                [
                    "sun2014structure",
                    "97.36",
                    "–",
                    "–"
                ],
                [
                    "sogaard2011semisupervised",
                    "97.50",
                    "–",
                    "–"
                ],
                [
                    "huang2015bidirectional",
                    "[BOLD] 97.55",
                    "–",
                    "–"
                ],
                [
                    "ma2016end",
                    "[BOLD] 97.55",
                    "–",
                    "–"
                ],
                [
                    "yang2017transfer",
                    "[BOLD] 97.55",
                    "–",
                    "–"
                ],
                [
                    "BiLSTM",
                    "97.35",
                    "254",
                    "22.50"
                ],
                [
                    "2 stacked BiLSTM",
                    "97.41",
                    "501",
                    "43.99"
                ],
                [
                    "3 stacked BiLSTM",
                    "97.40",
                    "746",
                    "64.96"
                ],
                [
                    "S-LSTM",
                    "[BOLD] 97.55",
                    "237",
                    "22.16"
                ]
            ]
        },
        "gold_description": "as can be seen in table 6 , s - lstm gives significantly better results compared with bilstm on the wsj dataset . it also gives competitive accuracies as compared with existing methods in the literature . stacking two layers of bilstms leads to improved results compared to one - layer bilstm , but the accuracy does not further improve with three layers of stacked lstms .",
        "generated_description": "we observe that the stacked bi - lstm model outperforms all previous models , achieving a state - of - the - art result of 97 . 55 % accuracy on the ptb test set . we also observe that even a single stacked birnn can achieve this performance , outperforming the previous best result of ma2016end by 0 . 8 % accuracy ."
    },
    {
        "table_id": "14",
        "table_info": {
            "table_caption": "Table 7: Results on CoNLL03 (NER)",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] F1",
                "[BOLD] Train (s)",
                "[BOLD] Test (s)"
            ],
            "table_content_values": [
                [
                    "collobert2011natural",
                    "89.59",
                    "–",
                    "–"
                ],
                [
                    "passos2014lexicon",
                    "90.90",
                    "–",
                    "–"
                ],
                [
                    "luo2015joint",
                    "91.20",
                    "–",
                    "–"
                ],
                [
                    "huang2015bidirectional",
                    "90.10",
                    "–",
                    "–"
                ],
                [
                    "lample2016neural",
                    "90.94",
                    "–",
                    "–"
                ],
                [
                    "ma2016end",
                    "91.21",
                    "–",
                    "–"
                ],
                [
                    "yang2017transfer",
                    "91.26",
                    "–",
                    "–"
                ],
                [
                    "rei:2017:Long",
                    "86.26",
                    "–",
                    "–"
                ],
                [
                    "peters2017semi",
                    "[BOLD] 91.93",
                    "–",
                    "–"
                ],
                [
                    "BiLSTM",
                    "90.96",
                    "82",
                    "9.89"
                ],
                [
                    "2 stacked BiLSTM",
                    "91.02",
                    "159",
                    "18.88"
                ],
                [
                    "3 stacked BiLSTM",
                    "91.06",
                    "235",
                    "30.97"
                ],
                [
                    "S-LSTM",
                    "[BOLD] 91.57*",
                    "79",
                    "9.78"
                ]
            ]
        },
        "gold_description": "for ner ( table 7 ) , s - lstm gives an f1 - score of 91 . 57 % on the conll test set , which is significantly better compared with bilstms . stacking more layers of bilstms leads to slightly better f1 - scores compared with a single - layer bilstm . our bilstm results are comparable to the results reported by ma and hovy ( 2016 ) and lample et al . ( 2016 ) , in contrast , s - lstm gives the best reported results under the same settings . in the second section of table 7 , yang et al . ( 2017 ) obtain an fscore of 91 . 26 %",
        "generated_description": "the s - lstm model achieved state - of - the - art results when trained and tested on the conll - 2009 spanish dataset . it significantly outperformed the previous best model , \\ newcitejuan2016neural , by 1 . 8 points in f1 score . our model also outperformed \\ newciteweisser2017semi , a neural semi - supervised model based on a multi - task learning framework , by 2 . 6 points . note that we only compare our model with the best published results , which are all based on neural semi neural models . we also observed that stacked bilstms performed the best in our experiments , despite the fact that they were not trained on the same data as our model ."
    },
    {
        "table_id": "15",
        "table_info": {
            "table_caption": "Table 2: E2E test set results. Own results correspond to avg±SD of ten runs and single result of best models on the development set.",
            "table_column_names": [
                "system",
                "BLEU",
                "ROUGE-L"
            ],
            "table_content_values": [
                [
                    "challenge",
                    "challenge",
                    "challenge"
                ],
                [
                    "baseline",
                    "65.9",
                    "68.5"
                ],
                [
                    "Thomson Reuters (np 3)",
                    "[BOLD] 68.1",
                    "69.3"
                ],
                [
                    "Thomson Reuters (np 4)",
                    "67.4",
                    "69.8"
                ],
                [
                    "HarvardNLP & H. Elder",
                    "67.4",
                    "[BOLD] 70.8"
                ],
                [
                    "own",
                    "own",
                    "own"
                ],
                [
                    "word",
                    "67.8±0.8",
                    "70.4±0.6"
                ],
                [
                    "character",
                    "64.6±6.0",
                    "67.9±4.7"
                ],
                [
                    "word (best on dev.)",
                    "67.8",
                    "70.2"
                ],
                [
                    "char. (best on dev.)",
                    "67.6",
                    "70.4"
                ]
            ]
        },
        "gold_description": "table 2 and 3 display the results on the e2e and webnlg test sets for models of the respective challenges and our own models on the e2e test set , our single best word - and character - based models reach comparable results to the best challenge submissions . the word - based models achieve significantly higher bleu and rouge - l scores than the character - based models .",
        "generated_description": "the baseline is a logistic regression classifier with cross - entropy as described in the main paper on the e2e dataset . our own results correspond to the best single - model results of ten runs and single result of best single model on the development set . both character and word embeddings outperform the baseline by a large margin , and the char . elder embedding achieves the best result on the challenge dataset , and outperforms the baseline in terms of bleu and rouge - l ."
    },
    {
        "table_id": "16",
        "table_info": {
            "table_caption": "Table 3: WebNLG test set results. Own results correspond to single best model on development set and avg±SD of ten runs.",
            "table_column_names": [
                "system",
                "BLEU",
                "ROUGE-L"
            ],
            "table_content_values": [
                [
                    "challenge",
                    "challenge",
                    "challenge"
                ],
                [
                    "baseline",
                    "32.1",
                    "43.3"
                ],
                [
                    "Melbourne",
                    "43.4",
                    "[BOLD] 61.0"
                ],
                [
                    "Tilburg-SMT",
                    "43.1",
                    "58.0"
                ],
                [
                    "UPF-FORGe",
                    "37.5",
                    "58.8"
                ],
                [
                    "own",
                    "own",
                    "own"
                ],
                [
                    "word (best on dev.)",
                    "[BOLD] 44.2",
                    "60.9"
                ],
                [
                    "char. (best on dev.)",
                    "41.3",
                    "58.4"
                ],
                [
                    "word",
                    "37.0±3.8",
                    "56.3±2.6"
                ],
                [
                    "character",
                    "39.7±1.7",
                    "58.4±0.7"
                ]
            ]
        },
        "gold_description": "on the webnlg test set , the bleu score of our best word - based model outperforms the best challenge submission by a small margin . the character - based model achieves a significantly higher rouge - l score than the wordbased model , whereas the bleu score difference is not significant .",
        "generated_description": "melbourne is the best performing system on the challenge , followed by tilburg - smt and upf - forge , and character - based systems are best on the development set . on the webnlg test set , our best results correspond to the char . ( best on dev . ) and word . ( avg . best on dev . ) , with improvements of 1 . 7 and 0 . 8 bleu and rouge - l respectively over the baseline . the difference between the dev . and test sets is small , less than 0 . 1 . despite the small size of the dataset , our results are competitive with the top systems ."
    },
    {
        "table_id": "17",
        "table_info": {
            "table_caption": "Table 4: Increasing the number of layers in SynST’s parse decoder significantly lowers the speedup while marginally impacting BLEU. Randomly sampling k from {1…6} during training boosts BLEU significantly with minimal impact on speedup.",
            "table_column_names": [
                "# Layers",
                "Max Chunk Size",
                "Speedup",
                "BLEU"
            ],
            "table_content_values": [
                [
                    "1",
                    "[ITALIC] k=6",
                    "3.8×",
                    "23.82"
                ],
                [
                    "2",
                    "[ITALIC] k=6",
                    "2.8×",
                    "23.98"
                ],
                [
                    "3",
                    "[ITALIC] k=6",
                    "2.2×",
                    "24.54"
                ],
                [
                    "4",
                    "[ITALIC] k=6",
                    "1.8×",
                    "24.04"
                ],
                [
                    "5",
                    "[ITALIC] k=6",
                    "1.4×",
                    "24.34"
                ],
                [
                    "1",
                    "[ITALIC] k∈{1…6}",
                    "3.1×",
                    "25.31"
                ]
            ]
        },
        "gold_description": "table 4 shows that increasing the number of layers from 1 to 5 results in a bleu increase of only 0 . 5 , while the speedup drops from 3 . 8 × to 1 . 4 × . the final row of table 4 shows that exposing the parse decoder to multiple possible chunkings of the same sentence during training allows it to choose a sequence of chunks that has a higher likelihood at test time , improving bleu by 1 . 5 while decreasing the speedup from 3 . 8 × to 3 . 1 × ;",
        "generated_description": "increasing the number of layers in synst ’ s parse decoder significantly decreases the speedup while marginally affecting bleu . we find that increasing the maximum chunk size from { 1 … 6 } to { 1 → 6 } results in a 3 . 1 × speedup and a 1 . 4 × bleus improvement , while having a negligible impact on speedup . increasing the size of the chunk size beyond 6 does not result in any significant gains in terms of speedup , and actually hampers the overall speedup ( theoretically , larger chunks allow for more capacity in the decoder to fit more sentences per second into the memory , thus potentially increasing the total number of parameters ) ."
    },
    {
        "table_id": "18",
        "table_info": {
            "table_caption": "Table 4: E2E and WebNLG development set results in the format avg±SD. Human results are averaged over using each human reference as prediction once.",
            "table_column_names": [
                "metric",
                "human",
                "word",
                "char."
            ],
            "table_content_values": [
                [
                    "E2E",
                    "E2E",
                    "E2E",
                    "E2E"
                ],
                [
                    "BLEU",
                    "55.5±0.7",
                    "68.2±1.4",
                    "65.8±2.6"
                ],
                [
                    "ROUGE-L",
                    "62.0±0.4",
                    "72.1±0.7",
                    "69.8±2.6"
                ],
                [
                    "WebNLG",
                    "WebNLG",
                    "WebNLG",
                    "WebNLG"
                ],
                [
                    "BLEU",
                    "48.3±0.7",
                    "40.6±4.2",
                    "43.7±2.4"
                ],
                [
                    "ROUGE-L",
                    "62.4±0.3",
                    "58.5±3.0",
                    "63.1±0.8"
                ]
            ]
        },
        "gold_description": "table 4 shows the bleu and rouge - l development set scores when treating each human reference as prediction once and evaluating it against the remaining references , compared to the scores of the word - based and character - based models strikingly , on the e2e development set , both model variants significantly outperform human texts by far with respect to both automatic evaluation measures . while the human bleu score is significantly higher than those of both systems on the webnlg development set , there is no statistical difference between human and system rouge - l scores .",
        "generated_description": "in the e2e dev set , we see that char . char . the human evaluation shows that webnlg benefits from using human - annotated references to improve bleu and rouge - l . however , the improvement in human evaluation is not as pronounced as in the dev set . we believe that this is due to the fact that human annotators have differing requirements for what constitutes a “ better ” or “ worse ” response depending on the context ."
    },
    {
        "table_id": "19",
        "table_info": {
            "table_caption": "Table 5: Percentage of affected instances in manual error analysis of 15 randomly selected development set instances for each input length.",
            "table_column_names": [
                "[EMPTY]",
                "E2E word",
                "E2E char.",
                "WebNLG word",
                "WebNLG char."
            ],
            "table_content_values": [
                [
                    "content errors",
                    "content errors",
                    "content errors",
                    "content errors",
                    "content errors"
                ],
                [
                    "info. dropped",
                    "40.0",
                    "30.0",
                    "42.9",
                    "66.7"
                ],
                [
                    "info. added",
                    "0.0",
                    "0.0",
                    "6.7",
                    "1.9"
                ],
                [
                    "info. modified",
                    "4.4",
                    "0.0",
                    "19.0",
                    "1.9"
                ],
                [
                    "info. repeated",
                    "0.0",
                    "0.0",
                    "15.2",
                    "28.6"
                ],
                [
                    "linguistic errors",
                    "linguistic errors",
                    "linguistic errors",
                    "linguistic errors",
                    "linguistic errors"
                ],
                [
                    "punctuation errors",
                    "5.6",
                    "5.6",
                    "8.6",
                    "3.8"
                ],
                [
                    "grammatical errors",
                    "13.3",
                    "14.4",
                    "15.2",
                    "12.4"
                ],
                [
                    "spelling mistakes",
                    "0.0",
                    "0.0",
                    "9.5",
                    "5.7"
                ],
                [
                    "overall correctness",
                    "overall correctness",
                    "overall correctness",
                    "overall correctness",
                    "overall correctness"
                ],
                [
                    "content correct",
                    "55.6",
                    "70.0",
                    "46.7",
                    "31.4"
                ],
                [
                    "language correct",
                    "83.3",
                    "81.1",
                    "69.5",
                    "79.0"
                ],
                [
                    "all correct",
                    "48.9",
                    "61.1",
                    "33.3",
                    "26.7"
                ]
            ]
        },
        "gold_description": "one annotator ( one of the authors of this paper ) manually assessed the outputs of the models that obtained the best development set bleu score as summarized in table 56 . as we can see from the bottom part of the table , all models struggle more with getting the content right than with producing linguistically correct texts ; 70 - 80 % of the texts generated by all models are completely correct linguistically . comparing the two datasets , we again observe that the webnlg dataset is much more challenging than the e2e dataset , especially with respect to correctly verbalizing the content . moreover , spelling mistakes only appeared in webnlg texts , mainly concerning omissions of accents or umlauts . the most frequent content error in both datasets concerns omission of information . information addition and repetition only occur in the webnlg dataset . the latter is an especially frequent problem of the character - based model , affecting more than a quarter of all texts . in comparison , character - based models reproduce the content more faithfully on the e2e dataset while offering the same level of linguistic quality as word - based models , leading to more correct outputs overall . on the webnlg dataset , the word - based model is more faithful to the inputs , probably because of the effective delexicalization strategy , whereas the character - based model errs less on the linguistic side . overall , the word - based model yields more correct texts , stressing the importance of delexicalization and data normalization in low resource settings .",
        "generated_description": "in terms of overall correctness , we can see that the char . version of e2e is slightly better than the word version for most of the content errors . however , there are a few minor spelling mistakes and punctuation errors , which can be handled by simply replacing the entire spelling with the correct spelling in the original text ( as done by \\ newcitedblp : journals / corr / abs - neco / webnlg ) . as for linguistic errors , content errors are the most common ones , which are caused by repeated ( or added ) information . the content errors of webnlg are less than 10 % for all information types . the linguistic errors of content are caused mostly by repetitions of the input text and misspellings ."
    },
    {
        "table_id": "20",
        "table_info": {
            "table_caption": "Table 6: Linguistic diversity of development set references and generated texts as avg±SD. ‘% new’ denotes the share of generated texts or sentences that do not appear in training references. Higher indicates more diversity for all measures.",
            "table_column_names": [
                "[EMPTY]",
                "E2E human",
                "E2E word",
                "E2E character",
                "WebNLG human",
                "WebNLG word",
                "WebNLG character"
            ],
            "table_content_values": [
                [
                    "unique sents.",
                    "866.3±16.5",
                    "203.5±30.6",
                    "366.8±60.0",
                    "1,185.0±12.6",
                    "603.7±144.3",
                    "875.4±30.2"
                ],
                [
                    "unique words",
                    "419.7±16.7",
                    "64.4±2.3",
                    "73.1±7.2",
                    "1447.3±7.4",
                    "620.3±35.5",
                    "881.5±26.0"
                ],
                [
                    "word E",
                    "6.5±0.0",
                    "5.1±0.0",
                    "5.5±0.0",
                    "7.1±0.0",
                    "6.3±0.0",
                    "6.6±0.0"
                ],
                [
                    "1-3-grams E",
                    "10.4±0.0",
                    "7.7±0.1",
                    "8.2±0.1",
                    "11.6±0.0",
                    "10.1±0.1",
                    "10.5±0.1"
                ],
                [
                    "% new texts",
                    "99.7±0.2",
                    "98.2±0.3",
                    "98.8±0.2",
                    "91.1±0.3",
                    "69.8±4.8",
                    "87.5±0.6"
                ],
                [
                    "% new sents.",
                    "85.1±1.1",
                    "61.8±6.4",
                    "71.4±4.7",
                    "87.4±0.4",
                    "57.2±5.8",
                    "82.1±1.2"
                ]
            ]
        },
        "gold_description": "table 6 shows automatically computed statistics on the diversity of the generated texts of both models and human texts and on the overlap of the ( generated ) texts with the training set . on both datasets , our systems produce significantly less varied outputs and reproduce more texts and sentences from the training data than the human texts . interestingly , however , the characterbased models generate significantly more unique sentences and copy significantly less from the training data than the word - based models , which copy about 40 % of their generated sentences from the training data .",
        "generated_description": "in terms of the number of unique sentences , e2e exhibits a higher percentage of sentences with unique sentences than webnlg . in addition , the percentage of completely new texts is much higher than in the case of human authored texts ( 99 . 7 % vs . 16 . 7 % ) . the character embeddings exhibit a slightly lower percentage of new texts than the human text . this is likely a result of the fact that the character embedding is better able to better represent the structure of sentences in the human language than the other embedding methods ."
    },
    {
        "table_id": "21",
        "table_info": {
            "table_caption": "Table 7: Manual evaluation of generated texts for 10 random test instances of a word-based model trained with synthetic training data from two templates. c@n: avg. number of correct texts (with respect to content and language) among the top n hypotheses.",
            "table_column_names": [
                "[EMPTY]",
                "c@1",
                "c@2",
                "c@5",
                "c@30"
            ],
            "table_content_values": [
                [
                    "template 1",
                    "0.8",
                    "0.8",
                    "0.9",
                    "1.7"
                ],
                [
                    "template 2",
                    "1.0",
                    "1.2",
                    "1.3",
                    "1.9"
                ],
                [
                    "template 1+2",
                    "0.9",
                    "1.6",
                    "2.2",
                    "3.3"
                ],
                [
                    "+ reranker",
                    "0.9",
                    "1.9",
                    "2.7",
                    "3.3"
                ]
            ]
        },
        "gold_description": "table 7 shows our manual evaluation of the top 30 hypotheses for 10 random e2e test inputs generated by models trained with data synthesized from the two templates . as is evident from the first two rows , all models learned to generalize from the training data to produce correct texts for novel inputs consisting of unseen combinations of input attributes . yet , the picture is a bit different for the model trained on data generated by both templates . while the top two hypotheses are equally distributed between adhering to template 1 and template 2 , more than 5 % among the lower - ranked hypotheses constitute a template combination such as the example shown in the bottom part of figure 2 . as can be seen in the final row of table 7 , this simple reranker successfully places correct hypotheses higher up in the ranking , improving the practical usability of the generation model by now offering almost three correct variants for each input among the top five hypotheses on average .",
        "generated_description": "as expected , template 1 produces the most correct texts ( avg . c @ n : avg . number of correct texts among the top 30 most frequent hypotheses ) , and template 2 produces the second - most correct texts , scoring 1 . 9 % and 1 . 3 % more correct texts than template { 1 + 2 } . adding a reranker further improves the performance to 1 . 7 % c @ 30 , and template { 2 + 2 } produces the next best text , scoring 3 . 3 % ."
    },
    {
        "table_id": "22",
        "table_info": {
            "table_caption": "Table 1: Experimental results of abstractive summarization on Gigaword test set with ROUGE metric. The top section is prefix baselines, the second section is recent unsupervised methods and ours, the third section is state-of-the-art supervised method along with our implementation of a seq-to-seq model with attention, and the bottom section is our model’s oracle performance. Wang and Lee (2018) is by author correspondence (scores differ because of evaluation setup). For another unsupervised work Fevry and Phang (2018), we attempted to replicate on our test set, but were unable to obtain results better than the baselines.",
            "table_column_names": [
                "Model",
                "R1",
                "R2",
                "RL"
            ],
            "table_content_values": [
                [
                    "Lead-75C",
                    "23.69",
                    "7.93",
                    "21.5"
                ],
                [
                    "Lead-8",
                    "21.30",
                    "7.34",
                    "19.94"
                ],
                [
                    "Schumann ( 2018 )",
                    "22.19",
                    "4.56",
                    "19.88"
                ],
                [
                    "Wang and Lee ( 2018 )",
                    "27.09",
                    "9.86",
                    "24.97"
                ],
                [
                    "Contextual Match",
                    "26.48",
                    "10.05",
                    "24.41"
                ],
                [
                    "Cao et al. ( 2018 )",
                    "37.04",
                    "19.03",
                    "34.46"
                ],
                [
                    "seq2seq",
                    "33.50",
                    "15.85",
                    "31.44"
                ],
                [
                    "Contextual Oracle",
                    "37.03",
                    "15.46",
                    "33.23"
                ]
            ]
        },
        "gold_description": "the automatic evaluation scores are presented in table 1 and table 2 . our method outperforms commonly used prefix baselines for this task which take the first 75 characters or 8 words of the source as a summary . our system achieves comparable results to wang and lee ( 2018 ) a system based on both gans and reinforcement training . in table 1 , we also list scores of the stateof - the - art supervised model , an attention based seq - to - seq model of our own implementation , as well as the oracle scores of our method obtained by choosing the best summary among all finished hypothesis from beam search . the oracle scores are much higher , indicating that our unsupervised method does allow summaries of better quality ,",
        "generated_description": "the first section is prefix baselines , the second section is recent unsupervised methods , the third section is the state - of - the - art supervised method along with our implementation of a seq - to - seq model with attention , the bottom section is our model ’ s implementation of an abstractive summarization model along with the contextual oracle . we note that the best score of cao et al . ( 2018 ) is by author correspondence , which we attempted to replicate on our test set , but were unable to replicate due to difficulty with the evaluation setup ."
    },
    {
        "table_id": "23",
        "table_info": {
            "table_caption": "Table 2: Experimental results of extractive summarization on Google data set. F1 is the token overlapping score, and CR is the compression rate. F&A is an unsupervised baseline used in Filippova and Altun (2013), and the middle section is supervised results.",
            "table_column_names": [
                "Model",
                "F1",
                "CR"
            ],
            "table_content_values": [
                [
                    "F&A Unsupervised",
                    "52.3",
                    "-"
                ],
                [
                    "Contextual Match",
                    "60.90",
                    "0.38"
                ],
                [
                    "Filippova et al. ( 2015 )",
                    "82.0",
                    "0.38"
                ],
                [
                    "Zhao et al. ( 2018 )",
                    "85.1",
                    "0.39"
                ],
                [
                    "Contextual Oracle",
                    "82.1",
                    "0.39"
                ]
            ]
        },
        "gold_description": "for extractive sentence summarization , our method achieves good compression rate and significantly raises a previous unsupervised baseline on token level f1 score .",
        "generated_description": "the supervised results are in the middle section of the table , where filippova and altun ( 2013 ) provide supervised results . the unsupervised f & a is an implementation of the supervised baseline . the contextual match baseline is the same as used in the previous section , and the contextual match is the supervised results obtained by applying the proposed contextual match algorithm . the oracle achieves 82 . 1 % token overlapping score and 0 . 39 % compression rate , which outperforms the previous best published results ."
    },
    {
        "table_id": "24",
        "table_info": {
            "table_caption": "Table 3: Comparison of different model choices. The top section evaluates the effects of contextual representation in the matching model, and the bottom section evaluates the effects of different smoothing methods in the fluency model.",
            "table_column_names": [
                "Models",
                "abstractive R1",
                "abstractive R2",
                "abstractive RL",
                "extractive F1",
                "extractive CR"
            ],
            "table_content_values": [
                [
                    "CS + cat",
                    "26.48",
                    "10.05",
                    "24.41",
                    "60.90",
                    "0.38"
                ],
                [
                    "CS + avg",
                    "26.34",
                    "9.79",
                    "24.23",
                    "60.09",
                    "0.38"
                ],
                [
                    "CS + top",
                    "26.21",
                    "9.69",
                    "24.14",
                    "62.18",
                    "0.34"
                ],
                [
                    "CS + mid",
                    "25.46",
                    "9.39",
                    "23.34",
                    "59.32",
                    "0.40"
                ],
                [
                    "CS + bot",
                    "15.29",
                    "3.95",
                    "14.06",
                    "21.14",
                    "0.23"
                ],
                [
                    "TEMP5 + cat",
                    "26.31",
                    "9.38",
                    "23.60",
                    "52.10",
                    "0.43"
                ],
                [
                    "TEMP10 + cat",
                    "25.63",
                    "8.82",
                    "22.86",
                    "42.33",
                    "0.47"
                ],
                [
                    "NA + cat",
                    "24.81",
                    "8.89",
                    "22.87",
                    "49.80",
                    "0.32"
                ]
            ]
        },
        "gold_description": "table 3 considers analysis of different aspects of the model . first , we look at the fluency model and compare the cluster smoothing ( cs ) approach with softmax temperature ( tempx with x being the temperature ) commonly used for generation in lm - integrated models ( chorowski and jaitly , 2016 ) as well as no adjustment ( na ) . second , we vary the 3 - layer representation out of elmo forward language model to do contextual matching ( bot / mid / top : bottom / middle / top layer only , avg : average of 3 layers , cat : concatenation of all layers ) . results show the effectiveness of our cluster smoothing method for the vocabulary adaptive language model pfm , although temperature smoothing is an option for abstractive datasets . when using word embeddings ( bottom layer only from elmo language model ) in our contextual matching model pcm , the summarization performance drops significantly to below simple baselines as demonstrated by score decrease .",
        "generated_description": "in order to evaluate the impact of contextual representation in the matching model , we conduct ablation experiments on the cs and extractive models . we can see that adding the cs features significantly improves the performance of the model . the extractive model benefits from having access to the context of the entire document as it is much more difficult to extractive than the abstractive model . in addition , the best performance is achieved by using the top - level representation ( cf . cs + top ) , mid - level ( cs + avg ) , and bottom - level representations ( caption + bot ) as the matching agents . the experimental results show that the representation of the context plays an important role in the model ’ s performance ."
    },
    {
        "table_id": "25",
        "table_info": {
            "table_caption": "Table 3: Official evaluation results of the submitted runs on the test set.",
            "table_column_names": [
                "[BOLD] Sub-task",
                "[BOLD] 1 Ext.",
                "[BOLD] 1 Class.",
                "[BOLD] 2 Ext.",
                "[BOLD] 2 Class.",
                "[ITALIC]  [BOLD] mv Ext.",
                "[ITALIC]  [BOLD] mv Class."
            ],
            "table_content_values": [
                [
                    "1.1",
                    "-",
                    "72.1",
                    "-",
                    "74.7",
                    "-",
                    "[BOLD] 76.7"
                ],
                [
                    "1.2",
                    "-",
                    "[BOLD] 83.2",
                    "-",
                    "82.9",
                    "-",
                    "80.1"
                ],
                [
                    "2",
                    "[BOLD] 37.4",
                    "[BOLD] 33.6",
                    "36.5",
                    "28.8",
                    "35.6",
                    "28.3"
                ]
            ]
        },
        "gold_description": "we select the 1st and 2nd best performing models on the development datasets as well as the majority vote ( mv ) of 5 models for the final submission . the final results are shown in table 3 .",
        "generated_description": "for the ext . and mv sub - tasks , we submitted models for official evaluation on the test set of the conll - 2009 ner task . the models were submitted using the official evaluation scripts provided by the ner evaluation committee . for each sub - task , models were trained and evaluated on the in - domain train set and tested on the out - of - domain test set . the results show that models trained on one task perform better than those trained on other for the both ext . and vc . for example , for the mv classifier , the model trained on task 1 ( ext . ) . the results for task 2 ( mv classifier ) are slightly worse than those for task 1 , indicating that there is still room for improvement for the model to improve ."
    },
    {
        "table_id": "26",
        "table_info": {
            "table_caption": "Table 3: F1 and exact match comparisons of predicted chunk sequences (from the parse decoder), ground-truth chunk sequences (from an external parser in the target language), and chunk sequences obtained after parsing the translation produced by the token decoder. First two columns show the improvement obtained by jointly training the two decoders. The third column shows that when the token decoder deviates from the predicted chunk sequence, it usually results in a translation that is closer to the ground-truth target syntax, while the fourth column shows that the token decoder closely follows the predicted chunk sequence.",
            "table_column_names": [
                "[EMPTY]",
                "[BOLD]  Predicted parse vs. Gold parse (separate) ",
                "[BOLD]  Predicted parse vs. Gold parse (joint) ",
                "[BOLD]  Parsed prediction vs. Gold parse ",
                "[BOLD]  Parsed prediction vs. Predicted parse "
            ],
            "table_content_values": [
                [
                    "F1",
                    "65.48",
                    "69.64",
                    "79.16",
                    "89.90"
                ],
                [
                    "Exact match",
                    "4.23%",
                    "5.24%",
                    "5.94%",
                    "43.10%"
                ]
            ]
        },
        "gold_description": "we evaluate two configurations of the parse decoder , one in which it is trained separately from the token decoder ( first column of table 3 ) , and the other where both decoders are trained jointly ( second column of ta ble 3 ) . we observe that joint training boosts the chunk f1 from 65 . 4 to 69 . 6 , although , in both cases the f1 scores are relatively low , which matches our intuition as most source sentences can be translated into multiple target syntactic forms . to measure how often the token decoder follows the predicted chunk sequence , we parse the generated translation and compute the f1 between the resulting chunk sequence and the parse decoder ' s prediction ( fourth column of table 3 ) . strong results of 89 . 9 f1 and 43 . 1 % exact match indicate that the token decoder is heavily reliant on the generated chunk sequences . the resulting f1 is indeed almost 10 points higher ( third column of table 3 ) , indicating that the token decoder does have the ability to correct mistakes . in section 5 . 3 ( see the final row of table 3 ) we consider the effect of randomly sampling the max chunk size k during training . this provides a considerable boost to bleu with a minimal impact to speedup .",
        "generated_description": "we first compare the predicted chunk sequences from the parse decoder to the ground - truth chunk sequences ( either extracted from a parser in the target language or from a external parser ) in terms of f1 score and exact match with the chunk sequences obtained after parsing the predicted parse . we also evaluate the quality of the translation produced by the token decoder . when the predicted parser diverges from the predicted sequence , it usually translates into a translation that is closer to the target syntax than to the parse output . on the other hand , when the decoder closely follows the chunk sequence ( i . e . , translating from a language that has a more similar grammar to the source language ) it tends to make a more accurate prediction . in all cases , the f1 scores obtained when jointly training the two decoders are higher than when they are trained separately ."
    },
    {
        "table_id": "27",
        "table_info": {
            "table_caption": "Table 1: The models here all have a bi-directional GRU as the encoder (dimensionality 300 in each direction). The default way of producing the representation is a concatenation of outputs from a global mean-pooling and a global max-pooling, while “⋅-MaxOnly” refers to the model with only global max-pooling. Bold numbers are the best results among all presented models. We found that 1) inputting correct words to an autoregressive decoder is not necessary; 2) predict-all-words decoders work roughly the same as autoregressive decoders; 3) mean+max pooling provides stronger transferability than the max-pooling alone does. The table supports our choice of the predict-all-words CNN decoder and the way of producing vector representations from the bi-directional RNN encoder.",
            "table_column_names": [
                "[BOLD] Decoder",
                "[BOLD] SICK-R",
                "[BOLD] SICK-E",
                "[BOLD] STS14",
                "[BOLD] MSRP (Acc/F1)",
                "[BOLD] SST",
                "[BOLD] TREC"
            ],
            "table_content_values": [
                [
                    "[BOLD] auto-regressive RNN as decoder",
                    "[BOLD] auto-regressive RNN as decoder",
                    "[BOLD] auto-regressive RNN as decoder",
                    "[BOLD] auto-regressive RNN as decoder",
                    "[BOLD] auto-regressive RNN as decoder",
                    "[BOLD] auto-regressive RNN as decoder",
                    "[BOLD] auto-regressive RNN as decoder"
                ],
                [
                    "Teacher-Forcing",
                    "0.8530",
                    "82.6",
                    "0.51/0.50",
                    "74.1 / 81.7",
                    "82.5",
                    "88.2"
                ],
                [
                    "Always Sampling",
                    "0.8576",
                    "83.2",
                    "0.55/0.53",
                    "74.7 / 81.3",
                    "80.6",
                    "87.0"
                ],
                [
                    "Uniform Sampling",
                    "0.8559",
                    "82.9",
                    "0.54/0.53",
                    "74.0 / 81.8",
                    "81.0",
                    "87.4"
                ],
                [
                    "[BOLD] auto-regressive CNN as decoder",
                    "[BOLD] auto-regressive CNN as decoder",
                    "[BOLD] auto-regressive CNN as decoder",
                    "[BOLD] auto-regressive CNN as decoder",
                    "[BOLD] auto-regressive CNN as decoder",
                    "[BOLD] auto-regressive CNN as decoder",
                    "[BOLD] auto-regressive CNN as decoder"
                ],
                [
                    "Teacher-Forcing",
                    "0.8510",
                    "82.8",
                    "0.49/0.48",
                    "74.7 / 82.8",
                    "81.4",
                    "82.6"
                ],
                [
                    "Always Sampling",
                    "0.8535",
                    "83.3",
                    "0.53/0.52",
                    "75.0 / 81.7",
                    "81.4",
                    "87.6"
                ],
                [
                    "Uniform Sampling",
                    "0.8568",
                    "83.4",
                    "0.56/0.54",
                    "74.7 / 81.4",
                    "83.0",
                    "88.4"
                ],
                [
                    "[BOLD] predict-all-words RNN as decoder",
                    "[BOLD] predict-all-words RNN as decoder",
                    "[BOLD] predict-all-words RNN as decoder",
                    "[BOLD] predict-all-words RNN as decoder",
                    "[BOLD] predict-all-words RNN as decoder",
                    "[BOLD] predict-all-words RNN as decoder",
                    "[BOLD] predict-all-words RNN as decoder"
                ],
                [
                    "RNN",
                    "0.8508",
                    "82.8",
                    "0.58/0.55",
                    "74.2 / 82.8",
                    "81.6",
                    "88.8"
                ],
                [
                    "[BOLD] predict-all-words CNN as decoder",
                    "[BOLD] predict-all-words CNN as decoder",
                    "[BOLD] predict-all-words CNN as decoder",
                    "[BOLD] predict-all-words CNN as decoder",
                    "[BOLD] predict-all-words CNN as decoder",
                    "[BOLD] predict-all-words CNN as decoder",
                    "[BOLD] predict-all-words CNN as decoder"
                ],
                [
                    "CNN",
                    "0.8530",
                    "82.6",
                    "[BOLD] 0.58/ [BOLD] 0.56",
                    "[BOLD] 75.6 / 82.9",
                    "82.8",
                    "89.2"
                ],
                [
                    "CNN-MaxOnly",
                    "0.8465",
                    "82.6",
                    "0.50/0.47",
                    "73.3 / 81.5",
                    "79.1",
                    "82.2"
                ],
                [
                    "Double-sized RNN Encoder",
                    "Double-sized RNN Encoder",
                    "Double-sized RNN Encoder",
                    "Double-sized RNN Encoder",
                    "Double-sized RNN Encoder",
                    "Double-sized RNN Encoder",
                    "Double-sized RNN Encoder"
                ],
                [
                    "CNN",
                    "[BOLD] 0.8631",
                    "[BOLD] 83.9",
                    "[BOLD] 0.58/0.55",
                    "74.7 /  [BOLD] 83.1",
                    "[BOLD] 83.4",
                    "[BOLD] 90.2"
                ],
                [
                    "CNN-MaxOnly",
                    "0.8485",
                    "83.2",
                    "0.47/0.44",
                    "72.9 / 80.8",
                    "82.2",
                    "86.6"
                ]
            ]
        },
        "gold_description": "the results are presented in table 1 ( top two subparts ) . as we can see , the three decoding settings do not differ significantly in terms of the performance on selected downstream tasks , with rnn or cnn as the decoder . the results are also presented in table 1 ( 3rd and 4th subparts ) . the performance of the predict - allwords rnn decoder does not significantly differ from that of any one of the autoregressive rnn de coders , and the same situation can be also observed in cnn decoders . in our proposed rnn - cnn model , we empirically show that the mean + max pooling provides stronger transferability than the max pooling alone does , and the results are presented in the last two sections of table 1 .",
        "generated_description": "we can see that using a cnn as the decoder results in worse performance than using a plain rnn as decoder , which is consistent with the observations in \\ newcitesidaw12acl . we also observe that using an always - sampling decoder ( i . e . , always sampling and uniform sampling ) , which is similar to our proposed teacher forcing method , does not perform as well as using a convolutional layer or an auto - regressive model ."
    },
    {
        "table_id": "28",
        "table_info": {
            "table_caption": "Table 4: Architecture Comparison. As shown in the table, our designed asymmetric RNN-CNN model (row 1,9, and 12) works better than other asymmetric models (CNN-LSTM, row 11), and models with symmetric structure (RNN-RNN, row 5 and 10).",
            "table_column_names": [
                "[BOLD] Encoder type",
                "[BOLD] Encoder dim",
                "[BOLD] Decoder type",
                "[BOLD] Decoder dim",
                "[BOLD] Hrs",
                "[BOLD] SICK-R",
                "[BOLD] SICK-E",
                "[BOLD] STS14",
                "[BOLD] MSRP (Acc/F1)",
                "[BOLD] SST",
                "[BOLD] TREC"
            ],
            "table_content_values": [
                [
                    "[BOLD] Dimension of Sentence Representation: 1200",
                    "[BOLD] Dimension of Sentence Representation: 1200",
                    "[BOLD] Dimension of Sentence Representation: 1200",
                    "[BOLD] Dimension of Sentence Representation: 1200",
                    "[BOLD] Dimension of Sentence Representation: 1200",
                    "[BOLD] Dimension of Sentence Representation: 1200",
                    "[BOLD] Dimension of Sentence Representation: 1200",
                    "[BOLD] Dimension of Sentence Representation: 1200",
                    "[BOLD] Dimension of Sentence Representation: 1200",
                    "[BOLD] Dimension of Sentence Representation: 1200",
                    "[BOLD] Dimension of Sentence Representation: 1200"
                ],
                [
                    "RNN",
                    "2x300",
                    "CNN",
                    "600-1200-300",
                    "20",
                    "0.8530",
                    "82.6",
                    "0.58/0.56",
                    "[BOLD] 75.6/ [BOLD] 82.9",
                    "82.8",
                    "[BOLD] 89.2"
                ],
                [
                    "RNN",
                    "2x300",
                    "CNN†",
                    "600-1200-300",
                    "21",
                    "0.8515",
                    "82.7",
                    "0.58/0.56",
                    "75.3/82.5",
                    "[BOLD] 82.9",
                    "85.2"
                ],
                [
                    "RNN",
                    "2x300",
                    "CNN(10)",
                    "600-1200-300",
                    "11",
                    "0.8474",
                    "82.9",
                    "0.57/0.55",
                    "74.2/81.6",
                    "82.8",
                    "88.0"
                ],
                [
                    "RNN",
                    "2x300",
                    "CNN(50)",
                    "600-1200-300",
                    "27",
                    "0.8533",
                    "82.5",
                    "0.57/0.55",
                    "74.7/82.2",
                    "81.5",
                    "86.2"
                ],
                [
                    "RNN",
                    "2x300",
                    "RNN",
                    "600",
                    "26",
                    "0.8530",
                    "82.6",
                    "0.51/0.50",
                    "74.1/81.7",
                    "81.0",
                    "89.0"
                ],
                [
                    "CNN",
                    "4x300 \\lx @ [ITALIC] sectionsign",
                    "CNN",
                    "600-1200-300",
                    "8",
                    "0.8117",
                    "80.5",
                    "0.44/0.42",
                    "72.7/80.7",
                    "78.4",
                    "85.0"
                ],
                [
                    "RNN",
                    "2x300",
                    "CNN",
                    "600-1200-2400-300",
                    "28",
                    "[BOLD] 0.8570",
                    "[BOLD] 84.0",
                    "0.58/0.56",
                    "74.3/81.5",
                    "82.8",
                    "88.2"
                ],
                [
                    "RNN",
                    "2x300",
                    "CNN",
                    "1200-2400-300",
                    "27",
                    "0.8541",
                    "83.0",
                    "[BOLD] 0.59/ [BOLD] 0.57",
                    "74.3/82.2",
                    "[BOLD] 82.9",
                    "89.0"
                ],
                [
                    "[BOLD] Dimension of Sentence Representation: 2400",
                    "[BOLD] Dimension of Sentence Representation: 2400",
                    "[BOLD] Dimension of Sentence Representation: 2400",
                    "[BOLD] Dimension of Sentence Representation: 2400",
                    "[BOLD] Dimension of Sentence Representation: 2400",
                    "[BOLD] Dimension of Sentence Representation: 2400",
                    "[BOLD] Dimension of Sentence Representation: 2400",
                    "[BOLD] Dimension of Sentence Representation: 2400",
                    "[BOLD] Dimension of Sentence Representation: 2400",
                    "[BOLD] Dimension of Sentence Representation: 2400",
                    "[BOLD] Dimension of Sentence Representation: 2400"
                ],
                [
                    "RNN",
                    "2x600",
                    "CNN",
                    "600-1200-300",
                    "25",
                    "0.8631",
                    "83.9",
                    "[BOLD] 0.58/ [BOLD] 0.55",
                    "[BOLD] 74.7/ [BOLD] 83.1",
                    "83.4",
                    "[BOLD] 90.2"
                ],
                [
                    "RNN",
                    "2x600",
                    "RNN",
                    "600",
                    "32",
                    "[BOLD] 0.8647",
                    "[BOLD] 84.2",
                    "0.52/0.51",
                    "74.0/81.2",
                    "[BOLD] 84.2",
                    "87.6"
                ],
                [
                    "CNN",
                    "3x800‡",
                    "RNN",
                    "600",
                    "8",
                    "0.8132",
                    "-",
                    "-",
                    "71.9/81.9",
                    "-",
                    "86.6"
                ],
                [
                    "[BOLD] Dimension of Sentence Representation: 4800",
                    "[BOLD] Dimension of Sentence Representation: 4800",
                    "[BOLD] Dimension of Sentence Representation: 4800",
                    "[BOLD] Dimension of Sentence Representation: 4800",
                    "[BOLD] Dimension of Sentence Representation: 4800",
                    "[BOLD] Dimension of Sentence Representation: 4800",
                    "[BOLD] Dimension of Sentence Representation: 4800",
                    "[BOLD] Dimension of Sentence Representation: 4800",
                    "[BOLD] Dimension of Sentence Representation: 4800",
                    "[BOLD] Dimension of Sentence Representation: 4800",
                    "[BOLD] Dimension of Sentence Representation: 4800"
                ],
                [
                    "RNN",
                    "2x1200",
                    "CNN",
                    "600-1200-300",
                    "34",
                    "[BOLD] 0.8698",
                    "[BOLD] 85.2",
                    "[BOLD] 0.59/ [BOLD] 0.57",
                    "[BOLD] 75.1/ [BOLD] 83.2",
                    "[BOLD] 84.1",
                    "[BOLD] 92.2"
                ],
                [
                    "Skip-thought (Kiros et al.,  2015 )",
                    "Skip-thought (Kiros et al.,  2015 )",
                    "Skip-thought (Kiros et al.,  2015 )",
                    "Skip-thought (Kiros et al.,  2015 )",
                    "336",
                    "0.8584",
                    "82.3",
                    "0.29/0.35",
                    "73.0/82.0",
                    "82.0",
                    "[BOLD] 92.2"
                ],
                [
                    "Skip-thought+LN (Ba et al.,  2016 )",
                    "Skip-thought+LN (Ba et al.,  2016 )",
                    "Skip-thought+LN (Ba et al.,  2016 )",
                    "Skip-thought+LN (Ba et al.,  2016 )",
                    "720",
                    "0.8580",
                    "79.5",
                    "0.44/0.45",
                    "-",
                    "82.9",
                    "88.4"
                ]
            ]
        },
        "gold_description": "we present the table 4 in the supplementary material and we summarise it as follows : 1 . decoding the next sentence performed similarly to decoding the subsequent contiguous words . 2 . decoding the subsequent 30 words , which was adopted from the skip - thought training code3 , gave reasonably good performance . more words for decoding didn ' t give us a significant performance gain , and took longer to train . 3 . adding more layers into the decoder and enlarging the dimension of the convolutional layers indeed sightly improved the performance on the three downstream tasks , but as training efficiency is one of our main concerns , it wasn ' t worth sacrificing training efficiency for the minor performance gain . 4 . increasing the dimensionality of the rnn encoder improved the model performance , and the additional training time required was less than needed for increasing the complexity in the cnn decoder . we report results from both smallest and largest models in table 2 . as the transferability of the models trained in both cases perform similarly on the evaluation tasks ( see rows 1 and 2 in table 4 ) , we focus on the simpler predictall - words cnn decoder that learns to reconstruct the next window of contiguous words . as stated in rows 1 , 3 , and 4 in table 4 , decoding short target sequences results in a slightly lower pearson score on sick , and decoding longer target sequences lead to a longer training time . we tweaked the cnn encoder , including different kernel size and activation function , and we report the best results of cnncnn model at row 6 in table 4 . the future predictor in ( gan et al . , 2017 ) also applies a cnn as the encoder , but the decoder is still an rnn , listed at row 11 in table 4 . compared to our designed cnn - cnn model , their cnn - lstm model contains more parameters than our model does , but they have similar performance on the evaluation",
        "generated_description": "our sentence encodings are obtained using bidirectional lstms with a sentence embedding dimension of 3200 and the maximum length of each sentence is 200 . we achieve the best results for the sentence encoder type rnn and the decoder type cnn with a dimension of 600 - 1200 - 300 and a decoding time of 20 hours . we also compare the performance of different combinations of rnn , cnn and lstm with different encoders and decoders . we find that a 2x300 convolutional rnn encoder with 20 hours of training time outperforms the rnn with 2 × 300 and 3 × 300 cnn encoder and decoder layers ."
    },
    {
        "table_id": "29",
        "table_info": {
            "table_caption": "Table 2: BLEU scores for training NMT models with full word and byte pair encoded vocabularies. Full word models limit vocabulary size to 50K. All models are trained with annealing Adam and scores are averaged over 3 optimizer runs.",
            "table_column_names": [
                "[EMPTY]",
                "WMT DE-EN",
                "WMT EN-FI",
                "WMT RO-EN",
                "IWSLT EN-FR",
                "IWSLT CS-EN"
            ],
            "table_content_values": [
                [
                    "Words 50K",
                    "31.6",
                    "12.6",
                    "27.1",
                    "33.6",
                    "21.0"
                ],
                [
                    "BPE 32K",
                    "[BOLD] 33.5",
                    "[BOLD] 14.7",
                    "[BOLD] 27.8",
                    "34.5",
                    "22.6"
                ],
                [
                    "BPE 16K",
                    "33.1",
                    "[BOLD] 14.7",
                    "[BOLD] 27.8",
                    "[BOLD] 34.8",
                    "[BOLD] 23.0"
                ]
            ]
        },
        "gold_description": "as shown in table 2 , sub - word systems outperform full - word systems across the board , despite having fewer total parameters . systems built on larger data generally benefit from larger vocabularies while smaller systems perform better with smaller vocabularies .",
        "generated_description": "for training nmt models we use full word and byte pair encoded vocabularies . even with a vocabulary size of 50k , the byte pair model performs better than the full word model . this shows that the byte - pair model is able to learn richer byte - level representations of the source and target languages which are beneficial for the downstream tasks ."
    },
    {
        "table_id": "30",
        "table_info": {
            "table_caption": "Table 1: Pre-selection results. PPL stands for test set perplexity, all models have ≈5M parameters.",
            "table_column_names": [
                "Model LSTM-Word",
                "PPL 88.0",
                "Model Char-CNN",
                "PPL 92.3"
            ],
            "table_content_values": [
                [
                    "Syl-LSTM",
                    "88.7",
                    "Syl-Avg",
                    "88.5"
                ],
                [
                    "Syl-CNN-2",
                    "86.6",
                    "Syl-Avg-A",
                    "91.4"
                ],
                [
                    "Syl-CNN-3",
                    "[BOLD] 84.6",
                    "Syl-Avg-B",
                    "88.5"
                ],
                [
                    "Syl-CNN-4",
                    "86.8",
                    "Syl-Concat",
                    "[BOLD] 83.7"
                ],
                [
                    "Syl-Sum",
                    "[BOLD] 84.6",
                    "[EMPTY]",
                    "[EMPTY]"
                ]
            ]
        },
        "gold_description": "the results of the pre - selection are reported in table 1 . all syllable - aware models comfortably outperform the char - cnn when the budget is limited to 5m parameters . surprisingly , a pure word - level model , 6 lstm - word , also beats the character - aware one under such budget . the three best configurations are syl - concat , syl - sum , and syl - cnn -",
        "generated_description": "syl - lstm , syl - cnn , and syl - avg perform better than the lstm - word models , however , the performance of the average and average - based models is slightly worse than that of the single character - level models . this may be due to the fact that the average representations are trained on the entire training set rather than just a subset of the positive and negative examples , which may introduce some noise into the resulting representations . on the other hand , the syl - sum model achieves the best performance on the test set , outperforming all the other models by a large margin . syl - combination and sum - a further improves the performance by 0 . 6 % and 0 . 8 % respectively ."
    },
    {
        "table_id": "31",
        "table_info": {
            "table_caption": "Table 3: Evaluation of the syllable-aware models against Char-CNN. In each case the smallest model, Syl-Concat, has 18%–33% less parameters than Char-CNN and is trained 1.2–2.2 times faster (Appendix C).",
            "table_column_names": [
                "Model",
                "EN",
                "FR",
                "ES",
                "DE",
                "CS",
                "RU",
                "[EMPTY]"
            ],
            "table_content_values": [
                [
                    "Char-CNN",
                    "[BOLD] 78.9",
                    "[BOLD] 184",
                    "[BOLD] 165",
                    "[BOLD] 239",
                    "[BOLD] 371",
                    "[BOLD] 261",
                    "DATA-S"
                ],
                [
                    "Syl-CNN",
                    "80.5",
                    "191",
                    "172",
                    "239",
                    "374",
                    "269",
                    "DATA-S"
                ],
                [
                    "Syl-Sum",
                    "80.3",
                    "193",
                    "170",
                    "243",
                    "389",
                    "273",
                    "DATA-S"
                ],
                [
                    "Syl-Concat",
                    "79.4",
                    "188",
                    "168",
                    "244",
                    "383",
                    "265",
                    "DATA-S"
                ],
                [
                    "Char-CNN",
                    "[BOLD] 160",
                    "[BOLD] 124",
                    "[BOLD] 118",
                    "[BOLD] 198",
                    "[BOLD] 392",
                    "[BOLD] 190",
                    "DATA-L"
                ],
                [
                    "Syl-CNN",
                    "–",
                    "–",
                    "–",
                    "–",
                    "–",
                    "–",
                    "DATA-L"
                ],
                [
                    "Syl-Sum",
                    "170",
                    "141",
                    "129",
                    "212",
                    "451",
                    "233",
                    "DATA-L"
                ],
                [
                    "Syl-Concat",
                    "176",
                    "139",
                    "129",
                    "225",
                    "449",
                    "225",
                    "DATA-L"
                ]
            ]
        },
        "gold_description": "the results of evaluating these three models on small ( 1m tokens ) and medium - sized ( 17m – 57m tokens ) data sets against char - cnn for different languages are provided in table 3 . the models demonstrate similar performance on small data , but char - cnn scales significantly better on medium - sized data . from the three syllable - aware models , syl - concat looks the most advantageous as it demonstrates stable results and has the least number of parameters . therefore in what follows we will make a more detailed comparison of sylconcat with char - cnn .",
        "generated_description": "char - cnns outperform syl - concat and syl - sum by a large margin in almost all languages , and the gains are statistically significant ( t - test , p - value < 0 . 01 ) across all language pairs . the character - based models outperform the two word - level models by a significant margin ( p - value < 0 . 05 ) in english , which is a morphologically - rich language with rich agglutinative features . the gains are smaller in languages with less rich morphology ( es , fr , es , de , cs , ru , cs and ru ) . for data partitioning ( data - s and data - l ) , the performance of the three models is very close and even surpasses the results of the combined model ."
    },
    {
        "table_id": "32",
        "table_info": {
            "table_caption": "Table 5: Replacing LSTM with Variational RHN.",
            "table_column_names": [
                "Model",
                "depth",
                "[ITALIC] dLM",
                "Size",
                "PPL"
            ],
            "table_content_values": [
                [
                    "RHN-Char-CNN",
                    "8",
                    "650",
                    "20M",
                    "67.6"
                ],
                [
                    "RHN-Syl-Concat",
                    "8",
                    "439",
                    "13M",
                    "72.0"
                ],
                [
                    "RHN-Syl-Concat",
                    "8",
                    "650",
                    "20M",
                    "69.4"
                ]
            ]
        },
        "gold_description": "to find out whether this was the case we replaced the lstm by a variational rhn ( zilly et al . , 2017 ) , and that resulted in a significant reduction of perplexities on ptb for both char - cnn and syl - concat ( table 5 ) . moreover , increasing dlm from 439 to 650 did result in better performance for syl - concat .",
        "generated_description": "to further improve the rhn model , we replace the lstm layer in rhn - char - cnn with a simple yet effective character - level character n - gram language model ( rhn - syl - concat ) . we observe that the perplexity decreases from 67 . 6 % to 72 . 0 % when the depth is increased from 8 to 439 , which shows the effectiveness of the proposed method . moreover , when we increase the depth to 8 and the dlm size from 650 to 20m , the perplexities further decrease to 69 . 4 % and 70 . 4 % , which shows that the model is able to utilize the character information to improve the language modeling performance ."
    },
    {
        "table_id": "33",
        "table_info": {
            "table_caption": "Table 1: Effect of adding titles to premises.",
            "table_column_names": [
                "Problem",
                "Support Accuracy",
                "Support Kappa",
                "Claim Accuracy",
                "Claim Kappa"
            ],
            "table_content_values": [
                [
                    "ESIM on FEVER One",
                    ".760",
                    ".260",
                    ".517",
                    ".297"
                ],
                [
                    "ESIM on FEVER Title One",
                    ".846",
                    ".394",
                    ".639",
                    ".433"
                ],
                [
                    "Transformer on FEVER Title One",
                    ".958",
                    ".660",
                    ".823",
                    ".622"
                ]
            ]
        },
        "gold_description": "on a support basis , we find a 52 % increase in kappa by adding the titles . cohen ' s kappa is 68 % higher than that for esim .",
        "generated_description": "in this section , we investigate the effect of adding titles to the premise of fever claims by adding the words “ support ” and “ claim ” to the input text . the results show that adding a title improves the ability of the model to produce good results . in particular , the support accuracy of the esim model increases from . 760 to . 846 and the claim accuracy from . 639 to . 622 . the transformer model achieves the best results with . 958 and . 823 in terms of support and claim accuracy respectively ."
    },
    {
        "table_id": "34",
        "table_info": {
            "table_caption": "Table 2: Concatenating evidence or not.",
            "table_column_names": [
                "Problem",
                "Support Accuracy",
                "Support Kappa",
                "Claim Accuracy",
                "Claim Kappa"
            ],
            "table_content_values": [
                [
                    "ESIM on FEVER Title Five Oracle",
                    "[EMPTY]",
                    "[EMPTY]",
                    ".591",
                    ".388"
                ],
                [
                    "ESIM on FEVER Title Five",
                    "[EMPTY]",
                    "[EMPTY]",
                    ".573",
                    ".110"
                ],
                [
                    "ESIM on FEVER Title One",
                    ".846",
                    ".394",
                    ".639",
                    ".433"
                ],
                [
                    "Transformer on FEVER Title Five Oracle",
                    "[EMPTY]",
                    "[EMPTY]",
                    ".673",
                    ".511"
                ],
                [
                    "Transformer on FEVER Title Five",
                    "[EMPTY]",
                    "[EMPTY]",
                    ".801",
                    ".609"
                ],
                [
                    "Transformer on FEVER Title One",
                    ".958",
                    ".660",
                    ".823",
                    ".622"
                ]
            ]
        },
        "gold_description": "makes the fever title five oracle performance better than fever title five . the transformer model is accurate enough that oracle guessing does not help .",
        "generated_description": "in this section , we analyze the effect of not concatenating the evidence with the fever claim or not . in the oracle setting , we concatenate the claim and the supporting evidence to obtain the final fever claims . we can see that there is no significant difference between the two setups . this shows that fever does not need to be concatenated with the claim ."
    },
    {
        "table_id": "35",
        "table_info": {
            "table_caption": "Table 3: Percentage of evidence retrieved from first half of development set. Single-evidence claims only.",
            "table_column_names": [
                "System",
                "Retrieval"
            ],
            "table_content_values": [
                [
                    "FEVER Baseline (TFIDF)",
                    "66.1%"
                ],
                [
                    "+ Titles in TFIDF",
                    "68.3%"
                ],
                [
                    "+ Titles + NE",
                    "80.8%"
                ],
                [
                    "+ Titles + NE + Film",
                    "81.2%"
                ],
                [
                    "Entire Articles + NE + Film",
                    "90.1%"
                ]
            ]
        },
        "gold_description": "the named entity retrieval strategy boosts the evidence retrieval rate to 80 . 8 % , the film retrievals raise evidence retrieval to 81 . 2 % .",
        "generated_description": "single - evidence claims refers to the claims that are only supported by a tfidf summary . we can see that the fever - based system achieves 66 . 1 % in terms of evidence retrieval , and 68 . 3 % if only the titles of the relevant documents are used as the evidence . providing full documents as additional evidence can increase this to 81 . 2 % and 90 . 1 % , which is an absolute improvement of 2 . 5 % ."
    },
    {
        "table_id": "36",
        "table_info": {
            "table_caption": "Table 4: FEVER Score of various systems. All use NE+Film retrieval.",
            "table_column_names": [
                "System",
                "Development",
                "Test"
            ],
            "table_content_values": [
                [
                    "FEVER Title Five Oracle",
                    ".5289",
                    "—"
                ],
                [
                    "FEVER Title Five",
                    ".5553",
                    "—"
                ],
                [
                    "FEVER Title One",
                    ".5617",
                    ".5539"
                ],
                [
                    "FEVER Title One (Narrow Evidence)",
                    ".5550",
                    "—"
                ],
                [
                    "FEVER Title One (Entire Articles)",
                    ".5844",
                    ".5736"
                ]
            ]
        },
        "gold_description": "limiting evidence in this way when only five statements are retrieved ( \" narrow evidence \" in table 4 ) pushes fever score down very little , to . 5550 from . 5617 on the development set , indeed , when the system reviews the extra evidence , fever score goes up to . 5844 on the development set .",
        "generated_description": "fever score refers to the percentage of claims that are deemed essential to the success of the system . an oracle score is used when the fever score on the development set is greater than or equal to 0 . 5 . the system using the ne + film retrieval model achieves . 5736 on development set , . 5539 on test set , and . 5844 on development and test set ."
    },
    {
        "table_id": "37",
        "table_info": {
            "table_caption": "Table 2: Projection accuracy for the isolated example experiment mapping from 2000 → 2001.",
            "table_column_names": [
                "Pairs (size)",
                "@1",
                "@5",
                "@10"
            ],
            "table_content_values": [
                [
                    "All (38)",
                    "44.7",
                    "73.7",
                    "84.2"
                ],
                [
                    "New (7)",
                    "14.3",
                    "28.6",
                    "42.9"
                ]
            ]
        },
        "gold_description": "then , this projection is applied to the second model embeddings of the 47 locations , which are subject to armed conflicts in the year 2001 ( 38 after skipping pairs with outof - vocabulary elements ) . table 2 demonstrates the resulting performance ( reflecting how close the predicted vectors are to the actual armed groups active in this or that location ) . note that out of 38 pairs from 2001 , 31 were already present in the previous data set ( ongoing conflicts ) . this explains why the evaluation on all the pairs gives high results . however , even for the new conflicts , the projection performance is encouraging .",
        "generated_description": "we can see that the majority of the image pairs ( 38 pairs ) are correct ( 84 . 2 % ) and the difference between the accuracy @ 1 and @ 5 and @ 10 is minimal ( 28 . 6 % ) . the least accurate pair is the new image pair ( 7 pairs ) which has an accuracy @ 10 of 42 . 9 % . this is due to the fact that this image pair was not seen during training and is therefore not part of the training set ."
    },
    {
        "table_id": "38",
        "table_info": {
            "table_caption": "Table 3: Average accuracies of predicting next-year insurgents on the basis of locations, using projections trained on the conflicts from all the preceding years (up-to-now) or the preceding year only (previous). Results for 3 baselines are shown along with the proposed incremental dynamic approach.",
            "table_column_names": [
                "[EMPTY]",
                "[BOLD] Only in-vocabulary pairs  [BOLD] up-to-now",
                "[BOLD] Only in-vocabulary pairs  [BOLD] up-to-now",
                "[BOLD] Only in-vocabulary pairs  [BOLD] up-to-now",
                "[BOLD] Only in-vocabulary pairs  [BOLD] previous",
                "[BOLD] Only in-vocabulary pairs  [BOLD] previous",
                "[BOLD] Only in-vocabulary pairs  [BOLD] previous",
                "[BOLD] All pairs, including OOV  [BOLD] up-to-now",
                "[BOLD] All pairs, including OOV  [BOLD] up-to-now",
                "[BOLD] All pairs, including OOV  [BOLD] up-to-now",
                "[BOLD] All pairs, including OOV  [BOLD] previous",
                "[BOLD] All pairs, including OOV  [BOLD] previous",
                "[BOLD] All pairs, including OOV  [BOLD] previous"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "@1",
                    "@5",
                    "@10",
                    "@1",
                    "@5",
                    "@10",
                    "@1",
                    "@5",
                    "@10",
                    "@1",
                    "@5",
                    "@10"
                ],
                [
                    "[BOLD] Separate",
                    "0.0",
                    "0.7",
                    "2.1",
                    "0.5",
                    "1.1",
                    "2.4",
                    "0.0",
                    "0.5",
                    "1.6",
                    "0.4",
                    "0.8",
                    "1.8"
                ],
                [
                    "[BOLD] Cumulative",
                    "1.7",
                    "8.3",
                    "13.8",
                    "2.9",
                    "9.6",
                    "15.2",
                    "1.5",
                    "7.4",
                    "12.2",
                    "2.5",
                    "8.5",
                    "13.4"
                ],
                [
                    "[BOLD] Incr. static",
                    "54.9",
                    "82.8",
                    "90.1",
                    "60.4",
                    "79.6",
                    "84.8",
                    "20.8",
                    "31.5",
                    "34.2",
                    "23.0",
                    "30.3",
                    "32.2"
                ],
                [
                    "[BOLD] Incr. dynamic",
                    "32.5",
                    "64.5",
                    "72.2",
                    "42.6",
                    "64.8",
                    "71.5",
                    "28.1",
                    "56.1",
                    "[BOLD] 62.9",
                    "[BOLD] 37.3",
                    "[BOLD] 56.7",
                    "62.6"
                ]
            ]
        },
        "gold_description": "table 3 presents the results for these experiments , as well as baselines ( averaged across 15 years ) . for the proposed incr . dynamic approach , the performance of the previous projections is comparable to that of the up - to - now projections on the accuracies @ 5 and @ 10 , and is even higher on the accuracy @ 1 ( statistically significant with t - test , p < 0 . 01 ) . thus , the single - year projections are somewhat more ' focused ' , while taking much less time to learn , because of less training pairs . the fact that our models were incrementally updated , not trained from scratch , is crucial . the results of the separate baseline look more like random jitter . the cumulative baseline results are slightly better , probably simply because they are trained on more data . however , they still perform much worse than the models trained using incremental updates . this is because the former models are not connected to each other , and thus are initialized with a different layout of words in the vector space . this gives rise to formally different directions of semantic relations in each yearly model ( the relations themselves are still there , but they are rotated and scaled differently ) . the results for the incr . static baseline , when tested only on the words present in the test model vocabulary ( the left part of the table ) , seem better than those of the proposed incr . dynamic approach . this stems from the fact that incremental updating with static vocabulary means that we never add new words to the models ; thus , they contain only the vocabulary learned from the 1994 texts . the result is that at test time we skip many more pairs than with the other approaches ( about 62 % in average ) . subsequently , the projections are tested only on a minor part of the test sets . of course , skipping large parts of the data would be a major drawback for any realistic application , so the incr . static baseline is not really plausible .",
        "generated_description": "as can be seen from the table , the results show that the best results are obtained using only in - vocabulary pairs ( i . e . , previous or current up - to - now ) and using all pairs , whereas the results obtained using separate embeddings ( separate ) do not achieve good results . separate performs worse than any of the other two approaches , which indicates that it is harder to learn a good embedding space for oovs . however , the result obtained using bilingual embedding is better than using the multilingual embedding ."
    },
    {
        "table_id": "39",
        "table_info": {
            "table_caption": "Table 4: Evaluation of relative time-lines for each model and loss function, where L∗ indicates the (unweighted) sum of Lτ, Lτce, and Lτh.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] TE3‡  [BOLD] P",
                "[BOLD] TE3‡  [BOLD] R",
                "[BOLD] TE3‡  [BOLD] F",
                "[BOLD] TD‡  [BOLD] P",
                "[BOLD] TD‡  [BOLD] R",
                "[BOLD] TD‡  [BOLD] F"
            ],
            "table_content_values": [
                [
                    "[ITALIC] Indirect:  [ITALIC] O( [ITALIC] n2)",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "TL2RTL ( [ITALIC] Lτ)",
                    "53.5",
                    "51.1",
                    "52.3",
                    "59.1",
                    "61.2",
                    "60.1"
                ],
                [
                    "TL2RTL ( [ITALIC] Lτce)",
                    "53.9",
                    "51.7",
                    "[BOLD] 52.8",
                    "61.2",
                    "60.7",
                    "60.9"
                ],
                [
                    "TL2RTL ( [ITALIC] Lτh)",
                    "52.8",
                    "51.1",
                    "51.9",
                    "57.9",
                    "60.6",
                    "59.2"
                ],
                [
                    "TL2RTL ( [ITALIC] L∗)",
                    "52.6",
                    "52.0",
                    "52.3",
                    "62.3",
                    "62.3",
                    "[BOLD] 62.3"
                ],
                [
                    "[ITALIC] Direct:  [ITALIC] O( [ITALIC] n)",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "S-TLM ( [ITALIC] Lτ)",
                    "50.1",
                    "50.4",
                    "50.2",
                    "57.8",
                    "59.5",
                    "[BOLD] 58.6"
                ],
                [
                    "S-TLM ( [ITALIC] Lτce)",
                    "50.1",
                    "50.0",
                    "50.1",
                    "53.4",
                    "53.5",
                    "53.5"
                ],
                [
                    "S-TLM ( [ITALIC] Lτh)",
                    "51.5",
                    "51.7",
                    "[BOLD] 51.6",
                    "55.1",
                    "56.4",
                    "55.7"
                ],
                [
                    "S-TLM ( [ITALIC] L∗)",
                    "50.9",
                    "51.0",
                    "51.0",
                    "56.5",
                    "55.3",
                    "55.9"
                ],
                [
                    "C-TLM ( [ITALIC] Lτ)",
                    "56.2",
                    "56.1",
                    "[BOLD] 56.1",
                    "57.1",
                    "59.7",
                    "[BOLD] 58.4"
                ],
                [
                    "C-TLM ( [ITALIC] Lτce)",
                    "54.4",
                    "55.4",
                    "54.9",
                    "52.4",
                    "57.3",
                    "54.7"
                ],
                [
                    "C-TLM ( [ITALIC] Lτh)",
                    "55.7",
                    "55.5",
                    "55.6",
                    "55.3",
                    "54.9",
                    "55.1"
                ],
                [
                    "C-TLM ( [ITALIC] L∗)",
                    "54.0",
                    "54.3",
                    "54.1",
                    "54.6",
                    "53.5",
                    "54.1"
                ]
            ]
        },
        "gold_description": "we compared our three proposed models for the three loss functions lτ , lτ ce , and lτ h , and their linear ( unweighted ) combination l ∗ , on te3 ‡ and td ‡ , for which the results are shown in table 4 . a trend that can be observed is that overall performance on td ‡ is higher than that of te3 ‡ if we compare loss functions lτ , lτ ce , and lτh , and combination l ∗ , it can be noticed that , although all loss functions seem to give fairly similar performance , lτ gives the most robust results ( never lowest ) , especially noticeable for the smaller dataset td ‡ . the combination of losses l ∗ shows mixed results , and has lower performance for s - tlm and c - tlm , but better performance for tl2rtl . moreover , we can clearly see that on te3 ‡ , ctlm performs better than the indirect models , across all loss functions . on td ‡ , the indirect models seem to perform slightly better . the difference between c - tlm and s - tlm is small on the smaller td ‡ on te3 ‡ , the larger dataset , c - tlm clearly outperforms s - tlm across all loss functions ,",
        "generated_description": "for tl2rtl ( lτ ) , we use the direct path prediction objective defined by \\ newcitelevy - etal - 2018 - indirect : o ( n2 ) = { o ( n1 , n2 ) = 1 , … , n + 1 , n , 1 } for our experiments . note that the best performance is achieved using direct path matching with the cosine similarity measure , which is the one used in the original paper . we also observe that the addition of the temporal latent variables , lτce and lτh , do not lead to significant improvements in terms of f - score . the reason may be that the local latent variables do not contribute to the prediction of the relation between two entities ."
    },
    {
        "table_id": "40",
        "table_info": {
            "table_caption": "Table 1: Jensen-Shannon divergence between word distribution in all Onion drug sites, Legal and Illegal Onion drug sites, and eBay sites. Each domain was also split in half for within-domain comparison.",
            "table_column_names": [
                "[EMPTY]",
                "[EMPTY]",
                "All Onion all",
                "All Onion half 1",
                "All Onion half 2",
                "eBay all",
                "eBay half 1",
                "eBay half 2",
                "Illegal Onion all",
                "Illegal Onion half 1",
                "Illegal Onion half 2",
                "Legal Onion all",
                "Legal Onion half 1",
                "Legal Onion half 2"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "all",
                    "[EMPTY]",
                    "0.23",
                    "0.25",
                    "0.60",
                    "0.61",
                    "0.61",
                    "0.33",
                    "0.39",
                    "0.41",
                    "0.35",
                    "0.41",
                    "0.42"
                ],
                [
                    "All Onion",
                    "half 1",
                    "0.23",
                    "[EMPTY]",
                    "0.43",
                    "0.60",
                    "0.62",
                    "0.62",
                    "0.37",
                    "0.33",
                    "0.50",
                    "0.40",
                    "0.36",
                    "0.52"
                ],
                [
                    "[EMPTY]",
                    "half 2",
                    "0.25",
                    "0.43",
                    "[EMPTY]",
                    "0.61",
                    "0.62",
                    "0.62",
                    "0.39",
                    "0.50",
                    "0.35",
                    "0.39",
                    "0.51",
                    "0.35"
                ],
                [
                    "[EMPTY]",
                    "all",
                    "0.60",
                    "0.60",
                    "0.61",
                    "[EMPTY]",
                    "0.23",
                    "0.25",
                    "0.59",
                    "0.60",
                    "0.60",
                    "0.66",
                    "0.67",
                    "0.67"
                ],
                [
                    "eBay",
                    "half 1",
                    "0.61",
                    "0.62",
                    "0.62",
                    "0.23",
                    "[EMPTY]",
                    "0.43",
                    "0.60",
                    "0.61",
                    "0.61",
                    "0.67",
                    "0.67",
                    "0.68"
                ],
                [
                    "[EMPTY]",
                    "half 2",
                    "0.61",
                    "0.62",
                    "0.62",
                    "0.25",
                    "0.43",
                    "[EMPTY]",
                    "0.60",
                    "0.61",
                    "0.61",
                    "0.67",
                    "0.68",
                    "0.68"
                ],
                [
                    "[EMPTY]",
                    "all",
                    "0.33",
                    "0.37",
                    "0.39",
                    "0.59",
                    "0.60",
                    "0.60",
                    "[EMPTY]",
                    "0.23",
                    "0.27",
                    "0.61",
                    "0.62",
                    "0.62"
                ],
                [
                    "Illegal Onion",
                    "half 1",
                    "0.39",
                    "0.33",
                    "0.50",
                    "0.60",
                    "0.61",
                    "0.61",
                    "0.23",
                    "[EMPTY]",
                    "0.45",
                    "0.62",
                    "0.63",
                    "0.62"
                ],
                [
                    "[EMPTY]",
                    "half 2",
                    "0.41",
                    "0.50",
                    "0.35",
                    "0.60",
                    "0.61",
                    "0.61",
                    "0.27",
                    "0.45",
                    "[EMPTY]",
                    "0.62",
                    "0.63",
                    "0.63"
                ],
                [
                    "[EMPTY]",
                    "all",
                    "0.35",
                    "0.40",
                    "0.39",
                    "0.66",
                    "0.67",
                    "0.67",
                    "0.61",
                    "0.62",
                    "0.62",
                    "[EMPTY]",
                    "0.26",
                    "0.26"
                ],
                [
                    "Legal onion",
                    "half 1",
                    "0.41",
                    "0.36",
                    "0.51",
                    "0.67",
                    "0.67",
                    "0.68",
                    "0.62",
                    "0.63",
                    "0.63",
                    "0.26",
                    "[EMPTY]",
                    "0.47"
                ],
                [
                    "[EMPTY]",
                    "half 2",
                    "0.42",
                    "0.52",
                    "0.35",
                    "0.67",
                    "0.68",
                    "0.68",
                    "0.62",
                    "0.62",
                    "0.63",
                    "0.26",
                    "0.47",
                    "[EMPTY]"
                ]
            ]
        },
        "gold_description": "table 1 presents our results . the self - distance f the ebay , legal onion and illegal onion corpora lies between 0 . 40 to 0 . 45 by the jensenshannon divergence , but the distance between each pair is 0 . 60 to 0 . 65 , with the three approximately forming an equilateral triangle in the space of word distributions . similar results are obtained using variational distance , and are omitted for brevity .",
        "generated_description": "as can be seen in the table , in general the performance of our approach is comparable to the one of \\ newciteallony and \\ newciteneelakantan - etal - 2019 - towards when all the features are used ( i . e . , 0 . 23 and 0 . 25 accuracy for all and half measures ) , but it is significantly lower when half measures are used . this shows that the effect of using half measures is negligible in comparison to using all features . however , in the case of the legal onion dataset , the accuracy is higher than that of the other two datasets , half measures being half measures and all measures being full measures . one possible reason for this is that the average length of the sentences in the illegal onion dataset is much longer than the one in the all - measures dataset ( 6 . 5 sentences vs . 4 sentences ) . therefore , the extraction of half measures from a long sentence is more difficult ."
    },
    {
        "table_id": "41",
        "table_info": {
            "table_caption": "Table 2: Average percentage of wikifiable named entities in a website per domain, with standard error.",
            "table_column_names": [
                "[EMPTY]",
                "% Wikifiable"
            ],
            "table_content_values": [
                [
                    "eBay",
                    "38.6±2.00"
                ],
                [
                    "Illegal Onion",
                    "32.5±1.35"
                ],
                [
                    "Legal Onion",
                    "50.8±2.31"
                ]
            ]
        },
        "gold_description": "each average . according to our results ( table 2 ) , the wikification success ratios of ebay and illegal onion named entities is comparable and relatively low . however , sites selling legal drugs on onion have a much higher wikification percentage .",
        "generated_description": "we define a wikifiable named entity as a website which has at least one associated wikipedia link . we define this as the percentage of entities in the entire website that are known to be associated with a wikipedia page . for example , for the illegal onion website , 32 . 5 % of its website ’ s users are also associated with wikipedia pages . for legal onion , we define this percentage to be 50 . 8 % . we observe a similar trend for the other website , with 38 . 6 % of all items in the legal onion domain being covered by wikifiable entities ."
    },
    {
        "table_id": "42",
        "table_info": {
            "table_caption": "Table 1: F1 results for knowledge-based systems on the Raganato et al. (2017a) dataset. Top rows show conflicting results for UKB. † for results reported in Raganato et al. (2017a), ‡ for results reported in Chaplot and Sakajhutdinov (2018). Best results in bold. S2 stands for Senseval-2, S3 for Senseval-3, S07 for Semeval-2007, S13 for Semeval-2013 and S15 for Semeval-2015.",
            "table_column_names": [
                "[EMPTY]",
                "All",
                "S2",
                "S3",
                "S07",
                "S13",
                "S15"
            ],
            "table_content_values": [
                [
                    "UKB (this work)",
                    "[BOLD] 67.3",
                    "68.8",
                    "66.1",
                    "53.0",
                    "[BOLD] 68.8",
                    "[BOLD] 70.3"
                ],
                [
                    "UKB (elsewhere)†‡",
                    "57.5",
                    "60.6",
                    "54.1",
                    "42.0",
                    "59.0",
                    "61.2"
                ],
                [
                    "Chaplot and Sakajhutdinov ( 2018 ) ‡",
                    "66.9",
                    "[BOLD] 69.0",
                    "[BOLD] 66.9",
                    "55.6",
                    "65.3",
                    "69.6"
                ],
                [
                    "Babelfy Moro et al. ( 2014 )†",
                    "65.5",
                    "67.0",
                    "63.5",
                    "51.6",
                    "66.4",
                    "70.3"
                ],
                [
                    "MFS",
                    "65.2",
                    "66.8",
                    "66.2",
                    "55.2",
                    "63.0",
                    "67.8"
                ],
                [
                    "Basile et al. ( 2014 )†",
                    "63.7",
                    "63.0",
                    "63.7",
                    "[BOLD] 56.7",
                    "66.2",
                    "64.6"
                ],
                [
                    "Banerjee and Pedersen ( 2003 )†",
                    "48.7",
                    "50.6",
                    "44.5",
                    "32.0",
                    "53.6",
                    "51.0"
                ]
            ]
        },
        "gold_description": "the two top rows in table 1 show conflicting results for ukb . as the results show , that paper reports a suboptimal use of ukb . the table also reports the best performing knowledge - based systems on this dataset . also report we ( chaplot and sakajhutdinov , 2018 ) , the latest work on this area , as well as the most frequent sense as given by wordnet counts the table shows that ukb yields the best overall result .",
        "generated_description": "s3 , s07 , s13 , s15 , and s15 . in particular , our model outperforms the previous state - of - the - art models across all test sets . across the board , the performance of our model is competitive with the best previously published results ."
    },
    {
        "table_id": "43",
        "table_info": {
            "table_caption": "Table 2: F1 results for supervised systems on the Raganato et al. (2017a) dataset. † for results reported in Raganato et al. (2017a). Best results in bold. Note that Raganato et al. (2017b) used S07 for development.",
            "table_column_names": [
                "[EMPTY]",
                "All",
                "S2",
                "S3",
                "S07",
                "S13",
                "S15"
            ],
            "table_content_values": [
                [
                    "Yuan et al. ( 2016 )",
                    "[BOLD] 71.5",
                    "[BOLD] 73.8",
                    "[BOLD] 71.8",
                    "63.5",
                    "[BOLD] 69.5",
                    "[BOLD] 72.6"
                ],
                [
                    "Raganato et al. ( 2017b )",
                    "69.9",
                    "72.0",
                    "69.1",
                    "[BOLD] 64.8",
                    "66.9",
                    "71.5"
                ],
                [
                    "Iacobacci et al. ( 2016 )†",
                    "69.7",
                    "73.3",
                    "69.6",
                    "61.1",
                    "66.7",
                    "70.4"
                ],
                [
                    "Melamud et al. ( 2016 )†",
                    "69.4",
                    "72.3",
                    "68.2",
                    "61.5",
                    "67.2",
                    "71.7"
                ],
                [
                    "IMS Zhong and Ng ( 2010 )†",
                    "68.8",
                    "72.8",
                    "69.2",
                    "60.0",
                    "65.0",
                    "69.3"
                ]
            ]
        },
        "gold_description": "table 2 reports the results of supervised systems on the same dataset , taken from the two works that use the dataset as expected , supervised systems outperform knowledge - based systems , by a small margin in some of the cases .",
        "generated_description": "s2 , s3 , s07 , s13 , and s15 . our best performing model outperforms the previous state - of - the - art results reported in raganato et al . ( 2017b ) by 1 . 8 f1 points , 1 . 7 f1 point , and 1 . 4 f1 score , respectively . note that we use the same hyperparameters for all of the systems as those reported in the original paper . our model performs best on s2 and s3 ."
    },
    {
        "table_id": "44",
        "table_info": {
            "table_caption": "Table 3: Additional results on other settings of UKB. nf subscript stands for “no sense frequency”. Top rows use a single sentence as context, while the bottom rows correspond to extended context (cf. Sect. 3). Best results in bold.",
            "table_column_names": [
                "[EMPTY]",
                "All",
                "S2",
                "S3",
                "S07",
                "S13",
                "S15"
            ],
            "table_content_values": [
                [
                    "Single context sentence",
                    "Single context sentence",
                    "Single context sentence",
                    "Single context sentence",
                    "Single context sentence",
                    "Single context sentence",
                    "Single context sentence"
                ],
                [
                    "ppr_w2w",
                    "66.9",
                    "[BOLD] 69.0",
                    "65.7",
                    "53.9",
                    "67.1",
                    "69.9"
                ],
                [
                    "dfs_ppr",
                    "65.2",
                    "67.5",
                    "65.6",
                    "53.6",
                    "62.7",
                    "68.2"
                ],
                [
                    "ppr",
                    "65.5",
                    "67.5",
                    "[BOLD] 66.5",
                    "[BOLD] 54.7",
                    "63.3",
                    "67.4"
                ],
                [
                    "ppr_w2wnf",
                    "60.2",
                    "63.7",
                    "55.1",
                    "42.2",
                    "63.5",
                    "63.8"
                ],
                [
                    "pprnf",
                    "57.1",
                    "60.5",
                    "53.8",
                    "41.3",
                    "58.0",
                    "61.4"
                ],
                [
                    "dfsnf",
                    "58.7",
                    "63.3",
                    "52.8",
                    "40.4",
                    "61.6",
                    "62.5"
                ],
                [
                    "One or more context sentences (# [ITALIC] words≥20)",
                    "One or more context sentences (# [ITALIC] words≥20)",
                    "One or more context sentences (# [ITALIC] words≥20)",
                    "One or more context sentences (# [ITALIC] words≥20)",
                    "One or more context sentences (# [ITALIC] words≥20)",
                    "One or more context sentences (# [ITALIC] words≥20)",
                    "One or more context sentences (# [ITALIC] words≥20)"
                ],
                [
                    "ppr_w2w",
                    "[BOLD] 67.3",
                    "68.8",
                    "66.1",
                    "53.0",
                    "[BOLD] 68.8",
                    "[BOLD] 70.3"
                ],
                [
                    "ppr",
                    "65.6",
                    "67.5",
                    "66.4",
                    "54.1",
                    "64.0",
                    "67.8"
                ],
                [
                    "dfs",
                    "65.7",
                    "67.9",
                    "65.9",
                    "54.5",
                    "64.2",
                    "68.1"
                ],
                [
                    "ppr_w2wnf",
                    "60.4",
                    "64.2",
                    "54.8",
                    "40.0",
                    "64.5",
                    "64.5"
                ],
                [
                    "pprnf",
                    "58.6",
                    "61.3",
                    "54.9",
                    "42.2",
                    "60.9",
                    "62.9"
                ],
                [
                    "dfsnf",
                    "59.1",
                    "62.7",
                    "54.4",
                    "39.3",
                    "62.8",
                    "62.2"
                ]
            ]
        },
        "gold_description": "ta the table shows that the key factor is the use of sense frequencies , and systems that do not use them ( those with a nf subscript ) suffer a loss between 7 and 8 percentage points in f1 . the table also shows that extending the context is mildly effective . regarding the algorithm , the table confirms that the best method is ppr w2w , followed by the subgraph approach ( dfs ) and ppr .",
        "generated_description": "as can be seen , the context - aware models outperform the single - context models by a large margin in terms of bleu score . the best overall performance is achieved by the pruning model ( ppr , w2w ) , although the performance of the other models is not as good as that of the ppr + w2w model . furthermore , we can see that the best context - only model is still dfs_ppr , which shows that context information is helpful even if the model only has a single context sentence as context ."
    },
    {
        "table_id": "45",
        "table_info": {
            "table_caption": "Table 2: Accuracy (in %) of our model and other text classification models on both versions of SWBD.",
            "table_column_names": [
                "[BOLD] Models",
                "[BOLD] SWBD",
                "[BOLD] SWBD2"
            ],
            "table_content_values": [
                [
                    "BoW + Logsitic",
                    "78.95",
                    "87.76"
                ],
                [
                    "BoW + SVM",
                    "73.68",
                    "[BOLD] 90.82"
                ],
                [
                    "Bigram + SVM",
                    "52.63",
                    "79.59"
                ],
                [
                    "BoW + TF-IDF + Logistic",
                    "52.63",
                    "81.63"
                ],
                [
                    "nGram + Logistic",
                    "52.63",
                    "78.57"
                ],
                [
                    "nGram + TF-IDF + Logistic",
                    "57.89",
                    "87.76"
                ],
                [
                    "Bag of Means + Logistic",
                    "78.95",
                    "87.76"
                ],
                [
                    "Avg. Skipgram + Logistic",
                    "26.32",
                    "59.18"
                ],
                [
                    "Doc2Vec + SVM",
                    "73.68",
                    "86.73"
                ],
                [
                    "HN",
                    "31.58",
                    "54.08"
                ],
                [
                    "HN-ATT Yang et al. ( 2016 )",
                    "73.68",
                    "85.71"
                ],
                [
                    "CNN Kim ( 2014 )",
                    "84.21",
                    "93.87"
                ],
                [
                    "HN-SA (our model)",
                    "[BOLD] 89.47",
                    "[BOLD] 95.92"
                ]
            ]
        },
        "gold_description": "we compare the performance of our model ( table 2 ) with traditional bag of words ( bow ) , tf - idf , and n - grams features based classifiers . we also compare against averaged skip - gram ( mikolov et al . , 2013 ) , doc2vec ( le and mikolov , 2014 ) , cnn ( kim , 2014 ) , hierarchical attention ( hn - att ) ( yang et al . , 2016 ) and hierarchical network ( hn ) models .",
        "generated_description": "we compare our proposed hn - sa model with the following state - of - the - art models : ( 1 ) cnn kim et al . our proposed model achieves 89 . 47 % and 95 . 92 % accuracy on swbd and swbd2 datasets , respectively , which are the new state of the art results on both datasets . note that the previous best accuracy ( cnn + svm ) is reported on the swbd dataset in 2014 . we can see that our proposed model outperforms cnn by a large margin , e . g . , it achieves 87 . 76 % and 93 . 87 % accuracy when using bow features and svm classifiers , respectively ."
    },
    {
        "table_id": "46",
        "table_info": {
            "table_caption": "Table 1: Exact matching accuracy on SQL queries.",
            "table_column_names": [
                "[BOLD] Approach",
                "[BOLD] Dev",
                "[BOLD] Test"
            ],
            "table_content_values": [
                [
                    "Seq2Seq",
                    "1.9%",
                    "3.7%"
                ],
                [
                    "Seq2Seq + Attention",
                    "1.8%",
                    "4.8%"
                ],
                [
                    "Seq2Seq + Copying",
                    "4.1%",
                    "5.3%"
                ],
                [
                    "TypeSQL",
                    "8.0%",
                    "8.2%"
                ],
                [
                    "SQLNet",
                    "10.9%",
                    "12.4%"
                ],
                [
                    "SyntaxSQLNet",
                    "18.9%",
                    "19.7%"
                ],
                [
                    "SyntaxSQLNet(augment)",
                    "24.8%",
                    "27.2%"
                ],
                [
                    "[BOLD] IRNet",
                    "[BOLD] 53.2%",
                    "[BOLD] 46.7%"
                ],
                [
                    "[BOLD] BERT",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "SyntaxSQLNet(BERT)",
                    "25.0%",
                    "25.4%"
                ],
                [
                    "[BOLD] IRNet(BERT)",
                    "[BOLD] 61.9%",
                    "[BOLD] 54.7%"
                ]
            ]
        },
        "gold_description": "table 1 presents the exact matching accuracy of irnet and various baselines on the development set and the test set . irnet clearly outperforms all the baselines by a substantial margin . it obtains 27 . 0 % absolute improvement over syntaxsqlit also obtains 19 . 5 % absolute net on test set . improvement over syntaxsqlnet ( augment ) that performs large - scale data augmentation . when incorporating bert , the performance of both syntaxsqlnet and irnet is substantially improved and the accuracy gap between them on both the development set and the test set is widened .",
        "generated_description": "comparing with existing approaches , we can see that there is a significant gap between the original seq2seq model and existing approaches . for example , compared with our proposed irnet , the exact matching accuracy drops from 53 . 2 % to 46 . 7 % and from 61 . 9 % to 54 . 7 % , respectively . this shows that the proposed framework is very effective in dealing with complex sql queries . we can also see that bert has a large impact on the performance as compared with the other two pre - trained language models . this is because bert is trained on a large general language model like scibert , and the performance of this model on sql queries is directly related to the accuracy of the corresponding sql query ."
    },
    {
        "table_id": "47",
        "table_info": {
            "table_caption": "Table 2: Exact matching accuracy of SyntaxSQLNet, SyntaxSQLNet(BERT), IRNet and IRNet(BERT) on test set by hardness level.",
            "table_column_names": [
                "[BOLD] Approach",
                "[BOLD] Easy",
                "[BOLD] Medium",
                "[BOLD] Hard",
                "[BOLD] Extra"
            ],
            "table_content_values": [
                [
                    "[BOLD] Approach",
                    "[BOLD] Easy",
                    "[BOLD] Medium",
                    "[BOLD] Hard",
                    "[BOLD] Hard"
                ],
                [
                    "SyntaxSQLNet",
                    "38.6%",
                    "17.6%",
                    "16.3%",
                    "4.9%"
                ],
                [
                    "SyntaxSQLNet",
                    "42.9%",
                    "24.9%",
                    "21.9%",
                    "8.6%"
                ],
                [
                    "(BERT)",
                    "42.9%",
                    "24.9%",
                    "21.9%",
                    "8.6%"
                ],
                [
                    "[BOLD] IRNet",
                    "[BOLD] 70.1%",
                    "[BOLD] 49.2%",
                    "[BOLD] 39.5%",
                    "[BOLD] 19.1%"
                ],
                [
                    "[BOLD] IRNet(BERT)",
                    "[BOLD] 77.2%",
                    "[BOLD] 58.7%",
                    "[BOLD] 48.1%",
                    "[BOLD] 25.3%"
                ]
            ]
        },
        "gold_description": "as shown in table 2 , irnet significantly outperforms syntaxsqlnet in all four hardness levels with or without bert . for example , compared with syntaxsqlnet , irnet obtains 23 . 3 % absolute improvement in hard level .",
        "generated_description": "from this table , we can see that the accuracy of different models on the hard level is higher than that of the easy level ( e . g . , 70 . 1 % for syntaxsqlnet , 58 . 7 % for irnet and 49 . 2 % for bert ) , while the accuracy on the medium and hard levels is lower ( 16 . 3 % and 19 . 1 % , respectively ) . this indicates that the problem of out - of - distribution sql models is more severe when the training set is hard . as a result , the exact matching accuracy of all the models is lower than 50 % . however , irnet ( bert ) is still the best one among all the compared models ."
    },
    {
        "table_id": "48",
        "table_info": {
            "table_caption": "Table 3: Exact matching accuracy on development set. The header ‘SQL’ means that the approaches are learned to generate SQL, while the header ‘SemQL’ indicates that they are learned to generate SemQL queries.",
            "table_column_names": [
                "[BOLD] Approach",
                "[BOLD] SQL",
                "[BOLD] SemQL"
            ],
            "table_content_values": [
                [
                    "Seq2Seq",
                    "1.9%",
                    "11.4%( [BOLD] +9.5)"
                ],
                [
                    "Seq2Seq + Attention",
                    "1.8%",
                    "14.7%( [BOLD] +12.9)"
                ],
                [
                    "Seq2Seq + Copying",
                    "4.1%",
                    "18.5%( [BOLD] +14.1)"
                ],
                [
                    "TypeSQL",
                    "8.0%",
                    "14.4%( [BOLD] +6.4)"
                ],
                [
                    "SQLNet",
                    "10.9%",
                    "17.5%( [BOLD] +6.6)"
                ],
                [
                    "SyntaxSQLNet",
                    "18.9%",
                    "27.5%( [BOLD] +8.6)"
                ],
                [
                    "[BOLD] BERT",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "SyntaxSQLNet(BERT)",
                    "25.0%",
                    "35.8%( [BOLD] +10.8)"
                ]
            ]
        },
        "gold_description": "as shown in table 3 , there are at least 6 . 6 % and up to 14 . 4 % absolute improvements on accuracy of exact matching on the development set . for example , when syntaxsqlnet is learned to generate semql queries instead of sql queries , it registers 8 . 6 % absolute improvement and even outperforms syntaxsqlnet ( augment ) which performs largescale data augmentation . table 4 presents the ablation study results . it is clear that our base model significantly outperforms syntaxsqlnet , syntaxsqlnet ( augment ) and syntaxsqlnet ( bert ) . performing schema linking ( ' + sl ' ) brings about 8 . 5 % and 6 . 4 % absolute improvement on irnet and irnet ( bert ) . the f1 score on the where clause increases by 12 . 5 % when irnet performs schema linking . the number of examples suffering from this problem decreases by 70 % , when using the memory augmented pointer network .",
        "generated_description": "from the table , we can see that all the three approaches are able to match the accuracy of sql queries generated by the semql pipeline better than that of semql queries , which indicates that syntactic knowledge is helpful to sql generation . the best matching accuracy is achieved by the proposed syntaxsqlnet model with 25 . 0 % and 35 . 8 % exact matching accuracy on sql and semql , respectively . note that the results are better than those of typesql and sqlnet , but not as good as those of seq2seq + copying and typesql . the reason is that those two approaches are designed for one - shot sql generation , while our approach is designed for two - stage sql generation in two stages , which makes them more suitable for multi - task learning ."
    },
    {
        "table_id": "49",
        "table_info": {
            "table_caption": "Table 3: Classifiers’ accuracy on the Supports and Refutes cases from the FEVER Dev set and on the Generated pairs for the Symmetric Test Set in the setting of without (base) and with (r.w) re-weight.",
            "table_column_names": [
                "Model",
                "FEVER Dev base",
                "FEVER Dev r.w",
                "Generated base",
                "Generated r.w"
            ],
            "table_content_values": [
                [
                    "NSMN",
                    "81.8",
                    "-",
                    "58.7",
                    "-"
                ],
                [
                    "ESIM",
                    "80.8",
                    "76.0",
                    "55.9",
                    "59.3"
                ],
                [
                    "BERT",
                    "[BOLD] 86.2",
                    "84.6",
                    "58.3",
                    "[BOLD] 61.6"
                ]
            ]
        },
        "gold_description": "table 3 summarizes the performance of the three models on the supports and refutes pairs from the fever dev set and on the created symmetric test set pairs . all models perform relatively well on fever dev but achieve less than 60 % accuracy on the synthetic ones . the re - weighting method increases the accuracy of the esim and bert models by an absolute 3 . 4 % and 3 . 3 % respectively . one can notice that this improvement comes at a cost in the accuracy over the fever dev pairs . applying the regularization method , using the same training data , helps to train a more robust model that performs better on our test set , where verification in context is a key requirement .",
        "generated_description": "the bert classifier achieves the highest accuracy on the supports and refutes cases , outperforming the other classifiers . the nsmn classifier has the lowest accuracy , which we attribute to its inability to distinguish between the strongly agree and strongly disagree cases . esim and bert have similar accuracies on the generated pairs , with bert slightly better than the other two ."
    },
    {
        "table_id": "50",
        "table_info": {
            "table_caption": "Table 7: Top 10 LMI-ranked bigrams in the train set of FEVER for Support.",
            "table_column_names": [
                "[BOLD] Bigram",
                "[BOLD] Train  [BOLD] LMI ⋅10−6",
                "[BOLD] Train  [ITALIC] p( [ITALIC] l| [ITALIC] w)",
                "[BOLD] Development  [BOLD] LMI ⋅10−6",
                "[BOLD] Development  [ITALIC] p( [ITALIC] l| [ITALIC] w)"
            ],
            "table_content_values": [
                [
                    "united states",
                    "271",
                    "0.64",
                    "268",
                    "0.44"
                ],
                [
                    "least one",
                    "269",
                    "0.90",
                    "267",
                    "0.77"
                ],
                [
                    "at least",
                    "256",
                    "0.72",
                    "163",
                    "0.48"
                ],
                [
                    "person who",
                    "162",
                    "0.90",
                    "135",
                    "0.61"
                ],
                [
                    "stars actor",
                    "143",
                    "0.86",
                    "111",
                    "0.71"
                ],
                [
                    "won award",
                    "133",
                    "0.80",
                    "50",
                    "0.56"
                ],
                [
                    "american actor",
                    "126",
                    "0.79",
                    "55",
                    "0.45"
                ],
                [
                    "starred movie",
                    "100",
                    "0.88",
                    "34",
                    "0.80"
                ],
                [
                    "from united",
                    "100",
                    "0.82",
                    "108",
                    "0.67"
                ],
                [
                    "from america",
                    "96",
                    "0.89",
                    "108",
                    "0.74"
                ]
            ]
        },
        "gold_description": "table 7 and table 8 summarize the top 10 bigrams for support and not enough info . the correlation between the biased phrases in the two dataset splits is not as strong as in the refute label , presented in the paper . however , one can notice that some of the biased bigrams in the training set , such as \" least one \" and \" starred movie \" , translate to cues that can help in predictions over the development set .",
        "generated_description": "for each bigram , we show the mean and standard deviation of its embedding vectors lmi ⋅ 10 − 6 with respect to the number of times it appears in the training set and the corresponding development set , and also the corresponding p ( l | w ) of the embedding of the bigram for each word w corresponding to that bigram . we see that words which appear in the development set tend to have higher values of lmi than those in the train set . for example , the words “ won award ” and “ american actor ” , which are used in the united states and america biographies respectively , have values lmi of 0 . 80 and 0 . 45 respectively . similarly , the term “ people who ” is also associated with a high lmi value , but its score is only 0 . 61 ."
    },
    {
        "table_id": "51",
        "table_info": {
            "table_caption": "Table 3: NewsQA to SQuAD. Exact match (EM) and span F1 results on SQuAD development set of a NewsQA BIDAF model baseline vs. one finetuned on SQuAD using the data generated by a 2-stage SynNet (Snet).",
            "table_column_names": [
                "System",
                "EM",
                "F1"
            ],
            "table_content_values": [
                [
                    "[ITALIC] Mnewsqa",
                    "46.3",
                    "60.8"
                ],
                [
                    "[ITALIC] Mnewsqa +  [ITALIC] Snet",
                    "47.9",
                    "61.5"
                ]
            ]
        },
        "gold_description": "report brief results on squad ( table 3 ) , we also evaluate the synnet on the newsqato - squad direction . we directly apply the best setting from the other direction and report the result in table 3 . the synnet improves over the baseline by 1 . 6 % in em and 0 . 7 % in f1 . limited by space , we leave out ablation studies in this direction .",
        "generated_description": "in order to put this into perspective , we compare the performance of a newsqa bidaf baseline vs . one finetuned on squad using the data generated by a 2 - stage synnet ( snet ) . the finetuning is done on the training set of the qa - related newswire qa task , mnewsqa . we find that the gap between the two approaches is small ( 0 . 2 and 0 . 3 in terms of em and f1 scores ) , which shows that the news - grounded qa models are able to bridge the gap when provided with the same amount of training data as the snet ."
    },
    {
        "table_id": "52",
        "table_info": {
            "table_caption": "Table 3: Performance on two datasets against the user simulator and human.",
            "table_column_names": [
                "Model",
                "Dataset 1  [ITALIC] vs. Simulator",
                "Dataset 1  [ITALIC] vs. Simulator",
                "Dataset 1  [ITALIC] vs. Human",
                "Dataset 1  [ITALIC] vs. Human",
                "Dataset 2  [ITALIC] vs. Simulator",
                "Dataset 2  [ITALIC] vs. Simulator",
                "Dataset 2  [ITALIC] vs. Human",
                "Dataset 2  [ITALIC] vs. Human"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "% Achieved",
                    "# Turns",
                    "% Achieved",
                    "# Turns",
                    "% Achieved",
                    "# Turns",
                    "% Achieved",
                    "# Turns"
                ],
                [
                    "Seq2Seq(goal)",
                    "76.00",
                    "4.74",
                    "67.74",
                    "7.87",
                    "67.10",
                    "7.38",
                    "54.1",
                    "7.56"
                ],
                [
                    "Seq2Seq(goal+state)",
                    "79.41",
                    "4.74",
                    "70.97",
                    "6.35",
                    "67.37",
                    "7.42",
                    "58.1",
                    "8.04"
                ],
                [
                    "Seq2Seq(goal+look)",
                    "80.64",
                    "6.54",
                    "74.19",
                    "5.41",
                    "83.54",
                    "[BOLD] 5.82",
                    "60.3",
                    "[BOLD] 6.94"
                ],
                [
                    "Seq2Seq(goal+look+state)",
                    "[BOLD] 85.07",
                    "[BOLD] 4.10",
                    "[BOLD] 77.42",
                    "[BOLD] 5.02",
                    "[BOLD] 83.58",
                    "6.36",
                    "[BOLD] 61.2",
                    "7.30"
                ]
            ]
        },
        "gold_description": "table 3 shows the performance of baselines against user simulator and human on the two datasets . equation ( 11 ) ) is also proven effective in deliver ing more achievement , which can be seen from the second and last rows of table 3 . the results are consistent with those in table 3 .",
        "generated_description": "in this section , we compare the seq2seq model with the human simulator and the goal - oriented dialogue agent . we observe that the goal oriented dialogue agent can achieve more than 70 % success rate when compared to the human agent . however , the number of action taken by the agent is lower than that achieved by humans . we conjecture that this is because humans tend to use the human language to encourage the agent to move faster and thus more efficiently search for the goal . on the other hand , our model outperforms humans in terms of goal success rate . we attribute this to the fact that our agent is able to exploit the multi - granularity information provided by the dialog state tracking module ."
    },
    {
        "table_id": "53",
        "table_info": {
            "table_caption": "Table 2: Correlation between Attribution word importance with POS tags, Fertility, and Syntactic Depth. Fertility can be categorized into 4 types: one-to-many (“≥2”), one-to-one (“1”), many-to-one (“(0,1)”), and null-aligned (“0”). Syntactic depth shows the depth of a word in the dependency tree. A lower tree depth indicates closer to the root node in the dependency tree, which might indicate a more important word.",
            "table_column_names": [
                "[BOLD] Type  [BOLD] POS Tags",
                "[BOLD] Type Noun",
                "[BOLD] Zh⇒En  [BOLD] 21.0%",
                "[BOLD] En⇒Fr 1.9%",
                "[BOLD] En⇒Ja 0.7%"
            ],
            "table_content_values": [
                [
                    "[BOLD] POS Tags",
                    "Verb",
                    "0.3%",
                    "[BOLD] 25.0%",
                    "0.3%"
                ],
                [
                    "[BOLD] POS Tags",
                    "Adj.",
                    "0.4%",
                    "9.3%",
                    "0.7%"
                ],
                [
                    "[BOLD] POS Tags",
                    "Prep.",
                    "1.3%",
                    "4.5%",
                    "[BOLD] 26.7%"
                ],
                [
                    "[BOLD] POS Tags",
                    "Dete.",
                    "3.0%",
                    "5.7%",
                    "2.1%"
                ],
                [
                    "[BOLD] POS Tags",
                    "Punc.",
                    "3.5%",
                    "[BOLD] 18.3%",
                    "[BOLD] 30.5%"
                ],
                [
                    "[BOLD] POS Tags",
                    "Others",
                    "0.5%",
                    "1.2%",
                    "4.7%"
                ],
                [
                    "[BOLD] Fertility",
                    "≥2",
                    "[BOLD] 50.2%",
                    "[BOLD] 21.4%",
                    "[BOLD] 21.7%"
                ],
                [
                    "[BOLD] Fertility",
                    "1",
                    "[BOLD] 15.4%",
                    "7.0%",
                    "3.1%"
                ],
                [
                    "[BOLD] Fertility",
                    "(0,1)",
                    "2.5%",
                    "0.4%",
                    "3.0%"
                ],
                [
                    "[BOLD] Fertility",
                    "0",
                    "0.0%",
                    "1.9%",
                    "3.8%"
                ],
                [
                    "[BOLD] Syntactic",
                    "Low",
                    "1.6%",
                    "2.5%",
                    "1.2%"
                ],
                [
                    "[BOLD] Syntactic",
                    "Middle",
                    "0.3%",
                    "0.8%",
                    "1.4%"
                ],
                [
                    "[BOLD] Syntactic",
                    "High",
                    "0.0%",
                    "0.1%",
                    "0.1%"
                ]
            ]
        },
        "gold_description": "table 2 : correlation between attribution word importance with pos tags , fertility , and syntactic depth . fertility can be categorized into 4 types : one - to - many ( \" ≥ 2 \" ) , one - to - one ( \" 1 \" ) , many - to - one ( \" ( 0 , 1 ) \" ) , and null - aligned ( \" 0 \" ) . syntactic depth shows the depth of a word in the dependency tree . a lower tree depth indicates closer to the root node in the dependency tree , which might indicate a more important word .",
        "generated_description": "for nouns , verbs , adverbs , adjectives , and conjunctions , the fraction of infrequent pos tags that do not have a fixed pos tag is 21 . 0 % , 1 . 9 % , and 0 . 4 % , respectively , in zh ⇒ en , en ⇇ fr , and ja , respectively . regarding other pos tags , a significant share ( 26 . 7 % ) of the sentences contains one of the following types : prepending , denotation , disambiguation , conjunct , quotation marks , and prepositions . in terms of fertility , we can see that a significant fraction ( 50 . 2 % ) of sentences containing a fertility term is also used as a pos tag , and the percentage of others is also present ( 4 . 7 % ) . we can also see that the proportion of others is also higher than the other types ( 21 . 7 % ) , and this is likely due to the fact that fertility terms are usually used in conjunction with other terms ( e . g . , “ in a relationship ” , “ wanting to get married ” ) ."
    },
    {
        "table_id": "54",
        "table_info": {
            "table_caption": "Table 1: Average accuracies and Macro-F1 scores over five runs with random initialization along with their standard deviations. Bold: best results or within std of them. ∗ indicates that the method’s result is significantly better than all baseline methods, † indicates that the method’s result is significantly better than all baselines methods that use the aspect-based data only, with p<0.05 according to a one-tailed unpaired t-test. The data annotations S, N and A indicate training with Sentence-level, Noisy sentence-level and Aspect-level data respectively. Numbers for TDLSTM+Att,ATAE-LSTM,MM,RAM and LSTM+SynATT+TarRep are from He et al. (2018a). Numbers for Semisupervised are from He et al. (2018b).",
            "table_column_names": [
                "Data",
                "Method",
                "SemEval-15 Acc.",
                "SemEval-15 Macro-F1",
                "SemEval-16 Acc.",
                "SemEval-16 Macro-F1"
            ],
            "table_content_values": [
                [
                    "A",
                    "TDLSTM+ATT Tang et al. ( 2016a )",
                    "77.10",
                    "59.46",
                    "83.11",
                    "57.53"
                ],
                [
                    "A",
                    "ATAE-LSTM Wang et al. ( 2016 )",
                    "78.48",
                    "62.84",
                    "83.77",
                    "61.71"
                ],
                [
                    "A",
                    "MM Tang et al. ( 2016b )",
                    "77.89",
                    "59.52",
                    "83.04",
                    "57.91"
                ],
                [
                    "A",
                    "RAM Chen et al. ( 2017 )",
                    "79.98",
                    "60.57",
                    "83.88",
                    "62.14"
                ],
                [
                    "A",
                    "LSTM+SynATT+TarRep He et al. ( 2018a )",
                    "81.67",
                    "66.05",
                    "84.61",
                    "67.45"
                ],
                [
                    "S+A",
                    "Semisupervised He et al. ( 2018b )",
                    "81.30",
                    "[BOLD] 68.74",
                    "85.58",
                    "69.76"
                ],
                [
                    "S",
                    "BiLSTM-104 Sentence Training",
                    "80.24 ± 1.64",
                    "61.89 ± 0.94",
                    "80.89 ± 2.79",
                    "61.40 ± 2.49"
                ],
                [
                    "S+A",
                    "BiLSTM-104 Sentence Training →Aspect Based Finetuning",
                    "77.75 ± 2.09",
                    "60.83 ± 4.53",
                    "84.87± 0.31",
                    "61.87 ± 5.44"
                ],
                [
                    "N",
                    "BiLSTM-XR-Dev Estimation",
                    "[BOLD] 83.31∗± 0.62",
                    "62.24 ± 0.66",
                    "[BOLD] 87.68∗± 0.47",
                    "63.23 ± 1.81"
                ],
                [
                    "N",
                    "BiLSTM-XR",
                    "[BOLD] 83.31∗± 0.77",
                    "64.42 ± 2.78",
                    "[BOLD] 88.12∗± 0.24",
                    "68.60 ± 1.79"
                ],
                [
                    "N+A",
                    "BiLSTM-XR →Aspect Based Finetuning",
                    "[BOLD] 83.44∗± 0.74",
                    "[BOLD] 67.23 ± 1.42",
                    "[BOLD] 87.66∗± 0.28",
                    "[BOLD] 71.19†± 1.40"
                ]
            ]
        },
        "gold_description": "table 1 : average accuracies and macro - f1 scores over five runs with random initialization along with their standard deviations . bold : best results or within std of them . ∗ indicates that the method ' s result is significantly better than all baseline methods , † indicates that the method ' s result is significantly better than all baselines methods that use the aspect - based data only , with p < 0 . 05 according to a one - tailed unpaired t - test . the data annotations s , and a indicate training with sentence - level , noisy sentence - level and aspect - level data respectively . numbers for tdlstm + att , atae - lstm , mm , ram and lstm + synatt + tarrep are from ( he et al . , 2018a ) . numbers for semisupervised are from ( he et al . , 2018b ) .",
        "generated_description": "semeval - 15 acc . and macro - f1 . from the table , we can see that : ( 1 ) semisupervised he et al . the sentence - level supervised method achieves the best performance on both datasets , which proves the effectiveness of the supervised sentence embeddings . ( 2 ) our bilstm - 104 sentence training model outperforms all the baselines and achieves the new state - of - the - art results on both the datasets . especially , our model achieves a significant improvement of 2 . 5 % and 2 . 3 % on semevals - 15 and - 16 , respectively , which verifies the effectiveness and generalizability of our model . ( 3 ) our model with sentence - encoding - based methods achieve the best results on two datasets , achieving the best accuracy and macro - f1 scores of 80 . 24 % and 69 . 76 % , respectively . this verifies that our sentence encoder is effective for the ner task . ( 4 ) we also compare our model with the semantic sentence sentence encoder heet al . ( 2018b ) , which is a deep neural network model based on multiple layers of sentence encoders and multi - task learning with supervision on both semantic and non - sentence long - term memory ( slm ) encoders . it can be seen that our model beats all the baseline methods by a large margin , which further verifies its effectiveness . ( 5 ) semantic sentence sentence modeling task is more challenging than short - term memory modeling task . the reason is that the length of semantic sentences is usually long , which makes it hard for the model to find the sentence boundary when there are multiple sentences in a single sentence ."
    },
    {
        "table_id": "55",
        "table_info": {
            "table_caption": "Table 4: Ablation Studies. Exact match (EM) and span F1 results on NewsQA test set of a BIDAF model finetuned with a 2-stage SynNet. In study A, we vary k, the number of mini-batches from SQuAD for every batch in NewsQA. In study B, we set k=0, and vary the answer type and how much of the paragraph we use for question synthesis. 2−sent refers to using two sentences before answer span, while all refers to using the entire paragraph. Aner refers to using an NER system and Aor refers to using the human-annotated answers to generate questions.",
            "table_column_names": [
                "[BOLD] A)",
                "EM",
                "F1",
                "[BOLD] B)",
                "EM",
                "F1"
            ],
            "table_content_values": [
                [
                    "k=0",
                    "27.2",
                    "40.5",
                    "2s +  [ITALIC] Aner",
                    "22.8",
                    "36.1"
                ],
                [
                    "k=2",
                    "29.8",
                    "43.9",
                    "all +  [ITALIC] Aner",
                    "27.2",
                    "40.5"
                ],
                [
                    "k=4",
                    "30.4",
                    "44.3",
                    "2s +  [ITALIC] Aoracle",
                    "31.3",
                    "45.2"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "all +  [ITALIC] Aoracle",
                    "32.5",
                    "46.8"
                ]
            ]
        },
        "gold_description": "conduct ablation studies ( table 4 ) , to better understand how various components in our training procedure and model impact overall performance we conduct several ablation studies , as summarized in table 4 . results in table 4 ( a ) show that using human - annotated answers to generate questions leads to a significant performance boost over using answers from an answer generation module . this supports the hypothesis that the answers humans choose to generate questions for provide important linguistic cues for finetuning the machine comprehension model . to see how copying impacts performance , we explore using the entire paragraph to generate the question vs . only the two sentences before and one sentence after the answer span and report results in table 4 ( b ) .",
        "generated_description": "we perform two sets of ablation studies on the bidaf model . in study a , we vary k , the number of mini - batches from squad for every batch in newsqa , and vary the answer type and how much of the paragraph we use for question synthesis . 2 − sent refers to using two sentences before answer span , while all refers to the entire paragraph . we find that using the human - annotated answers rather than the ner system results in a significant boost in both em and f1 . we also find that both using the full paragraph and using the answer span to generate questions ( i . e . , all + aoracle ) outperform using the n - gram aner system . finally , we find that the combination of all three components , i . e . using the complete paragraph as the answer , outperforms using the 2 - sentence version of the answer and the 3 - sentences version . this shows the importance of taking advantage of all available information from the paragraph to generate the question ."
    },
    {
        "table_id": "56",
        "table_info": {
            "table_caption": "Table 2: Validation BLEU for English-Spanish models with transfer learning. We use the final three models in our submission.",
            "table_column_names": [
                "[BOLD] Transfer learning schedule",
                "[BOLD] es2en  [BOLD] Khresmoi",
                "[BOLD] es2en  [BOLD] Health",
                "[BOLD] es2en  [BOLD] Bio",
                "[BOLD] en2es  [BOLD] Khresmoi",
                "[BOLD] en2es  [BOLD] Health",
                "[BOLD] en2es  [BOLD] Bio"
            ],
            "table_content_values": [
                [
                    "Health",
                    "45.1",
                    "35.7",
                    "34.0",
                    "41.2",
                    "34.7",
                    "36.1"
                ],
                [
                    "All-biomed",
                    "49.8",
                    "35.4",
                    "35.7",
                    "43.4",
                    "33.9",
                    "37.5"
                ],
                [
                    "All-biomed → Health",
                    "48.9",
                    "36.4",
                    "35.9",
                    "43.0",
                    "35.2",
                    "38.0"
                ],
                [
                    "All-biomed → Bio",
                    "48.0",
                    "34.6",
                    "37.2",
                    "43.2",
                    "34.1",
                    "40.5"
                ],
                [
                    "Health → All-biomed",
                    "[BOLD] 52.1",
                    "36.7",
                    "37.0",
                    "44.2",
                    "35.0",
                    "39.0"
                ],
                [
                    "Health → All-biomed → Health",
                    "51.1",
                    "[BOLD] 37.0",
                    "37.2",
                    "44.0",
                    "[BOLD] 36.3",
                    "39.5"
                ],
                [
                    "Health → All-biomed → Bio",
                    "50.6",
                    "36.0",
                    "[BOLD] 38.0",
                    "[BOLD] 45.2",
                    "35.3",
                    "[BOLD] 41.3"
                ]
            ]
        },
        "gold_description": "table 2 gives single model validation scores for es2en and en2es models with standard and iterative transfer learning . we find that the all - biomed domain gains 1 - 2 bleu points from transfer learning .",
        "generated_description": "s5ss0sss0px3 health , bio . the health schedule outperforms the bio schedule in general , and the health → all - biomed schedule is the best overall , showing that the health and bio schedules are complementary to each other ."
    },
    {
        "table_id": "57",
        "table_info": {
            "table_caption": "Table 3: Validation and test BLEU for models used in English-Spanish language pair submissions.",
            "table_column_names": [
                "[EMPTY]",
                "[BOLD] es2en  [BOLD] Khresmoi",
                "[BOLD] es2en  [BOLD] Health",
                "[BOLD] es2en  [BOLD] Bio",
                "[BOLD] es2en  [BOLD] Test",
                "[BOLD] en2es  [BOLD] Khresmoi",
                "[BOLD] en2es  [BOLD] Health",
                "[BOLD] en2es  [BOLD] Bio",
                "[BOLD] en2es  [BOLD] Test"
            ],
            "table_content_values": [
                [
                    "Health → All-biomed",
                    "52.1",
                    "36.7",
                    "37.0",
                    "42.4",
                    "44.2",
                    "35.0",
                    "39.0",
                    "44.9"
                ],
                [
                    "Health → All-biomed → Health",
                    "51.1",
                    "37.0",
                    "37.2",
                    "-",
                    "44.0",
                    "36.3",
                    "39.5",
                    "-"
                ],
                [
                    "Health → All-biomed → Bio",
                    "50.6",
                    "36.0",
                    "38.0",
                    "-",
                    "45.2",
                    "35.3",
                    "41.3",
                    "-"
                ],
                [
                    "Uniform ensemble",
                    "[BOLD] 52.2",
                    "36.9",
                    "37.9",
                    "[BOLD] 43.0",
                    "[BOLD] 45.1",
                    "35.6",
                    "40.2",
                    "45.4"
                ],
                [
                    "BI ensemble ( [ITALIC] α=0.5)",
                    "52.1",
                    "[BOLD] 37.0",
                    "[BOLD] 38.1",
                    "42.9",
                    "44.5",
                    "[BOLD] 35.7",
                    "[BOLD] 41.2",
                    "[BOLD] 45.6"
                ]
            ]
        },
        "gold_description": "for de2en and es2en , uniform ensembling performs similarly to the oracles , and performs similarly to bi . we submitted three runs to the wmt19 biomedical task for each language pair : the best single all - biomed model , a uniform ensemble of models on two en - de and three es - en domains , and an ensemble with bayesian interpolation . tables 3 and 4 give validation and test scores . that a uniform multi - domain ensemble performs well , giving 0 . 5 - 1 . 2 bleu improvement on the test set over strong single models . we see small gains from using bi with ensembles on most validation sets , but only on en2es test . we noted that , in general , we could predict bi ( α = 0 . 5 ) performance by comparing the uniform ensemble with the oracle model performing best on each validation domain . for en2es uniform ensembling underperforms the health and bio oracle models on their validation sets , and the uniform ensemble slightly underperforms bi on the test data .",
        "generated_description": "uniform ensembling . health → all - biomed → health , health → bio , and bio → bio . we see that in general , training a single model to perform well on both health and bio tasks is difficult . uniform ensemble outperforms all other models in all cases . in particular , in health , uniform ensemble yields a bleu score of 52 . 2 on es2en and 43 . 0 on bio , which outperforms the best single model by a large margin . in contrast , in the health case uniform ensemble performs worse than training one single model on the bio task . we hypothesize that the health model is overfitted on the in - domain data while the bio model is pretrained on all the biomedical data in the other direction . uniform ensemble also performs better than training single models across all domains ."
    },
    {
        "table_id": "58",
        "table_info": {
            "table_caption": "Table 4: Validation and test BLEU for models used in English-German language pair submissions.",
            "table_column_names": [
                "[EMPTY]",
                "[BOLD] de2en  [BOLD] Khresmoi",
                "[BOLD] de2en  [BOLD] Cochrane",
                "[BOLD] de2en  [BOLD] Test",
                "[BOLD] en2de  [BOLD] Khresmoi",
                "[BOLD] en2de  [BOLD] Cochrane",
                "[BOLD] en2de  [BOLD] Test"
            ],
            "table_content_values": [
                [
                    "News",
                    "43.8",
                    "46.8",
                    "-",
                    "30.4",
                    "40.7",
                    "-"
                ],
                [
                    "News → All-biomed",
                    "44.5",
                    "47.6",
                    "27.4",
                    "31.1",
                    "39.5",
                    "26.5"
                ],
                [
                    "Uniform ensemble",
                    "45.3",
                    "48.4",
                    "[BOLD] 28.6",
                    "[BOLD] 32.6",
                    "42.9",
                    "[BOLD] 27.2"
                ],
                [
                    "BI ensemble ( [ITALIC] α=0.5)",
                    "[BOLD] 45.4",
                    "[BOLD] 48.8",
                    "28.5",
                    "32.4",
                    "[BOLD] 43.1",
                    "26.4"
                ]
            ]
        },
        "gold_description": "we see small gains from using bi with ensembles on most validation sets , but only on en2es test . for en2de , by contrast , uniform ensembling is consistently better than oracles on the dev sets , and outperforms bi on the test data .",
        "generated_description": "in the english - german language pair , we use the wmt ’ 17 en - de evaluation campaign sponsored by the conference on spoken language translation and evaluation ( cochrane et al . , 2017 ) to evaluate models for both the news and the news → all - biomed language pairs . the bleu scores are given in terms of the number of correctly translated sentences in the test set . uniform ensembling results in large improvements for the news - only and news - all biomedicine - based models , and small improvements in the case of the cochrane model . in the en2en - to - german setting , the uniform ensemble outperforms all other ensembles by a small but consistent margin . bi ensemble results in further improvements , reaching a maximum of 1 . 8 bleus on the final test set , and a mean of 2 . 2 bleùs ."
    },
    {
        "table_id": "59",
        "table_info": {
            "table_caption": "Table 5: Comparing uniform ensembles and BI with varying smoothing factor on the WMT19 test data. Small deviations from official test scores on submitted runs are due to tokenization differences. α=0.5 was chosen for submission based on results on available development data.",
            "table_column_names": [
                "[EMPTY]",
                "[BOLD] es2en",
                "[BOLD] en2es",
                "[BOLD] de2en",
                "[BOLD] en2de"
            ],
            "table_content_values": [
                [
                    "Uniform",
                    "[BOLD] 43.2",
                    "45.3",
                    "28.3",
                    "25.9"
                ],
                [
                    "BI ( [ITALIC] α=0.5)",
                    "43.0",
                    "[BOLD] 45.5",
                    "28.2",
                    "25.2"
                ],
                [
                    "BI ( [ITALIC] α=0.1)",
                    "[BOLD] 43.2",
                    "[BOLD] 45.5",
                    "[BOLD] 28.5",
                    "[BOLD] 26.0"
                ]
            ]
        },
        "gold_description": "consequently in table 5 we experiment with bi ( α = 0 . 1 ) . in this case bi matches or out - performs the uniform ensemble . notably , for en2es , where bi ( α = 0 . 5 ) performed well , taking α = 0 . 1 does not harm performance .",
        "generated_description": "uniform ensembles were used for submission based on results on available development data . bi was applied on the wmt19 test data to fine - tune models for a range of α from 0 . 1 to 0 . 5 . we observe that bi leads to small but consistent improvements across all language pairs . small deviations from the official test scores on en2en and en2es are due to tokenization differences . when scaled to α = 0 . 5 , bi produces models with a uniform uniform ensembling and a small number of distinct models per language pair . however , as α increases , bi tends to produce models with more distinct models across languages . this suggests that adaptive smoothing has a positive impact on both uniform and distributed models ."
    },
    {
        "table_id": "60",
        "table_info": {
            "table_caption": "Table 1: BLEU Scores of Data Sets",
            "table_column_names": [
                "dev",
                "train",
                "eScape"
            ],
            "table_content_values": [
                [
                    "77.15",
                    "77.42",
                    "37.68"
                ]
            ]
        },
        "gold_description": "we then tested the bleu scores between machine translation results and corresponding gold standard post - editing results on the original development set , the training set and the synthetic data as shown in table 1 . table 1 shows that there is a significant gap between the synthetic escape data set ( negri et al . , 2018 ) and the real - life data sets ( the development set and the original training set from posteditors ) ,",
        "generated_description": "we conduct experiments on two data sets to evaluate the performance of the proposed model . the first data set is named scrawl treebank and the second one is called escape extractive treebank ( escape treebank ) . the bleu score of the model on the development set is 77 . 15 and the score on the test set is 37 . 68 . the models are trained and tested on both the training and development sets ."
    },
    {
        "table_id": "61",
        "table_info": {
            "table_caption": "Table 3: Results on the Test Set",
            "table_column_names": [
                "Models",
                "TER",
                "BLEU"
            ],
            "table_content_values": [
                [
                    "MT as PE",
                    "16.84",
                    "74.73"
                ],
                [
                    "Gaussian",
                    "16.79",
                    "75.03"
                ],
                [
                    "Uniform",
                    "16.80",
                    "75.03"
                ],
                [
                    "Ensemble x5",
                    "16.77",
                    "75.03"
                ]
            ]
        },
        "gold_description": "even the ensemble of 5 models did not result in significant differences especially in bleu scores .",
        "generated_description": "ensemble x5 is the ensemble of 5 models trained with different random seeds and different random hyper - parameter initializations , and it achieves the best result in terms of ter and bleu score ."
    },
    {
        "table_id": "62",
        "table_info": {
            "table_caption": "Table 2: BLEU Scores on the Development Set",
            "table_column_names": [
                "Models",
                "BLEU"
            ],
            "table_content_values": [
                [
                    "MT as PE",
                    "76.76"
                ],
                [
                    "Processed MT",
                    "76.61"
                ],
                [
                    "Base",
                    "76.91 ∼ 77.13"
                ],
                [
                    "Gaussian",
                    "76.94 ∼ 77.08"
                ],
                [
                    "Uniform",
                    "77.01 ∼ 77.10"
                ],
                [
                    "Ensemble x5",
                    "[BOLD] 77.22"
                ]
            ]
        },
        "gold_description": "table 2 shows that the performance got slightly hurt ( comparing \" processed mt \" with \" mt as pe \" ) with pre - processing and post - processing procedures which are normally applied in training seq2seq models for reducing vocabulary size . the multi - source transformer ( base ) model achieved the highest single model bleu score without joint training with the de - noising encoder task . even with the ensembled model , our ape approach does not significantly improve machine translation outputs measured in bleu ( + 0 . 46 ) .",
        "generated_description": "uniform and ensemble x5 . the bleu score for process mt is 76 . 61 % and 77 . 13 % , respectively . the ensemble x5 model achieves 77 . 22 % , which is + 0 . 5 points higher than the previous best result ."
    },
    {
        "table_id": "63",
        "table_info": {
            "table_caption": "Table 1: Variation across conditions for a range of metrics (defined in § 3.4). Bold indicates a statistically significant difference compared to the baseline at the 0.05 level, and a † indicates significance at the 0.01 level, both after applying the Holm–-Bonferroni method across each row Holm (1979).",
            "table_column_names": [
                "Condition",
                "Accuracy (%) Corr",
                "Accuracy (%) Gram",
                "Time (s)",
                "Diversity Distinct",
                "Diversity PINC"
            ],
            "table_content_values": [
                [
                    "Baseline",
                    "74",
                    "97",
                    "36",
                    "99",
                    "68"
                ],
                [
                    "Lexical Examples",
                    "[BOLD] 90†",
                    "98",
                    "[BOLD] 27",
                    "[BOLD] 93",
                    "[BOLD] 55†"
                ],
                [
                    "Mixed Examples",
                    "[BOLD] 89†",
                    "96",
                    "36",
                    "[BOLD] 87†",
                    "[BOLD] 58†"
                ],
                [
                    "No Examples",
                    "84",
                    "96",
                    "30",
                    "95",
                    "63"
                ],
                [
                    "Novelty Bonus",
                    "72",
                    "96",
                    "30",
                    "99",
                    "69"
                ],
                [
                    "No Bonus",
                    "78",
                    "94",
                    "28",
                    "99",
                    "66"
                ],
                [
                    "One Paraphrase",
                    "82",
                    "[BOLD] 89",
                    "38",
                    "96",
                    "65"
                ],
                [
                    "Chain",
                    "68",
                    "94",
                    "[BOLD] 25",
                    "98",
                    "[BOLD] 74"
                ],
                [
                    "Answers",
                    "80",
                    "94",
                    "[BOLD] 29",
                    "96",
                    "65"
                ],
                [
                    "advising",
                    "78",
                    "94",
                    "31",
                    "97",
                    "70"
                ],
                [
                    "geoquery",
                    "77",
                    "[BOLD] 85†",
                    "[BOLD] 25†",
                    "[BOLD] 94",
                    "[BOLD] 63"
                ],
                [
                    "wsj",
                    "68",
                    "[BOLD] 90",
                    "[BOLD] 61†",
                    "[BOLD] 94†",
                    "[BOLD] 38†"
                ],
                [
                    "ubuntu",
                    "[BOLD] 56†",
                    "92",
                    "44",
                    "97",
                    "67"
                ]
            ]
        },
        "gold_description": "we consider variations in instructions , incentives , data domains , and workflows . we manually analyzed paraphrases for correctness , grammaticality , and linguistic diversity . our observations provide new insight into the trade - offs between accuracy and diversity in crowd responses that arise as a result of task design , our analysis shows that the most important factor is how workers are primed for a task , with the choice of examples and the prompt sentence affecting diversity and correctness significantly . there was relatively little variation in grammaticality or time across the conditions . priming had a major impact , with the shift to lexical examples leading to a significant improvement in correctness , but much lower diversity . the surprising increase in correctness when providing no examples changing the incentives by providing either a bonus for novelty , or no bonus at all , did not substantially impact any of the metrics . changing the number of paraphrases written by each worker did not significantly impact diversity ( we worried that collecting more than one may lead to a decrease ) . the one paraphrase condition did have lower grammaticality , changing the source of the prompt sentence to create a chain of paraphrases led to a significant increase in diversity . showing the answer to the question being para phrased did not significantly affect correctness or diversity ,",
        "generated_description": "we can see that the best results are obtained for the first condition , which is the one that has the highest number of examples in terms of grammatical diversity . the second condition is the hardest to predict because it requires the most extensive understanding of the text and the longest average time to answer the questions . the third condition is confusing because it is the only one that requires a background knowledge about the text other than what is contained in the training set . in terms of time , it is surprisingly easy to predict the correct answer for most of the conditions without any prior knowledge about what the text is about . the hardest conditions are the advisory and geoquery conditions , where the amount of training data is small and the model is not trained on enough examples . the fourth and fifth conditions are lexical examples , which are the most time consuming since they require the model to go through the entire training set to select a set of examples . finally , the last condition is unique to the qa - qa task and results in the lowest diversity ."
    },
    {
        "table_id": "64",
        "table_info": {
            "table_caption": "Table 3: Frame evaluation results on the triples from the FrameNet 1.7 corpus Baker et al. (1998). The results are sorted by the descending order of the Frame F1-score. Best results are boldfaced.",
            "table_column_names": [
                "[BOLD] Method",
                "[BOLD] Verb  [BOLD] nmPU",
                "[BOLD] Verb  [BOLD] niPU",
                "[BOLD] Verb  [BOLD] F1",
                "[BOLD] Subject  [BOLD] nmPU",
                "[BOLD] Subject  [BOLD] niPU",
                "[BOLD] Subject  [BOLD] F1",
                "[BOLD] Object  [BOLD] nmPU",
                "[BOLD] Object  [BOLD] niPU",
                "[BOLD] Object  [BOLD] F1",
                "[BOLD] Frame  [BOLD] nmPU",
                "[BOLD] Frame  [BOLD] niPU",
                "[BOLD] Frame  [BOLD] F1"
            ],
            "table_content_values": [
                [
                    "Triframes Watset",
                    "42.84",
                    "88.35",
                    "[BOLD] 57.70",
                    "54.22",
                    "81.40",
                    "65.09",
                    "53.04",
                    "83.25",
                    "64.80",
                    "55.19",
                    "60.81",
                    "[BOLD] 57.87"
                ],
                [
                    "HOSG Cotterell et al. ( 2017 )",
                    "44.41",
                    "68.43",
                    "53.86",
                    "52.84",
                    "74.53",
                    "61.83",
                    "54.73",
                    "74.05",
                    "62.94",
                    "55.74",
                    "50.45",
                    "52.96"
                ],
                [
                    "NOAC Egurnov et al. ( 2017 )",
                    "20.73",
                    "88.38",
                    "33.58",
                    "57.00",
                    "80.11",
                    "[BOLD] 66.61",
                    "57.32",
                    "81.13",
                    "[BOLD] 67.18",
                    "44.01",
                    "63.21",
                    "51.89"
                ],
                [
                    "Triadic Spectral",
                    "49.62",
                    "24.90",
                    "33.15",
                    "50.07",
                    "41.07",
                    "45.13",
                    "50.50",
                    "41.82",
                    "45.75",
                    "52.05",
                    "28.60",
                    "36.91"
                ],
                [
                    "Triadic  [ITALIC] k-Means",
                    "[BOLD] 63.87",
                    "23.16",
                    "33.99",
                    "[BOLD] 63.15",
                    "38.20",
                    "47.60",
                    "[BOLD] 63.98",
                    "37.43",
                    "47.23",
                    "[BOLD] 63.64",
                    "24.11",
                    "34.97"
                ],
                [
                    "LDA-Frames Materna ( 2013 )",
                    "26.11",
                    "66.92",
                    "37.56",
                    "17.28",
                    "83.26",
                    "28.62",
                    "20.80",
                    "90.33",
                    "33.81",
                    "18.80",
                    "71.17",
                    "29.75"
                ],
                [
                    "Triframes CW",
                    "7.75",
                    "6.48",
                    "7.06",
                    "3.70",
                    "14.07",
                    "5.86",
                    "51.91",
                    "76.92",
                    "61.99",
                    "21.67",
                    "26.50",
                    "23.84"
                ],
                [
                    "Singletons",
                    "0.00",
                    "25.23",
                    "0.00",
                    "0.00",
                    "25.68",
                    "0.00",
                    "0.00",
                    "20.80",
                    "0.00",
                    "32.34",
                    "22.15",
                    "26.29"
                ],
                [
                    "Whole",
                    "3.62",
                    "[BOLD] 100.0",
                    "6.98",
                    "2.41",
                    "[BOLD] 98.41",
                    "4.70",
                    "2.38",
                    "[BOLD] 100.0",
                    "4.64",
                    "2.63",
                    "[BOLD] 99.55",
                    "5.12"
                ]
            ]
        },
        "gold_description": "in table 3 and figure 1 , the results of the experiment are presented . triframes based on watset clustering outperformed the other methods on both verb f1 and overall frame f1 . the use of the watset fuzzy clustering algorithm that splits the hubs by disambiguating them leads to the best results ( see table 3 ) .",
        "generated_description": "s4ss3sss0px3 comparison with baselines . we compare our method with the hosg framework proposed by \\ newcitecotterell et al . as can be seen from the table , our method outperforms the baselines by a large margin on the verb and frame tasks , and achieves the best performance on the object and verb tasks . on the object task , the f1 score of our method is higher than that of the baseline by about 2 points , and on the subject and frame task by around 2 points . note that the performance of the verb task is better than the frame task because the latter task is more difficult due to the presence of noisy parts in the vocabulary ."
    },
    {
        "table_id": "65",
        "table_info": {
            "table_caption": "Table 4: Evaluation results on the dataset of polysemous verb classes by Korhonen et al. (2003).",
            "table_column_names": [
                "[BOLD] Method",
                "[BOLD] nmPU",
                "[BOLD] niPU",
                "[BOLD] F1"
            ],
            "table_content_values": [
                [
                    "LDA-Frames",
                    "[BOLD] 52.60",
                    "45.84",
                    "[BOLD] 48.98"
                ],
                [
                    "Triframes Watset",
                    "40.05",
                    "62.09",
                    "48.69"
                ],
                [
                    "NOAC",
                    "37.19",
                    "64.09",
                    "47.07"
                ],
                [
                    "HOSG",
                    "38.22",
                    "43.76",
                    "40.80"
                ],
                [
                    "Triadic Spectral",
                    "35.76",
                    "38.96",
                    "36.86"
                ],
                [
                    "Triadic  [ITALIC] k-Means",
                    "52.22",
                    "27.43",
                    "35.96"
                ],
                [
                    "Triframes CW",
                    "18.05",
                    "12.72",
                    "14.92"
                ],
                [
                    "Whole",
                    "24.14",
                    "[BOLD] 79.09",
                    "36.99"
                ],
                [
                    "Singletons",
                    "0.00",
                    "27.21",
                    "0.00"
                ]
            ]
        },
        "gold_description": "table 4 presents results on the second dataset for the best models identified on the first dataset . the lda - frames yielded the best results with our approach performing comparably in terms of the f1 - score .",
        "generated_description": "we can see that lda - frames outperforms all the other methods and achieves the highest scores on both datasets . on the nipu dataset , the performance of the whole model is 24 . 14 % and the best result is obtained by the k - means method , which achieves a f1 - score of 36 . 99 % ."
    },
    {
        "table_id": "66",
        "table_info": {
            "table_caption": "Table 1: French-English Performance. Baseline indicates current state of the art performance.",
            "table_column_names": [
                "Method",
                "Max F1",
                "11-point IAP"
            ],
            "table_content_values": [
                [
                    "Baseline",
                    "54.92",
                    "50.99"
                ],
                [
                    "RR",
                    "62.94",
                    "59.62"
                ],
                [
                    "RR_FR_1step",
                    "68.35",
                    "64.42"
                ],
                [
                    "RR_FR_2step",
                    "69.72",
                    "67.29"
                ]
            ]
        },
        "gold_description": "table 1 shows these performance measures for french - english , using global structure greatly improves upon the state of the art baseline performance .",
        "generated_description": "rr achieves a max f1 score of 54 . 92 % and 50 . 99 % for french - english and 59 . 62 % for english - french , respectively . the rr model achieves a 10 - point improvement over the current state - of - the - art . the best performing model is rr_fr_1step which achieves 68 . 35 % of max f1 and 64 . 42 % of 11 - point iap . this result shows the effectiveness of our method in the context of multi - step document extraction ."
    },
    {
        "table_id": "67",
        "table_info": {
            "table_caption": "Table 4: French-English Performance (large data). Baseline indicates state of the art performance.",
            "table_column_names": [
                "Method",
                "Max F1",
                "11-point IAP"
            ],
            "table_content_values": [
                [
                    "Baseline",
                    "55.08",
                    "51.35"
                ],
                [
                    "RR",
                    "60.88",
                    "58.79"
                ],
                [
                    "RR_FR_1step",
                    "65.87",
                    "63.55"
                ],
                [
                    "RR_FR_2step",
                    "65.76",
                    "65.26"
                ]
            ]
        },
        "gold_description": "tables 4 through 6 show the summary metrics for the three language pairs for the large data experiments . we can see that the reverse rank and forward rank methods of taking into account the global structure of interactions among predictions is still helpful , providing large improvements in performance even in this challenging large data condition over strong state of the art baselines",
        "generated_description": "rr achieves 60 . 88 % and 58 . 79 % in terms of max f1 and 11 - point iap , respectively , compared to the state of the art of 51 . 35 % reported by \\ newcitedblp : journals / corr / abs - 1907 - 0909 - 10683 . we observe that rr is able to improve the state - of - the - art by 1 . 87 % and 1 . 55 % absolute f1 , respectively . the rr_fr_1step method performs slightly worse than rr due to the small amount of training data . however , rr based methods outperform the rr baseline by 2 . 76 % and 2 . 26 % absolute on the same evaluation measures . this result shows the effectiveness of our rr method ."
    },
    {
        "table_id": "68",
        "table_info": {
            "table_caption": "Table 2: List of the target NER data sets and their specifications. Size is shown in number of tokens.",
            "table_column_names": [
                "[BOLD] Data set",
                "[BOLD] Size",
                "[BOLD] Entity Types",
                "[BOLD] Description"
            ],
            "table_content_values": [
                [
                    "CADEC",
                    "120,341",
                    "Adverse Drug Event, Disease, Drug, Finding, Symptom",
                    "Posts taken from AskaPatient, which is a forum where consumers can discuss their experiences with medications."
                ],
                [
                    "CoNLL2003",
                    "301,418",
                    "Person, Organization, Location, Miscellany",
                    "Newswire from the Reuters RCV1 corpus."
                ],
                [
                    "CRAFT",
                    "561,015",
                    "Cell, Chemical entity, Biological taxa, Protein, Biomacromolecular sequence, Entrez gene, Biological process and molecular function, Cellular component",
                    "Full-length, open-access journal articles about biology."
                ],
                [
                    "JNLPBA",
                    "593,590",
                    "Protein, DNA, RNA, Cell line and Cell type",
                    "Abstract of journal articles about biology."
                ],
                [
                    "ScienceIE",
                    "99,555",
                    "Process (including methods, equipment), Task and Material (including corpora, physical materials)",
                    "Journal articles about Computer Science, Material Sciences and Physics."
                ],
                [
                    "Wetlab",
                    "220,618",
                    "Action, 9 object-based (Amount, Concentration, Device, Location, Method, Reagent, Speed, Temperature, Time) entity types, 5 measure-based (Numerical, Generic-Measure, Size, pH, Measure-Type) and 3 other (Mention, Modifier, Seal) types",
                    "Protocols written by researchers about conducting biology and chemistry experiments."
                ]
            ]
        },
        "gold_description": "five source and six target ner data sets , each selected to provide a range of fields ( i . e . , biology , computer science , medications , local business ) and tenors ( i . e . , encyclopedia articles , journal articles , experimental protocols , online reviews ) . we use five data sets as source data , covering a range of fields ( i . e . , clinical , biomedical , local business and wiki with diverse fields ) and tenors ( i . e . , popular reporting , notes , scholarly publications , online reviews and encyclopedia ) . details of these target data are listed in table 2 .",
        "generated_description": "we perform experiments on four datasets related to biology : cadec , conll2003 , craft , jnlpba , and scienceie . the first set of datasets is derived from the reuters rcv1 corpus , which contains newswire articles about medical problems and treatments . the second set is extracted from the askapatient dataset , which is a forum where users can discuss their experiences with medications . the third set is obtained from the commoncapsnets , which are related to patents and drug companies . the fourth set is from the ge healthcare data set , which consists of 9 object - based types , i . e . , “ action , consumptions , device , location , method , reagent , speed , time ” , “ numerical , generic - label , ph , type ” and “ mention , modifier , seal ” . each of these types is associated with a set of domain - specific entity types , e . g . “ disease name ” or “ allurencence ” in the description of a drug . for each of the above types , we use the top 10 most frequently occurring entity types from the original wikipedia as features . then , we fine - tune the feature embeddings for each of these entity types on the training set of each dataset , and select the features with the best performance on the validation set of that type ."
    },
    {
        "table_id": "69",
        "table_info": {
            "table_caption": "Table 4: Correlation coefficients between similarity measures and the effectiveness of pretrained models. The coefficients vary between -1 (negative correlation) and 1 (positive correlation). Zero means no correlation.",
            "table_column_names": [
                "[EMPTY]",
                "[BOLD] Word vectors",
                "[BOLD] LMs"
            ],
            "table_content_values": [
                [
                    "TVC",
                    "00.454",
                    "00.666"
                ],
                [
                    "TVcC",
                    "00.469",
                    "00.739"
                ],
                [
                    "PPL",
                    "-0.398",
                    "-0.618"
                ],
                [
                    "WVV",
                    "-0.406",
                    "-0.747"
                ]
            ]
        },
        "gold_description": "the results in table 4 show that our proposed similarity measures are predictive of the effectiveness of the pretraining data . vccr is the most informative factor in predicting the effectiveness of pretrained word vectors given a target data set .",
        "generated_description": "we first investigate the correlation coefficients between the word vectors and the effectiveness of the pretrained models . we observe that the scores of tvc and tvcc are positively correlated with the scores obtained by the pretraining models . this indicates that word vectors learned from the tvc corpus are more transferable and effective than the ones learned from wvv corpus . we also observe that ppl is negatively correlated with pretraining performance , which implies that the language model quality is influenced by the quality of pretraining data obtained from tvc rather than the quality obtained from word vectors ."
    },
    {
        "table_id": "70",
        "table_info": {
            "table_caption": "Table 5: Comparison between our best performance pretrained models and the publicly available ones, which are pretrained on much larger corpora.",
            "table_column_names": [
                "[EMPTY]",
                "[BOLD] Word vectors GloVe",
                "[BOLD] Word vectors Ours",
                "[BOLD] LMs ELMo",
                "[BOLD] LMs Ours"
            ],
            "table_content_values": [
                [
                    "CADEC",
                    "[BOLD] 70.30",
                    "70.27",
                    "[BOLD] 71.91",
                    "70.46"
                ],
                [
                    "CoNLL2003",
                    "[BOLD] 90.25",
                    "86.36",
                    "[BOLD] 91.34",
                    "89.78"
                ],
                [
                    "CRAFT",
                    "74.22",
                    "[BOLD] 75.45",
                    "[BOLD] 75.77",
                    "75.45"
                ],
                [
                    "JNLPBA",
                    "73.19",
                    "[BOLD] 73.24",
                    "73.65",
                    "[BOLD] 74.29"
                ],
                [
                    "ScienceIE",
                    "37.10",
                    "[BOLD] 37.91",
                    "41.15",
                    "[BOLD] 42.07"
                ],
                [
                    "WetLab",
                    "[BOLD] 79.15",
                    "78.93",
                    "79.57",
                    "[BOLD] 79.62"
                ]
            ]
        },
        "gold_description": "we find that word vectors and lms pretrained on small similar sources can achieve competitive or even better performance than the ones pretrained on larger sources ( table 5 ) . on jnlpba , scienceie and wetlab , lms pretrained on the small similar source perform better , while word vectors pretrained on the small similar source perform better on craft , jnlpba , and scienceie .",
        "generated_description": "we first compare our best publicly available pretrained models against the ones which are pretrained on much larger publicly available datasets , which are all supervised by elmo peters et al . we observe that our best elmo model outperforms the glove pretrained model on cadec , conll2003 , craft , jnlpba , scienceie , and wetlab , and is only slightly worse than our best pretrained word vectors on the wiki dataset ."
    },
    {
        "table_id": "71",
        "table_info": {
            "table_caption": "Table 6: Impact of hyper-parameter setting on the effectiveness of pretrained word vectors. ‘Opt’ is hyper-parameter setting proposed in (Chiu et al., 2016), whereas ‘Def’ is the default setting in word2vec.",
            "table_column_names": [
                "[EMPTY]",
                "[BOLD] ScienceIE  [BOLD] Def",
                "[BOLD] ScienceIE  [BOLD] Opt",
                "[BOLD] WetLab  [BOLD] Def",
                "[BOLD] WetLab  [BOLD] Opt"
            ],
            "table_content_values": [
                [
                    "1BWB",
                    "34.40",
                    "34.57",
                    "78.66",
                    "79.12"
                ],
                [
                    "MIMIC",
                    "31.23",
                    "34.14",
                    "78.68",
                    "78.65"
                ],
                [
                    "PubMed",
                    "[BOLD] 37.91",
                    "[BOLD] 38.86",
                    "[BOLD] 78.93",
                    "[BOLD] 79.28"
                ],
                [
                    "Wiki",
                    "36.15",
                    "35.63",
                    "78.45",
                    "78.99"
                ],
                [
                    "Yelp",
                    "33.92",
                    "34.25",
                    "78.48",
                    "78.78"
                ]
            ]
        },
        "gold_description": "our results suggest that this hyper - parameter setting can overall ( except wiki - scienceie and mimic - wetlab pairs ) produce better performance compare to the default setting ( table 6 ) .",
        "generated_description": "s4ss3sss0px2 how does the quality of pretrained word vectors affect the effectiveness of pre - trained word embeddings ? we conduct an ablation study on word2vec to investigate the impact of hyper - parameter settings on the overall effectiveness of our proposed method . for a fair comparison , we also set the default setting of our method as ‘ def ’ , while ‘ opt ’ is used in the original word2vec model . we observe that our method significantly outperforms the default on all the datasets ."
    },
    {
        "table_id": "72",
        "table_info": {
            "table_caption": "Table 3: Types of discrepancy in context-agnostic translation caused by deixis (excluding anaphora)",
            "table_column_names": [
                "[BOLD] type of discrepancy",
                "[BOLD] frequency"
            ],
            "table_content_values": [
                [
                    "T-V distinction",
                    "67%"
                ],
                [
                    "speaker/addressee gender:",
                    "[EMPTY]"
                ],
                [
                    "same speaker",
                    "22%"
                ],
                [
                    "different speaker",
                    "09%"
                ],
                [
                    "other",
                    "02%"
                ]
            ]
        },
        "gold_description": "most errors in our annotated corpus are related to person deixis , specifically gender marking in the russian translation , and the t - v distinction between informal and formal you ( latin \" tu \" and \" vos \" ) . from table 3 , we see that the most frequent error category related to deixis in our annotated corpus is the inconsistency of t - v forms when translating second person pronouns .",
        "generated_description": "in order to better understand the types of discrepancy in context - agnostic translation caused by deixis , we looked into the output of the lstm - cpt system . we observe that 73 % of the errors are caused by t - v distinction . the most common error type is caused by the speaker / addressee gender : 22 % and 10 % for same and different speakers respectively . we also observe other errors such as the speaker being different or difficult to distinguish ( e . g . anaphora ) and the context being too long or too short ( exact gender ) ."
    },
    {
        "table_id": "73",
        "table_info": {
            "table_caption": "Table 1: Evaluation on subsets of THYME Dev (in F-measure). The subsets of Event×Event (EE) and Timex3×Event (TE) relation pairs are of sizes 3.3k and 2.7k respectively. The intervals 0-100, 100-500 and 500+ are subsets reflecting average argument token frequency in the training data (of sizes 2.2k, 2.2k and 1.8k respectively).",
            "table_column_names": [
                "Model",
                "EE",
                "TE",
                "0-100",
                "100-500",
                "500+",
                "All"
            ],
            "table_content_values": [
                [
                    "RC (random initializations)",
                    "44.5",
                    "64.4",
                    "40.5",
                    "57.8",
                    "63.4",
                    "53.4"
                ],
                [
                    "RC (SG initializations)",
                    "49.5",
                    "68.6",
                    "44.1",
                    "62.5",
                    "67.0",
                    "57.3"
                ],
                [
                    "RC (SG fixed)",
                    "48.9",
                    "[BOLD] 68.7",
                    "44.1",
                    "62.7",
                    "67.3",
                    "57.6"
                ],
                [
                    "RC + SG",
                    "51.6",
                    "67.4",
                    "[BOLD] 46.4",
                    "62.5",
                    "66.8",
                    "58.2"
                ],
                [
                    "RC + SGLR",
                    "[BOLD] 51.7",
                    "68.5",
                    "45.3",
                    "[BOLD] 63.0",
                    "[BOLD] 68.1",
                    "[BOLD] 58.4"
                ]
            ]
        },
        "gold_description": "in table 1 we can see that also for our model , ee relations are harder to recognize than the te relations , as all models achieve higher scores for te compared to ee relations . what is interesting to see is that when training with the combined loss ( sg or sglr ) we obtain a clear improvement on the more difficult ee relations , and perform slightly worse on te relations compared to using pre - trained embeddings ( the three upper settings ) . what can be observed is that the rc + sg model performs best for low - frequency words , and rc + sglr performs best for the higher frequency ranges . when evaluating on the full dev set , both combined loss settings outperform the baselines consistently .",
        "generated_description": "sglr consistently improves over random initializations for all relation types , and outperforms rc with fixed sg initializations across all intervals . furthermore , the performance of rc with sg fixed initializations is consistently better than rc with initializations of 0 – 100 , 100 – 500 , and 500 + relations . this shows that initializing with sg is beneficial for relation types with low average argument token frequency in the training data ."
    },
    {
        "table_id": "74",
        "table_info": {
            "table_caption": "Table 2: THYME test set results, reporting precision (P), recall (R) and F-measure (F), macro-averaged over three runs. The standard deviation for F is also given.",
            "table_column_names": [
                "Model",
                "P",
                "R",
                "F"
            ],
            "table_content_values": [
                [
                    "[ITALIC] With specialized resources:",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Best Clinical TempEval (2016)",
                    "58.8",
                    "55.9",
                    "57.3"
                ],
                [
                    "Lin et al. (2016)",
                    "66.9",
                    "53.4",
                    "59.4"
                ],
                [
                    "Leeuwenberg et al. (2017)",
                    "-",
                    "-",
                    "60.8"
                ],
                [
                    "Tourille et al. (2017)",
                    "65.7",
                    "57.5",
                    "61.3"
                ],
                [
                    "Lin et al. (2017)",
                    "66.2",
                    "58.5",
                    "62.1"
                ],
                [
                    "[ITALIC] No specialized resources:",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "RC (random initialization)",
                    "67.9",
                    "52.1",
                    "58.9±0.2"
                ],
                [
                    "RC (SG initialization)",
                    "[BOLD] 71.2",
                    "52.0",
                    "60.0±1.2"
                ],
                [
                    "RC (SG fixed)",
                    "68.9",
                    "54.6",
                    "60.9±0.8"
                ],
                [
                    "RC + SG",
                    "66.2",
                    "[BOLD] 59.7",
                    "[BOLD] 62.8±0.2"
                ],
                [
                    "RC + SGLR",
                    "68.7",
                    "57.5",
                    "62.5±0.3"
                ]
            ]
        },
        "gold_description": "table 2 shows that initializing the model with the pre - trained embeddings gives a significant 4 1 . 1 point increase in f - measure compared to random initialization , due to an increase in precision . fixing the embeddings gives slightly better performance than using them as initialization , an increase of 0 . 9 point in f - measure , mostly due to higher recall . when extending the loss with the sglr loss , we gain6 1 . 6 in f - measure compared to fixing the word embeddings , and also surpass the state of the art by 0 . 4 even without specialized resources . if we train our model using the sg loss extension we obtain the best results , and gain6 1 . 9 points in f - measure compared to using pre - trained fixed word embeddings . this setting also exceeds the state of the art ( lin et al . , 2017 ) by 0 . 7 points in f - measure , due to a gain of 1 . 2 points in recall , again without using any specialized clinical nlp tools for feature engineering , in contrast to all state - of - the - art baselines .",
        "generated_description": "we compare the performance of our rc model with the state - of - the - art models and the baselines in the clinical tempeval 2016 challenge on the mimic - iii benchmark . the evaluation metric used in this benchmark is precision , recall , and f1 score . training with a random initialization of the sg embeddings leads to a significant drop in performance across the board . however , when we fix the embedding vectors during training and use the same sg embedding initialization as in the previous experiment , the performance significantly improves , ranging from 1 . 6 % to 3 . 8 % absolute in terms of f1 - score . this shows that our model benefits from having access to a larger set of specialized resources ."
    },
    {
        "table_id": "75",
        "table_info": {
            "table_caption": "Table 3: Error analysis on 50 FP and 50 FN (random from test) for different settings. Clause boundaries are: newlines and sub-clause or sentence boundaries. Error categories are not mutually exclusive.",
            "table_column_names": [
                "Error Type",
                "RC + SG",
                "RC (SG fixed)",
                "RC (SG init.)"
            ],
            "table_content_values": [
                [
                    "Cross-Clause Relations (CCR)",
                    "42",
                    "39",
                    "36"
                ],
                [
                    "Infrequent Arguments (<10)",
                    "11",
                    "15",
                    "26"
                ],
                [
                    "Frequent Arguments (>250)",
                    "37",
                    "50",
                    "40"
                ],
                [
                    "Mistake in Ground-Truth",
                    "10",
                    "8",
                    "5"
                ],
                [
                    "Other",
                    "21",
                    "15",
                    "28"
                ]
            ]
        },
        "gold_description": "from table 3 we can see that all models have difficulties with distant relations that cross sentence or clause boundaries ( ccr ) . furthermore , arguments that are frequent in the supervised data ( > 250 ) are a dominant error category . furthermore it can be noticed that rc + sg has less errors for infrequent arguments ( < 10 ) in the supervised data .",
        "generated_description": "we perform an error analysis on 50 fp and 50 fn ( random sample from test set ) to see how our model performs with respect to the ground truth . we notice that the initial sg setting performs better than rc for most of the error types . however , the performance of rc tends to be better for infrequent arguments and cross - clause relations ( ccr ) but worse for frequent arguments and mistake in ground - truth . a possible explanation for this is that the model has not yet learned to recognize the regular expression of the clause boundary , and thus it tends to insert new clauses that are not part of the same clause ( e . g . new lines ) ."
    },
    {
        "table_id": "76",
        "table_info": {
            "table_caption": "Table 2: Results for all the models on the three datasets in our experiment. Marked with ∗ are the results which significantly improve over LING and LING+random (p<0.05, also for the following results); ⋄ indicates a significant improvement over LING+PV; † a significant improvement over LING+N2V.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Sentiment",
                "[BOLD] Stance",
                "[BOLD] Hate"
            ],
            "table_content_values": [
                [
                    "Frequency",
                    "0.332",
                    "0.397",
                    "0.057"
                ],
                [
                    "LING",
                    "0.676",
                    "0.569",
                    "0.624"
                ],
                [
                    "LING+random",
                    "0.657",
                    "0.571",
                    "0.600"
                ],
                [
                    "LING+PV",
                    "0.671",
                    "0.601∗",
                    "0.667∗"
                ],
                [
                    "LING+N2V",
                    "0.672",
                    "0.629∗⋄",
                    "0.656∗"
                ],
                [
                    "LING+GAT",
                    "0.666",
                    "0.640∗⋄†",
                    "0.674∗⋄†"
                ]
            ]
        },
        "gold_description": "in table 2 we report the results , that we compute as the average of ten runs with random parameter initialization . 9 we use the unpaired welch ' s t test to check for statistically significant difference between models . the results show that social information helps improve the performance on stance and hate speech detection , while it has no effect for sentiment analysis . ling + random never improves over ling : we find that both pv and n2v user representations lead to an improvement over ling . where ling + n2v outperforms ling + pv , while for hate speech the performance of the two models is comparable ( the difference between ling + pv and ling + n2v is not statistically significative due to the high variance of the ling + pv results - see extended results table in the supplementary material ) . our model outperforms any other model on both stance and hate speech detection .",
        "generated_description": "results for all the models are shown in tab . as can be seen , ling + n2v significantly outperforms ling in all three tasks , and in two of the cases , the difference is statistically significant ( p < 0 . 05 , also for the following results ) . the difference between random and pv is not statistically significant ."
    },
    {
        "table_id": "77",
        "table_info": {
            "table_caption": "Table 1: Overview of annotated parallel sentences per language pair",
            "table_column_names": [
                "[BOLD] Languages  [BOLD] EN–BG",
                "[BOLD] # sents 306,380",
                "[BOLD] Languages  [BOLD] EN–IT",
                "[BOLD] # sents 1,297,635"
            ],
            "table_content_values": [
                [
                    "[BOLD] EN–CS",
                    "491,848",
                    "[BOLD] EN–LT",
                    "481,570"
                ],
                [
                    "[BOLD] EN–DA",
                    "1,421,197",
                    "[BOLD] EN–LV",
                    "487,287"
                ],
                [
                    "[BOLD] EN–DE",
                    "1,296,843",
                    "[BOLD] EN–NL",
                    "1,419,359"
                ],
                [
                    "[BOLD] EN–EL",
                    "921,540",
                    "[BOLD] EN–PL",
                    "478,008"
                ],
                [
                    "[BOLD] EN–ES",
                    "1,419,507",
                    "[BOLD] EN–PT",
                    "1,426,043"
                ],
                [
                    "[BOLD] EN–ET",
                    "494,645",
                    "[BOLD] EN–RO",
                    "303,396"
                ],
                [
                    "[BOLD] EN–FI",
                    "1,393,572",
                    "[BOLD] EN–SK",
                    "488,351"
                ],
                [
                    "[BOLD] EN–FR",
                    "1,440,620",
                    "[BOLD] EN–SL",
                    "479,313"
                ],
                [
                    "[BOLD] EN–HU",
                    "251,833",
                    "[BOLD] EN–SV",
                    "1,349,472"
                ]
            ]
        },
        "gold_description": "an overview of the language pairs as well as the amount of annotated parallel sentences per language pair is given in table 1 . overview of annotated parallel sentences per language pair",
        "generated_description": "for simplicity , we focus on the number of sentences , i . e . number of segments , to show the size of the extracted segmentation results . the number of inserted segmented sentences is proportional to number of words in the original text . for instance , for the language group bulgarian ( see en – bg ) , we obtain 1 . 4m fewer sentences than english ( 1 . 2m vs . 1 . 7m ) . for other language groups , such as english - bulgarian ( en – bg ) and english - romanian , we obtain more than 1 . 6m and 1 . 8m sentence pairs , respectively . in addition , we also acquire 1 . 3m and 2 . 1m wikipedia articles for bulgarian and bulgarian ( en - bg and en - bl ) , respectively . we also acquire a large number of articles for several language groups including english - portuguese ."
    },
    {
        "table_id": "78",
        "table_info": {
            "table_caption": "Table 4: Types of discrepancy in context-agnostic translation caused by ellipsis",
            "table_column_names": [
                "[BOLD] type of discrepancy",
                "[BOLD] frequency"
            ],
            "table_content_values": [
                [
                    "wrong morphological form",
                    "66%"
                ],
                [
                    "wrong verb (VP-ellipsis)",
                    "20%"
                ],
                [
                    "other error",
                    "14%"
                ]
            ]
        },
        "gold_description": "we classified ellipsis examples which lead to errors in sentence - level translations by the type of error they cause . results are provided in table 4 . from table 4 , we see that the two most frequent types of ambiguity caused by the presence of an elliptical structure have different nature , hence we construct individual test sets for each of them .",
        "generated_description": "in order to better understand the types of discrepancy in context - agnostic translation caused by ellipsis , we perform an error analysis . we randomly sample 20 % of the sentences in the validation set of the mscoco test set that do not contain any ellipse . the most common error is caused by wrong morphological form ( 66 % ) , followed by wrong verb ( 20 % ) and other error ( 14 % ) . we observe that the most frequent error type involves the derivation of wrong verbs ( vp - ellipsis ) , while morphological forms are the least common error ."
    },
    {
        "table_id": "79",
        "table_info": {
            "table_caption": "Table 1: Results. Dev Acc. represents accuracy on SST-2 dev set. F - M represents difference between means of predicted positive class probabilities for sentences with female nouns and sentences with male nouns. ** denotes statistical significance with p<0.01 (after applying Bonferroni correction).",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Dev Acc.",
                "[BOLD] F - M"
            ],
            "table_content_values": [
                [
                    "[BOLD] M.1 (BoW+LogReg)",
                    "0.827",
                    "0.035**"
                ],
                [
                    "[BOLD] M.2 (BiLSTM)",
                    "0.841",
                    "0.077**"
                ],
                [
                    "[BOLD] M.3 (BERT)",
                    "0.930",
                    "-0.040**"
                ]
            ]
        },
        "gold_description": "the main results of our experiments are shown in table 1 . we notice that m . 1 ( bag - of - words + logistic regression ) and m . 2 ( bilstm ) show a statistically significant difference between the two genders , with higher predicted positive class probabilities for sentences with female nouns . on the contrary , m . 3 ( bert ) shows that sentences with male nouns have a statistically significant higher predicted positive class probability than sentences with female nouns .",
        "generated_description": "we observe that both bilstm and bert models significantly outperform the bow + logreg model , with bert being the best performing model on the dev set . we also observe that the model with the highest accuracy on the development set , m . 3 ( bert ) , is significantly different from the models with the lowest accuracy ( m . 1 ) on the sst - 2 dev set ( p < 0 . 01 using bonferroni correction ) . therefore , we conclude that bert may be overfitting the training data ."
    },
    {
        "table_id": "80",
        "table_info": {
            "table_caption": "Table 1: Average cosine similarity score and Delta-E distance over 5 runs. A smaller Delta-E distance means a less significant difference between two colors. Bold: best performance. Hard: the hard ensemble model. WM18∗: the performance from WM18 paper. See Supplementary Material for example outputs and ensemble analysis.",
            "table_column_names": [
                "Test Condition",
                "[BOLD] Cosine Similarity ± SD  (↑)  [BOLD] RGB",
                "[BOLD] Cosine Similarity ± SD  (↑)  [BOLD] WM18",
                "[BOLD] Cosine Similarity ± SD  (↑)  [BOLD] HSV",
                "[BOLD] Cosine Similarity ± SD  (↑)  [BOLD] Ensemble",
                "[BOLD] Cosine Similarity ± SD  (↑)  [BOLD] WM18∗"
            ],
            "table_content_values": [
                [
                    "Seen Pairings",
                    "0.954±0.001",
                    "0.953±0.000",
                    "0.934±0.089",
                    "[BOLD] 0.954±0.0",
                    "0.68"
                ],
                [
                    "Unseen Pairings",
                    "0.799±0.044",
                    "0.771±0.032",
                    "[BOLD] 0.843±0.144",
                    "0.797±0.0",
                    "0.68"
                ],
                [
                    "Unseen Ref. Color",
                    "0.781±0.015",
                    "0.767±0.010",
                    "[BOLD] 0.945±0.019",
                    "0.804±0.0",
                    "0.40"
                ],
                [
                    "Unseen Modifiers",
                    "0.633±0.042",
                    "0.637±0.032",
                    "[BOLD] 0.724±0.131",
                    "0.629±0.0",
                    "0.41"
                ],
                [
                    "Fully Unseen",
                    "0.370±0.029",
                    "0.358±0.038",
                    "[BOLD] 0.919±0.026",
                    "0.445±0.0",
                    "-0.21"
                ],
                [
                    "Overall",
                    "0.858±0.006",
                    "0.856±0.003",
                    "[BOLD] 0.911±0.057",
                    "0.868±0.0",
                    "0.65"
                ],
                [
                    "[EMPTY]",
                    "[BOLD] Delta-E ± SD  (↓)",
                    "[BOLD] Delta-E ± SD  (↓)",
                    "[BOLD] Delta-E ± SD  (↓)",
                    "[BOLD] Delta-E ± SD  (↓)",
                    "[BOLD] Delta-E ± SD  (↓)"
                ],
                [
                    "Test Condition",
                    "RGB",
                    "WM18",
                    "HSV",
                    "Ensemble",
                    "WM18∗"
                ],
                [
                    "Seen Pairings",
                    "[BOLD] 3.121±0.027",
                    "3.188±0.062",
                    "5.380±4.846",
                    "4.093±0.1",
                    "6.1"
                ],
                [
                    "Unseen Pairings",
                    "6.454±0.233",
                    "6.825±0.093",
                    "11.701±3.358",
                    "[BOLD] 5.873±0.0",
                    "7.9"
                ],
                [
                    "Unseen Ref. Color",
                    "7.456±0.184",
                    "7.658±0.363",
                    "10.429±2.523",
                    "[BOLD] 7.171±0.0",
                    "11.4"
                ],
                [
                    "Unseen Modifiers",
                    "13.288±1.082",
                    "13.891±1.077",
                    "14.183±5.175",
                    "10.927±0.0",
                    "[BOLD] 10.5"
                ],
                [
                    "Fully Unseen",
                    "13.859±0.874",
                    "14.516±0.587",
                    "[BOLD] 12.432±2.170",
                    "13.448±0.0",
                    "15.9"
                ],
                [
                    "Overall",
                    "[BOLD] 5.412±0.169",
                    "5.595±0.128",
                    "7.487±3.940",
                    "5.777±0.0",
                    "6.8"
                ]
            ]
        },
        "gold_description": "because of the small size of the dataset , we report the average performance over 5 runs with different random seeds . table 1 shows the results . compared with wm18 , our rgb model outperforms under all conditions . according to the cosine similarity , the hsv model is superior for most test conditions ( confirming our hypothesis about simpler modifier behaviour in this space ) . however for delta - e , the rgb model and ensemble perform better . unlike cosine , delta - e is sensitive to differences in vector length , and we would argue it is the most appropriate metric because lengths are critical to measuring the extent of lightness and darkness of colors . accordingly the hsv model does worse under this metric , as it more directly models the direction of color modifiers , but as a consequence this leads to errors in its length predictions . over - all the ensemble does well according to both met rics , and has the best performance for several test conditions with delta - e .",
        "generated_description": "in order to evaluate the performance of the ensembled model , we use the standard dot - product similarity metric to compute the cosine similarity between the word embedding vector and the bleu score . the metric is defined as follows : for each test condition , we report the mean and standard deviation of cosine similarities of the embedding vectors of the model ’ s predictions on the test set . seen pairings : this is the subset of the test conditions that are present in the training set . unseen relationships : these are the unseen pairs that are not present in training set but are part of the unseen relations test set ( i . e . are present at test time ) . unseen modifiers : the unseen modifiers are the ones that were not present at training time . in this subset of test conditions , we also report the results of the ensemble model . the standard deviation is computed by summing the standard deviations of the individual models ’ predictions across all test conditions . the results show that the ensemble achieves the best performance in the unseen pairings and unseen modifiers . in addition , the ensemble outperforms the simple dot product similarity metric in the case of both rgb and hsv . this is due to the fact that the combination of the two similarity measures ( cosine similarity and similarity sd ) produces better results than the individual similarity measures alone ."
    },
    {
        "table_id": "81",
        "table_info": {
            "table_caption": "Table 6: BLEU scores. CADec trained with p=0.5. Scores for CADec are not statistically different from the baseline (6m).",
            "table_column_names": [
                "[BOLD] model",
                "[BOLD] BLEU"
            ],
            "table_content_values": [
                [
                    "baseline (1.5m)",
                    "29.10"
                ],
                [
                    "baseline (6m)",
                    "[BOLD] 32.40"
                ],
                [
                    "concat",
                    "31.56"
                ],
                [
                    "s-hier-to-2.tied",
                    "26.68"
                ],
                [
                    "CADec",
                    "[BOLD] 32.38"
                ]
            ]
        },
        "gold_description": "bleu scores for our model and the baselines are given in table 6 . 5 for context - aware models , all sentences in a group were translated , and then only the current sentence is evaluated . we also report bleu for the context - agnostic baseline trained only on 1 . 5m dataset to show how the performance is influenced by the amount of data . we observe that our model is no worse in bleu than the baseline despite the second - pass model being trained only on a fraction of the data . in contrast , the concatenation baseline , trained on a mixture of data with and without context is about 1 bleu below the context - agnostic baseline and our model when using all 3 context sentences . cadec ' s performance remains the same independently from the number of context sentences ( 1 , 2 or 3 ) as measured with bleu . s - hier - to - 2 . tied performs worst in terms of bleu , but note that this is a shallow recurrent model , while others are transformer - based . it also suffers from the asymmetric data setting , like the concatenation baseline .",
        "generated_description": "results for cadec trained with p = 0 . 5 are shown in table 6 . these results show that cadec outperforms the baseline by 0 . 38 bleu points ."
    },
    {
        "table_id": "82",
        "table_info": {
            "table_caption": "Table 2: F&C dataset size. All Labels represent the original dataset with all the labels. Subset Labels are the subset labels which are inferable by the resource.",
            "table_column_names": [
                "All Labels",
                "Orig F&C",
                "Train 587",
                "Dev 5,418",
                "Test 6,007",
                "All 12,012"
            ],
            "table_content_values": [
                [
                    "All Labels",
                    "No-Leak F&C",
                    "712",
                    "3,000",
                    "4,497",
                    "8,209"
                ],
                [
                    "Subset Labels",
                    "Orig F&C",
                    "361",
                    "3,311",
                    "3,650",
                    "7,322"
                ],
                [
                    "Subset Labels",
                    "Clean F&C",
                    "173",
                    "1,268",
                    "1,523",
                    "2,964"
                ]
            ]
        },
        "gold_description": "we reorganized the train / dev / test sets , forming new splits , which we refer to as no - leak f & c . the new split sizes can be found in table 2 . we re - ran the current models on no - leak f & c and , as expected , we observe a drop of 5 - 6 % in accuracy : from the original 76 % accuracy on the dev / test sets , to 70 % and 71 % accuracy , respectively .",
        "generated_description": "the f & c dataset is divided into a train set , a dev set , and a test set . we use the dev set to tune the parameters , and use the test set for hyperparameter tuning . we train the model on the original dataset , and evaluate it on the dev and test sets . we also evaluate the model ’ s performance on the “ no - leak ” and “ clean ” versions of the dataset , which denote the cases where the label is not known in advance . we find that the model performs well on the clean labels , but poorly on the noisy ones . the dataset has a large number of noisy labels ( observed by the annotation resource ) , which poses a challenge for the model to learn to distinguish between the different labels ."
    },
    {
        "table_id": "83",
        "table_info": {
            "table_caption": "Table 4: Results on the noun comparison datasets.",
            "table_column_names": [
                "Model/Dataset",
                "F&C Clean Dev",
                "F&C Clean Test",
                "New Data Dev",
                "New Data Test"
            ],
            "table_content_values": [
                [
                    "Majority",
                    "0.54",
                    "0.57",
                    "0.51",
                    "0.50"
                ],
                [
                    "Yang et al. (PCE LSTM)",
                    "[BOLD] 0.86",
                    "[BOLD] 0.87",
                    "0.60",
                    "0.57"
                ],
                [
                    "DoQ",
                    "0.78",
                    "0.77",
                    "[BOLD] 0.62",
                    "[BOLD] 0.62"
                ],
                [
                    "DoQ + 10-distance",
                    "0.78",
                    "0.77",
                    "[BOLD] 0.62",
                    "[BOLD] 0.62"
                ],
                [
                    "DoQ + 3-distance",
                    "0.81",
                    "0.80",
                    "[BOLD] 0.62",
                    "0.61"
                ]
            ]
        },
        "gold_description": "the left column of table 4 presents results for the cleaned version of the forbes and choi ( 2017 ) dataset . results on the new objects comparison dataset we created are shown in the rightmost column of table 4 . we get better results than previous methods on this dataset : 63 % and 61 % accuracy on the dev / test sets compared to 60 % and 57 % . these relatively low results on this new dataset indicate that it is more challenging .",
        "generated_description": "the doq model outperforms the majority baseline by a large margin in both clean and new data conditions , with 10 - distance and 3 - distance sampling giving the best performance on the new data ."
    },
    {
        "table_id": "84",
        "table_info": {
            "table_caption": "Table 5: Results on the Relative dataset. Yang et al. (2018) result was achieved by running their model on their training set, and using it as a transfer method on Relative. Finally, we present our own predictions, with different thresholds, which surpass previous work.",
            "table_column_names": [
                "Model",
                "Accuracy"
            ],
            "table_content_values": [
                [
                    "Chance",
                    "0.5"
                ],
                [
                    "Bagherinezhad et al.",
                    "0.835"
                ],
                [
                    "Yang et al. (Transfer)",
                    "0.858"
                ],
                [
                    "DoQ",
                    "0.872"
                ],
                [
                    "DoQ + 10-distance",
                    "[BOLD] 0.877"
                ],
                [
                    "DoQ + 3-distance",
                    "0.858"
                ]
            ]
        },
        "gold_description": "noun comparatives is on relative ( bagherinezhad et al . , 2016 ) , presented in table 5 . we report the results of the original work , where the best score used a combination of visual and textual signals , achieving 83 . 5 % accuracy . we also tested the method by yang et al . ( 2018 ) on this dataset . the accuracy achieved by this method is 85 . 8 % , surpassing the previous method by more than 2 points . we evaluated our method on this dataset , achieving a new state - of - the - art result of 87 . 7 % accuracy with k = 10 as a filter method .",
        "generated_description": "finally , we present our own models with different thresholds , with 10 - distance and 3 - distance , which surpass previous work . yang et al . ’ s result was achieved by running their model on their training set , and using it as a transfer method on relative . we transfer the transfer learning method to the relative dataset to evaluate the quality of the generated questions . we present results for the best performing transfer method , which is doq + 10 - distance with a score of 0 . 877 , which achieves a new state - of - the - art ."
    },
    {
        "table_id": "85",
        "table_info": {
            "table_caption": "Table 7: Intrinsic Evaluation. Accuracy of the number of objects which our proposed median fall into range of the object, given the dimension.",
            "table_column_names": [
                "Method/Data",
                "Mass",
                "Length",
                "Speed",
                "Currency",
                "All"
            ],
            "table_content_values": [
                [
                    "Indian Annotators",
                    "0.61",
                    "0.79",
                    "0.77",
                    "0.58",
                    "0.69"
                ],
                [
                    "US Annotators",
                    "-",
                    "-",
                    "-",
                    "0.76",
                    "-"
                ]
            ]
        },
        "gold_description": "the results of the intrinsic evaluation on a sample of doq are shown in table 7 . the total agreement is 69 % , while the specific agreements for mass , length , speed and currency are 61 % , 79 % , 77 % and 58 % respectively . we re - annotated the samples in the currency category with annotators from the u . s . and found a much higher agreement score : 76 % .",
        "generated_description": "we perform an intrinsic evaluation of our object embedding model by asking annotators to identify the number of objects in the corpus which fall into range of the described object , given a given dimension . annotators are given a list of annotated objects and their speed , mass , length , and currency . they are then asked to select the 1 , 000 most frequently occurring objects from this list . accuracy is evaluated on a held - out subset of the annotated corpus , in order to see how well the annotators approximate the expected performance . we observe that the indian annotators perform better than the us annotators on average . however , the performance of our proposed model is competitive with the us annotation ."
    },
    {
        "table_id": "86",
        "table_info": {
            "table_caption": "Table 1: Significant components of our logistic regression model using the Coh-Metrix features. Variables are also separated by their association with either satire or fake news. Bold: the remaining features following the step-wise backward elimination. Note: *** p < 0.001, ** p < 0.01, * p < 0.05.",
            "table_column_names": [
                "Satire associated",
                "PCA Component  [BOLD] RC19",
                "Description First person singular pronoun incidence",
                "estimate 1.80",
                "std.error 0.41",
                "statistic 4.38",
                "***"
            ],
            "table_content_values": [
                [
                    "Satire associated",
                    "[BOLD] RC5",
                    "Sentence length, number of words",
                    "0.66",
                    "0.18",
                    "3.68",
                    "***"
                ],
                [
                    "Satire associated",
                    "[BOLD] RC15",
                    "Estimates of hypernymy for nouns",
                    "0.61",
                    "0.19",
                    "3.18",
                    "**"
                ],
                [
                    "Satire associated",
                    "[BOLD] RC49",
                    "Word Concreteness",
                    "0.54",
                    "0.17",
                    "3.18",
                    "**"
                ],
                [
                    "Satire associated",
                    "[BOLD] RC35",
                    "Ratio of casual particles to causal verbs",
                    "0.56",
                    "0.18",
                    "3.10",
                    "**"
                ],
                [
                    "Satire associated",
                    "[BOLD] RC91",
                    "Text Easability PC Referential cohesion",
                    "0.45",
                    "0.16",
                    "2.89",
                    "**"
                ],
                [
                    "Satire associated",
                    "[BOLD] RC20",
                    "Incidence score of gerunds",
                    "0.43",
                    "0.16",
                    "2.77",
                    "**"
                ],
                [
                    "Satire associated",
                    "RC32",
                    "Expanded temporal connectives incidence",
                    "0.44",
                    "0.16",
                    "2.75",
                    "**"
                ],
                [
                    "Satire associated",
                    "[BOLD] RC9",
                    "Third person singular pronoun incidence",
                    "0.44",
                    "0.16",
                    "2.67",
                    "**"
                ],
                [
                    "Satire associated",
                    "RC43",
                    "Word length, number of letters",
                    "0.45",
                    "0.20",
                    "2.27",
                    "*"
                ],
                [
                    "Satire associated",
                    "RC46",
                    "Verb phrase density",
                    "0.37",
                    "0.16",
                    "2.25",
                    "*"
                ],
                [
                    "Satire associated",
                    "[BOLD] RC97",
                    "Coh-Metrix L2 Readability",
                    "0.34",
                    "0.16",
                    "2.16",
                    "*"
                ],
                [
                    "Satire associated",
                    "[BOLD] RC61",
                    "Average word frequency for all words",
                    "0.50",
                    "0.24",
                    "2.13",
                    "*"
                ],
                [
                    "Satire associated",
                    "RC84",
                    "The average givenness of each sentence",
                    "0.37",
                    "0.18",
                    "2.11",
                    "*"
                ],
                [
                    "Satire associated",
                    "RC65",
                    "Text Easability PC Syntactic simplicity",
                    "0.38",
                    "0.18",
                    "2.08",
                    "*"
                ],
                [
                    "Satire associated",
                    "RC50",
                    "Lexical diversity",
                    "0.37",
                    "0.18",
                    "2.05",
                    "*"
                ],
                [
                    "Fake news associated",
                    "[BOLD] RC30",
                    "Agentless passive voice density",
                    "-1.05",
                    "0.21",
                    "-4.96",
                    "***"
                ],
                [
                    "Fake news associated",
                    "[BOLD] RC73",
                    "Average word frequency for content words",
                    "-0.72",
                    "0.20",
                    "-3.68",
                    "***"
                ],
                [
                    "Fake news associated",
                    "[BOLD] RC59",
                    "Adverb incidence",
                    "-0.62",
                    "0.18",
                    "-3.43",
                    "***"
                ],
                [
                    "Fake news associated",
                    "[BOLD] RC55",
                    "Number of sentences",
                    "-0.79",
                    "0.26",
                    "-3.09",
                    "**"
                ],
                [
                    "Fake news associated",
                    "RC62",
                    "Causal and intentional connectives",
                    "-0.42",
                    "0.15",
                    "-2.72",
                    "**"
                ],
                [
                    "Fake news associated",
                    "[BOLD] RC34",
                    "LSA overlap between verbs",
                    "-0.35",
                    "0.16",
                    "-2.22",
                    "*"
                ],
                [
                    "Fake news associated",
                    "[BOLD] RC44",
                    "LSA overlap, adjacent sentences",
                    "-0.36",
                    "0.16",
                    "-2.16",
                    "*"
                ],
                [
                    "Fake news associated",
                    "RC47",
                    "Sentence length, number of words",
                    "-0.36",
                    "0.18",
                    "-2.03",
                    "*"
                ],
                [
                    "Fake news associated",
                    "RC89",
                    "LSA overlap, all sentences in paragraph",
                    "-0.34",
                    "0.17",
                    "-1.97",
                    "*"
                ],
                [
                    "[EMPTY]",
                    "(Intercept)",
                    "[EMPTY]",
                    "-0.54",
                    "0.19",
                    "-2.91",
                    "[EMPTY]"
                ]
            ]
        },
        "gold_description": "as our dependent variable . significant features of the logistic regression model are shown in table 1 with the respective significance levels . we also run a step - wise backward elimination regression . those components that are also significant in the step - wise model appear in bold . observing the significant features , in bold in table 1 , we see a combination of surface level related features , such as sentence length and average word frequency , as well as semantic features including lsa ( latent semantic analysis ) overlaps between verbs and between adjacent sentences . semantic features which are associated with the gist representation of content are particularly interesting to see among the predictors since based on fuzzytrace theory ( reyna , 2012 ) , a well - known theory of decision making under risk , gist representation of content drives individual ' s decision to spread misinformation online . also among the significant features , we observe the causal connectives , that are proven to be important in text comprehension , and two indices related to the text easability and readability , both suggesting that satire articles are more sophisticated , or less easy to read , than fake news articles .",
        "generated_description": "for instance , the pca component for the first person singular pronoun in a satire has a value of 1 . 80 , which is the proportion of singular pronouns that appear in the training data , and a std . error of 4 . 38 , meaning that the incidence of such a pronoun in the test data is significantly different from that seen in training data . the word concreteness component in the word concreteness metric ( rc49 ) measures the word embeddings of nouns and adjectives that are described in satire . a value of 0 . 54 indicates that a noun is described as “ very concrete ” ( meaning it is highly likely to appear in multiple nouns ) , while a score of 3 . 18 indicates that it is “ peaky ” . the pca component for the casual particles and causal verbs in the rc35 and rc91 metrics are the same , which indicates that casual particles are infrequent in satire , while causal verbs are more frequent in general . in addition , pca components for the expanded temporal connectives in the incidence score ( rc32 and rc20 ) are also found to be the most important ones in the causal verbs and gerunds in the satire ."
    },
    {
        "table_id": "87",
        "table_info": {
            "table_caption": "Table 2: Results of classification between fake news and satire articles using BERT pre-trained models, based on the headline, body and full text. Bold: best performing model. P: Precision, and R: Recall",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] P",
                "[BOLD] R",
                "[BOLD] F1"
            ],
            "table_content_values": [
                [
                    "Headline only",
                    "0.46",
                    "0.89",
                    "0.61"
                ],
                [
                    "Text body only",
                    "0.78",
                    "0.52",
                    "0.62"
                ],
                [
                    "Headline + text body",
                    "[BOLD] 0.81",
                    "[BOLD] 0.75",
                    "[BOLD] 0.78"
                ]
            ]
        },
        "gold_description": "text of a story , and in combination . results are shown in table 2 . the models based on the headline and text body give a similar f1 score . however , while the headline model performs poorly on precision , perhaps due to the short text , the model based on the text body performs poorly on recall . the model based on the full text of headline and body gives the best performance .",
        "generated_description": "in order to classify the fake news and satire articles , we use bert pre - trained models , based on the headline , body and full text of the news articles , using precision , recall and f1 score . the model with the headline and text body combined achieves the best performance , obtaining 0 . 81 precision , 0 . 75 recall and an f1 - score of 0 . 78 . in contrast , the model with only the text body alone performs poorly , obtaining a precision of only 0 . 52 . this shows the importance of including the whole article along with the text ."
    },
    {
        "table_id": "88",
        "table_info": {
            "table_caption": "Table 7: Accuracy for deixis and lexical cohesion.",
            "table_column_names": [
                "[EMPTY]",
                "total",
                "latest relevant context 1st",
                "latest relevant context 2nd",
                "latest relevant context 3rd"
            ],
            "table_content_values": [
                [
                    "[BOLD] deixis",
                    "[BOLD] deixis",
                    "[BOLD] deixis",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "baseline",
                    "50.0",
                    "50.0",
                    "50.0",
                    "50.0"
                ],
                [
                    "concat",
                    "[BOLD] 83.5",
                    "[BOLD] 88.8",
                    "[BOLD] 85.6",
                    "[BOLD] 76.4"
                ],
                [
                    "s-hier-to-2.tied",
                    "60.9",
                    "83.0",
                    "50.1",
                    "50.0"
                ],
                [
                    "CADec",
                    "81.6",
                    "84.6",
                    "84.4",
                    "75.9"
                ],
                [
                    "[BOLD] lexical cohesion",
                    "[BOLD] lexical cohesion",
                    "[BOLD] lexical cohesion",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "baseline",
                    "45.9",
                    "46.1",
                    "45.9",
                    "45.4"
                ],
                [
                    "concat",
                    "47.5",
                    "48.6",
                    "46.7",
                    "46.7"
                ],
                [
                    "s-hier-to-2.tied",
                    "48.9",
                    "53.0",
                    "46.1",
                    "45.4"
                ],
                [
                    "CADec",
                    "[BOLD] 58.1",
                    "[BOLD] 63.2",
                    "[BOLD] 52.0",
                    "[BOLD] 56.7"
                ]
            ]
        },
        "gold_description": "for all tasks , we observe a large improvement from using context . for deixis , the concatenation model ( concat ) and cadec improve over the baseline by 33 . 5 and 31 . 6 percentage points , respectively . on the lexical cohesion test set , cadec shows a large improvement over the context - agnostic baseline ( 12 . 2 percentage points ) , while concat performs similarly to the baseline . when looking only at the scores where the latest relevant context is in the model ' s context window ( column 2 in table 7 ) , s - hier - to - 2 . tied outperforms the concatenation baseline for lexical cohesion , but remains behind the performance of cadec .",
        "generated_description": "as expected , the best performing model is the one using the concatenation of the first relevant context and the latest relevant context . however , the results for the deixis and lexical cohesion tasks are not as good as for the context - aware model . this is likely due to the fact that these tasks are harder to learn with the current context , as they require deeper understanding of the context than the ones used in the baseline and cadec models . in addition , cadec performs better than the s - hier - to - 2 . tied model in terms of using the same relevant context as the baseline model , which suggests that the model benefits from having access to the same amount of relevant context in the first iteration of the model ."
    },
    {
        "table_id": "89",
        "table_info": {
            "table_caption": "Table 3: Summary of results of classification between fake news and satire articles using the baseline Multinomial Naive Bayes method, the linguistic cues of text coherence and semantic representation with a pre-trained BERT model. Statistically significant differences with the baseline are marked with ’*’. Bold: best performing model. P: Precision, and R: Recall. For Coh-Metrix, we report the mean Precision, Recall, and F1 on the test set.",
            "table_column_names": [
                "[BOLD] Method",
                "[BOLD] P",
                "[BOLD] R",
                "[BOLD] F1"
            ],
            "table_content_values": [
                [
                    "Baseline",
                    "0.70",
                    "0.64",
                    "0.67"
                ],
                [
                    "Coh-Metrix",
                    "0.72",
                    "0.66",
                    "0.74*"
                ],
                [
                    "Pre-trained BERT",
                    "[BOLD] 0.81",
                    "[BOLD] 0.75",
                    "[BOLD] 0.78*"
                ]
            ]
        },
        "gold_description": "table 3 provides a summary of the results . we compare the results of our methods of the pre - trained bert , using both the headline and text body , and the coh - mertix approach , to the language - based baseline with multinomial naive bayes from ( golbeck et al . , 2018 ) 2 . both the semantic cues with bert and the linguistic cues with coh - metrix significantly outperform the baseline on the f1 score . the two - tailed paired t - test with a 0 . 05 significance level was used for testing statistical significance of performance differences . the best result is given by the bert model . overall , these results provide an answer to research question rq1 regarding the existence of semantic and linguistic difference between fake news and satire .",
        "generated_description": "the linguistic cues of text coherence and semantic representation were extracted using a pre - trained bert model . we then trained a model on the corpus of fake news and satire articles and tested it on the test set of the coh coh - metrix task . for the baseline model , we used the multinomial naive bayes classifier with linear kernel we report the mean precision , recall , and f1 - score of the classifier using the number of articles classified as fake news , satire articles , and non - fake news . the results show that the model trained on a large corpus of news outperformed the baseline by a significant margin . in addition , the results of the pre - processed bert shows statistically significant differences with the baseline when the model is applied to the coh - metrix test set ."
    },
    {
        "table_id": "90",
        "table_info": {
            "table_caption": "Table 1: F1 scores on AIDA-B (test set).",
            "table_column_names": [
                "Methods",
                "Aida-B"
            ],
            "table_content_values": [
                [
                    "Q15-1011",
                    "88.7"
                ],
                [
                    "guorobust",
                    "89.0"
                ],
                [
                    "P16-1059",
                    "91.0"
                ],
                [
                    "K16-1025",
                    "91.5"
                ],
                [
                    "D17-1276",
                    "92.22±0.14"
                ],
                [
                    "rel-norm",
                    "92.41±0.19"
                ],
                [
                    "ment-norm",
                    "[BOLD] 93.07±0.27"
                ],
                [
                    "ment-norm ( [ITALIC] K=1)",
                    "92.89±0.21"
                ],
                [
                    "ment-norm (no pad)",
                    "92.37±0.26"
                ]
            ]
        },
        "gold_description": "table 1 shows micro f1 scores on aida - b of the sota methods and ours , which all use wikipedia and yago mention - entity index . to our knowledge , ours are the only ( unsupervisedly ) inducing and employing more than one relations on this dataset . the others use only one relation , coreference , which is given by simple heuristics or supervised third - party resolvers . all four our models outperform any previous method , with ment - norm achieving the best results , 0 . 85 % higher than that of ganea and hofmann ( 2017 ) . the experimental results show that ment - norm outperforms rel - norm , and that mention padding plays an important role .",
        "generated_description": "the ment - norm method outperforms the previous state - of - the - art methods on aida - b dataset by a large margin . we also compare our results with the supervised baselines , including guorobust and the original mentoring model . we observe that the ment - regularized version of our model ( no pad ) is comparable to the supervised rel - norm , which is the one used by \\ newcited17 - 1276 . this result shows that our unsupervised learning of embeddings can benefit from the supervision of the word embedding matrix . we leave the implementation of the pad embedding as our future work ."
    },
    {
        "table_id": "91",
        "table_info": {
            "table_caption": "Table 2: F1 scores on five out-domain test sets. Underlined scores show cases where the corresponding model outperforms the baseline.",
            "table_column_names": [
                "Methods",
                "MSNBC",
                "AQUAINT",
                "ACE2004",
                "CWEB",
                "WIKI",
                "Avg"
            ],
            "table_content_values": [
                [
                    "milne2008learning",
                    "78",
                    "85",
                    "81",
                    "64.1",
                    "81.7",
                    "77.96"
                ],
                [
                    "D11-1072",
                    "79",
                    "56",
                    "80",
                    "58.6",
                    "63",
                    "67.32"
                ],
                [
                    "P11-1138",
                    "75",
                    "83",
                    "82",
                    "56.2",
                    "67.2",
                    "72.68"
                ],
                [
                    "cheng-roth:2013:EMNLP",
                    "90",
                    "[BOLD] 90",
                    "86",
                    "67.5",
                    "73.4",
                    "81.38"
                ],
                [
                    "guorobust",
                    "92",
                    "87",
                    "88",
                    "77",
                    "[BOLD] 84.5",
                    "[BOLD] 85.7"
                ],
                [
                    "D17-1276",
                    "93.7 ± 0.1",
                    "88.5 ± 0.4",
                    "88.5 ± 0.3",
                    "[BOLD] 77.9 ± 0.1",
                    "77.5 ± 0.1",
                    "85.22"
                ],
                [
                    "rel-norm",
                    "92.2 ± 0.3",
                    "86.7 ± 0.7",
                    "87.9 ± 0.3",
                    "75.2 ± 0.5",
                    "76.4 ± 0.3",
                    "83.67"
                ],
                [
                    "ment-norm",
                    "[BOLD] 93.9 ± 0.2",
                    "88.3 ± 0.6",
                    "[BOLD] 89.9 ± 0.8",
                    "77.5 ± 0.1",
                    "78.0 ± 0.1",
                    "85.51"
                ],
                [
                    "ment-norm ( [ITALIC] K=1)",
                    "93.2 ± 0.3",
                    "88.4 ± 0.4",
                    "88.9 ± 1.0",
                    "77.0 ± 0.2",
                    "77.2 ± 0.1",
                    "84.94"
                ],
                [
                    "ment-norm (no pad)",
                    "93.6 ± 0.3",
                    "87.8 ± 0.5",
                    "[BOLD] 90.0 ± 0.3",
                    "77.0 ± 0.2",
                    "77.3 ± 0.3",
                    "85.13"
                ]
            ]
        },
        "gold_description": "table 2 shows micro f1 scores on 5 out - domain test sets . besides ours , only cheng and roth ( 2013 ) employs several mention relations . mentnorm achieves the highest f1 scores on msnbc and ace2004 . on average , ment - norm ' s f1 score is 0 . 3 % higher than that of ganea and hofmann ( 2017 ) , but 0 . 2 % lower than guo and barbosa ( 2016 ) ' s . it is worth noting that guo and barbosa ( 2016 ) performs exceptionally well on wiki , but substantially worse than ment - norm on all other datasets . our other three models , however , have lower average f1 scores compared to the best previous model . the experimental results show that ment - norm outperforms rel - norm , and that mention padding plays an important role .",
        "generated_description": "on average , guorobust obtains an f1 score of 92 . 2 % , outperforming the previous state - of - the - art by 0 . 8 % and 0 . 5 % on the msnbc , aquaint , ace2004 , cweb , and wiki datasets , respectively . our rel - norm also achieves the best average score on all the datasets , outperforming all the previous methods , except for \\ newcitecheng - roth : 2013 : emnlp on the wikis dataset . note that the performance of the other methods on the other datasets are slightly worse than the ones reported in the original paper , which can be attributed to the small size of the dataset ."
    },
    {
        "table_id": "92",
        "table_info": {
            "table_caption": "Table 2: Performance of the proposed LSTM-based variants with the traditional cross-validation setup. Due to overlapping sub-dialogues in the train and test sets, the performance of the LSTM-based models achieve unrealistically high performance.",
            "table_column_names": [
                "[EMPTY]",
                "[ITALIC] UAR",
                "[ITALIC] κ",
                "[ITALIC] ρ",
                "[ITALIC] eA",
                "[ITALIC] Ep."
            ],
            "table_content_values": [
                [
                    "LSTM",
                    "0.78",
                    "0.85",
                    "0.91",
                    "[BOLD] 0.99",
                    "101"
                ],
                [
                    "BiLSTM",
                    "[BOLD] 0.78",
                    "[BOLD] 0.85",
                    "[BOLD] 0.92",
                    "[BOLD] 0.99",
                    "100"
                ],
                [
                    "LSTM+att",
                    "0.74",
                    "0.82",
                    "0.91",
                    "[BOLD] 0.99",
                    "101"
                ],
                [
                    "BiLSTM+att",
                    "0.75",
                    "0.83",
                    "0.91",
                    "[BOLD] 0.99",
                    "93"
                ],
                [
                    "Rach et al. ( 2017 )",
                    "0.55",
                    "0.68",
                    "0.83",
                    "0.94",
                    "-"
                ],
                [
                    "Ultes et al. ( 2015 )",
                    "0.55",
                    "-",
                    "-",
                    "0.89",
                    "-"
                ]
            ]
        },
        "gold_description": "the results are presented in table 2 . the results in table 2 show very high performance , which is likely to further increase with ongoing training .",
        "generated_description": "ultes et al . from the table we can see that the lstm - based models outperform the previous state - of - the - art models by a large margin . due to the large number of overlapping sub - dialogues in the train and test sets , the bilstm models achieve unrealistically high uar and ea scores . moreover , the attention mechanism gives the model the ability to attend to relevant segments in the context to decide when to stop ."
    },
    {
        "table_id": "93",
        "table_info": {
            "table_caption": "Table 8: Accuracy on ellipsis test set.",
            "table_column_names": [
                "[EMPTY]",
                "[BOLD] ellipsis (infl.)",
                "[BOLD] ellipsis (VP)"
            ],
            "table_content_values": [
                [
                    "baseline",
                    "53.0",
                    "28.4"
                ],
                [
                    "concat",
                    "[BOLD] 76.2",
                    "76.6"
                ],
                [
                    "s-hier-to-2.tied",
                    "66.4",
                    "65.6"
                ],
                [
                    "CADec",
                    "72.2",
                    "[BOLD] 80.0"
                ]
            ]
        },
        "gold_description": "for ellipsis , both models improve substantially over the baseline ( by 19 - 51 percentage points ) , with concat stronger for inflection tasks and cadec stronger for vpellipsis .",
        "generated_description": "the results show that both concat and cadec significantly outperform the baseline and s - hier - to - 2 . tied on the ellipsis test set . the concat model is better than the baseline on both inflected and verb - inflated forms , while cadec is better on the vp version . on the inflated version , cadec outperforms the baseline by a large margin and is within 3 points of the best model . the model with the best accuracy on the verb portion of the test set ( 80 . 0 % ) is very close to the best performance on the noun portion ( 75 . 6 % ) . the results indicate that cadec has better generalizability than the other models ."
    },
    {
        "table_id": "94",
        "table_info": {
            "table_caption": "Table 3: Performance of the proposed LSTM-based variants with the dialogue-wise cross-validation setup. The models by Rach et al. (2017) and Ultes et al. (2015) have been re-implemented. The BiLSTM with attention mechanism performs best in all evaluation metrics.",
            "table_column_names": [
                "[EMPTY]",
                "[ITALIC] UAR",
                "[ITALIC] κ",
                "[ITALIC] ρ",
                "[ITALIC] eA",
                "[ITALIC] Ep."
            ],
            "table_content_values": [
                [
                    "LSTM",
                    "0.51",
                    "0.63",
                    "0.78",
                    "0.93",
                    "8"
                ],
                [
                    "BiLSTM",
                    "0.53",
                    "0.63",
                    "0.78",
                    "0.93",
                    "8"
                ],
                [
                    "LSTM+att",
                    "0.52",
                    "0.63",
                    "0.79",
                    "0.92",
                    "40"
                ],
                [
                    "BiLSTM+att",
                    "[BOLD] 0.54",
                    "[BOLD] 0.65",
                    "[BOLD] 0.81",
                    "[BOLD] 0.94",
                    "40"
                ],
                [
                    "Rach et al. ( 2017 )",
                    "0.45",
                    "0.58",
                    "0.79",
                    "0.88",
                    "82"
                ],
                [
                    "Ultes et al. ( 2015 )",
                    "0.44",
                    "0.53",
                    "0.69",
                    "0.86",
                    "-"
                ]
            ]
        },
        "gold_description": "all results of these experiments are presented in table 3 with the absolute improvement of the two main measures uar and ea over the svm - based approach",
        "generated_description": "as can be seen , the proposed bilstm - based models significantly outperform the lstm baselines in terms of all evaluation metrics . we observe that the attention mechanism gives the best performance in the uar , κ , ρ and ea metrics . the best performance is achieved by the attention - based model with a ρ value of 0 . 81 and an ep . number of 8 . the performance of the rach et al . from the results , we can see that the proposed models perform best in the dialogue - wise cross - validation setup . this is in line with the results of the previous work ."
    },
    {
        "table_id": "95",
        "table_info": {
            "table_caption": "Table 1: Evaluation of our models for NER performance with our dataset. We report F1-measure results over the test portion of our dataset averaged over 10 replications of the training with the same hyper parameters.",
            "table_column_names": [
                "This work Model",
                "This work Mean F1-measure"
            ],
            "table_content_values": [
                [
                    "ner",
                    "81.07"
                ],
                [
                    "joint1",
                    "81.28"
                ],
                [
                    "joint2",
                    "81.84"
                ],
                [
                    "j_multi",
                    "[BOLD] 83.21"
                ],
                [
                    "Previous work",
                    "Previous work"
                ],
                [
                    "ext_m_feat",
                    "[BOLD] 83.47"
                ]
            ]
        },
        "gold_description": "the results are shown in table 1 . we see that the mean ner performance increases in joint models . as one can see from the table , it achieved the best results compared to our joint models . however , we cannot confirm the difference between ext m feat and j multi models as the calculated p is well above . 05 .",
        "generated_description": "our best model ext_m_feat achieves an f1 - measure of 83 . 47 % on the test portion of our dataset averaged over 10 replications of the training with the same hyper parameters as the previous work . our model improves over the previous state - of - the - art ner model by 0 . 8 % absolute in terms of f1 score and over 0 . 7 % absolute on the joint1 and joint2 datasets ."
    },
    {
        "table_id": "96",
        "table_info": {
            "table_caption": "Table 2: Evaluation of our models for MD performance. As in the NER evaluation, we report accuracies over the test dataset averaged over 10 replications of the training.",
            "table_column_names": [
                "This work Model",
                "This work Mean Accuracy"
            ],
            "table_content_values": [
                [
                    "md",
                    "88.61"
                ],
                [
                    "joint1",
                    "88.17"
                ],
                [
                    "joint2",
                    "86.86"
                ],
                [
                    "j_multi",
                    "88.05"
                ],
                [
                    "Previous work",
                    "Previous work"
                ],
                [
                    "yuret2006decision_lists",
                    "89.55"
                ],
                [
                    "shen2016role",
                    "[BOLD] 91.03"
                ]
            ]
        },
        "gold_description": "as can be seen from table 2 , we are very close to the state of the art md performance even if we only trained with a low number of parameters as stated in the beginning of this section . we have to also note that in contrast with the ner task , the md task did not enjoy a performance increase from joint learning .",
        "generated_description": "for md , we report accuracy over the test dataset averaged over 10 replications of the training and development sets , using the same hyperparameter settings as in the ner evaluation . joint1 and joint2 achieve slightly lower accuracy than joint1 , and j_multi slightly lower than joint2 . in addition , we also report the results of \\ newciteyuret2006decision_lists , a recent list ranking model shen2016role , which achieves a state - of - the - art performance of 91 . 03 % on the msrp dataset ."
    },
    {
        "table_id": "97",
        "table_info": {
            "table_caption": "Table 1: Performance of our models (bottom four) along with the state-of-the-art baseline models (top four). Baseline results for fastText, GloVe are obtained from Romanov and Shivade (2018). Adding knowledge graph information to the base models showed an absolute improvement of 4.97% in case of fastText and 1.36% in case of GloVe. The baseline model utilizing BioELMo as base embeddings Jin et al. (2019) showed an accuracy of 78.2%. On adding knowledge graph information, we were able to improve these results to 78.76% and on further addition of sentiment information, the accuracy rose to 79.04%",
            "table_column_names": [
                "Model",
                "Accuracy"
            ],
            "table_content_values": [
                [
                    "fastText",
                    "68.7%"
                ],
                [
                    "GloVe",
                    "73.1%"
                ],
                [
                    "[ITALIC] BioELMo Jin et al. ( 2019 )",
                    "78.2%"
                ],
                [
                    "[ITALIC] ESIMw/ [ITALIC] K Lu et al. ( 2019 )",
                    "77.8%"
                ],
                [
                    "fastText w/ KG+Sentiment",
                    "73.67%"
                ],
                [
                    "GloVe w/ KG+Sentiment",
                    "74.46%"
                ],
                [
                    "BioELMo w/ KG",
                    "78.76%"
                ],
                [
                    "[BOLD] BioELMo w/ KG+Sentiment",
                    "[BOLD] 79.04%"
                ]
            ]
        },
        "gold_description": "we report accuracy as the performance metric . table 1 represents the performance comparison of our proposed models and the baselines , which shows that incorporation of knowledge graph embeddings helps to improve the model performance . all results are summarized in table 1 .",
        "generated_description": "besides , we were able to observe an absolute improvement of 4 . 97 % in case of fasttext and 1 . 36 % for glove and a slight increase of 1 . 76 % in the case of bioelmo . besides , on further addition of sentiment information , the accuracy rose to 79 . 04 % , which is a new state - of - the - art result ."
    },
    {
        "table_id": "98",
        "table_info": {
            "table_caption": "Table 9: Results for different probabilities of using corrupted reference at training time. BLEU for 3 context sentences. For ellipsis, we show inflection/VP scores.",
            "table_column_names": [
                "[ITALIC] p",
                "[BOLD] BLEU",
                "[BOLD] deixis",
                "[BOLD] lex. c.",
                "[BOLD] ellipsis"
            ],
            "table_content_values": [
                [
                    "[ITALIC] p=0",
                    "32.34",
                    "84.1",
                    "48.7",
                    "65 / 75"
                ],
                [
                    "[ITALIC] p=0.25",
                    "32.31",
                    "83.3",
                    "52.4",
                    "67 / 78"
                ],
                [
                    "[ITALIC] p=0.5",
                    "32.38",
                    "81.6",
                    "58.1",
                    "72 / 80"
                ],
                [
                    "[ITALIC] p=0.75",
                    "32.45",
                    "80.0",
                    "65.0",
                    "70 / 80"
                ]
            ]
        },
        "gold_description": "results for different values of p are given in table 9 . all models have about the same bleu , not statistically significantly different from the baseline , but they are quite different in terms of incorporating context . the denoising positively influences almost all tasks except for deixis , yielding the largest improvement on lexical cohesion .",
        "generated_description": "for ellipsis and deixis , we show scores for the best probability of using the corrupted reference at training time . as expected , the model is more likely to use the incorrect reference when the probability is high . however , when the p is low ( e . g . , 0 . 25 or 0 . 75 ) , the bleu scores of all 3 context sentences are higher than the ones with the correct reference . this indicates that the model has already learned to associate the context sentences with the corrupted references ."
    },
    {
        "table_id": "99",
        "table_info": {
            "table_caption": "Table 5: The accuracy of the traditional classifier in Phase 2 given documents from seen classes only.",
            "table_column_names": [
                "Dataset Input ∖ Unseen rate",
                "DBpedia 50%",
                "DBpedia 25%",
                "20news 50%",
                "20news 25%"
            ],
            "table_content_values": [
                [
                    "[ITALIC] vw",
                    "0.993",
                    "0.992",
                    "0.878",
                    "0.861"
                ]
            ]
        },
        "gold_description": "the purpose of table 5 is to show that the traditional cnn classifier in phase 2 was highly accurate .",
        "generated_description": "in phase 2 , we evaluate the performance of the traditional classifier on the unseen classes first , and then analyse its performance given the seen classes only . to this end , we filter out half of the documents in phase 2 from the original training set and train a new classifier based on the remaining documents . we compare the accuracy of the classifier with respect to the number of instances for which the document is not present in phase 1 ( i . e . , the percentage of instances that are present in the training set ) . for example , given a document ( e . g . , “ water in a container ” ) as input , we would expect to see a drop of 0 . 5 % in accuracy in the case of dbpedia as the document contains water in the form of a container , while this is not the case for 20news , which contains a number of different kinds of water in its form , such as a “ flavors ” , “ filtering ” and “ towards ” . the results show that the performance does not drop significantly when the proportion of seen documents is reduced from 50 % to 25 % ."
    },
    {
        "table_id": "100",
        "table_info": {
            "table_caption": "Table 1: Macro-averaged P/R/F1 on the dev set for the entity typing task of Choi et al. (2018) comparing various systems. ELMo gives a substantial improvement over baselines. Over an ELMo-equipped model, data augmentation using the method of Choi et al. (2018) gives no benefit. However, our denoising technique allow us to effectively incorporate distant data, matching the results of a BERT model on this task Devlin et al. (2018).",
            "table_column_names": [
                "Model",
                "Total P",
                "Total R",
                "Total F1",
                "General P",
                "General R",
                "General F1",
                "Fine P",
                "Fine R",
                "Fine F1",
                "Ultra-Fine P",
                "Ultra-Fine R",
                "Ultra-Fine F1"
            ],
            "table_content_values": [
                [
                    "Ours + GloVe w/o augmentation",
                    "46.4",
                    "23.3",
                    "31.0",
                    "57.7",
                    "65.5",
                    "61.4",
                    "41.3",
                    "31.3",
                    "35.6",
                    "42.4",
                    "9.2",
                    "15.1"
                ],
                [
                    "Ours + ELMo w/o augmentation",
                    "[BOLD] 55.6",
                    "28.1",
                    "37.3",
                    "[BOLD] 69.3",
                    "77.3",
                    "73.0",
                    "[BOLD] 47.9",
                    "35.4",
                    "40.7",
                    "[BOLD] 48.9",
                    "12.6",
                    "20.0"
                ],
                [
                    "Ours + ELMo w augmentation",
                    "55.2",
                    "26.4",
                    "35.7",
                    "69.4",
                    "72.0",
                    "70.7",
                    "46.6",
                    "38.5",
                    "42.2",
                    "48.7",
                    "10.3",
                    "17.1"
                ],
                [
                    "Ours + ELMo w augmentation",
                    "50.7",
                    "[BOLD] 33.1",
                    "[BOLD] 40.1",
                    "66.9",
                    "[BOLD] 80.7",
                    "73.2",
                    "41.7",
                    "46.2",
                    "43.8",
                    "45.6",
                    "[BOLD] 17.4",
                    "[BOLD] 25.2"
                ],
                [
                    "+ filter & relabel",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "BERT-Base, Uncased",
                    "51.6",
                    "32.8",
                    "[BOLD] 40.1",
                    "67.4",
                    "80.6",
                    "[BOLD] 73.4",
                    "41.6",
                    "[BOLD] 54.7",
                    "[BOLD] 47.3",
                    "46.3",
                    "15.6",
                    "23.4"
                ],
                [
                    "Choi et al. ( 2018 ) w augmentation",
                    "48.1",
                    "23.2",
                    "31.3",
                    "60.3",
                    "61.6",
                    "61.0",
                    "40.4",
                    "38.4",
                    "39.4",
                    "42.8",
                    "8.8",
                    "14.6"
                ]
            ]
        },
        "gold_description": "table 1 compares the performance of these systems on the development set . our model with no augmentation already matches the system of choi et al . ( 2018 ) with augmentation , and incorporating elmo gives further gains on both precision and recall . on top of this model , adding the distantly - annotated data lowers the performance ; the loss function - based approach of ( choi et al . , 2018 ) does not sufficiently mitigate the noise in this data . however , denoising makes the distantlyannotated data useful , improving recall by a substantial margin especially in the general class . bert performs similarly to elmo with denoised distant data . as can be seen in the performance breakdown , bert gains from improvements in recall in the fine class .",
        "generated_description": "compared with the original glove embeddings , our approach significantly improves the performance in general and fine - grained extraction as well as the ultra - fine extraction . as for the augmentation process , the performance of our approach is consistently better than that of the original elmo - based model , which verifies the effectiveness of our method ."
    },
    {
        "table_id": "101",
        "table_info": {
            "table_caption": "Table 2: Macro-averaged P/R/F1 on the test set for the entity typing task of Choi et al. (2018). Our denoising approach gives substantial gains over naive augmentation and matches the performance of a BERT model.",
            "table_column_names": [
                "Model",
                "P",
                "R",
                "F1"
            ],
            "table_content_values": [
                [
                    "Ours + GloVe w/o augmentation",
                    "47.6",
                    "23.3",
                    "31.3"
                ],
                [
                    "Ours + ELMo w/o augmentation",
                    "[BOLD] 55.8",
                    "27.7",
                    "37.0"
                ],
                [
                    "Ours + ELMo w augmentation",
                    "55.5",
                    "26.3",
                    "35.7"
                ],
                [
                    "Ours + ELMo w augmentation",
                    "51.5",
                    "[BOLD] 33.0",
                    "[BOLD] 40.2"
                ],
                [
                    "+ filter & relabel",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "BERT-Base, Uncased",
                    "51.6",
                    "[BOLD] 33.0",
                    "[BOLD] 40.2"
                ],
                [
                    "Choi et al. ( 2018 ) w augmentation",
                    "47.1",
                    "24.2",
                    "32.0"
                ],
                [
                    "LabelGCN Xiong et al. ( 2019 )",
                    "50.3",
                    "29.2",
                    "36.9"
                ]
            ]
        },
        "gold_description": "table 2 shows the performance of all settings on the test set , with the same trend as the performance on the development set . our approach outperforms the concurrently - published xiong et al . ( 2019 ) ;",
        "generated_description": "we find that our denoising approach gives substantial improvements over the naive augmentation of the glove embeddings and elmo , and matches the performance of a bert - base model trained without any augmentation on the entity typing task of choi et al . our model also achieves competitive performance compared to labelgcn xiong et al . , which is trained with the same data as our model ."
    },
    {
        "table_id": "102",
        "table_info": {
            "table_caption": "Table 3: Macro-averaged P/R/F1 on the dev set for the entity typing task of Choi et al. (2018) with various types of augmentation added. The customized loss from Choi et al. (2018) actually causes a decrease in performance from adding any of the datasets. Heuristics can improve incorporation of this data: a relabeling heuristic (Pair) helps on HEAD and a filtering heuristic (Overlap) is helpful in both settings. However, our trainable filtering and relabeling models outperform both of these techniques.",
            "table_column_names": [
                "Type",
                "Denoising Method",
                "EL&HEAD P",
                "EL&HEAD R",
                "EL&HEAD F1",
                "EL P",
                "EL R",
                "EL F1",
                "HEAD P",
                "HEAD R",
                "HEAD F1"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "Raw Data",
                    "[BOLD] 55.2",
                    "26.4",
                    "35.7",
                    "52.3",
                    "26.1",
                    "34.8",
                    "[BOLD] 52.8",
                    "28.4",
                    "36.9"
                ],
                [
                    "Heuristic Baselines",
                    "Synonyms&Hypernyms",
                    "43.0",
                    "30.0",
                    "35.3",
                    "47.5",
                    "26.3",
                    "33.9",
                    "44.8",
                    "31.7",
                    "37.1"
                ],
                [
                    "[EMPTY]",
                    "Pair",
                    "50.2",
                    "29.0",
                    "36.8",
                    "49.6",
                    "27.0",
                    "35.0",
                    "50.6",
                    "31.2",
                    "38.6"
                ],
                [
                    "[EMPTY]",
                    "Overlap",
                    "50.0",
                    "32.3",
                    "39.2",
                    "49.5",
                    "[BOLD] 30.8",
                    "38.0",
                    "50.6",
                    "31.4",
                    "38.7"
                ],
                [
                    "Proposed Approach",
                    "Filter",
                    "53.1",
                    "28.2",
                    "36.8",
                    "51.9",
                    "26.5",
                    "35.1",
                    "51.2",
                    "31.2",
                    "38.7"
                ],
                [
                    "[EMPTY]",
                    "Relabel",
                    "52.1",
                    "32.2",
                    "39.8",
                    "50.2",
                    "31.4",
                    "38.6",
                    "50.2",
                    "31.8",
                    "38.9"
                ],
                [
                    "[EMPTY]",
                    "Filter&Relabel",
                    "50.7",
                    "[BOLD] 33.1",
                    "[BOLD] 40.1",
                    "[BOLD] 52.7",
                    "30.5",
                    "[BOLD] 38.7",
                    "50.7",
                    "[BOLD] 32.1",
                    "[BOLD] 39.3"
                ],
                [
                    "[EMPTY]",
                    "Choi et al. ( 2018 )",
                    "48.1",
                    "23.2",
                    "31.3",
                    "50.3",
                    "19.6",
                    "28.2",
                    "48.4",
                    "22.3",
                    "30.6"
                ]
            ]
        },
        "gold_description": "table 3 compares the results on the development set . on top of the baseline original , adding synonyms and hypernyms by consulting external knowledge does not improve the performance . the pair technique results in small gains over original . overlap is the most ef fective heuristic technique . this simple heuristic improves recall on el . filter , gives similar improvements to pair and overlap on the head setting , relabel and overlap both improve performance on both el and head while other methods do poorly on el . filter & relabel outperforms all the baselines .",
        "generated_description": "the heuristic baselines are based on the raw data and the original el and head annotations . as can be seen , the proposed approach significantly outperforms the other baselines across all types of denoising methods and achieves the best overall performance ."
    },
    {
        "table_id": "103",
        "table_info": {
            "table_caption": "Table 4: Test results on OntoNotes. Denoising helps substantially even in this reduced setting. Using fewer distant examples, we nearly match the performance using the data from Choi et al. (2018) (see text).",
            "table_column_names": [
                "Model",
                "Acc.",
                "Ma-F1",
                "Mi-F1"
            ],
            "table_content_values": [
                [
                    "Ours + ELMo w/o augmentation",
                    "42.7",
                    "72.7",
                    "66.7"
                ],
                [
                    "Ours + ELMo w augmentation",
                    "59.3",
                    "76.5",
                    "70.7"
                ],
                [
                    "Ours + ELMo w augmentation",
                    "63.9",
                    "[BOLD] 84.5",
                    "78.9"
                ],
                [
                    "+ filter & relabel",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Ours + ELMo w augmentation",
                    "[BOLD] 64.9",
                    "[BOLD] 84.5",
                    "[BOLD] 79.2"
                ],
                [
                    "by Choi et al. ( 2018 )",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "BERT-Base, Uncased",
                    "51.8",
                    "76.6",
                    "69.1"
                ],
                [
                    "Shimaoka et al. ( 2017 )",
                    "51.7",
                    "70.9",
                    "64.9"
                ],
                [
                    "AFET Ren et al. ( 2016a )",
                    "55.1",
                    "71.1",
                    "64.7"
                ],
                [
                    "PLE Ren et al. ( 2016b )",
                    "57.2",
                    "71.5",
                    "66.1"
                ],
                [
                    "Choi et al. ( 2018 )",
                    "59.5",
                    "76.8",
                    "71.8"
                ],
                [
                    "LabelGCN Xiong et al. ( 2019 )",
                    "59.6",
                    "77.8",
                    "72.2"
                ]
            ]
        },
        "gold_description": "table 4 lists the results on the ontonotes test set following the adaptation setting of choi et al . ( 2018 ) . denoising significantly improves over naive incorporation of distant data , bert still performs well but not as well as our model with augmented training data .",
        "generated_description": "since elmo is pre - trained on a large amount of unlabeled data , we use the uncased version of bert devlin et al . as can be seen , our approach significantly outperforms the bert - base and elmo - augmented models and achieves the new state - of - the - art performance across all metrics , which verifies the effectiveness of our approach . especially , when combined with elmo , our model with the filter & relabeling heuristic achieves a new state of the art performance of 64 . 9 % accuracy and 79 . 2 % mi - f1 ."
    },
    {
        "table_id": "104",
        "table_info": {
            "table_caption": "Table 5: The average number of types added or deleted by the relabeling function per example. The right-most column shows that the rate of examples discarded by the filtering function.",
            "table_column_names": [
                "Data",
                "General Add",
                "General Del",
                "Fine Add",
                "Fine Del",
                "Ultra-Fine Add",
                "Ultra-Fine Del",
                "Filter (%)"
            ],
            "table_content_values": [
                [
                    "EL",
                    "0.87",
                    "0.01",
                    "0.36",
                    "0.17",
                    "2.03",
                    "0.12",
                    "9.4"
                ],
                [
                    "HEAD",
                    "1.18",
                    "0.00",
                    "0.51",
                    "0.01",
                    "1.15",
                    "0.16",
                    "10.0"
                ]
            ]
        },
        "gold_description": "table 5 reports the average numbers of types added / deleted by the relabeling function and the ratio of examples discarded by the filtering function . the head examples have more general types added than the el examples since the noisy head labels are typically finer . fine - grained types are added to both el and head examples less frequently . ultra - fine examples are frequently added to both datasets , with more added to el ; the filtering function discards similar numbers of examples for the el and head data : 9 . 4 % and 10 % respectively .",
        "generated_description": "we can see that the filtering function tends to delete more types than the general add and delete methods in general . however , for the head dataset , the ratio of added types is much higher than for el and head . we assume that this is due to the fact that the average number of types added or deleted by the relabeling function per example is only 1 . 15 and 1 . 16 for the general and fine - tuned methods , respectively ."
    },
    {
        "table_id": "105",
        "table_info": {
            "table_caption": "Table 1: Properties of the Ubuntu and Samsung QA dataset. The message and response are {context}, {response} in Ubuntu and {question}, {answer} in the Samsung QA dataset.",
            "table_column_names": [
                "[BOLD] Dataset",
                "[BOLD] # Samples Train",
                "[BOLD] # Samples Val.",
                "[BOLD] # Samples Test",
                "[BOLD] Message (Avg.) # tokens",
                "[BOLD] Message (Avg.) # groups",
                "[BOLD] Message (Avg.) # tokens /group",
                "[BOLD] Response (Avg.) # tokens",
                "[BOLD] Response (Avg.) # groups",
                "[BOLD] Response (Avg.) # tokens /group"
            ],
            "table_content_values": [
                [
                    "Ubuntu-v1",
                    "1M",
                    "35,609",
                    "35,517",
                    "162.47 ±132.47",
                    "8.43 ±6.32",
                    "20.14 ±18.41",
                    "14.44 ±13.93",
                    "1",
                    "-"
                ],
                [
                    "Ubuntu-v2",
                    "1M",
                    "19,560",
                    "18,920",
                    "85.92 ±74.71",
                    "4.95 ±2.98",
                    "20.73 ±20.19",
                    "17.01 ±16.41",
                    "1",
                    "-"
                ],
                [
                    "Samsung QA",
                    "163,616",
                    "10,000",
                    "10,000",
                    "12.84 ±6.42",
                    "1",
                    "-",
                    "173.48 ±192.12",
                    "6.09 ±5.58",
                    "29.28 ±31.91"
                ]
            ]
        },
        "gold_description": "we found that it still showed a limitation when we consider very large sequential length data such as 162 steps average in the ubuntu dialogue corpus dataset ( see table 1 ) . table 1 shows properties of the ubuntu dataset . we generated ( { question } , { answer } , flag ) triples ( see table 1 ) . the maximum time step for calculating gradient of the rnn is determined according to the input data statistics in table 1 .",
        "generated_description": "on the ubuntu dataset , we use the { context } , { response } in { context } to indicate whether a message and a response are composed of the same words or not . the value of context is 1 for ubuntu - v1 and 1 for the samsung qa dataset . on the contrary , the value of response is much higher than that of message on the ubuntu v2 dataset . this is due to the fact that the response contains a large number of groups and values of words which are extracted from the whole message while the message contains only a small number of them . the values of context are also higher than those of response ."
    },
    {
        "table_id": "106",
        "table_info": {
            "table_caption": "Table 3: Model performance results for the Ubuntu-v1 dataset. Models [1-4] are from Lowe et al. (2015); Kadlec et al. (2015); Wang and Jiang (2016); Wang et al. (2017), respectively.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Ubuntu-v1 1 in 2R@1",
                "[BOLD] Ubuntu-v1 1 in 10R@1",
                "[BOLD] Ubuntu-v1 1 in 10R@2",
                "[BOLD] Ubuntu-v1 1 in 10R@5"
            ],
            "table_content_values": [
                [
                    "TF-IDF [1]",
                    "0.659",
                    "0.410",
                    "0.545",
                    "0.708"
                ],
                [
                    "CNN [2]",
                    "0.848",
                    "0.549",
                    "0.684",
                    "0.896"
                ],
                [
                    "LSTM [2]",
                    "0.901",
                    "0.638",
                    "0.784",
                    "0.949"
                ],
                [
                    "CompAgg [3]",
                    "0.884",
                    "0.631",
                    "0.753",
                    "0.927"
                ],
                [
                    "BiMPM [4]",
                    "0.897",
                    "0.665",
                    "0.786",
                    "0.938"
                ],
                [
                    "RDE",
                    "0.898 ±0.002",
                    "0.643 ±0.009",
                    "0.784 ±0.007",
                    "0.945 ±0.002"
                ],
                [
                    "RDE-LTC",
                    "0.903 ±0.001",
                    "0.656 ±0.003",
                    "0.794 ±0.003",
                    "0.948 ±0.001"
                ],
                [
                    "HRDE",
                    "0.915 ±0.001",
                    "0.681 ±0.001",
                    "0.820 ±0.001",
                    "0.959 ±0.001"
                ],
                [
                    "HRDE-LTC",
                    "[BOLD] 0.916 ±0.001",
                    "[BOLD] 0.684 ±0.001",
                    "[BOLD] 0.822 ±0.001",
                    "[BOLD] 0.960 ±0.001"
                ]
            ]
        },
        "gold_description": "as table 3 shows , our proposed hrde and hrde - ltc models achieve the best performance for the ubuntu - v1 dataset . we also find that the rde - ltc model shows improvements from the baseline model , rde .",
        "generated_description": "results on ubuntu - v1 . we compare our model with the following baselines : tf - idf [ 1 ] , cnn [ 2 ] , lstm [ 2 ] and cnn + rde [ 3 ] . we can see that our model significantly outperforms all these baselines in terms of all metrics on both datasets . compagg [ 3 ] and bimpm [ 4 ] are very similar to our rde and hrde , but their results are significantly worse than our model . we conjecture that this is due to the multi - task learning nature of these two models , which makes them better at dealing with long - term dependencies than our models . we can also see that the ltc - based rde outperforms the rde - ltc model , which further verifies the effectiveness of modeling the interaction between the utterance and the response ."
    },
    {
        "table_id": "107",
        "table_info": {
            "table_caption": "Table 4: Model performance results for the Ubuntu-v2 dataset. Models [1,3-6] are from Lowe et al. (2015); Wang and Jiang (2016); Wang et al. (2017); Baudiš et al. (2016); Tan et al. (2015), respectively.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Ubuntu-v2 1 in 2R@1",
                "[BOLD] Ubuntu-v2 1 in 10R@1",
                "[BOLD] Ubuntu-v2 1 in 10R@2",
                "[BOLD] Ubuntu-v2 1 in 10R@5"
            ],
            "table_content_values": [
                [
                    "LSTM [1]",
                    "0.869",
                    "0.552",
                    "0.721",
                    "0.924"
                ],
                [
                    "RNN [5]",
                    "0.907 ±0.002",
                    "0.664 ±0.004",
                    "0.799 ±0.004",
                    "0.951 ±0.001"
                ],
                [
                    "CNN [5]",
                    "0.863 ±0.003",
                    "0.587 ±0.004",
                    "0.721 ±0.005",
                    "0.907 ±0.003"
                ],
                [
                    "RNN-CNN [5]",
                    "0.911 ±0.001",
                    "[BOLD] 0.672 ±0.002",
                    "0.809 ±0.002",
                    "0.956 ±0.001"
                ],
                [
                    "Attention \\scriptsize{[6]}(RNN-CNN)",
                    "0.903 ±0.002",
                    "0.653 ±0.005",
                    "0.788 ±0.005",
                    "0.945 ±0.002"
                ],
                [
                    "CompAgg [3]",
                    "0.895",
                    "0.641",
                    "0.776",
                    "0.937"
                ],
                [
                    "BiMPM [4]",
                    "0.877",
                    "0.611",
                    "0.747",
                    "0.921"
                ],
                [
                    "RDE",
                    "0.894 ±0.002",
                    "0.610 ±0.008",
                    "0.776 ±0.006",
                    "0.947 ±0.002"
                ],
                [
                    "RDE-LTC",
                    "0.899 ±0.002",
                    "0.625 ±0.004",
                    "0.788 ±0.004",
                    "0.951 ±0.001"
                ],
                [
                    "HRDE",
                    "0.914 ±0.001",
                    "0.649 ±0.001",
                    "0.813 ±0.001",
                    "0.964 ±0.001"
                ],
                [
                    "HRDE-LTC",
                    "[BOLD] 0.915 ±0.002",
                    "0.652 ±0.003",
                    "[BOLD] 0.815 ±0.001",
                    "[BOLD] 0.966 ±0.001"
                ]
            ]
        },
        "gold_description": "table 4 reveals that the hrde - ltc model is best for three cases ( 1 in 2 r @ 1 , 1 in 10 r @ 2 and 1 in 10 r @ 5 ) . we see improvements from the rde model to the hrde model and additional improvements with the ltc module in all test cases ( the ubuntuv1 / v2 and the samsung qa ) .",
        "generated_description": "compagg outperforms lstm , cnn , rnn , and rnn - cnn in terms of 1 in 2r @ 1 and 1 in 10r @ k metrics , but underperforms on the other metrics . compagg is better than rnns , but worse than cnns , which implies that the sentence - level information plays a more important role in this task . attention \\ script { [ 6 ] } ( rnn - conv - cnn ) obtains comparable results with the best performing compagg model , but suffers from worse performance than the attention - based models . this shows that the attention mechanism is more effective than simply concatenating the whole sentence together as in compagg ."
    },
    {
        "table_id": "108",
        "table_info": {
            "table_caption": "Table 5: Model performance results for the Samsung QA dataset.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Samsung QA 1 in 2R@1",
                "[BOLD] Samsung QA 1 in 10R@1",
                "[BOLD] Samsung QA 1 in 10R@2",
                "[BOLD] Samsung QA 1 in 10R@5"
            ],
            "table_content_values": [
                [
                    "TF-IDF",
                    "0.939",
                    "0.834",
                    "0.897",
                    "0.953"
                ],
                [
                    "RDE",
                    "0.978 ±0.002",
                    "0.869 ±0.009",
                    "0.966 ±0.003",
                    "0.997 ±0.001"
                ],
                [
                    "RDE-LTC",
                    "0.981 ±0.002",
                    "0.880 ±0.009",
                    "0.970 ±0.003",
                    "0.997 ±0.001"
                ],
                [
                    "HRDE",
                    "0.981 ±0.002",
                    "0.885 ±0.011",
                    "0.971 ±0.004",
                    "0.997 ±0.001"
                ],
                [
                    "HRDE-LTC",
                    "[BOLD] 0.983 ±0.002",
                    "[BOLD] 0.890 ±0.010",
                    "[BOLD] 0.972 ±0.003",
                    "[BOLD] 0.998 ±0.001"
                ]
            ]
        },
        "gold_description": "table 5 indicates the proposed rde - ltc , hrde , and the that hrde - ltc model show performance improvements when compared to the baseline model , tfidf and rde .",
        "generated_description": "comparing with tf - idf , hrde and rde - ltc , we can see that the performance of the proposed hrde is comparable to that of the baseline rde . this indicates that our model is able to find the correct answer when there is a large number of possible answers to choose from , which is in line with the findings of previous works that show that this is not always the case ( e . g . , in some cases there are only one or two correct answers given by one or the other model ) . furthermore , when compared with rde , our model outperforms the baseline by a large margin in terms of all the metrics . in particular , the results of the “ 1 in 2r @ 1 ” metric , which measures the ratio of questions that are ranked in the top 1 result of the ranked list , increases from 0 . 834 ( rde ) to 0 . 972 ( hrde ) when the number of correct answers is 1 , 2 , and 5 , respectively ."
    },
    {
        "table_id": "109",
        "table_info": {
            "table_caption": "Table 2: Comparison of our model with the baselines. “*” is our QG module.",
            "table_column_names": [
                "Model",
                "BLEU-1",
                "BLEU-2",
                "BLEU-3",
                "BLEU-4",
                "METEOR",
                "ROUGE-L"
            ],
            "table_content_values": [
                [
                    "Zhou et al. ( 2017 )",
                    "-",
                    "-",
                    "-",
                    "13.29",
                    "-",
                    "-"
                ],
                [
                    "Zhao et al. ( 2018 )*",
                    "45.69",
                    "29.58",
                    "22.16",
                    "16.85",
                    "20.62",
                    "44.99"
                ],
                [
                    "Kim et al. ( 2019 )",
                    "-",
                    "-",
                    "-",
                    "16.17",
                    "-",
                    "-"
                ],
                [
                    "Liu et al. ( 2019 )",
                    "46.58",
                    "30.90",
                    "22.82",
                    "17.55",
                    "21.24",
                    "44.53"
                ],
                [
                    "[BOLD] IWAQG",
                    "[BOLD] 47.69",
                    "[BOLD] 32.24",
                    "[BOLD] 24.01",
                    "[BOLD] 18.53",
                    "[BOLD] 22.33",
                    "[BOLD] 46.94"
                ]
            ]
        },
        "gold_description": "our model outperforms all other models in all the metrics . this improvement is consistent , around 2 % .",
        "generated_description": "zhao et al . from the table , we can see that our proposed iwaqg model significantly outperforms all the baselines in terms of all the evaluation metrics , which indicates the effectiveness of the proposed model . especially , our model achieves significant improvement on bleu - 1 , 2 , 3 , 4 , meteor and rouge - l scores , which verifies the effectiveness and generalizability of our proposed model in conversational response generation ."
    },
    {
        "table_id": "110",
        "table_info": {
            "table_caption": "Table 3: Performance of the QG model with respect to the accuracy of the interrogative-word classifier. “*” is our implementation of the QG module without our interrogative-word classifier Zhao et al. (2018).",
            "table_column_names": [
                "Accuracy",
                "BLEU-1",
                "BLEU-2",
                "BLEU-3",
                "BLEU-4",
                "METEOR",
                "ROUGE-L"
            ],
            "table_content_values": [
                [
                    "[BOLD] Only QG*",
                    "[BOLD] 45.63",
                    "[BOLD] 30.43",
                    "[BOLD] 22.51",
                    "[BOLD] 17.30",
                    "[BOLD] 21.06",
                    "[BOLD] 45.42"
                ],
                [
                    "60%",
                    "45.80",
                    "30.61",
                    "22.57",
                    "17.30",
                    "21.47",
                    "44.70"
                ],
                [
                    "70%",
                    "47.05",
                    "31.62",
                    "23.46",
                    "18.05",
                    "22.00",
                    "45.88"
                ],
                [
                    "[BOLD] IWAQG (73.8%)",
                    "[BOLD] 47.69",
                    "[BOLD] 32.24",
                    "[BOLD] 24.01",
                    "[BOLD] 18.53",
                    "[BOLD] 22.33",
                    "[BOLD] 46.94"
                ],
                [
                    "80%",
                    "48.11",
                    "32.36",
                    "24.00",
                    "18.42",
                    "22.43",
                    "47.22"
                ],
                [
                    "90%",
                    "49.33",
                    "33.43",
                    "24.91",
                    "19.20",
                    "22.98",
                    "48.41"
                ],
                [
                    "[BOLD] Upper Bound (100%)",
                    "[BOLD] 50.51",
                    "[BOLD] 34.28",
                    "[BOLD] 25.60",
                    "[BOLD] 19.75",
                    "[BOLD] 23.45",
                    "[BOLD] 49.65"
                ]
            ]
        },
        "gold_description": "table 3 and figure 3 show a linear relationship between the accuracy of the classifier and the iwaqg . this demonstrates the effectiveness of our pipelined approach regardless of the interrogative - word classifier model .",
        "generated_description": "we observe that our iwaqg model outperforms the “ only qg * ” model by a large margin in terms of bleu - 1 , 2 , 3 and meteor scores , which confirms the importance of having a large fraction of human - annotated qg examples in the video captioning task . moreover , we observe that the accuracy decreases as the number of videos in the training set increases , as shown in the last row of the table . this suggests that there is a trade - off between the accuracy and the performance of the captioning model , as it is difficult to achieve high accuracy with a small dataset size ."
    },
    {
        "table_id": "111",
        "table_info": {
            "table_caption": "Table 4: Recall of interrogative words of the QG model. “*” is our implementation of the QG module without our interrogative-word classifier Zhao et al. (2018).",
            "table_column_names": [
                "Model",
                "What",
                "Which",
                "Where",
                "When",
                "Who",
                "Why",
                "How",
                "Others",
                "Total"
            ],
            "table_content_values": [
                [
                    "Only QG*",
                    "82.24%",
                    "0.29%",
                    "51.90%",
                    "60.82%",
                    "68.34%",
                    "12.66%",
                    "60.62%",
                    "2.13%",
                    "68.29%"
                ],
                [
                    "IWAQG",
                    "87.66%",
                    "1.46%",
                    "66.24%",
                    "49.41%",
                    "76.41%",
                    "50.63%",
                    "70.26%",
                    "14.89%",
                    "74.10%"
                ],
                [
                    "Upper Bound",
                    "99.87%",
                    "99.71%",
                    "100.00%",
                    "99.71%",
                    "99.84%",
                    "98.73%",
                    "99.67%",
                    "89.36%",
                    "99.72%"
                ]
            ]
        },
        "gold_description": "we analyze the recall of the interrogative words generated by our pipelined system . as shown in the table 4 , the total recall of using only the qg module is 68 . 29 % , while the recall of our proposed system , iwaqg , is 74 . 10 % , an improvement of almost 6 % . furthermore , if we assume a perfect interrogative - word classifier , the recall would be 99 . 72 % , a dramatic improvement which proves the validity of our hypothesis .",
        "generated_description": "zhao et al . ’ s implementation of the qg module without our interrogative - word classifier . “ only qg * ” is our implementation of qg which uses only the text of the interrogative words to predict the answer . we can see that the percentage of the correct answers is only 82 . 24 % and 68 . 34 % for the who and what questions , respectively , which is much lower than the accuracy of 86 . 66 % and 90 . 36 % of the answers obtained by zhao ’ s system . our iwaqg module outperforms the original qg by a large margin , achieving 99 . 84 % and 99 . 72 % on the questions of what , what , where , when and what , and who questions ."
    },
    {
        "table_id": "112",
        "table_info": {
            "table_caption": "Table 6: Ablation Study of our interrogative-word classifier.",
            "table_column_names": [
                "Classifier",
                "Accuracy"
            ],
            "table_content_values": [
                [
                    "CLS",
                    "56.0%"
                ],
                [
                    "CLS + NER",
                    "56.6%"
                ],
                [
                    "CLS + AE",
                    "70.3%"
                ],
                [
                    "CLS + AT",
                    "73.3%"
                ],
                [
                    "[BOLD] CLS + AT + NER",
                    "[BOLD] 73.8%"
                ]
            ]
        },
        "gold_description": "we tried to combine different features shown in table 6 for the interrogative - word classifier . the first model is only using the [ cls ] bert token embedding the second model is the previous one with the entity type of the answer as an additional feature . the performance of this model is a bit better than the first one but it is not enough to be utilized effectively for our pipeline . as we can see , the performance noticeably increased , which indicates that answer information is the key to predict the interrogative word needed . the fourth model , clearly outperforms the previous one , the fifth model is the same as the previous one but with the addition of the entitytype embedding of the answer . the combination of the three features ( answer , answer entity type , and passage ) yields to the best performance .",
        "generated_description": "our interrogative - word classifier is designed to identify if a given interrogative word is semantically related to the interrogative verb ( i . e . interrogative ) or not . in order to evaluate the effectiveness of our proposed classifier , we train a number of variants : ( 1 ) using only the cls classifier ( accuracy = 56 . 0 % ) , ( 2 ) adding ae and at features ( acc = 70 . 3 % and 73 . 3 % , respectively ) , ( 3 ) adding ner features ( ner ) further improves the accuracy to 56 . 6 % . ( 4 ) finally , we add both at and ner to our classifier and achieve an accuracy of 73 . 8 % ."
    },
    {
        "table_id": "113",
        "table_info": {
            "table_caption": "Table 7: Recall and precision of interrogative words of our interrogative-word classifier.",
            "table_column_names": [
                "Class",
                "Recall",
                "Precision"
            ],
            "table_content_values": [
                [
                    "What",
                    "87.7%",
                    "76.0%"
                ],
                [
                    "Which",
                    "1.4%",
                    "38.0%"
                ],
                [
                    "Where",
                    "65.9%",
                    "55.8%"
                ],
                [
                    "When",
                    "49.2%",
                    "69.8%"
                ],
                [
                    "Who",
                    "76.9%",
                    "66.7%"
                ],
                [
                    "Why",
                    "50.1%",
                    "74.1%"
                ],
                [
                    "How",
                    "70.5%",
                    "79.0%"
                ],
                [
                    "Others",
                    "10.5%",
                    "57.0%"
                ]
            ]
        },
        "gold_description": "in addition , we provide the recall and precision per class for our final interrogative - word classifier ( cls + at + ner in table 7 ) . however , the recall of which is very low . our model has also problem with why lastly , the recall of ' when is also low",
        "generated_description": "to evaluate the performance of our interrogative - word classifier , we manually annotated the number of interrogative words of each individual class . we observe that the classifier achieves high precision for the ‘ who ’ and ‘ what ’ classes , and low precision for ‘ when ’ , which is mainly because interrogators tend to use these words to refer to situations where their emotions are not fully developed . in addition , the model achieves high recall for the other classes , however , its precision is lower than that of the who and what classes . we conjecture that this is because a large number of questions are ‘ difficult ’ or ‘ unanswerable ’ which makes it difficult for the model to determine the answers ."
    },
    {
        "table_id": "114",
        "table_info": {
            "table_caption": "Table 1: Statistics on forests generated with various γ (upper half) and K (lower half) on the development set.",
            "table_column_names": [
                "[ITALIC] γ",
                "#Edge/#Node",
                "LAS",
                "Conn. Ratio(%)"
            ],
            "table_content_values": [
                [
                    "0.05",
                    "2.09",
                    "92.5",
                    "100.0"
                ],
                [
                    "0.1",
                    "1.57",
                    "91.2",
                    "99.5"
                ],
                [
                    "0.2",
                    "1.34",
                    "90.5",
                    "94.2"
                ],
                [
                    "0.3",
                    "1.04",
                    "88.0",
                    "77.6"
                ],
                [
                    "[ITALIC] K",
                    "#Edge/#Node",
                    "LAS",
                    "Conn. Ratio(%)"
                ],
                [
                    "1",
                    "1.00",
                    "86.4",
                    "100.0"
                ],
                [
                    "2",
                    "1.03",
                    "87.3",
                    "100.0"
                ],
                [
                    "5",
                    "1.09",
                    "89.1",
                    "100.0"
                ],
                [
                    "10",
                    "1.14",
                    "89.8",
                    "100.0"
                ]
            ]
        },
        "gold_description": "table 1 demonstrates several characteristics of the generated forests of both the edgewise and kbesteisner algorithms in section 5 . 1 , where \" # edge / # sent \" measures the forest density with the number of edges divided by the sentence length , \" las \" represents the oracle las score on 100 biomedical sentences with manually annotated dependency trees , and \" conn . ratio ( % ) \" shows the percentage of forests where both related entity mentions are connected . regarding the forest density , forests produced by edgewise generally contain more edges than those from kbesteisner . for connectivity , kbesteisner guarantees to generate spanning forests . on the other hand , the connectivity ratio for the forests produced by edgewise drops when increasing the threshold γ . we can have more than 94 % being connected with γ ≤ 0 . 2 .",
        "generated_description": "the higher the value of γ , the higher the edge ratio of node nodes , the more edges are generated , but the lower the maximal edge ratio is . we can see that the number of edges and the average node ratio of the forest with γ = 1 can be very large ( e . g . , 1 . 00 and 1 . 14 ) , but the las and conn . ratio ( also known as the ratio of edges / node ) can also be large as well . the result shows that a large γ produces very long forests , but a small γ also produces long forests . we also observe that a larger γ and larger k can also lead to shorter trees , which is consistent with the observation in \\ newcitekim2015tree ."
    },
    {
        "table_id": "115",
        "table_info": {
            "table_caption": "Table 2: Test results of Biocreative VI CPR. † indicates previously reported numbers. ** means significant over DepTree at p<0.01 with 1000 bootstrap tests (Efron and Tibshirani, 1994).",
            "table_column_names": [
                "Model",
                "F1 score"
            ],
            "table_content_values": [
                [
                    "GRU+Attn (Liu et al.,  2017 )†",
                    "49.5"
                ],
                [
                    "Bran (Verga et al.,  2018 )†",
                    "50.8"
                ],
                [
                    "TextOnly",
                    "50.6"
                ],
                [
                    "DepTree",
                    "51.4"
                ],
                [
                    "KBestEisnerPS",
                    "**52.4**"
                ],
                [
                    "EdgewisePS",
                    "** [BOLD] 53.4**"
                ]
            ]
        },
        "gold_description": "table 2 shows the main comparison results on the biocreative cpr testset , with comparisons to the previous state - of - the - art and our baselines . textonly gives a performance comparable with bran . with 1 - best dependency trees , our deptree baseline gives better performances than the previous state of the art . both kbesteisnerps and edgewiseps obtain significantly higher numbers than deptree .",
        "generated_description": "the proposed edgewiseps outperforms deptree at a significant level ( p < 0 . 01 with bootstrap tests ) and achieves a new state - of - the - art f1 score of 53 . 4 % , which significantly outperforms both the deptree and the textonly baselines ."
    },
    {
        "table_id": "116",
        "table_info": {
            "table_caption": "Table 3: Main results on PGR testest. † denotes previous numbers rounded into 3 significant digits. * and ** indicate significance over DepTree at p<0.05 and p<0.01 with 1000 bootstrap tests.",
            "table_column_names": [
                "Model",
                "F1 score"
            ],
            "table_content_values": [
                [
                    "BO-LSTM (Lamurias et al.,  2019 )†",
                    "52.3"
                ],
                [
                    "BioBERT (Lee et al.,  2019 )†",
                    "67.2"
                ],
                [
                    "TextOnly",
                    "76.0"
                ],
                [
                    "DepTree",
                    "78.9"
                ],
                [
                    "KBestEisnerPS",
                    "*83.6*"
                ],
                [
                    "EdgewisePS",
                    "** [BOLD] 85.7**"
                ]
            ]
        },
        "gold_description": "table 3 shows the comparison with previous work on the pgr testset , where our models are significantly better than the existing models . with 1 - best trees , deptree is 2 . 9 points better than textonly , both kbesteisnerps and edgewiseps significantly outperform deptr",
        "generated_description": "the deptree model achieves 78 . 9 % in f1 score , which is a significant improvement over the previous state - of - the - art ( i . e . , lee et al . , from the results , we see that the proposed edgewiseps outperforms the deptree at p < 0 . 01 with 1000 bootstrap tests . we also find that kbesteisnerps achieves the best performance with 83 . 6 % , which is 2 . 6 % higher than the previous best performance ."
    },
    {
        "table_id": "117",
        "table_info": {
            "table_caption": "Table 4: Main results on SemEval-2010 task 8 testest. † denotes previous numbers.",
            "table_column_names": [
                "Model",
                "F1 score"
            ],
            "table_content_values": [
                [
                    "C-GCN (Zhang et al.,  2018b )†",
                    "84.8"
                ],
                [
                    "C-AGGCN (Guo et al.,  2019 )†",
                    "85.7"
                ],
                [
                    "DepTree",
                    "84.6"
                ],
                [
                    "KBestEisnerPS",
                    "85.8"
                ],
                [
                    "EdgewisePS",
                    "86.3"
                ]
            ]
        },
        "gold_description": "as shown in table 4 , we conduct a preliminary study on semeval - 2010 task 8 deptree achieves similar performance as cgcn and is slightly worse than c - aggcn , both kbesteisnerps and edgewiseps outperform deptree they show comparable and slightly better performances than c - aggcn . edgewiseps is better than kbesteisnerps ,",
        "generated_description": "we can see that our proposed model edgewiseps achieves the best f1 score of 86 . 3 % , which is + 0 . 8 % absolute improvement over the previous state - of - the - art ."
    },
    {
        "table_id": "118",
        "table_info": {
            "table_caption": "Table 1: Accuracy (%) of Hits@K on relations with training instances fewer than 100/200.",
            "table_column_names": [
                "Training Instances Hits@K (Macro)",
                "Training Instances Hits@K (Macro)",
                "<100 10",
                "<100 15",
                "<100 20",
                "<200 10",
                "<200 15",
                "<200 20"
            ],
            "table_content_values": [
                [
                    "CNN",
                    "+ATT",
                    "<5.0",
                    "<5.0",
                    "18.5",
                    "<5.0",
                    "16.2",
                    "33.3"
                ],
                [
                    "[EMPTY]",
                    "+HATT",
                    "5.6",
                    "31.5",
                    "57.4",
                    "22.7",
                    "43.9",
                    "65.1"
                ],
                [
                    "[EMPTY]",
                    "+KATT",
                    "[BOLD] 9.1",
                    "[BOLD] 41.3",
                    "[BOLD] 58.5",
                    "[BOLD] 23.3",
                    "[BOLD] 44.1",
                    "[BOLD] 65.4"
                ],
                [
                    "PCNN",
                    "+ATT",
                    "<5.0",
                    "7.4",
                    "40.7",
                    "17.2",
                    "24.2",
                    "51.5"
                ],
                [
                    "[EMPTY]",
                    "+HATT",
                    "29.6",
                    "51.9",
                    "61.1",
                    "41.4",
                    "60.6",
                    "68.2"
                ],
                [
                    "[EMPTY]",
                    "+KATT",
                    "[BOLD] 35.3",
                    "[BOLD] 62.4",
                    "[BOLD] 65.1",
                    "[BOLD] 43.2",
                    "[BOLD] 61.3",
                    "[BOLD] 69.2"
                ]
            ]
        },
        "gold_description": "from the results shown in table 1 , we observe that for both cnn and pcnn models , our model outperforms the plain attention model and the hatt model .",
        "generated_description": "from the results , we can see that our model consistently outperforms all the baselines and achieves new state - of - the - art performance in terms of hits @ 10 , hits @ 15 and hits @ 20 . more importantly , our katt - enhanced models consistently outperform the pcnn and cnn baselines , which demonstrates the effectiveness of our framework in dealing with long - distance relations ."
    },
    {
        "table_id": "119",
        "table_info": {
            "table_caption": "Table 2: Results of ablation study with PCNN.",
            "table_column_names": [
                "Training Instances Hits@K (Macro)",
                "<100 10",
                "<100 15",
                "<100 20",
                "<200 10",
                "<200 15",
                "<200 20"
            ],
            "table_content_values": [
                [
                    "+KATT",
                    "[BOLD] 35.3",
                    "[BOLD] 62.4",
                    "[BOLD] 65.1",
                    "[BOLD] 43.2",
                    "[BOLD] 61.3",
                    "[BOLD] 69.2"
                ],
                [
                    "w/o hier",
                    "34.2",
                    "62.1",
                    "65.1",
                    "42.5",
                    "60.2",
                    "68.1"
                ],
                [
                    "w/o GCNs",
                    "30.5",
                    "61.9",
                    "63.1",
                    "39.5",
                    "58.4",
                    "66.1"
                ],
                [
                    "Word2vec",
                    "30.2",
                    "62.0",
                    "62.5",
                    "39.6",
                    "57.5",
                    "65.8"
                ],
                [
                    "w/o KG",
                    "30.0",
                    "61.0",
                    "61.3",
                    "39.5",
                    "56.5",
                    "62.5"
                ]
            ]
        },
        "gold_description": "from the evaluation results in table 2 , we observe that the performance slightly degraded without coarse - tofine attention , we also noticed that the performance slightly degraded without kg or using word embeddings , and the performance significantly degraded when we removed gcns .",
        "generated_description": "from the results , we can see that our model + katt significantly outperforms all the other models in terms of both hits @ k and macro - f1 score , which proves the effectiveness of our proposed katt . moreover , our katt + katt model also achieves the best performance when the training instance size is small , i . e . , 10 , 15 , 20 , and 40 . in such a small training instance , katt can learn a good representation of the target relation so that it can improve the performance of gcns and word2vec ."
    },
    {
        "table_id": "120",
        "table_info": {
            "table_caption": "Table 1: Spearman correlations with WordNet similarities (left) and human judgments (right) ×100.",
            "table_column_names": [
                "WordNet",
                "LCH  [ITALIC] 100",
                "ShP  [ITALIC] 100",
                "WuP  [ITALIC] 100",
                "LCH 51.3",
                "ShP 51.3",
                "WuP 47.4"
            ],
            "table_content_values": [
                [
                    "path2vec",
                    "[BOLD] 93.5",
                    "[BOLD] 95.2",
                    "[BOLD] 93.1",
                    "[BOLD] 53.2",
                    "[BOLD] 55.5",
                    "[BOLD] 55.5"
                ],
                [
                    "TransR",
                    "77.6",
                    "77.6",
                    "72.5",
                    "38.6",
                    "38.6",
                    "38.6"
                ],
                [
                    "node2vec",
                    "75.9",
                    "75.9",
                    "78.7",
                    "46.2",
                    "46.2",
                    "46.2"
                ],
                [
                    "DeepWalk",
                    "86.8",
                    "86.8",
                    "85.0",
                    "53.3",
                    "53.3",
                    "53.3"
                ],
                [
                    "FSE",
                    "90.0",
                    "90.0",
                    "89.0",
                    "55.6",
                    "55.6",
                    "55.6"
                ]
            ]
        },
        "gold_description": "discussion of results the left part of table 1 shows results with the wordnet similarity scores used as gold standard . path2vec outperforms other graph embeddings , achieving high correlations with wordnet similarities . this shows that our model efficiently approximates different graph measures . the right part of table 1 shows results for the correlations with human judgments ( simlex999 ) . we report the results for the best models for each method , all of them ( except fse ) using vector size 300 for comparability .",
        "generated_description": "we observe that path2vec and path2vec perform consistently better than all the other embedding models on both lch and wup , and outperform transr and node2vec by a large margin on the wup . all the neural models outperform fse , which is the best performing model on the lch task . however , the performance of deepwalk and fse on wup is much lower than that on lch . we conjecture that this is because the neural methods for wup are not tuned specifically to capture the hierarchical structure of the wordnet , while the models for lch are optimized for the single - class nature of the wordnet ."
    },
    {
        "table_id": "121",
        "table_info": {
            "table_caption": "Table 2: F1 scores of a graph-based WSD algorithm on WordNet versus its vectorized counterparts.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Senseval2",
                "[BOLD] Senseval3",
                "[BOLD] SemEval-15"
            ],
            "table_content_values": [
                [
                    "Random sense",
                    "0.381",
                    "0.312",
                    "0.393"
                ],
                [
                    "[ITALIC] Graph-based vs vector-based measures",
                    "[ITALIC] Graph-based vs vector-based measures",
                    "[ITALIC] Graph-based vs vector-based measures",
                    "[ITALIC] Graph-based vs vector-based measures"
                ],
                [
                    "LCH (WordNet)",
                    "0.547↓0.000",
                    "0.494↓0.000",
                    "0.550↓0.000"
                ],
                [
                    "LCH (path2vec)",
                    "0.527↓0.020",
                    "0.472↓0.022",
                    "0.536↓0.014"
                ],
                [
                    "ShP (WordNet)",
                    "0.548↓0.000",
                    "0.495↓0.000",
                    "0.550↓0.000"
                ],
                [
                    "ShP (path2vec)",
                    "0.534↓0.014",
                    "0.489↓0.006",
                    "0.563↑0.013"
                ],
                [
                    "WuP (WordNet)",
                    "0.547↓0.000",
                    "0.487↓0.000",
                    "0.542↓0.000"
                ],
                [
                    "WuP (path2vec)",
                    "0.543↓0.004",
                    "0.489↑0.002",
                    "0.545↑0.003"
                ],
                [
                    "[ITALIC] Various baseline graph embeddings trained on WordNet",
                    "[ITALIC] Various baseline graph embeddings trained on WordNet",
                    "[ITALIC] Various baseline graph embeddings trained on WordNet",
                    "[ITALIC] Various baseline graph embeddings trained on WordNet"
                ],
                [
                    "TransR",
                    "0.540",
                    "0.466",
                    "0.536"
                ],
                [
                    "node2vec",
                    "0.503",
                    "0.467",
                    "0.489"
                ],
                [
                    "DeepWalk",
                    "0.528",
                    "0.476",
                    "0.552"
                ],
                [
                    "FSE",
                    "0.536",
                    "0.476",
                    "0.523"
                ]
            ]
        },
        "gold_description": "discussion of results table 2 presents the wsd micro - f1 scores using raw wordnet similarities , 300d path2vec , deepwalk and node2vec models , and the 128d fse model . we evaluate on the following all - words english wsd test sets : senseval - 2 ( palmer et al . , 2001 ) , senseval - 3 ( mihalcea et al . , 2004 ) , and semeval - 15 task 13 ( moro and navigli , 2015 ) . the raw wordnet similarities have a small edge over their vector approximations in the majority of the cases yet the path2vec models consistently closely follow them while outperforming other graph embedding baselines : we indicate the differences with respect to the original with a subscript number .",
        "generated_description": "we compare the performance of the graph - based vs vector - based measures as well as the neural network - based and word embedding - based models . the neural network models outperform the other two baselines on all of the three datasets . we observe that the lch ( lch with path2vec ) and the wup ( wup ) models perform better than the random sense model . the lch and wup models have better performance than the other vector models on the senseval - 3 and semeval - 15 datasets , while the winobias model performs the worst . this is due to the fact that the embedding space of lch is much smaller than that of the vector models , which makes it hard for them to learn good embeddings . on the other hand , the vectors can learn representations that are more flexible than those of the graphs ."
    },
    {
        "table_id": "122",
        "table_info": {
            "table_caption": "Table 3: Similarity results on the RareWord set, measured as Spearman’s ρ×100. VarEmbed was trained on a 20-million token dataset, Polyglot on a 1.7B-token dataset.",
            "table_column_names": [
                "[EMPTY]",
                "Emb.",
                "Vocab",
                "Polyglot",
                "All"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "dim",
                    "size",
                    "in-vocab",
                    "pairs"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[ITALIC] N=862",
                    "[ITALIC] N=2034"
                ],
                [
                    "VarEmbed",
                    "128",
                    "100K",
                    "41.9",
                    "25.5"
                ],
                [
                    "Polyglot",
                    "64",
                    "100K",
                    "40.8",
                    "8.7"
                ],
                [
                    "mimick",
                    "64",
                    "0",
                    "17.9",
                    "17.5"
                ],
                [
                    "Polyglot",
                    "64",
                    "100K",
                    "40.8",
                    "27.0"
                ],
                [
                    "+mimick",
                    "64",
                    "100K",
                    "40.8",
                    "27.0"
                ],
                [
                    "Fasttext",
                    "300",
                    "2.51M",
                    "[EMPTY]",
                    "47.3"
                ]
            ]
        },
        "gold_description": "the results , shown in table 3 , demonstrate that the mimick rnn recovers about half of the loss in performance incurred by the original polyglot training model due to out - of - vocabulary words in the \" all pairs \" condition . mimick also outperforms varembed . fasttext can be considered an upper bound : with a vocabulary that is 25 times larger than the other models , it was missing words from only 44 pairs on this data .",
        "generated_description": "for comparison , we trained varembed and fasttext on a 20 - million token and polyglot versions of the rareword dataset to see how well they perform on a 1 . 7b - token english vocabulary . the results show that the polyglot version performs better than the poly - embedding version for all vocabularies , and outperforms fasttext for in - vocabulary words . however , it does not perform as well on out - of - vocab words . we suspect that this is because in rareword many of the most frequent words are out of the vocabulary ( e . g . , rare and rare - word adjectives ) and it is hard to find poly - embedding models that can learn to represent them accurately ."
    },
    {
        "table_id": "123",
        "table_info": {
            "table_caption": "Table 3: Results on argument generation by BLEU and METEOR (MTR), with system retrieved evidence and oracle retrieval. The best performing model is highlighted in bold per metric. Our separate decoder models, with and without keyphrase attention, statistically significantly outperform all seq2seq-based models based on approximation randomization testing Noreen (1989), p<0.0001.",
            "table_column_names": [
                "[EMPTY]",
                "[ITALIC] w/ System Retrieval  [BOLD] BLEU",
                "[ITALIC] w/ System Retrieval  [BOLD] MTR",
                "[ITALIC] w/ System Retrieval  [BOLD] Len",
                "[ITALIC] w/ Oracle Retrieval  [BOLD] BLEU",
                "[ITALIC] w/ Oracle Retrieval  [BOLD] MTR",
                "[ITALIC] w/ Oracle Retrieval  [BOLD] Len"
            ],
            "table_content_values": [
                [
                    "[BOLD] Baseline",
                    "[BOLD] Baseline",
                    "[BOLD] Baseline",
                    "[BOLD] Baseline",
                    "[BOLD] Baseline",
                    "[BOLD] Baseline",
                    "[BOLD] Baseline"
                ],
                [
                    "Retrieval",
                    "15.32",
                    "[BOLD] 12.19",
                    "151.2",
                    "10.24",
                    "[BOLD] 16.22",
                    "132.7"
                ],
                [
                    "[BOLD] Comparisons",
                    "[BOLD] Comparisons",
                    "[BOLD] Comparisons",
                    "[BOLD] Comparisons",
                    "[BOLD] Comparisons",
                    "[BOLD] Comparisons",
                    "[BOLD] Comparisons"
                ],
                [
                    "Seq2seq",
                    "10.21",
                    "5.74",
                    "34.9",
                    "7.44",
                    "5.25",
                    "31.1"
                ],
                [
                    "+  [ITALIC] encode evd",
                    "18.03",
                    "7.32",
                    "67.0",
                    "13.79",
                    "10.06",
                    "68.1"
                ],
                [
                    "+  [ITALIC] encode KP",
                    "21.94",
                    "8.63",
                    "74.4",
                    "12.96",
                    "10.50",
                    "78.2"
                ],
                [
                    "[BOLD] Our Models",
                    "[BOLD] Our Models",
                    "[BOLD] Our Models",
                    "[BOLD] Our Models",
                    "[BOLD] Our Models",
                    "[BOLD] Our Models",
                    "[BOLD] Our Models"
                ],
                [
                    "Dec-shared",
                    "21.22",
                    "8.91",
                    "69.1",
                    "15.78",
                    "11.52",
                    "68.2"
                ],
                [
                    "+  [ITALIC] attend KP",
                    "[BOLD] 24.71",
                    "10.05",
                    "74.8",
                    "11.48",
                    "10.08",
                    "40.5"
                ],
                [
                    "Dec-separate",
                    "24.24",
                    "10.63",
                    "88.6",
                    "17.48",
                    "13.15",
                    "86.9"
                ],
                [
                    "+  [ITALIC] attend KP",
                    "24.52",
                    "11.27",
                    "88.3",
                    "[BOLD] 17.80",
                    "13.67",
                    "86.8"
                ]
            ]
        },
        "gold_description": "as can be seen from table 3 , our models produce better bleu scores than almost all the comparisons . especially , our models with separate decoder yield significantly higher bleu and meteor scores than all seq2seq - based models ( approximation randomization testing , p < 0 . 0001 ) do . better meteor scores are achieved by the retrieval baseline , mainly due to its significantly longer arguments . moreover , utilizing attention over both input and the generated keyphrases further boosts our models ' performance . interestingly , utilizing system retrieved evidence yields better bleu scores than using oracle retrieval for testing .",
        "generated_description": "our baseline is a seq2seq model with the standard seq2seq decoder and decoder . the oracle score of bleu is 15 . 32 , mtr is 10 . 24 and len is 132 . 7 . with oracle retrieval , our model significantly outperforms the baseline model and achieves the best mtr score of 16 . 22 , 12 . 19 and 151 . 2 under the settings w / / or retrieval and w / or lexicalized similarity . in addition , we add the encode evd module to our baseline model to get more details about the encoding process ."
    },
    {
        "table_id": "124",
        "table_info": {
            "table_caption": "Table 5: POS tagging accuracy (UD 1.4 Test). Bold (Italic) indicates significant improvement (degradation) by McNemar’s test, p",
            "table_column_names": [
                "[EMPTY]",
                "[ITALIC] Ntrain=5000 No-Char",
                "[ITALIC] Ntrain=5000 mimick",
                "[ITALIC] Ntrain=5000 char",
                "[ITALIC] Ntrain=5000 Both",
                "Full data  [ITALIC] Ntrain",
                "Full data No-Char",
                "Full data mimick",
                "Full data char",
                "Both",
                "PSG"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "→tag",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "→tag",
                    "[EMPTY]",
                    "2016*"
                ],
                [
                    "kk",
                    "—",
                    "—",
                    "—",
                    "—",
                    "4,949",
                    "81.94",
                    "83.95",
                    "83.64",
                    "84.88",
                    "[EMPTY]"
                ],
                [
                    "ta",
                    "82.30",
                    "81.55",
                    "84.97",
                    "85.22",
                    "6,329",
                    "80.44",
                    "[BOLD] 82.96",
                    "84.11",
                    "84.46",
                    "[EMPTY]"
                ],
                [
                    "lv",
                    "80.44",
                    "[BOLD] 84.32",
                    "84.49",
                    "[BOLD] 85.91",
                    "13,781",
                    "85.77",
                    "[BOLD] 87.95",
                    "89.55",
                    "89.99",
                    "[EMPTY]"
                ],
                [
                    "vi",
                    "85.67",
                    "[ITALIC] 84.22",
                    "84.85",
                    "85.43",
                    "31,800",
                    "89.94",
                    "90.34",
                    "90.50",
                    "90.19",
                    "[EMPTY]"
                ],
                [
                    "hu",
                    "82.88",
                    "[BOLD] 88.93",
                    "85.83",
                    "[BOLD] 88.34",
                    "33,017",
                    "91.52",
                    "[BOLD] 93.88",
                    "94.07",
                    "93.74",
                    "[EMPTY]"
                ],
                [
                    "tr",
                    "83.69",
                    "[BOLD] 85.60",
                    "84.23",
                    "[BOLD] 86.25",
                    "41,748",
                    "90.19",
                    "[BOLD] 91.82",
                    "93.11",
                    "92.68",
                    "[EMPTY]"
                ],
                [
                    "el",
                    "93.10",
                    "[BOLD] 93.63",
                    "94.05",
                    "[BOLD] 94.64",
                    "47,449",
                    "97.27",
                    "[BOLD] 98.08",
                    "98.09",
                    "98.22",
                    "[EMPTY]"
                ],
                [
                    "bg",
                    "90.97",
                    "[BOLD] 93.16",
                    "93.03",
                    "[BOLD] 93.52",
                    "50,000",
                    "96.63",
                    "[BOLD] 97.29",
                    "97.95",
                    "97.78",
                    "98.23"
                ],
                [
                    "sv",
                    "90.87",
                    "[BOLD] 92.30",
                    "92.27",
                    "[BOLD] 93.02",
                    "66,645",
                    "95.26",
                    "[BOLD] 96.27",
                    "96.69",
                    "96.87",
                    "96.60"
                ],
                [
                    "eu",
                    "82.67",
                    "[BOLD] 84.44",
                    "86.01",
                    "[BOLD] 86.93",
                    "72,974",
                    "91.67",
                    "[BOLD] 93.16",
                    "94.46",
                    "94.29",
                    "95.38"
                ],
                [
                    "ru",
                    "87.40",
                    "[BOLD] 89.72",
                    "88.65",
                    "[BOLD] 90.91",
                    "79,772",
                    "92.59",
                    "[BOLD] 95.21",
                    "95.98",
                    "95.84",
                    "[EMPTY]"
                ],
                [
                    "da",
                    "89.46",
                    "90.13",
                    "89.96",
                    "90.55",
                    "88,980",
                    "94.14",
                    "[BOLD] 95.04",
                    "96.13",
                    "96.02",
                    "96.16"
                ],
                [
                    "id",
                    "89.07",
                    "89.34",
                    "89.81",
                    "90.21",
                    "97,531",
                    "92.92",
                    "93.24",
                    "93.41",
                    "[BOLD] 93.70",
                    "93.32"
                ],
                [
                    "zh",
                    "80.84",
                    "[BOLD] 85.69",
                    "81.84",
                    "[BOLD] 85.53",
                    "98,608",
                    "90.91",
                    "[BOLD] 93.31",
                    "93.36",
                    "93.72",
                    "[EMPTY]"
                ],
                [
                    "fa",
                    "93.50",
                    "93.58",
                    "93.53",
                    "93.71",
                    "121,064",
                    "96.77",
                    "[BOLD] 97.03",
                    "97.20",
                    "97.16",
                    "97.60"
                ],
                [
                    "he",
                    "90.73",
                    "[BOLD] 91.69",
                    "91.93",
                    "91.70",
                    "135,496",
                    "95.65",
                    "[BOLD] 96.15",
                    "96.59",
                    "96.37",
                    "96.62"
                ],
                [
                    "ro",
                    "87.73",
                    "[BOLD] 89.18",
                    "88.96",
                    "[BOLD] 89.38",
                    "163,262",
                    "95.68",
                    "[BOLD] 96.72",
                    "97.07",
                    "97.09",
                    "[EMPTY]"
                ],
                [
                    "en",
                    "87.48",
                    "[BOLD] 88.45",
                    "88.89",
                    "88.89",
                    "204,587",
                    "93.39",
                    "[BOLD] 94.04",
                    "94.90",
                    "94.70",
                    "95.17"
                ],
                [
                    "ar",
                    "89.01",
                    "[BOLD] 90.58",
                    "90.49",
                    "90.62",
                    "225,853",
                    "95.51",
                    "[BOLD] 95.72",
                    "96.37",
                    "96.24",
                    "98.87"
                ],
                [
                    "hi",
                    "87.89",
                    "87.77",
                    "87.92",
                    "88.09",
                    "281,057",
                    "96.31",
                    "96.45",
                    "96.64",
                    "96.61",
                    "96.97"
                ],
                [
                    "it",
                    "91.35",
                    "[BOLD] 92.50",
                    "92.45",
                    "[BOLD] 93.01",
                    "289,440",
                    "97.22",
                    "97.47",
                    "97.76",
                    "97.69",
                    "97.90"
                ],
                [
                    "es",
                    "90.54",
                    "[BOLD] 91.41",
                    "91.71",
                    "91.78",
                    "382,436",
                    "94.68",
                    "94.84",
                    "95.08",
                    "95.05",
                    "95.67"
                ],
                [
                    "cs",
                    "87.97",
                    "[BOLD] 90.81",
                    "90.17",
                    "[BOLD] 91.29",
                    "1,173,282",
                    "96.34",
                    "[BOLD] 97.62",
                    "98.18",
                    "[ITALIC] 97.93",
                    "98.02"
                ]
            ]
        },
        "gold_description": "we report the results on the full sets and on = 5000 tokens in table 5 ( partof - speech tagging accuracy ) for pos , the largest margins are in the slavic languages ( russian , czech , bulgarian ) , where word order is relatively free and thus rich word representations are imperative . chinese also exhibits impressive improvement across all settings , perhaps due to the large character inventory ( > 12 , 000 ) , for which a model such as mimick can learn well - informed embeddings using the large polyglot vocabulary dataset , overcoming both word - and characterlevel sparsity in the ud corpus .",
        "generated_description": "in the full data scenario , the character model outperforms the no - char model by a large margin in all languages . in the case of the smaller mimick setting , character models outperform the no - char models by a small margin but still perform on par with the state - of - the - art systems using full data . in contrast , the performance of the psg model is strong across all languages , outperforming the state of the art by an average of 0 . 8 % ( p < 0 . 01 ) and 0 . 7 % ( 0 . 2 % ) for pos tagging and pos tagging ."
    },
    {
        "table_id": "125",
        "table_info": {
            "table_caption": "Table 6: Micro-F1 for morphosyntactic attributes (UD 1.4 Test). Bold (Italic) type indicates significant improvement (degradation) by a bootstrapped Z-test, p",
            "table_column_names": [
                "[EMPTY]",
                "[ITALIC] Ntrain=5000 No-Char",
                "[ITALIC] Ntrain=5000 mimick",
                "[ITALIC] Ntrain=5000 char",
                "[ITALIC] Ntrain=5000 Both",
                "Full data No-Char",
                "Full data mimick",
                "Full data char",
                "Full data Both"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "→tag",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "→tag",
                    "[EMPTY]"
                ],
                [
                    "kk",
                    "—",
                    "—",
                    "—",
                    "—",
                    "21.48",
                    "20.07",
                    "28.47",
                    "20.98"
                ],
                [
                    "ta",
                    "80.68",
                    "[BOLD] 81.96",
                    "84.26",
                    "[BOLD] 85.63",
                    "79.90",
                    "[BOLD] 81.93",
                    "84.55",
                    "85.01"
                ],
                [
                    "lv",
                    "56.98",
                    "[BOLD] 59.86",
                    "64.81",
                    "[BOLD] 65.82",
                    "66.16",
                    "66.61",
                    "76.11",
                    "75.44"
                ],
                [
                    "hu",
                    "73.13",
                    "[BOLD] 76.30",
                    "73.62",
                    "[BOLD] 76.85",
                    "80.04",
                    "80.64",
                    "86.43",
                    "84.12"
                ],
                [
                    "tr",
                    "69.58",
                    "[BOLD] 75.21",
                    "75.81",
                    "[BOLD] 78.93",
                    "78.31",
                    "[BOLD] 83.32",
                    "91.51",
                    "90.86"
                ],
                [
                    "el",
                    "86.87",
                    "[ITALIC] 86.07",
                    "86.40",
                    "[BOLD] 87.50",
                    "94.64",
                    "[BOLD] 94.96",
                    "96.55",
                    "[BOLD] 96.76"
                ],
                [
                    "bg",
                    "78.26",
                    "[BOLD] 81.77",
                    "82.74",
                    "[BOLD] 84.93",
                    "91.98",
                    "[BOLD] 93.48",
                    "96.12",
                    "95.96"
                ],
                [
                    "sv",
                    "82.09",
                    "[BOLD] 84.12",
                    "85.26",
                    "[BOLD] 88.16",
                    "92.45",
                    "[BOLD] 94.20",
                    "96.37",
                    "[BOLD] 96.57"
                ],
                [
                    "eu",
                    "65.29",
                    "[BOLD] 66.00",
                    "70.67",
                    "[ITALIC] 70.27",
                    "82.75",
                    "[BOLD] 84.74",
                    "90.58",
                    "[BOLD] 91.39"
                ],
                [
                    "ru",
                    "77.31",
                    "[BOLD] 81.84",
                    "79.83",
                    "[BOLD] 83.53",
                    "88.80",
                    "[BOLD] 91.24",
                    "93.54",
                    "93.56"
                ],
                [
                    "da",
                    "80.26",
                    "[BOLD] 82.74",
                    "83.59",
                    "82.65",
                    "92.06",
                    "[BOLD] 94.14",
                    "96.05",
                    "95.96"
                ],
                [
                    "zh",
                    "63.29",
                    "[BOLD] 71.44",
                    "63.50",
                    "[BOLD] 74.66",
                    "84.95",
                    "85.70",
                    "84.86",
                    "85.87"
                ],
                [
                    "fa",
                    "84.73",
                    "[BOLD] 86.07",
                    "85.94",
                    "81.75",
                    "95.30",
                    "[BOLD] 95.55",
                    "96.90",
                    "96.80"
                ],
                [
                    "he",
                    "75.35",
                    "68.57",
                    "81.06",
                    "75.24",
                    "90.25",
                    "[BOLD] 90.99",
                    "93.35",
                    "93.63"
                ],
                [
                    "ro",
                    "84.20",
                    "[BOLD] 85.64",
                    "85.61",
                    "[BOLD] 87.31",
                    "94.97",
                    "[BOLD] 96.10",
                    "97.18",
                    "97.14"
                ],
                [
                    "en",
                    "86.71",
                    "[BOLD] 87.99",
                    "88.50",
                    "[BOLD] 89.61",
                    "95.30",
                    "[BOLD] 95.59",
                    "96.40",
                    "96.30"
                ],
                [
                    "ar",
                    "84.14",
                    "84.17",
                    "81.41",
                    "[ITALIC] 81.11",
                    "94.43",
                    "[BOLD] 94.85",
                    "95.50",
                    "95.37"
                ],
                [
                    "hi",
                    "83.45",
                    "[BOLD] 86.89",
                    "85.64",
                    "85.27",
                    "96.15",
                    "96.21",
                    "96.59",
                    "[BOLD] 96.67"
                ],
                [
                    "it",
                    "89.96",
                    "[BOLD] 92.07",
                    "91.27",
                    "[BOLD] 92.62",
                    "97.32",
                    "[BOLD] 97.80",
                    "98.18",
                    "98.31"
                ],
                [
                    "es",
                    "88.11",
                    "[BOLD] 89.81",
                    "88.58",
                    "[BOLD] 89.63",
                    "94.84",
                    "[BOLD] 95.44",
                    "96.21",
                    "[BOLD] 96.84"
                ],
                [
                    "cs",
                    "68.66",
                    "[BOLD] 72.65",
                    "71.02",
                    "[BOLD] 73.61",
                    "91.75",
                    "[BOLD] 93.71",
                    "95.29",
                    "95.31"
                ]
            ]
        },
        "gold_description": "table 6 ( morphosyntactic attribute tagging micro - f1 ) . in morphosyntactic tagging , gains are apparent for slavic languages and chinese , but also for agglutinative languages — especially tamil and turkish — where the stable morpheme representation makes it easy for subword modeling to provide a type - level signal .",
        "generated_description": "in the no - char setting , we see that the character model outperforms the no - char model by a large margin in terms of mimick score . in the full data setting , the performance is close to that of the full data setting and outperforms by a small margin when using the char model . in addition , we observe that using both the character and the tag embeddings together yields the best overall performance ."
    },
    {
        "table_id": "126",
        "table_info": {
            "table_caption": "Table 7: Absolute gain in POS tagging accuracy from using mimick for 10,000-token datasets (all tokens for Tamil and Kazakh). Bold denotes statistical significance (McNemar’s test,p<0.01).",
            "table_column_names": [
                "Test set embeddings",
                "Missing embeddings",
                "Full vocabulary",
                "Full vocabulary",
                "OOV (UD)",
                "OOV (UD)"
            ],
            "table_content_values": [
                [
                    "char→tag",
                    "[EMPTY]",
                    "w/o",
                    "with",
                    "w/o",
                    "with"
                ],
                [
                    "Persian",
                    "2.2%",
                    "0.03",
                    "[BOLD] 0.41",
                    "[BOLD] 0.83",
                    "[BOLD] 0.81"
                ],
                [
                    "Hindi",
                    "3.8%",
                    "[BOLD] 0.59",
                    "0.21",
                    "[BOLD] 3.61",
                    "0.36"
                ],
                [
                    "English",
                    "4.5%",
                    "[BOLD] 0.83",
                    "0.25",
                    "[BOLD] 3.26",
                    "0.49"
                ],
                [
                    "Spanish",
                    "5.2%",
                    "0.33",
                    "-0.26",
                    "1.03",
                    "-0.66"
                ],
                [
                    "Italian",
                    "6.6%",
                    "[BOLD] 0.84",
                    "0.28",
                    "[BOLD] 1.83",
                    "0.21"
                ],
                [
                    "Danish",
                    "7.8%",
                    "0.65",
                    "[BOLD] 0.99",
                    "[BOLD] 2.41",
                    "[BOLD] 1.72"
                ],
                [
                    "Hebrew",
                    "9.2%",
                    "[BOLD] 1.25",
                    "[BOLD] 0.40",
                    "[BOLD] 3.03",
                    "0.06"
                ],
                [
                    "Swedish",
                    "9.2%",
                    "[BOLD] 1.50",
                    "[BOLD] 0.55",
                    "[BOLD] 4.75",
                    "[BOLD] 1.79"
                ],
                [
                    "Bulgarian",
                    "9.4%",
                    "[BOLD] 0.96",
                    "0.12",
                    "[BOLD] 1.83",
                    "-0.11"
                ],
                [
                    "Czech",
                    "10.6%",
                    "[BOLD] 2.24",
                    "[BOLD] 1.32",
                    "[BOLD] 5.84",
                    "[BOLD] 2.20"
                ],
                [
                    "Latvian",
                    "11.1%",
                    "[BOLD] 2.87",
                    "[BOLD] 1.03",
                    "[BOLD] 7.29",
                    "[BOLD] 2.71"
                ],
                [
                    "Hungarian",
                    "11.6%",
                    "[BOLD] 2.62",
                    "[BOLD] 2.01",
                    "[BOLD] 5.76",
                    "[BOLD] 4.85"
                ],
                [
                    "Turkish",
                    "14.5%",
                    "[BOLD] 1.73",
                    "[BOLD] 1.69",
                    "[BOLD] 3.58",
                    "[BOLD] 2.71"
                ],
                [
                    "Tamil*",
                    "16.2%",
                    "[BOLD] 2.52",
                    "0.35",
                    "2.09",
                    "1.35"
                ],
                [
                    "Russian",
                    "16.5%",
                    "[BOLD] 2.17",
                    "[BOLD] 1.82",
                    "[BOLD] 4.55",
                    "[BOLD] 3.52"
                ],
                [
                    "Greek",
                    "17.5%",
                    "[BOLD] 1.07",
                    "0.34",
                    "[BOLD] 3.30",
                    "1.17"
                ],
                [
                    "Indonesian",
                    "19.1%",
                    "[BOLD] 0.46",
                    "0.25",
                    "[BOLD] 1.19",
                    "0.75"
                ],
                [
                    "Kazakh*",
                    "21.0%",
                    "2.01",
                    "1.24",
                    "[BOLD] 5.34",
                    "[BOLD] 4.20"
                ],
                [
                    "Vietnamese",
                    "21.9%",
                    "0.53",
                    "[BOLD] 1.18",
                    "1.07",
                    "[BOLD] 5.73"
                ],
                [
                    "Romanian",
                    "27.1%",
                    "[BOLD] 1.49",
                    "[BOLD] 0.47",
                    "[BOLD] 4.22",
                    "[BOLD] 1.24"
                ],
                [
                    "Arabic",
                    "27.1%",
                    "[BOLD] 1.23",
                    "[BOLD] 0.32",
                    "[BOLD] 2.15",
                    "0.22"
                ],
                [
                    "Basque",
                    "35.3%",
                    "[BOLD] 2.39",
                    "[BOLD] 1.06",
                    "[BOLD] 5.42",
                    "[BOLD] 1.68"
                ],
                [
                    "Chinese",
                    "69.9%",
                    "[BOLD] 4.19",
                    "[BOLD] 2.57",
                    "[BOLD] 9.52",
                    "[BOLD] 5.24"
                ]
            ]
        },
        "gold_description": "table 7 presents the pos tagging improvements that mimick achieves over the pre - trained polyglot models , with and without char → tag concatenation , with 10 , 000 tokens of training data . we obtain statistically significant improvements in most languages , even when char → tag is included . these improvements are particularly substantial for test - set tokens outside the ud training set , as shown in the right two columns . while test set oovs are a strength of the char → tag model ( plank et al . , 2016 ) , in many languages there are still considerable improvements to be obtained from the application of mimick initialization .",
        "generated_description": "we see that the number of oov words in the test sets is reduced in both full vocabulary and oov ( ud ) setting when the char → tag embedding matrix is concatenated with the pre - trained word embeddings . this is consistent with the findings of \\ newcitehashimoto - etal : 2017 : emnlp : long \\ shortcitearaghri2017 : emnlp2017 : long which showed that character – word co - occurrences are important for morphologically rich languages like persian and hindi . we also observe that the vocabulary reduction in the oov setting is more significant in the full vocabulary setting than in the character → tag setting , suggesting that the character – tag information helps to reduce the vocabulary size of the test set . in addition , we also compare the effect of the different kinds of missing embedding in our model . the results show that the model significantly benefits from the following types of missing information : ( 1 ) missing word embedding vectors , ( 2 ) missing character embedding vector , and ( 3 ) missing vocabulary information ."
    },
    {
        "table_id": "127",
        "table_info": {
            "table_caption": "Table 3: Accuracies (in %) and perplexities for different models and scenarios. The script model substantially outperforms linguistic and base models (with p<0.001, significance tested with McNemar’s test [Everitt1992]). As expected, the human prediction model outperforms the script model (with p<0.001, significance tested by McNemar’s test).",
            "table_column_names": [
                "Scenario",
                "Human Model Accuracy",
                "Human Model Perplexity",
                "Script Model Accuracy",
                "Script Model Perplexity",
                "Linguistic Model Accuracy",
                "Linguistic Model Perplexity",
                "Tily Model Accuracy",
                "Tily Model Perplexity"
            ],
            "table_content_values": [
                [
                    "Grocery Shopping",
                    "74.80",
                    "2.13",
                    "68.17",
                    "3.16",
                    "53.85",
                    "6.54",
                    "32.89",
                    "24.48"
                ],
                [
                    "Repairing a flat bicycle tyre",
                    "78.34",
                    "2.72",
                    "62.09",
                    "3.89",
                    "51.26",
                    "6.38",
                    "29.24",
                    "19.08"
                ],
                [
                    "Riding a public bus",
                    "72.19",
                    "2.28",
                    "64.57",
                    "3.67",
                    "52.65",
                    "6.34",
                    "32.78",
                    "23.39"
                ],
                [
                    "Getting a haircut",
                    "71.06",
                    "2.45",
                    "58.82",
                    "3.79",
                    "42.82",
                    "7.11",
                    "28.70",
                    "15.40"
                ],
                [
                    "Planting a tree",
                    "71.86",
                    "2.46",
                    "59.32",
                    "4.25",
                    "47.80",
                    "7.31",
                    "28.14",
                    "24.28"
                ],
                [
                    "Borrowing book from library",
                    "77.49",
                    "1.93",
                    "64.07",
                    "3.55",
                    "43.29",
                    "8.40",
                    "33.33",
                    "20.26"
                ],
                [
                    "Taking Bath",
                    "81.29",
                    "1.84",
                    "67.42",
                    "3.14",
                    "61.29",
                    "4.33",
                    "43.23",
                    "16.33"
                ],
                [
                    "Going on a train",
                    "70.79",
                    "2.39",
                    "58.73",
                    "4.20",
                    "47.62",
                    "7.68",
                    "30.16",
                    "35.11"
                ],
                [
                    "Baking a cake",
                    "76.43",
                    "2.16",
                    "61.79",
                    "5.11",
                    "46.40",
                    "9.16",
                    "24.07",
                    "23.67"
                ],
                [
                    "Flying in an airplane",
                    "62.04",
                    "3.08",
                    "61.31",
                    "4.01",
                    "48.18",
                    "7.27",
                    "30.90",
                    "30.18"
                ],
                [
                    "Average",
                    "73.63",
                    "2.34",
                    "62.63",
                    "3.88",
                    "49.52",
                    "7.05",
                    "31.34",
                    "23.22"
                ]
            ]
        },
        "gold_description": "see table 3 for the averages across 10 scenarios . as we can see from table 3 , the perplexity scores are consistent with the accuracies : the script model again outperforms other methods , and , as expected , all the models are weaker than humans .",
        "generated_description": "the results show that the linguistic accuracy is the lowest across all the scenarios , which indicates that the model tends to generate generic and generic responses rather than specific and specific words to address a user ’ s needs . for example , “ riding a public bus ” and “ borrowing book from library ” are two examples of generic responses , where the model simply uses generic “ i don ’ t need to fix that ” instead of specific words ( e . g . “ wanting a haircut ” or “ a new pair of pants ” ) . the script accuracy is higher than human accuracy for most of the scenarios but for “ fixing a flat bicycle tyre ” , the linguistic accuracy is slightly lower than that of the human response . we conjecture that this is due to the fact that the dialogue history in this scenario is relatively short and the model has to memorize all words rather than just memorizing specific words like “ riding a bicycle ” since it is a generic response ."
    },
    {
        "table_id": "128",
        "table_info": {
            "table_caption": "Table 4: Evaluation on topic relevance—models that generate arguments highly related with OP should be ranked high by a separately trained relevance estimation model, i.e., higher Mean Reciprocal Rank (MRR) and Precision at 1 (P@1) scores. All models trained with evidence significantly outperform seq2seq trained without evidence (approximation randomization testing, p<0.0001).",
            "table_column_names": [
                "[EMPTY]",
                "[BOLD] Standard Decoder MRR",
                "[BOLD] Standard Decoder P@1",
                "[BOLD] Our Decoder MRR",
                "[BOLD] Our Decoder P@1"
            ],
            "table_content_values": [
                [
                    "[BOLD] Baseline",
                    "[BOLD] Baseline",
                    "[BOLD] Baseline",
                    "[BOLD] Baseline",
                    "[BOLD] Baseline"
                ],
                [
                    "Retrieval",
                    "81.08",
                    "65.45",
                    "-",
                    "-"
                ],
                [
                    "[BOLD] Comparisons",
                    "[BOLD] Comparisons",
                    "[BOLD] Comparisons",
                    "[BOLD] Comparisons",
                    "[BOLD] Comparisons"
                ],
                [
                    "Seq2seq",
                    "75.29",
                    "58.85",
                    "74.46",
                    "57.06"
                ],
                [
                    "+  [ITALIC] encode evd",
                    "83.73",
                    "71.59",
                    "88.24",
                    "78.76"
                ],
                [
                    "[BOLD] Our Models",
                    "[BOLD] Our Models",
                    "[BOLD] Our Models",
                    "[BOLD] Our Models",
                    "[BOLD] Our Models"
                ],
                [
                    "Dec-shared",
                    "79.80",
                    "65.57",
                    "[BOLD] 95.18",
                    "[BOLD] 90.91"
                ],
                [
                    "+  [ITALIC] attend KP",
                    "[BOLD] 94.33",
                    "[BOLD] 89.76",
                    "93.48",
                    "87.91"
                ],
                [
                    "Dec-separate",
                    "86.85",
                    "76.74",
                    "91.70",
                    "84.72"
                ],
                [
                    "+  [ITALIC] attend KP",
                    "88.53",
                    "79.05",
                    "92.77",
                    "86.46"
                ]
            ]
        },
        "gold_description": "ranking metrics of mrr and precision at 1 ( p @ 1 ) are utilized , with results reported in table 4 . the ranker yields significantly better scores over arguments generated from models trained with evidence , compared to arguments generated by seq2seq model .",
        "generated_description": "our decoder outperforms the standard seq2seq model and the retrieval - based baseline by a large margin , which demonstrates the importance of having a strong encoder - decoder architecture . our model with attend kp as encoder gives the best mrr and p @ 1 performance , which indicates that the knowledge encoded in the encoder contributes the most to the final performance . our decoders also outperform the retrieval baselines , which shows that our encoder is capable of finding the relevant information in the context and store it in the decoder ."
    },
    {
        "table_id": "129",
        "table_info": {
            "table_caption": "Table 5: Coefficients obtained from regression analysis for different models. Two NP types considered: full NP and Pronoun/ProperNoun, with base class full NP. Significance: ‘***’ <0.001, ‘**’ <0.01, ‘*’ <0.05, and ‘.’ <0.1.",
            "table_column_names": [
                "[EMPTY]",
                "Estimate Human",
                "Estimate Script",
                "Estimate Linguistic",
                "Estimate Base",
                "Std. Error Human",
                "Std. Error Script",
                "Std. Error Linguistic",
                "Std. Error Base",
                "Pr(>∣ [ITALIC] z∣) Human",
                "Pr(>∣ [ITALIC] z∣) Script",
                "Pr(>∣ [ITALIC] z∣) Linguistic",
                "Pr(>∣ [ITALIC] z∣) Base"
            ],
            "table_content_values": [
                [
                    "(Intercept)",
                    "-3.4",
                    "-3.418",
                    "-3.245",
                    "-3.061",
                    "0.244",
                    "0.279",
                    "0.321",
                    "0.791",
                    "<2e-16 ***",
                    "<2e-16 ***",
                    "<2e-16 ***",
                    "0.00011 ***"
                ],
                [
                    "recency",
                    "1.322",
                    "1.322",
                    "1.324",
                    "1.322",
                    "0.095",
                    "0.095",
                    "0.096",
                    "0.097",
                    "<2e-16 ***",
                    "<2e-16 ***",
                    "<2e-16 ***",
                    "<2e-16 ***"
                ],
                [
                    "frequency",
                    "0.097",
                    "0.103",
                    "0.112",
                    "0.114",
                    "0.098",
                    "0.097",
                    "0.098",
                    "0.102",
                    "0.317",
                    "0.289",
                    "0.251",
                    "0.262"
                ],
                [
                    "pastObj",
                    "0.407",
                    "0.396",
                    "0.423",
                    "0.395",
                    "0.293",
                    "0.294",
                    "0.295",
                    "0.3",
                    "0.165",
                    "0.178",
                    "0.151",
                    "0.189"
                ],
                [
                    "pastSubj",
                    "-0.967",
                    "-0.973",
                    "-0.909",
                    "-0.926",
                    "0.559",
                    "0.564",
                    "0.562",
                    "0.565",
                    "0.0838 .",
                    "0.0846 .",
                    "0.106",
                    "0.101"
                ],
                [
                    "pastExpPronoun",
                    "1.603",
                    "1.619",
                    "1.616",
                    "1.602",
                    "0.21",
                    "0.207",
                    "0.208",
                    "0.245",
                    "2.19e-14 ***",
                    "5.48e-15 ***",
                    "7.59e-15 ***",
                    "6.11e-11 ***"
                ],
                [
                    "depTypeSubj",
                    "2.939",
                    "2.942",
                    "2.656",
                    "2.417",
                    "0.299",
                    "0.347",
                    "0.429",
                    "1.113",
                    "<2e-16 ***",
                    "<2e-16 ***",
                    "5.68e-10 ***",
                    "0.02994 *"
                ],
                [
                    "depTypeObj",
                    "1.199",
                    "1.227",
                    "0.977",
                    "0.705",
                    "0.248",
                    "0.306",
                    "0.389",
                    "1.109",
                    "1.35e-06 ***",
                    "6.05e-05 ***",
                    "0.0119 *",
                    "0.525"
                ],
                [
                    "surprisal",
                    "-0.04",
                    "-0.006",
                    "0.002",
                    "-0.131",
                    "0.099",
                    "0.097",
                    "0.117",
                    "0.387",
                    "0.684",
                    "0.951",
                    "0.988",
                    "0.735"
                ],
                [
                    "residualEntropy",
                    "-0.009",
                    "0.023",
                    "-0.141",
                    "-0.128",
                    "0.088",
                    "0.128",
                    "0.168",
                    "0.258",
                    "0.916",
                    "0.859",
                    "0.401",
                    "0.619"
                ]
            ]
        },
        "gold_description": "the results of all four logistic regression models are shown in table 5 . the results for the full dataset are fully consistent with the findings shown in table 5 : there was no significant effect of surprisal on referring expression type . in order to replicate their settings as closely as possible , we also included residualentropy as a predictor in our model ( see last predictor in table 5 ) ; however , this did not change the results .",
        "generated_description": "we find that the linguistic error is lowest for ( intercept ) and ( recency ) , and the human error is highest for ( already ) , indicating that utterances tend to start and end with a relatively abrupt change in script . we also find that both human and linguistic error are higher for triggers that are more subtle ( e . g . , “ i don ’ t know ” or “ that ’ s all i needed ” ) , which is consistent with the findings in [ hovy - etal : 2015 : naacl - hlt ] . we also observe that the overall frequency of utterances tends to be lower than the other two , which is possibly due to the fact that the recording device was placed closer to the speaker who uttered the most during the recording , thus the signal from the speaker was stronger ."
    },
    {
        "table_id": "130",
        "table_info": {
            "table_caption": "Table 1: Results on different SP acquisition evaluation sets. As Keller is created based on the PP distribution and has relatively small size while SP-10K is created based on random sampling and has a much larger size, we treat the performance on SP-10K as the main evaluation metric. Spearman’s correlation between predicated plausibility and annotations are reported. The best performing models are denoted with bold font. † indicates statistical significant (p <0.005) overall baseline methods.",
            "table_column_names": [
                "Model",
                "Downstream",
                "Keller dobj",
                "Keller amod",
                "Keller average",
                "SP-10K nsubj",
                "SP-10K dobj",
                "SP-10K amod",
                "SP-10K average"
            ],
            "table_content_values": [
                [
                    "word2vec",
                    "Friendly",
                    "0.29",
                    "0.28",
                    "0.29",
                    "0.32",
                    "0.53",
                    "0.62",
                    "0.49"
                ],
                [
                    "GloVe",
                    "Friendly",
                    "0.37",
                    "0.32",
                    "0.35",
                    "0.57",
                    "0.60",
                    "0.68",
                    "0.62"
                ],
                [
                    "D-embeddings",
                    "Friendly",
                    "0.19",
                    "0.22",
                    "0.21",
                    "0.66",
                    "0.71",
                    "0.77",
                    "0.71"
                ],
                [
                    "ELMo",
                    "Friendly",
                    "0.23",
                    "0.06",
                    "0.15",
                    "0.09",
                    "0.29",
                    "0.38",
                    "0.25"
                ],
                [
                    "BERT (static)",
                    "Friendly",
                    "0.11",
                    "0.05",
                    "0.08",
                    "0.25",
                    "0.32",
                    "0.27",
                    "0.28"
                ],
                [
                    "BERT (dynamic)",
                    "Friendly",
                    "0.19",
                    "0.23",
                    "0.21",
                    "0.35",
                    "0.45",
                    "0.51",
                    "0.41"
                ],
                [
                    "PP",
                    "Unfriendly",
                    "[BOLD] 0.66",
                    "0.26",
                    "0.46",
                    "0.75",
                    "0.74",
                    "0.75",
                    "0.75"
                ],
                [
                    "DS",
                    "Unfriendly",
                    "0.53",
                    "0.32",
                    "0.43",
                    "0.59",
                    "0.65",
                    "0.67",
                    "0.64"
                ],
                [
                    "NN",
                    "Unfriendly",
                    "0.16",
                    "0.13",
                    "0.15",
                    "0.70",
                    "0.68",
                    "0.68",
                    "0.69"
                ],
                [
                    "MWE",
                    "Friendly",
                    "0.63",
                    "[BOLD] 0.43†",
                    "[BOLD] 0.53†",
                    "[BOLD] 0.76",
                    "[BOLD] 0.79†",
                    "[BOLD] 0.78",
                    "[BOLD] 0.78†"
                ]
            ]
        },
        "gold_description": "the experimental results are shown in table 1 . as keller is created based on the pp distribution and have relatively small size while sp - 10k is created based on random sampling and has a much larger size , we treat the performance on sp - 10k as the major evaluation . our embeddings significantly outperform other baselines , especially embedding based baselines . the only exception is pp on the keller dataset due to its biased distribution . in addition , there are other interesting observations . first , compared with ' dobj ' and ' nsubj ' , ' amod ' is simpler for word2vec and glove . the reason behind is that conventional embeddings only capture the co - occurrence information , which is enough to predict the selectional preference of",
        "generated_description": "the table shows that the dynamic bert embeddings perform the best across the two tasks , with an average score of 0 . 28 for the keller task and 0 . 25 for the sp - 10k task . this indicates that the model is able to benefit from information coming from the downstream tasks when training on a large scale corpus , and to generalize well across downstream tasks ."
    },
    {
        "table_id": "131",
        "table_info": {
            "table_caption": "Table 3: Spearman’s correlation of different embeddings for the WS measurement. ‘nsubj’, ‘dobj’, ‘amod’ represents the embeddings of the corresponding relation and ‘center’ indicates the center embeddings. h, t, h+t, and [h,t] refer to the head, tail, sum of two embeddings, and the concatenation of them, respectively. The best scores are marked in bold fonts.",
            "table_column_names": [
                "Model word2vec",
                "Model word2vec",
                "noun 0.41",
                "verb 0.28",
                "adjective 0.44",
                "overall 0.38"
            ],
            "table_content_values": [
                [
                    "Glove",
                    "Glove",
                    "0.40",
                    "0.22",
                    "0.53",
                    "0.37"
                ],
                [
                    "D-embedding",
                    "D-embedding",
                    "0.41",
                    "0.27",
                    "0.38",
                    "0.36"
                ],
                [
                    "nsubj",
                    "h",
                    "0.46",
                    "0.29",
                    "0.54",
                    "0.43"
                ],
                [
                    "nsubj",
                    "t",
                    "0.45",
                    "0.25",
                    "0.48",
                    "0.40"
                ],
                [
                    "nsubj",
                    "h+t",
                    "0.44",
                    "0.23",
                    "0.50",
                    "0.40"
                ],
                [
                    "nsubj",
                    "[h,t]",
                    "0.47",
                    "0.27",
                    "0.51",
                    "0.42"
                ],
                [
                    "dobj",
                    "h",
                    "0.46",
                    "0.27",
                    "0.45",
                    "0.41"
                ],
                [
                    "dobj",
                    "t",
                    "0.45",
                    "0.23",
                    "0.46",
                    "0.40"
                ],
                [
                    "dobj",
                    "h+t",
                    "0.45",
                    "0.20",
                    "0.45",
                    "0.38"
                ],
                [
                    "dobj",
                    "[h,t]",
                    "0.46",
                    "0.25",
                    "0.48",
                    "0.42"
                ],
                [
                    "amod",
                    "h",
                    "0.47",
                    "0.25",
                    "0.52",
                    "0.37"
                ],
                [
                    "amod",
                    "t",
                    "0.46",
                    "0.24",
                    "0.50",
                    "0.38"
                ],
                [
                    "amod",
                    "h+t",
                    "0.46",
                    "0.24",
                    "0.52",
                    "0.38"
                ],
                [
                    "amod",
                    "[h,t]",
                    "0.47",
                    "0.26",
                    "0.52",
                    "0.38"
                ],
                [
                    "center",
                    "h",
                    "0.51",
                    "[BOLD] 0.33",
                    "[BOLD] 0.57",
                    "[BOLD] 0.48"
                ],
                [
                    "center",
                    "t",
                    "0.51",
                    "0.30",
                    "0.56",
                    "0.47"
                ],
                [
                    "center",
                    "h+t",
                    "[BOLD] 0.52",
                    "0.31",
                    "0.54",
                    "0.46"
                ],
                [
                    "center",
                    "[h,t]",
                    "0.51",
                    "0.32",
                    "0.57",
                    "[BOLD] 0.48"
                ]
            ]
        },
        "gold_description": "results are reported in table 3 with several observations . first , our model achieves the best overall performance and significantly better on nouns , which can be explained by that nouns appear in all 9the only possible sp relation between nouns and adjectives is ' amod ' , while multiple sp relations could exist between nouns and verbs , and co occurrence information cannot effectively distinguish them . ( 2017 ) that three relations while most of the verbs and adjectives only appear in one or two relations . this result is promising since it is analyzed by solovyev et al . two - thirds of the frequent words are nouns ; thus there are potential benefits if our embeddings are used in downstream nlp tasks . second , the center embeddings achieve the best performance against all the other relationdependent embeddings , which demonstrates the effectiveness of our model in learning relationdependent information over words and also enhancing their overall semantics .",
        "generated_description": "in this section , we present the results of our experiments with different versions of the dobj and nsubj subj - embeddings , as well as the glove and word2vec embeddings . we observe that the d - embedding performs better than the glove embedding for nouns , verbs , adjectives and adverbs , and the overall score is close to 0 . 38 . this is due to the fact that the adverbs and nouns are the most difficult ones to classify , which is also why glove performs better . we also observe that in most cases , the best performing embedding is a combination of both d - and glove vectors ."
    },
    {
        "table_id": "132",
        "table_info": {
            "table_caption": "Table 4: Comparison of MWE against language models on the WS task. Overall performance, embedding dimension, and training time (days) on a single GPU are reported.",
            "table_column_names": [
                "Model",
                "WS",
                "Dimension",
                "Training Time"
            ],
            "table_content_values": [
                [
                    "ELMo",
                    "0.434",
                    "512",
                    "≈40"
                ],
                [
                    "BERT",
                    "0.486",
                    "768",
                    "≈300"
                ],
                [
                    "MWE",
                    "0.476",
                    "300",
                    "4.17"
                ]
            ]
        },
        "gold_description": "we also compare mwe with pre - trained contextualized word embedding models in table 4 for this task , with overall performance , embedding dimensions , and training times reported . it is observed that that mwe outperforms elmo and achieves comparable results with bert with smaller embedding dimension and much less training complexities .",
        "generated_description": "we can see that mwe is significantly faster than elmo and bert , and is comparable to elmo in terms of embedding dimension , training time , and embedding size . however , the training time of elmo is ≈ 40 times longer than that of mwe . bert is trained on a gpu cluster on a single nvidia titan x gpu for about 300k iterations , while mwe requires approximately 20k iterations . we attribute this difference to the fact that bert uses a much larger number of parameters ( 512 rather than 768 ) and is trained for a much longer time . on the other hand , mwe relies on fewer parameters and is faster due to its simple cross - entropy loss ."
    },
    {
        "table_id": "133",
        "table_info": {
            "table_caption": "Table 5: Comparisons of different training strategies.",
            "table_column_names": [
                "Training Strategy",
                "Averaged SPA",
                "Overall WS"
            ],
            "table_content_values": [
                [
                    "[ITALIC] λ = 1",
                    "0.762",
                    "0.476"
                ],
                [
                    "[ITALIC] λ = 0",
                    "0.073",
                    "0.018"
                ],
                [
                    "[ITALIC] λ = 0.5",
                    "0.493",
                    "0.323"
                ],
                [
                    "Alternating optimization",
                    "0.775",
                    "0.476"
                ]
            ]
        },
        "gold_description": "as shown in table 5 , we compare our model with several different strategies . the first one is to put all weights to the center embedding ( fix λ to 1 ) , which never updates the local relational embeddings . as a result , it can achieve similar performance on word similarity measurement but is inferior in sp acquisition because no relationdependent information is preserved .",
        "generated_description": "alternating optimization refers to the case where we first train the model with λ = 1 , then choose the hyperparameter λ to train the classifier , and train it alternately with different values of λ . it can be observed that training with the proposed training strategy leads to better results than the other two training strategies . the reason is that the training of the document - level semantic parser is much more difficult than that of the word - level parser . in addition , the training data is larger and the number of epochs to train is much less than that in the word level parser , which makes the training more efficient . alternating training leads to the best results ."
    },
    {
        "table_id": "134",
        "table_info": {
            "table_caption": "Table 3: Single Transformers trained to convergence on 1M WAT Ja-En, batch size 4096",
            "table_column_names": [
                "Representation",
                "Batches / update",
                "Learning rate",
                "Test BLEU"
            ],
            "table_content_values": [
                [
                    "Plain BPE",
                    "1",
                    "0.025",
                    "27.5"
                ],
                [
                    "Plain BPE",
                    "1",
                    "0.2",
                    "27.2"
                ],
                [
                    "Plain BPE",
                    "8",
                    "0.2",
                    "28.9"
                ],
                [
                    "Linearized derivation",
                    "1",
                    "0.025",
                    "25.6"
                ],
                [
                    "Linearized derivation",
                    "1",
                    "0.2",
                    "25.6"
                ],
                [
                    "Linearized derivation",
                    "8",
                    "0.2",
                    "28.7"
                ]
            ]
        },
        "gold_description": "results in table 3 show that large batch training can significantly improve the performance of single transformers , particularly when trained to produce longer sequences . accumulating the gradient over 8 batches of size 4096 gives a 3 bleu improvement for the linear derivation model . it has been suggested that decaying the learning rate can have a similar effect to large batch training ( smith et al . , 2017 ) , but reducing the initial learning rate by a factor of 8 alone did not give the same improvements .",
        "generated_description": "single transformers are trained to convergence on 1m wat ja - en data set with batch size 4096 . for the plain bpe representation , the linearized derivation model is able to achieve a lower test bleu score than the uni - directional model with a smaller batch size . however , when the number of updates per batch is increased from 8 to 25 , the bpe model is unable to maintain the convergence rate and suffers from a large learning rate drop . in the following experiments , we increase the batch size to 8 and the learning rate from 0 . 025 to 0 . 2 ."
    },
    {
        "table_id": "135",
        "table_info": {
            "table_caption": "Table 4: Single models on Ja-En. Previous evaluation result included for comparison.",
            "table_column_names": [
                "Architecture",
                "Representation",
                "Dev BLEU",
                "Test BLEU"
            ],
            "table_content_values": [
                [
                    "Seq2seq (8-model ensemble)",
                    "Best WAT17 result Morishita et al. ( 2017 )",
                    "-",
                    "28.4"
                ],
                [
                    "Seq2seq",
                    "Plain BPE",
                    "21.6",
                    "21.2"
                ],
                [
                    "Seq2seq",
                    "Linearized derivation",
                    "21.9",
                    "21.2"
                ],
                [
                    "Transformer",
                    "Plain BPE",
                    "28.0",
                    "28.9"
                ],
                [
                    "Transformer",
                    "Linearized tree",
                    "28.2",
                    "28.4"
                ],
                [
                    "Transformer",
                    "Linearized derivation",
                    "28.5",
                    "28.7"
                ],
                [
                    "Transformer",
                    "POS/BPE",
                    "28.5",
                    "29.1"
                ]
            ]
        },
        "gold_description": "our plain bpe baseline ( table 4 ) outperforms the current best system on wat ja - en , an 8 - model ensemble ( morishita et al . , 2017 ) . our syntax models achieve similar results despite producing much longer sequences .",
        "generated_description": "as can be seen , the vanilla seq2seq model achieves 21 . 2 bleu score on the test set , which is lower than morishita et al . ’ s 28 . 4 . the transformer outperforms the seq2seq baseline by 1 . 3 bleus . we also observe that the pos / bpe representation yields a further 0 . 8 bbleu score improvement over the plain bpe representation . we conjecture that this is due to the fact that the transformer is able to capture richer contextual information than bpe ."
    },
    {
        "table_id": "136",
        "table_info": {
            "table_caption": "Table 5: Ja-En Transformer ensembles: † marks significant improvement on plain BPE baseline shown in Table 4 (p<0.05 using bootstrap resampling (Koehn et al., 2007)).",
            "table_column_names": [
                "External representation",
                "Internal representation",
                "Test BLEU"
            ],
            "table_content_values": [
                [
                    "Plain BPE",
                    "Plain BPE",
                    "29.2"
                ],
                [
                    "Linearized derivation",
                    "Linearized derivation",
                    "28.8"
                ],
                [
                    "Linearized tree",
                    "Plain BPE",
                    "28.9"
                ],
                [
                    "Plain BPE",
                    "Linearized derivation",
                    "28.8"
                ],
                [
                    "Linearized derivation",
                    "Plain BPE",
                    "29.4†"
                ],
                [
                    "POS/BPE",
                    "Plain BPE",
                    "29.3†"
                ],
                [
                    "Plain BPE",
                    "POS/BPE",
                    "29.4†"
                ]
            ]
        },
        "gold_description": "ensembles of two identical models trained with different seeds only slightly improve over the single model ( table 5 ) . however , an ensemble of models producing plain bpe and linearized derivations improves by 0 . 5 bleu over the plain bpe baseline .",
        "generated_description": "the plain bpe model achieves 29 . 2 bleu on the test set , a significant improvement over the previous state - of - the - art by bootstrap resampling ( p < 0 . 05 ) . however , the linearized variants underperform by 0 . 8 - 0 . 9 in terms of both test and internal representations . in addition , the addition of pos and bpe embeddings improves performance to 29 . 3 and 29 . 4 bleus respectively ."
    },
    {
        "table_id": "137",
        "table_info": {
            "table_caption": "Table 1: BLEU score variation across WMT’17 language arcs for cased (top) and uncased (bottom) BLEU. Each column varies the processing of the “online-B” system output and its references. basic denotes basic user-supplied tokenization, split adds compound splitting, unk replaces words not appearing at least twice in the training data with UNK, and metric denotes the metric-supplied tokenization used by WMT. The range row lists the difference between the smallest and largest scores, excluding unk.",
            "table_column_names": [
                "config",
                "English→⋆ en-cs",
                "English→⋆ en-de",
                "English→⋆ en-fi",
                "English→⋆ en-lv",
                "English→⋆ en-ru",
                "English→⋆ en-tr",
                "⋆→English cs-en",
                "⋆→English de-en",
                "⋆→English fi-en",
                "⋆→English lv-en",
                "⋆→English ru-en",
                "⋆→English tr-en"
            ],
            "table_content_values": [
                [
                    "basic",
                    "20.7",
                    "25.8",
                    "22.2",
                    "16.9",
                    "33.3",
                    "18.5",
                    "26.8",
                    "31.2",
                    "26.6",
                    "21.1",
                    "36.4",
                    "24.4"
                ],
                [
                    "split",
                    "20.7",
                    "26.1",
                    "22.6",
                    "17.0",
                    "33.3",
                    "18.7",
                    "26.9",
                    "31.7",
                    "26.9",
                    "21.3",
                    "36.7",
                    "24.7"
                ],
                [
                    "unk",
                    "20.9",
                    "26.5",
                    "25.4",
                    "18.7",
                    "33.8",
                    "20.6",
                    "26.9",
                    "31.4",
                    "27.6",
                    "22.7",
                    "37.5",
                    "25.2"
                ],
                [
                    "metric",
                    "20.1",
                    "26.6",
                    "22.0",
                    "17.9",
                    "32.0",
                    "19.9",
                    "27.4",
                    "33.0",
                    "27.6",
                    "22.0",
                    "36.9",
                    "25.6"
                ],
                [
                    "[ITALIC] range",
                    "0.6",
                    "0.8",
                    "0.6",
                    "1.0",
                    "1.3",
                    "1.4",
                    "0.6",
                    "1.8",
                    "1.0",
                    "0.9",
                    "0.5",
                    "1.2"
                ],
                [
                    "basic [ITALIC] lc",
                    "21.2",
                    "26.3",
                    "22.5",
                    "17.4",
                    "33.3",
                    "18.9",
                    "27.7",
                    "32.5",
                    "27.5",
                    "22.0",
                    "37.3",
                    "25.2"
                ],
                [
                    "split [ITALIC] lc",
                    "21.3",
                    "26.6",
                    "22.9",
                    "17.5",
                    "33.4",
                    "19.1",
                    "27.8",
                    "32.9",
                    "27.8",
                    "22.2",
                    "37.5",
                    "25.4"
                ],
                [
                    "unk [ITALIC] lc",
                    "21.4",
                    "27.0",
                    "25.6",
                    "19.1",
                    "33.8",
                    "21.0",
                    "27.8",
                    "32.6",
                    "28.3",
                    "23.6",
                    "38.3",
                    "25.9"
                ],
                [
                    "metric [ITALIC] lc",
                    "20.6",
                    "27.2",
                    "22.4",
                    "18.5",
                    "32.8",
                    "20.4",
                    "28.4",
                    "34.2",
                    "28.5",
                    "23.0",
                    "37.8",
                    "26.4"
                ],
                [
                    "[ITALIC] rangelc",
                    "0.6",
                    "0.9",
                    "0.5",
                    "1.1",
                    "0.6",
                    "1.5",
                    "0.7",
                    "1.7",
                    "1.0",
                    "1.0",
                    "0.5",
                    "1.2"
                ]
            ]
        },
        "gold_description": "table 1 demonstrates the effect of computing bleu scores with different reference tokenizations . the changes in each column show the effect these different schemes have , as high as 1 . 8 for one arc , and averaging around 1 . 0 . the biggest is the treatment of case , which is well known , yet many papers are not clear about whether they report cased or case - insensitive bleu . the variations in table 1 are only some of the possible configurations , since there is no limit to the preprocessing that a group could apply .",
        "generated_description": "we observe that split consistently outperforms basic and unk across all language pairs and setups , with the exception of fi - en and tr - en , where the best performance is obtained by using the unk configuration . this shows that multilingual pre - trained embeddings benefit from additional monolingual data when used in a multilingual setup ."
    },
    {
        "table_id": "138",
        "table_info": {
            "table_caption": "Table 2: System performance on the SPMRL and Wiki5K datasets.",
            "table_column_names": [
                "[EMPTY]",
                "[BOLD] % perf",
                "[BOLD] P",
                "[BOLD] R",
                "[BOLD] F"
            ],
            "table_content_values": [
                [
                    "[BOLD] SPMRL",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[ITALIC] baseline",
                    "69.65",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[ITALIC] UDPipe",
                    "89.65",
                    "93.52",
                    "68.82",
                    "79.29"
                ],
                [
                    "[ITALIC] yap",
                    "94.25",
                    "86.33",
                    "96.33",
                    "91.05"
                ],
                [
                    "[ITALIC] RF",
                    "[BOLD] 98.19",
                    "[BOLD] 97.59",
                    "[BOLD] 96.57",
                    "[BOLD] 97.08"
                ],
                [
                    "[ITALIC] DNN",
                    "97.27",
                    "95.90",
                    "95.01",
                    "95.45"
                ],
                [
                    "[BOLD] Wiki5K",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[ITALIC] baseline",
                    "67.61",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[ITALIC] UDPipe",
                    "87.39",
                    "92.03",
                    "64.88",
                    "76.11"
                ],
                [
                    "[ITALIC] yap",
                    "92.66",
                    "85.55",
                    "92.34",
                    "88.81"
                ],
                [
                    "[ITALIC] RF",
                    "[BOLD] 97.63",
                    "[BOLD] 97.41",
                    "[BOLD] 95.31",
                    "[BOLD] 96.35"
                ],
                [
                    "[ITALIC] DNN",
                    "95.72",
                    "94.95",
                    "92.22",
                    "93.56"
                ]
            ]
        },
        "gold_description": "table 2 shows the results of several systems on both datasets . 7 the column ' % perf ' indicates the proportion of perfectly segmented super - tokens , while the next three columns indicate precision , recall and f - score for boundary detection , not including the trivial final position characters . the first baseline strategy of not segmenting anything is given in the first row , and unsurprisingly gets many cases right , but performs badly overall . a more intelligent baseline is provided by udpipe ( straka et al . 2016 ; retrained on the spmrl data ) , which , for super - tokens in morphologically rich languages such as hebrew , implements a ' most common segmentation ' baseline ( i . e . each super - token is given its most common segmentation from training data , forgoing segmentation for oov items ) . 8 results for yap represent pure segmentation performance from the previous state of the art ( more and tsarfaty , 2016 ) . the best two approaches in the present paper are represented next : the extra trees random forest variant , 9 called rftokenizer , is labeled rf and the dnn - based system is labeled dnn . surprisingly , while the dnn is a close runner up , the best performance is achieved by the rftokenizer , de spite not having access to word embeddings . its high performance on the spmrl dataset makes it difficult to converge to a better solution using the dnn , though it is conceivable that substantially more data , a better feature representation and / or more hyperparameter tuning could equal or surpass the rftokenizer ' s performance . coupled with a lower cost in system resources and external dependencies , and the ability to forgo large model files to store word embeddings , we consider the rftokenizer solution to be better given the current training data size . performance on the out of domain dataset is",
        "generated_description": "we can see that the performance of udpipe and udpipe - based models are very close to each other , and both outperform the baseline by a large margin , indicating the robustness of these two systems . we can also see that both rf and dnn perform better than the baseline in terms of perf . however , the gap is much smaller than for spmrl , which suggests that there is still a lot of room for improvement for these systems to improve ."
    },
    {
        "table_id": "139",
        "table_info": {
            "table_caption": "Table 3: Effects of removing features on performance, ordered by descending F-score impact on SPMRL.",
            "table_column_names": [
                "[EMPTY]",
                "[BOLD] % perf",
                "[BOLD] P",
                "[BOLD] R",
                "[BOLD] F"
            ],
            "table_content_values": [
                [
                    "[BOLD] SPMRL",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "FINAL",
                    "98.19",
                    "97.59",
                    "96.57",
                    "97.08"
                ],
                [
                    "-expansion",
                    "98.01",
                    "97.25",
                    "96.35",
                    "96.80"
                ],
                [
                    "-vowels",
                    "97.99",
                    "97.55",
                    "95.97",
                    "96.75"
                ],
                [
                    "-letters",
                    "97.77",
                    "96.98",
                    "95.73",
                    "96.35"
                ],
                [
                    "-letr-vowl",
                    "97.57",
                    "97.56",
                    "94.44",
                    "95.97"
                ],
                [
                    "-lexicon",
                    "94.79",
                    "92.08",
                    "91.46",
                    "91.77"
                ],
                [
                    "[BOLD] Wiki5K",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "FINAL",
                    "97.63",
                    "97.41",
                    "95.31",
                    "96.35"
                ],
                [
                    "-expansion",
                    "97.33",
                    "96.64",
                    "95.31",
                    "95.97"
                ],
                [
                    "-vowels",
                    "97.51",
                    "97.56",
                    "94.87",
                    "96.19"
                ],
                [
                    "-letters",
                    "97.27",
                    "96.89",
                    "94.71",
                    "95.79"
                ],
                [
                    "-letr-vowl",
                    "96.72",
                    "97.17",
                    "92.77",
                    "94.92"
                ],
                [
                    "-lexicon",
                    "94.72",
                    "92.53",
                    "91.51",
                    "92.01"
                ]
            ]
        },
        "gold_description": "table 3 gives an overview of the impact on performance when specific features are removed : the entire lexicon , lexicon expansion , letter identity , ' vowel ' features from section 3 . 1 , and both of the latter . performance is high even in ablation scenarios , though we keep in mind that baselines for the task are high ( e . g . ' most frequent lookup ' , the udpipe strategy , achieves close to 90 % ) . the results show the centrality of the lexicon : removing lexicon lookup features degrades performance by about 3 . 5 % perfect accuracy , or 5 . 5 f - score points . all other ablations impact performance by less than 1 % or 1 . 5 f - score points . expanding the lexicon using wikipedia data offers a contribution of 0 . 3 – 0 . 4 points , confirming the original lexicon ' s incompleteness . 10 looking more closely at the other features , it is surprising that identity of the letters is not crucial , as long as we have access to dictionary lookup using the letters . nevertheless , removing letter identity impacts especially boundary recall , perhaps does not break down drastically . the impact on wiki5k is stronger , possibly because the necessary memorization of familiar contexts is less effective out of domain . because some letters receive identical lookup values ( e . g . single letter prepositions such as b ' in ' , l ' to ' ) but have different segmentation likelihoods . the ' vowel ' features , though ostensibly redundant with letter identity , help a little , causing 0 . 33 spmrl f - score point degradation if removed . a",
        "generated_description": "- expansion has the worst perf and f1 score on the spmrl test set for the final test set . the values of \" - vowels \" and \" - letters \" indicate that we removed the vowels and letters from the vocabulary , and \" - letr - vowl \" or \" - lexicon \" denotes the setting where the vocabulary is restricted to the words of the lexicon . we can see that \" - expansion \" has the highest percentage of perf on both datasets , which shows that it is necessary to keep the vocabulary expansion in the final set . on the wiki5k test set , the values of - lexicon have the highest values of f1 , which proves that lexicon - based features are more important than the letters and vowels ."
    },
    {
        "table_id": "140",
        "table_info": {
            "table_caption": "Table 3: Maximum perturbation space size in the SST and AG News test set using word / character substitutions, which is the maximum number of forward passes per sentence to evaluate in the exhaustive verification.",
            "table_column_names": [
                "Perturbation radius",
                "[ITALIC] δ=1",
                "[ITALIC] δ=2",
                "[ITALIC] δ=3"
            ],
            "table_content_values": [
                [
                    "SST-word",
                    "49",
                    "674",
                    "5,136"
                ],
                [
                    "SST-character",
                    "206",
                    "21,116",
                    "1,436,026"
                ],
                [
                    "AG-character",
                    "722",
                    "260,282",
                    "-"
                ]
            ]
        },
        "gold_description": "the allowable input perturbation space is much larger than for word - level synonym substitutions , as shown in table 3 . in table 3 , we show the maximum perturbation space size in the sst and ag news test set for different perturbation radii δ . this number grows exponentially as δ increases .",
        "generated_description": "for the word - level perturbation , we use the sst - word and ag news datasets with a maximum of 49 and 674 forward passes per sentence , which is the maximum number of times the model is allowed to evaluate in the exhaustive verification . for the character level perturbations , the model uses the ag news dataset with 260 , 282 characters . the size of the character space is smaller than that of the word space because of the fewer forward passes required to generate the word and character substitutions in the larger space ."
    },
    {
        "table_id": "141",
        "table_info": {
            "table_caption": "Table 1: Experimental results for changes up to δ=3 and δ=2 symbols on SST and AG dataset, respectively. We compare normal training, adversarial training, data augmentation and IBP-verifiable training, using three metrics on the test set: the nominal accuracy, adversarial accuracy, and exhaustively verified accuracy (Oracle) (%).",
            "table_column_names": [
                "[BOLD] Training",
                "[BOLD] Acc.",
                "[BOLD] SST-Char-Level  [BOLD] Adv. Acc.",
                "[BOLD] Oracle",
                "[BOLD] Acc.",
                "[BOLD] SST-Word-Level  [BOLD] Adv. Acc.",
                "[BOLD] Oracle",
                "[BOLD] Acc.",
                "[BOLD] AG-Char-Level  [BOLD] Adv. Acc.",
                "[BOLD] Oracle"
            ],
            "table_content_values": [
                [
                    "Normal",
                    "79.8",
                    "36.5",
                    "10.3",
                    "84.8",
                    "71.3",
                    "69.8",
                    "89.5",
                    "75.4",
                    "65.1"
                ],
                [
                    "Adversarial",
                    "79.0",
                    "[BOLD] 74.9",
                    "25.8",
                    "[BOLD] 85.0",
                    "76.8",
                    "74.6",
                    "[BOLD] 90.5",
                    "85.5",
                    "81.6"
                ],
                [
                    "Data aug.",
                    "79.8",
                    "37.8",
                    "13.7",
                    "85.4",
                    "72.7",
                    "71.6",
                    "88.4",
                    "77.5",
                    "72.0"
                ],
                [
                    "Verifiable (IBP)",
                    "74.2",
                    "73.1",
                    "[BOLD] 73.1",
                    "81.7",
                    "[BOLD] 77.2",
                    "[BOLD] 76.5",
                    "87.6",
                    "[BOLD] 87.1",
                    "[BOLD] 87.1"
                ]
            ]
        },
        "gold_description": "table 1 shows the results of ibp training and baseline models under δ = 3 and δ = 24 perturbations on sst and ag news , respectively . in table 1 , comparing adversarial accuracy with exhaustive verification accuracy ( oracle ) , we observe that although adversarial training is effective at defending against hotflip attacks ( 74 . 9 / 76 . 8 / 85 . 5 % ) , the oracle adversarial accuracy under exhaustive testing ( 25 . 8 / 74 . 6 / 81 . 6 % ) is much lower in sst - character / sst - word / ag - character level , respectively . in table 1 , when the perturbation space is larger ( sst character - level vs . sst word - level ) , ( a ) across models , there is a larger gap in adversarial accuracy and true robustness ( oracle ) ; ( b ) the difference in oracle robustness between ibp and adversarial training is even larger ( 73 . 1 % vs . 25 . 8 % and 76 . 5 % vs . 74 . 6 % ) . the resulting models achieve the highest exhaustively verified accuracy at the cost of only moderate deterioration in nominal accuracy ( table 1 ) .",
        "generated_description": "the oracle performance is the average of the ten runs with ten different random seeds , and is reported as the performance of the best run using the cross - validation on the test set . as can be seen , adversarial training ( ‘ normal ’ and ‘ adversarial ’ ) deteriorates the accuracy of the model when using adversarial data augmentation . however , when using the verifiable training data , the model achieves the best oracle accuracy on three of the four classification tasks . for the sst - char - level classifier , the oracle is an improvement of 1 . 4 % over the normal training set and 1 . 8 % over adversarial adversarial and ibp - generated data , respectively ."
    },
    {
        "table_id": "142",
        "table_info": {
            "table_caption": "Table 2: Macro precision (P), recall (R), F1 for the binary violation prediction task (± std. dev.).",
            "table_column_names": [
                "[EMPTY]",
                "P",
                "R",
                "F1"
            ],
            "table_content_values": [
                [
                    "majority",
                    "32.9 ± 0.0",
                    "50.0 ± 0.0",
                    "39.7 ± 0.0"
                ],
                [
                    "coin-toss",
                    "50.4 ± 0.7",
                    "50.5 ± 0.8",
                    "49.1 ± 0.7"
                ],
                [
                    "[BOLD] Non-Anonymized",
                    "[BOLD] Non-Anonymized",
                    "[BOLD] Non-Anonymized",
                    "[BOLD] Non-Anonymized"
                ],
                [
                    "bow-svm",
                    "71.5 ± 0.0",
                    "72.0 ± 0.0",
                    "71.8 ± 0.0"
                ],
                [
                    "bigru-att",
                    "87.1 ± 1.0",
                    "77.2 ± 3.4",
                    "79.5 ± 2.7"
                ],
                [
                    "han",
                    "88.2 ± 0.4",
                    "78.0 ± 0.2",
                    "80.5 ± 0.2"
                ],
                [
                    "bert",
                    "24.0 ± 0.2",
                    "50.0 ± 0.0",
                    "17.0 ± 0.5"
                ],
                [
                    "hier-bert",
                    "[BOLD] 90.4 ± 0.3",
                    "[BOLD] 79.3 ± 0.9",
                    "[BOLD] 82.0 ± 0.9"
                ],
                [
                    "[BOLD] Anonymized",
                    "[BOLD] Anonymized",
                    "[BOLD] Anonymized",
                    "[BOLD] Anonymized"
                ],
                [
                    "bow-svm",
                    "71.6 ± 0.0",
                    "70.5 ± 0.0",
                    "70.9 ± 0.0"
                ],
                [
                    "bigru-att",
                    "[BOLD] 87.0 ± 1.0",
                    "76.6 ± 1.9",
                    "78.9 ± 1.9"
                ],
                [
                    "han",
                    "85.2 ± 4.9",
                    "[BOLD] 78.3 ± 2.0",
                    "[BOLD] 80.2 ± 2.7"
                ],
                [
                    "bert",
                    "17.0 ± 3.0",
                    "50.0 ± 0.0",
                    "25.4 ± 0.4"
                ],
                [
                    "hier-bert",
                    "85.2 ± 0.3",
                    "78.1 ± 1.3",
                    "80.1 ± 1.1"
                ]
            ]
        },
        "gold_description": "table 2 ( upper part ) shows the results for binary violation . we evaluate models using macroaveraged precision ( p ) , recall ( p ) , f1 . the weak baselines ( majority , coin - toss ) are widely outperformed by the rest of the methods . bigruatt outperforms in f1 ( 79 . 5 vs . 71 . 8 ) the previous best performing method ( aletras et al . , 2016 ) in english judicial prediction . han slightly improves over bigru - att ( 80 . 5 vs . 79 . 5 ) , while being more robust across runs ( 0 . 2 % vs . 2 . 7 % std . dev . ) . the results in table 2 indicate that performance is comparable even when this information is masked , with the exception of hier - bert that has quite worse results ( 2 % ) compared to using non - anonymized data , suggesting model bias .",
        "generated_description": "for the majority of the examples in the ner dataset , the accuracies of the random classifiers are quite low . however , they outperform the majority classifier for bow - svm , bigru - att , han , and bert , indicating that they are able to generalize to unseen words . among the top - performing models for the anonymized dataset , hier - bert obtains the highest accuracies for both the named and unseen words , and its f1 - score is the highest among the non - anonymized ones ."
    },
    {
        "table_id": "143",
        "table_info": {
            "table_caption": "Table 1: Comparison of labeling strategies using many-to-one mapping for target languages with available test data, using 500 clusters or number of states. Accuracy is shown in percentage points.",
            "table_column_names": [
                "Seq. Tagger",
                "en",
                "de",
                "fr",
                "ru",
                "fa",
                "sw"
            ],
            "table_content_values": [
                [
                    "brown",
                    "81.37",
                    "[BOLD] 81.28",
                    "84.81",
                    "[BOLD] 79.78",
                    "[BOLD] 86.94",
                    "87.35"
                ],
                [
                    "marlin",
                    "[BOLD] 81.53",
                    "81.25",
                    "[BOLD] 85.4",
                    "79.14",
                    "86.64",
                    "[BOLD] 88.81"
                ],
                [
                    "a-hmm",
                    "77.12",
                    "74.85",
                    "81.48",
                    "73.88",
                    "80.25",
                    "76.69"
                ],
                [
                    "e-kmeans",
                    "63.01",
                    "65.14",
                    "68.68",
                    "70.80",
                    "76.94",
                    "65.08"
                ]
            ]
        },
        "gold_description": "we evaluate our four labeling strategies using the many - to - one approach , as presented in table 1 . in all cases , clustering by type with brown - based algorithms works better than using a sophisticated tagger such as a - hmm .",
        "generated_description": "the results using many - to - one mapping are shown in table 1 , for target languages with available test data , using 500 clusters or number of states . across all languages , marlin outperforms the other tagging strategies by a large margin . the disadvantage of a - hmm and e - kmeans is that they do not perform as well as marlin for the target languages , suggesting that they might be less effective when there is not sufficient training data ."
    },
    {
        "table_id": "144",
        "table_info": {
            "table_caption": "Table 2: Performance of cipher grounder using brown (|C|=500) as labeler. The best PL for each CL besides itself, is shown in bold. The artificial case where we have CL POS data (PL=CL) is shown for comparison, as is the ultimately used cipher-avg method.",
            "table_column_names": [
                "CL",
                "Parent Language (PL) en",
                "Parent Language (PL) de",
                "Parent Language (PL) fr",
                "Parent Language (PL) it",
                "Parent Language (PL) es",
                "Parent Language (PL) ja",
                "Parent Language (PL) cs",
                "Parent Language (PL) ru",
                "Parent Language (PL) ar",
                "Parent Language (PL) sw",
                "cipher-avg",
                "PL=CL"
            ],
            "table_content_values": [
                [
                    "en",
                    "-",
                    "57.1",
                    "[BOLD] 60.4",
                    "59.9",
                    "59.4",
                    "25.1",
                    "52.8",
                    "49.0",
                    "30.7",
                    "28.4",
                    "56.4",
                    "73.9"
                ],
                [
                    "fr",
                    "58.1",
                    "56.0",
                    "-",
                    "68.6",
                    "[BOLD] 71.9",
                    "23.6",
                    "48.3",
                    "47.8",
                    "35.0",
                    "26.7",
                    "58.6",
                    "76.7"
                ],
                [
                    "fa",
                    "13.8",
                    "32.3",
                    "29.7",
                    "22.7",
                    "33.3",
                    "19.7",
                    "33.3",
                    "[BOLD] 43.5",
                    "37.0",
                    "38.2",
                    "37.4",
                    "73.3"
                ],
                [
                    "sw",
                    "24.9",
                    "14.3",
                    "37.3",
                    "21.2",
                    "35.9",
                    "21.3",
                    "25.8",
                    "27.9",
                    "[BOLD] 38.96",
                    "-",
                    "37.8",
                    "69.4"
                ]
            ]
        },
        "gold_description": "table 2 presents the intrinsic performance of the cipher grounder over all pl - cl pairs considered . the difference between the best and the worst performing pl for each cl ranges from 24 . 62 percentage points for swahili to 48 . 34 points for french , and an average difference of 34 . 5 points among all languages . the case when pl = cl is also presented in table 2 as a reference and provides a reliable upper - bound under zero - resource conditions . it is worth noting the difference in accuracy when comparing the best performing pl for each cl with its corresponding pl = cl upper - bound . among all cls , the best cipher grounder for french ( es - fr ) gets the closest to its upper - bound with just 4 . 81 percentage points of difference , followed by the english grounder ( fr - en ) with 13 . 53 points of difference . as shown in table 2 , this model , cipher - avg , obtains accuracy scores of 56 . 4 , 58 . 6 , 37 . 4 , and 37 . 8 % for en , fr , fa , and sw , respectively . when compared to the best performing pl for each cl ( see bold cells in table 2 ) , it can be noticed that the performance gap ranges from just 1 . 2 percentage points for swahili to 13 . 3 points for french , with an average of 6 . 1 points among all target languages .",
        "generated_description": "we observe that the cipher - avg model outperforms the cl model across all language pairs with an average improvement of 2 . 3 bleu points , and the improvement is statistically significant ( t - test with p - value < 0 . 01 ) across all parent pairs ."
    },
    {
        "table_id": "145",
        "table_info": {
            "table_caption": "Table 3: Comparison of performance over the NOUN tag, as measured by precision (P), recall (R), and F1 scores, between our combined cipher grounder (cipher-avg) and a supervised tagger.",
            "table_column_names": [
                "CL",
                "cipher-avg P",
                "cipher-avg R",
                "cipher-avg F1",
                "Supervised P",
                "Supervised R",
                "Supervised F1"
            ],
            "table_content_values": [
                [
                    "en",
                    "47.70",
                    "64.4",
                    "54.81",
                    "94.04",
                    "90.44",
                    "92.20"
                ],
                [
                    "fr",
                    "56.26",
                    "78.82",
                    "65.65",
                    "96.15",
                    "93.72",
                    "94.92"
                ],
                [
                    "fa",
                    "64.94",
                    "51.23",
                    "57.27",
                    "96.48",
                    "97.77",
                    "97.12"
                ],
                [
                    "sw",
                    "53.46",
                    "51.82",
                    "52.63",
                    "98.88",
                    "97.50",
                    "98.18"
                ]
            ]
        },
        "gold_description": "let us now compare the performance of cipher - avg with that of a vanilla supervised neural model . 11 table 3 shows precision , recall , and f1 scores for the noun tag . even though cipher - avg achieved mixed results ( mid to low accuracy ) , the model robustly achieves mid - range performance according to f1 - score for all cls . the results are even more optimistic in terms of recall for english and french , and in terms of precision for farsi and swahili .",
        "generated_description": "our combined cipher - avg model outperforms the supervised tagger in terms of precision , recall , and f1 score across all languages except for french , where the noun tagger outperforms our combined cipher grounder by a substantial margin ."
    },
    {
        "table_id": "146",
        "table_info": {
            "table_caption": "Table 4: Impact of grounded unsupervised POS tagging on MaLOPa’s ‘zero-resource’ condition. Bold entries indicate an improvement over the baseline condition of having no POS tag information (beyond punctuation)",
            "table_column_names": [
                "Test Tags",
                "de UAS",
                "de LAS",
                "fr UAS",
                "fr LAS",
                "es UAS",
                "es LAS",
                "it UAS",
                "it LAS",
                "pt UAS",
                "pt LAS",
                "sv UAS",
                "sv LAS"
            ],
            "table_content_values": [
                [
                    "gold",
                    "65.57",
                    "52.37",
                    "71.27",
                    "59.80",
                    "73.26",
                    "63.13",
                    "71.46",
                    "59.66",
                    "63.28",
                    "54.93",
                    "77.50",
                    "64.90"
                ],
                [
                    "none",
                    "40.90",
                    "18.61",
                    "51.14",
                    "30.91",
                    "43.82",
                    "17.67",
                    "48.22",
                    "33.29",
                    "37.89",
                    "16.72",
                    "38.15",
                    "17.96"
                ],
                [
                    "cipher (this work)",
                    "38.31",
                    "[BOLD] 24.72",
                    "[BOLD] 54.46",
                    "[BOLD] 41.04",
                    "[BOLD] 55.56",
                    "[BOLD] 41.16",
                    "[BOLD] 54.05",
                    "[BOLD] 39.78",
                    "[BOLD] 46.97",
                    "[BOLD] 36.07",
                    "[BOLD] 55.06",
                    "[BOLD] 36.51"
                ]
            ]
        },
        "gold_description": "likewise , the utility of cipher - avg tags for dependency parsing under zero - resource scenarios is summarized in table 4 and table 5 . we first analyze the effect of pos tag information at test time for the malopa setup in table 4 . first we remove all pos signal except trivial punctuation information ( none row ) , and , predictably , the scores drop significantly across all target languages . then , we use our cipher tags ( cipher row ) and see improvements for all languages in las and for all but one language in uas ( de ) .",
        "generated_description": "in this section , we investigate the impact of grounding unsupervised pos tags on malms . for this experiment , we use the gold standard and the cipher algorithm proposed by \\ newcitehovy - etal : 2017 : emnlp2014 : acl - ijcnlp2014 as the test sets . the results show that our model outperforms the gold tagger and the baselines by a large margin ."
    },
    {
        "table_id": "147",
        "table_info": {
            "table_caption": "Table 5: Changing to unsupervised muse embeddings boosts MaLOPa’s zero-resource performance significantly (bold entries), in many cases doing so even without any POS tag information (italic entries), however noisy decipherment-based POS tags are no longer helpful.",
            "table_column_names": [
                "Embeddings",
                "Test Tags",
                "de UAS",
                "de LAS",
                "fr UAS",
                "fr LAS",
                "es UAS",
                "es LAS",
                "it UAS",
                "it LAS",
                "pt UAS",
                "pt LAS",
                "sv UAS",
                "sv LAS"
            ],
            "table_content_values": [
                [
                    "guo",
                    "gold",
                    "65.57",
                    "52.37",
                    "71.27",
                    "59.80",
                    "73.26",
                    "63.13",
                    "71.46",
                    "59.66",
                    "63.28",
                    "54.93",
                    "[BOLD] 77.50",
                    "[BOLD] 64.90"
                ],
                [
                    "muse",
                    "gold",
                    "[BOLD] 66.19",
                    "[BOLD] 56.28",
                    "[BOLD] 80.86",
                    "[BOLD] 72.65",
                    "[BOLD] 81.06",
                    "[BOLD] 73.62",
                    "[BOLD] 82.08",
                    "[BOLD] 72.40",
                    "[BOLD] 81.17",
                    "[BOLD] 76.17",
                    "72.46",
                    "61.71"
                ],
                [
                    "muse",
                    "none",
                    "57.26",
                    "45.10",
                    "[ITALIC] 73.84",
                    "[ITALIC] 63.09",
                    "[ITALIC] 77.01",
                    "[ITALIC] 67.06",
                    "71.36",
                    "[ITALIC] 60.48",
                    "[ITALIC] 75.31",
                    "[ITALIC] 68.36",
                    "60.82",
                    "45.25"
                ],
                [
                    "muse",
                    "cipher",
                    "48.56",
                    "37.13",
                    "69.94",
                    "59.22",
                    "73.86",
                    "61.68",
                    "69.30",
                    "56.85",
                    "73.41",
                    "65.23",
                    "57.39",
                    "41.49"
                ]
            ]
        },
        "gold_description": "likewise , the utility of cipher - avg tags for dependency parsing under zero - resource scenarios is summarized in table 4 and table 5 . we then take the next logical step and remove the parallel data - grounded embeddings , replacing them with fully unsupervised muse embeddings . table 5 summarizes these results . it can be observed that pos signal improves performance greatly for all languages when using muse embeddings . here we note a mixed result : whilst de , sv , and it do benefit from pos information , the other languages do not , obtaining great improvements from muse embed dings instead . finally , consider muse - cipher ( gold pos tags during training , cipher tags during testing ) . when compared to muse - none setup , it can be observed that , unfortunately , the heuristic pos tagger is too noisy and gets in muse ' s way .",
        "generated_description": "guo and muse . we observe that guo is slightly better than guo for unlabeled attachment score ( uas ) but worse for logistic regression ( las ) . muse outperforms guo by a large margin for both uas and las . the scores of it are slightly worse for las than for fr but better for uas . finally , the scores for it are higher than those for fr , suggesting that it is a more difficult language ."
    },
    {
        "table_id": "148",
        "table_info": {
            "table_caption": "Table 3: Micro precision, recall, F1 in multi-label violation for all, frequent, and few training instances.",
            "table_column_names": [
                "overall (all labels)",
                "overall (all labels)  [BOLD] P",
                "overall (all labels)  [BOLD] R",
                "overall (all labels)  [BOLD] F1"
            ],
            "table_content_values": [
                [
                    "bow-svm",
                    "56.3 ± 0.0",
                    "45.5 ± 0.0",
                    "50.4 ± 0.0"
                ],
                [
                    "bigru-att",
                    "62.6 ± 1.2",
                    "50.9 ± 1.5",
                    "56.2 ± 1.3"
                ],
                [
                    "han",
                    "65.0 ± 0.4",
                    "[BOLD] 55.5 ± 0.7",
                    "59.9 ± 0.5"
                ],
                [
                    "lwan",
                    "62.5 ± 1.0",
                    "53.5 ± 1.1",
                    "57.6 ± 1.0"
                ],
                [
                    "hier-bert",
                    "[BOLD] 65.9 ± 1.4",
                    "55.1 ± 3.2",
                    "[BOLD] 60.0 ± 1.3"
                ],
                [
                    "frequent (≥50)",
                    "frequent (≥50)",
                    "frequent (≥50)",
                    "frequent (≥50)"
                ],
                [
                    "bow-svm",
                    "56.3 ± 0.0",
                    "45.6 ± 0.0",
                    "50.4 ± 0.0"
                ],
                [
                    "bigru-att",
                    "62.7 ± 1.2",
                    "52.2 ± 1.6",
                    "57.0 ± 1.4"
                ],
                [
                    "han",
                    "65.1 ± 0.3",
                    "[BOLD] 57.0 ± 0.8",
                    "[BOLD] 60.8 ± 1.3"
                ],
                [
                    "lwan",
                    "62.8 ± 1.2",
                    "54.7 ± 1.2",
                    "58.5 ± 1.0"
                ],
                [
                    "hier-bert",
                    "[BOLD] 66.0 ± 1.4",
                    "56.5 ± 3.3",
                    "[BOLD] 60.8 ± 1.3"
                ],
                [
                    "few ([1,50))",
                    "few ([1,50))",
                    "few ([1,50))",
                    "few ([1,50))"
                ],
                [
                    "bow-svm",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "bigru-att",
                    "36.3 ± 13.8",
                    "03.2 ± 23.1",
                    "05.6 ± 03.8"
                ],
                [
                    "han",
                    "30.2 ± 35.1",
                    "01.6 ± 01.2",
                    "02.8 ± 01.9"
                ],
                [
                    "lwan",
                    "24.9 ± 06.3",
                    "[BOLD] 07.0 ± 04.1",
                    "[BOLD] 10.6 ± 05.2"
                ],
                [
                    "hier-bert",
                    "[BOLD] 43.6 ± 14.5",
                    "05.0 ± 02.8",
                    "08.9 ± 04.9"
                ]
            ]
        },
        "gold_description": "table 3 reports micro - averaged precision ( p ) , recall ( r ) , and f1 results for all methods , now including lwan , in multi - label violation prediction . the results are also grouped by label frequency for all ( overall ) , frequent , and few labels ( articles ) , counting frequencies on the training subset . we observe that predicting specific articles that have been violated is a much more difficult task than predicting if any article has been violated in a binary setup ( cf . table 2 ) . overall , hier - bert outperforms bigru - att and lwan ( 60 . 0 vs . 57 . micro - f1 ) , which is tailored for multi - labeling tasks , while being comparable with han ( 60 . 0 vs . 59 . 9 micro - f1 ) . all models under - perform in labels with few training examples , demonstrating the difficulty of few - shot learning in echr legal judgment prediction .",
        "generated_description": "as a supplement to the main results , we also report precision , recall , and f1 scores for the frequent and hier - bert variants . as can be seen , unsurprisingly , the bow - svm variant outperforms the bigru - att variant in terms of f1 score . however , the results of the variants with hierarchical masking are slightly worse than the ones with frequent masking . this may be due to the fact that in addition to masking rare words , adding more frequent words into the model hurts performance on rare words ."
    },
    {
        "table_id": "149",
        "table_info": {
            "table_caption": "Table 1: Historical datasets used in the experiments",
            "table_column_names": [
                "[BOLD] Dataset/Language",
                "[BOLD] Dataset/Language",
                "[BOLD] Time Period",
                "[BOLD] Genre",
                "[BOLD] Tokens Train",
                "[BOLD] Tokens Dev",
                "[BOLD] Tokens Test"
            ],
            "table_content_values": [
                [
                    "DEA",
                    "German (Anselm)",
                    "14th–16th c.",
                    "Religious",
                    "233,947",
                    "45,996",
                    "45,999"
                ],
                [
                    "DER",
                    "German (RIDGES)",
                    "1482–1652",
                    "Science",
                    "41,857",
                    "9,712",
                    "9,587"
                ],
                [
                    "EN",
                    "English",
                    "1386–1698",
                    "Letters",
                    "147,826",
                    "16,334",
                    "17,644"
                ],
                [
                    "ES",
                    "Spanish",
                    "15th–19th c.",
                    "Letters",
                    "97,320",
                    "11,650",
                    "12,479"
                ],
                [
                    "HU",
                    "Hungarian",
                    "1440–1541",
                    "Religious",
                    "134,028",
                    "16,707",
                    "16,779"
                ],
                [
                    "IS",
                    "Icelandic",
                    "15th c.",
                    "Religious",
                    "49,633",
                    "6,109",
                    "6,037"
                ],
                [
                    "PT",
                    "Portuguese",
                    "15th–19th c.",
                    "Letters",
                    "222,525",
                    "26,749",
                    "27,078"
                ],
                [
                    "SLB",
                    "Slovene (Bohorič)",
                    "1750–1840s",
                    "Diverse",
                    "50,023",
                    "5,841",
                    "5,969"
                ],
                [
                    "SLG",
                    "Slovene (Gaj)",
                    "1840s–1899",
                    "Diverse",
                    "161,211",
                    "20,878",
                    "21,493"
                ],
                [
                    "SV",
                    "Swedish",
                    "1527–1812",
                    "Diverse",
                    "24,458",
                    "2,245",
                    "29,184"
                ]
            ]
        },
        "gold_description": "table 1 gives an overview of the historical datasets . covering eight languages from different language families — english , german , hungarian , icelandic , spanish , portuguese , slovene , and swedish — as well as different text genres and time periods .",
        "generated_description": "for the cross - lingual experiments , we use the universal dependencies ( ud ) and semeval ( se ) datasets to train and test our models . the ud and se datasets were created by the darpa ’ s taxonomy of word extraction ( rwe ) and statistical machine translation ( smt ) ( denoted by dea and der ) . the media and pred datasets were already available but were not split into training , development , and test sets . the religious datasets are from the united states , the uk , and the european union ( en , es , and pt ) . the der and semeval datasets are for german ( anselm ) and german ( ridges ) , the former a collection of german letters written by members of the religious community while the latter is a corpus of germanic and slavic letters . the ep data for spanish ( es , hu , and it ) and portuguese ( pt ) come from the same period ( 15th to 19th century ) and belong to the same genre ( e . g . portuguese vs . spanish letters ) . for the rest of the datasets , the religious data is from the period between 14th and 16th c . ( ranging from 1 , 000 to 2 , 000 letters ) . we use the development , test , and development sets to train our models and evaluate them on the held - out test set . the development and test set portions of the ep datasets are the same as the training set . we perform experiments on three datasets for each language : english , german , spanish , and russian . the experiments are carried out on a linux machine ( cpu : intel xeon e5 - 2690 v2 . 2ghz , gpu : nvidia tesla p100 , ram : nvidia geforce gtx 1080ti ) . we perform 5 epochs of training ( 20 epochs for english , 10 epochs in german , 5 epoch for spanish and portuguese"
    },
    {
        "table_id": "150",
        "table_info": {
            "table_caption": "Table 2: Word accuracy of different normalization methods on the test sets of the historical datasets, in percent; best result for each dataset in bold; results marked with an asterisk (*) are not significantly different from the best result using McNemar’s test at p<0.05. † indicates scores that were not (re)produced here, but reported in previous work; they might not be strictly comparable due to differences in data preprocessing (cf. Sec. 3). Additionally, Identity shows the accuracy when leaving all word forms unchanged, while Maximum gives the theoretical maximum accuracy with purely token-level methods.",
            "table_column_names": [
                "[BOLD] Method",
                "[BOLD] Dataset DEA",
                "[BOLD] Dataset DER",
                "[BOLD] Dataset EN",
                "[BOLD] Dataset ES",
                "[BOLD] Dataset HU",
                "[BOLD] Dataset IS",
                "[BOLD] Dataset PT",
                "[BOLD] Dataset SLB",
                "[BOLD] Dataset SLG",
                "[BOLD] Dataset SV"
            ],
            "table_content_values": [
                [
                    "[ITALIC] Identity",
                    "[ITALIC] 30.63",
                    "[ITALIC] 44.36",
                    "[ITALIC] 75.29",
                    "[ITALIC] 73.40",
                    "[ITALIC] 17.53",
                    "[ITALIC] 47.62",
                    "[ITALIC] 65.19",
                    "[ITALIC] 40.74",
                    "[ITALIC] 85.38",
                    "[ITALIC] 58.59"
                ],
                [
                    "[ITALIC] Maximum",
                    "[ITALIC] 94.64",
                    "[ITALIC] 96.46",
                    "[ITALIC] 98.57",
                    "[ITALIC] 97.40",
                    "[ITALIC] 98.70",
                    "[ITALIC] 93.46",
                    "[ITALIC] 97.65",
                    "[ITALIC] 98.71",
                    "[ITALIC] 98.96",
                    "[ITALIC] 98.97"
                ],
                [
                    "Norma, Lookup",
                    "83.86",
                    "82.15",
                    "92.45",
                    "92.51",
                    "74.58",
                    "82.84",
                    "91.67",
                    "81.76",
                    "93.90",
                    "83.80"
                ],
                [
                    "Norma, Rule-based",
                    "76.48",
                    "82.52",
                    "90.85",
                    "88.59",
                    "78.73",
                    "83.72",
                    "86.33",
                    "86.09",
                    "91.63",
                    "85.23"
                ],
                [
                    "Norma, Distance-based",
                    "58.92",
                    "73.30",
                    "83.92",
                    "84.41",
                    "62.38",
                    "69.95",
                    "77.28",
                    "71.02",
                    "88.20",
                    "76.03"
                ],
                [
                    "Norma (Combined)",
                    "88.02",
                    "86.55",
                    "94.60",
                    "94.41",
                    "86.83",
                    "*86.85",
                    "94.19",
                    "89.45",
                    "91.44",
                    "87.12"
                ],
                [
                    "cSMTiser",
                    "88.82",
                    "*88.06",
                    "*95.21",
                    "*95.01",
                    "*91.63",
                    "*87.10",
                    "*95.09",
                    "*93.18",
                    "*95.99",
                    "[BOLD] 91.13"
                ],
                [
                    "cSMTiser+LM",
                    "86.69",
                    "*88.19",
                    "[BOLD] 95.24",
                    "[BOLD] 95.02",
                    "[BOLD] 91.70",
                    "*86.83",
                    "[BOLD] 95.18",
                    "[BOLD] 93.30",
                    "[BOLD] 96.01",
                    "*91.11"
                ],
                [
                    "NMT (Bollmann,  2018 )",
                    "89.16",
                    "*88.07",
                    "94.80",
                    "*94.83",
                    "91.17",
                    "86.45",
                    "94.64",
                    "91.61",
                    "95.19",
                    "90.27"
                ],
                [
                    "NMT (Tang et al.,  2018 )",
                    "[BOLD] 89.64",
                    "[BOLD] 88.22",
                    "94.95",
                    "*94.84",
                    "*91.65",
                    "[BOLD] 87.31",
                    "94.51",
                    "92.60",
                    "*95.85",
                    "90.39"
                ],
                [
                    "†SMT (Pettersson et al.,  2014 )",
                    "–",
                    "–",
                    "94.3–",
                    "–",
                    "80.1–",
                    "71.8–",
                    "–",
                    "–",
                    "–",
                    "92.9–"
                ],
                [
                    "†NMT (Tang et al.,  2018 )",
                    "–",
                    "–",
                    "94.69",
                    "–",
                    "91.69",
                    "87.59",
                    "–",
                    "–",
                    "–",
                    "91.56"
                ]
            ]
        },
        "gold_description": "table 2 shows the results of this evaluation . the extent of spelling variation varies greatly between datasets , with less than 15 % of tokens requiring normalization ( slg ) to more than 80 % ( hu ) . the maximum accuracy is above 97 % for most datasets , for the normalization systems , we observe significantly better word accuracy with smt than nmt on four of the datasets , and non - significant differences on five others . there is only one dataset ( dea ) where the nmt system by tang et al . ( 2018 ) gets significantly better word accuracy than other systems . overall , the deep nmt model by tang et al . ( 2018 ) consistently outperforms the shallow one by bollmann ( 2018 ) . finally , while norma does produce competitive results on sev eral datasets ( particularly in the \" combined \" setting ) , it is generally significantly behind the smt and nmt methods .",
        "generated_description": "the best results are highlighted in bold . we can observe that the best performance of each method is achieved on most of the datasets and almost always outperforms the best baseline . the only exceptions are the maximum and norma , lookup methods on the spanish and russian datasets , where the best results belong to the identity method . this is due to the fact that these two approaches have been shown to perform well on similar corpora ( e . g . , newswire ) but not on dissimilar ones ( i . e . , web pages ) ."
    },
    {
        "table_id": "151",
        "table_info": {
            "table_caption": "Table 4: Mean Absolute Error and Spearman’s ρ for case importance. Importance ranges from 1 (most important) to 4 (least). * Not Applicable.",
            "table_column_names": [
                "[EMPTY]",
                "MAE",
                "spearman’s  [ITALIC] ρ"
            ],
            "table_content_values": [
                [
                    "majority",
                    "[BOLD] .369 ± .000",
                    "[ITALIC] N/ [ITALIC] A*"
                ],
                [
                    "bow-svr",
                    ".585 ± .000",
                    ".370 ± .000"
                ],
                [
                    "bigru-att",
                    ".539 ± .073",
                    ".459 ± .034"
                ],
                [
                    "han",
                    ".524 ± .049",
                    ".437 ± .018"
                ],
                [
                    "hier-bert",
                    ".437 ± .018",
                    "[BOLD] .527 ± .024"
                ]
            ]
        },
        "gold_description": "bow - svr performs worse than bigru - att , while han is 10 % and 3 % better , respectively . hier - bert further improves the results , outperforming han by 17 % . hier - bert has the best ρ ( . 527 ) , indicating a moderate positive correlation ( > 0 . 5 ) , which is not the case for the rest of the methods .",
        "generated_description": "the case importance scores range from 1 ( most important ) to 4 ( not applicable ) , with 1 being the most important case and 4 the least . spearman ’ s ρ then measures the correlation between mae and the case importance score . as expected , the mae score for the majority baseline is high ( but still small ) and the correlation is low ( but significant ) for the bigru - att and han models ."
    },
    {
        "table_id": "152",
        "table_info": {
            "table_caption": "Table 1: 2015 & 2017 test set Smatch scores",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] 2015",
                "[BOLD] 2017"
            ],
            "table_content_values": [
                [
                    "Ours",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "local edge + projective decoder",
                    "70.2 \\scalebox 0.8$±0.3$",
                    "[BOLD] 71.0 ±0.5"
                ],
                [
                    "local edge + fixed-tree decoder",
                    "69.4 \\scalebox 0.8$±0.6$",
                    "70.2 \\scalebox 0.8$±0.5$"
                ],
                [
                    "K&G edge + projective decoder",
                    "68.6 \\scalebox 0.8$±0.7$",
                    "69.4 \\scalebox 0.8$±0.4$"
                ],
                [
                    "K&G edge + fixed-tree decoder",
                    "69.6 \\scalebox 0.8$±0.4$",
                    "69.9 \\scalebox 0.8$±0.2$"
                ],
                [
                    "Baselines",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "fixed-tree (type-unaware)",
                    "26.0 \\scalebox 0.8$±0.6$",
                    "27.9 \\scalebox 0.8$±0.6$"
                ],
                [
                    "JAMR-style",
                    "66.1",
                    "66.2"
                ],
                [
                    "Previous work",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "CAMR Wang et al. ( 2015 )",
                    "66.5",
                    "-"
                ],
                [
                    "JAMR Flanigan et al. ( 2016 )",
                    "67",
                    "-"
                ],
                [
                    "E17-1051",
                    "64",
                    "-"
                ],
                [
                    "van2017neural",
                    "68.5",
                    "[BOLD] 71.0"
                ],
                [
                    "foland2017abstract",
                    "[BOLD] 70.7",
                    "-"
                ],
                [
                    "buys2017oxford",
                    "-",
                    "61.9"
                ]
            ]
        },
        "gold_description": "table 1 shows the smatch scores ( cai and knight , 2013 ) of our models , compared to a selection of previously published results . the fixed - tree decoder seems to work well with either edge model , but performance of the projective decoder drops with the k & g edge scores . as expected , the type - unaware baseline has low recall , the fact that our models outperform the jamr - style baseline so clearly is an indication that",
        "generated_description": "our local edge + projective decoder and k & g edge + fixed - tree decoder models outperform the previous state - of - the - art camr wang et al . ( cf . as can be seen , our proposed model outperforms previous approaches by a large margin , e . g . , compared to the local edge + projective model , our local edge model achieves an improvement of 2 . 8 \\ scalebox points ( 70 . 2 \\ scalebox bleu score ) and outperforms camr by 2 . 7 \\ scalepoint bleus ( 71 . 0 \\ scalepoint auc score ) . compared to the jamr - style method , our model also outperforms this method , achieving a relative improvement of 6 . 7 \\ scaleunk bleu points ( 27 . 9 % relative improvement ) on the development set . we attribute this improvement to the fact that our model is able to learn the compositional structure of the whole sentence , which is more suitable for compositional dependency parsing ."
    },
    {
        "table_id": "153",
        "table_info": {
            "table_caption": "Table 2: Details for the LDC2015E86 and LDC2017T10 test sets",
            "table_column_names": [
                "[BOLD] Metric",
                "[BOLD] 2015 W’15",
                "[BOLD] 2015 F’16",
                "[BOLD] 2015 D’17",
                "[BOLD] 2015 PD",
                "[BOLD] 2015 FTD",
                "[BOLD] 2017 vN’17",
                "[BOLD] 2017 PD",
                "[BOLD] 2017 FTD"
            ],
            "table_content_values": [
                [
                    "Smatch",
                    "67",
                    "67",
                    "64",
                    "[BOLD] 70",
                    "[BOLD] 70",
                    "[BOLD] 71",
                    "[BOLD] 71",
                    "70"
                ],
                [
                    "Unlabeled",
                    "69",
                    "69",
                    "69",
                    "[BOLD] 73",
                    "[BOLD] 73",
                    "[BOLD] 74",
                    "[BOLD] 74",
                    "[BOLD] 74"
                ],
                [
                    "No WSD",
                    "64",
                    "68",
                    "65",
                    "[BOLD] 71",
                    "70",
                    "[BOLD] 72",
                    "[BOLD] 72",
                    "70"
                ],
                [
                    "Named Ent.",
                    "75",
                    "79",
                    "[BOLD] 83",
                    "79",
                    "78",
                    "[BOLD] 79",
                    "78",
                    "77"
                ],
                [
                    "Wikification",
                    "0",
                    "[BOLD] 75",
                    "64",
                    "71",
                    "72",
                    "65",
                    "[BOLD] 71",
                    "[BOLD] 71"
                ],
                [
                    "Negations",
                    "18",
                    "45",
                    "48",
                    "[BOLD] 52",
                    "[BOLD] 52",
                    "[BOLD] 62",
                    "57",
                    "55"
                ],
                [
                    "Concepts",
                    "80",
                    "83",
                    "83",
                    "83",
                    "[BOLD] 84",
                    "82",
                    "[BOLD] 84",
                    "[BOLD] 84"
                ],
                [
                    "Reentrancies",
                    "41",
                    "42",
                    "41",
                    "[BOLD] 46",
                    "44",
                    "[BOLD] 52",
                    "49",
                    "46"
                ],
                [
                    "SRL",
                    "60",
                    "60",
                    "56",
                    "[BOLD] 63",
                    "61",
                    "[BOLD] 66",
                    "64",
                    "62"
                ]
            ]
        },
        "gold_description": "table 2 analyzes the performance of our two best systems ( pd = projective , ftd = fixed - tree ) in more detail , and compares them to wang ' s , flanigan ' s , and damonte ' s amr parsers on the 2015 set and , and van noord and bos ( 2017b ) for the 2017 dataset . the good scores we achieve on reentrancy identification ,",
        "generated_description": "in order to better understand the performance of our system across different metrics , we perform an ablation study to gain insight into how each of them individually contributes to the overall performance . we observe that most of the metrics have small or negative effects on the performance , with the exception of ‘ no wsd ’ and ‘ unlabeled ’ , which are the two most underrepresented categories in the three benchmark datasets ."
    },
    {
        "table_id": "154",
        "table_info": {
            "table_caption": "Table 2: Main results.",
            "table_column_names": [
                "System",
                "Dev",
                "Test"
            ],
            "table_content_values": [
                [
                    "TSP-gen",
                    "21.12",
                    "22.44"
                ],
                [
                    "JAMR-gen",
                    "23.00",
                    "23.00"
                ],
                [
                    "All",
                    "[BOLD] 25.24",
                    "[BOLD] 25.62"
                ],
                [
                    "NoInducedRule",
                    "16.75",
                    "17.43"
                ],
                [
                    "NoConceptRule",
                    "23.99",
                    "24.86"
                ],
                [
                    "NoMovingDistance",
                    "23.48",
                    "24.06"
                ],
                [
                    "NoReorderModel",
                    "25.09",
                    "25.43"
                ]
            ]
        },
        "gold_description": "over the rules used on the 1 - best result , more than 30 % are non - terminal rules , on the other hand , 30 % are glue rules . finally , terminal rules take the largest percentage ,",
        "generated_description": "we can see that all the proposed rule - based models outperform tsp - gen and jamr - gen by a large margin , which indicates the effectiveness of our proposed rules . the noinducedrule and noconceptrule models perform the worst among all the models , which shows that rule induction is the most important part of our model . we can also see that the noreorder model achieves the best performance , which proves that the reordering mechanism can help the model to find the correct order ."
    },
    {
        "table_id": "155",
        "table_info": {
            "table_caption": "Table 3: Rules used for decoding.",
            "table_column_names": [
                "[EMPTY]",
                "Glue",
                "Nonterminal",
                "Terminal"
            ],
            "table_content_values": [
                [
                    "1-best",
                    "30.0%",
                    "30.1%",
                    "39.9%"
                ]
            ]
        },
        "gold_description": "over the rules used on the 1 - best result , more than 30 % are non - terminal rules . on the other hand , 30 % are glue rules . finally , terminal rules take the largest percentage",
        "generated_description": "rules used for decoding are listed in table 3 . the rules for glue consist of two rules : 1 - best and nonterminal . nonterminal rules are used to distinguish between terminal and terminal tokens . in order to have a fair comparison , the rules for glue are different from those for the other two rules ."
    },
    {
        "table_id": "156",
        "table_info": {
            "table_caption": "Table 2: Label distribution of train, dev, and test set",
            "table_column_names": [
                "[EMPTY]",
                "happy",
                "angry",
                "sad",
                "others",
                "size"
            ],
            "table_content_values": [
                [
                    "Train",
                    "14.07%",
                    "18.26%",
                    "18.11%",
                    "49.56%",
                    "30160"
                ],
                [
                    "Dev",
                    "5.15%",
                    "5.44%",
                    "4.54%",
                    "84.86%",
                    "2755"
                ],
                [
                    "Test",
                    "4.28%",
                    "5.57%",
                    "4.45%",
                    "85.70%",
                    "5509"
                ]
            ]
        },
        "gold_description": "according to the description in ( codalab , 2019 ) , the label distribution for dev and test sets are roughly 4 % for each of the emotions . however , from the dev set ( table 2 ) we know that the proportions of each of the emotion categories are better described as % 5 each , thereby we use % 5 as the empirical estimation of distribution pte ( xtr i ) . we did not use the exact proportion of dev set as the estimation to prevent the overfitting towards dev set . the sample distribution of the train set is used as ptr ( xtr i ) .",
        "generated_description": "we observe that the ratio of happy ( 18 . 26 % ) , angry ( 49 . 56 % ) , and sad ( 84 . 86 % ) , and others ( 25 . 86 % ) are the most frequent labels . the dev set has 5 . 15 % , 5 . 44 % , and 5 . 57 % of the labels being labeled as happy , angry , sad , and others . the ratio of others ( 85 . 7 % ) is found to be slightly higher than the dev set but still below the test set ( 5 . 57 % ) ."
    },
    {
        "table_id": "157",
        "table_info": {
            "table_caption": "Table 1: Macro-F1 scores and its harmonic means of the four models",
            "table_column_names": [
                "[EMPTY]",
                "F1",
                "Happy",
                "Angry",
                "Sad",
                "Harm. Mean"
            ],
            "table_content_values": [
                [
                    "SL",
                    "Dev",
                    "0.6430",
                    "0.7530",
                    "0.7180",
                    "0.7016"
                ],
                [
                    "SL",
                    "Test",
                    "0.6400",
                    "0.7190",
                    "0.7300",
                    "0.6939"
                ],
                [
                    "SLD",
                    "Dev",
                    "0.6470",
                    "0.7610",
                    "0.7360",
                    "0.7112"
                ],
                [
                    "SLD",
                    "Test",
                    "0.6350",
                    "0.7180",
                    "0.7360",
                    "0.6934"
                ],
                [
                    "HRLCE",
                    "Dev",
                    "0.7460",
                    "0.7590",
                    "0.8100",
                    "[BOLD] 0.7706"
                ],
                [
                    "HRLCE",
                    "Test",
                    "0.7220",
                    "0.766",
                    "0.8180",
                    "[BOLD] 0.7666"
                ],
                [
                    "BERT",
                    "Dev",
                    "0.7138",
                    "0.7736",
                    "0.8106",
                    "0.7638"
                ],
                [
                    "BERT",
                    "Test",
                    "0.7151",
                    "0.7654",
                    "0.8157",
                    "0.7631"
                ]
            ]
        },
        "gold_description": "the results are shown in table 1 . it shows that the proposed hrlce model performs the best . the performance of sld and sl are very close to each other , on the dev set , sld performs better than sl but they have almost the same overall scores on the test set . the macrof1 scores of each emotion category are very different from each other : the classification accuracy for emotion sad is the highest in most of the cases , while the emotion happy is the least accurately classified by all the models . we also noticed that the performance on the dev set is generally slightly better than that on the test set .",
        "generated_description": "the macro - f1 score is used to measure the harmonic mean of the f1 score of the four models , which is the harmonic score divided by the number of happy / angry / disgust scores . as we can see , bert achieves the highest f1 scores for all the three emotions and the highest harmonic mean score for the happy and sadness emotions , which means that bert can generate more appropriate and natural - looking happy and sad texts than the sl and sld models . compared with the sld model , the hrlce model achieves the best scores in all the emotions and achieves the lowest harmonic mean scores for the sadness and anger emotions ."
    },
    {
        "table_id": "158",
        "table_info": {
            "table_caption": "Table 3: The intrinsic evaluation results.",
            "table_column_names": [
                "Aligner",
                "Alignment F1",
                "Oracle’s Smatch"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "(on hand-align)",
                    "(on dev. dataset)"
                ],
                [
                    "JAMR",
                    "90.6",
                    "91.7"
                ],
                [
                    "Our",
                    "95.2",
                    "94.7"
                ]
            ]
        },
        "gold_description": "table 3 shows the intrinsic evaluation results , in which our alignment intrinsically outperforms jamr aligner by achieving better alignment f1 score and leading to a higher scored oracle parser .",
        "generated_description": "the intrinsic evaluation results are based on the alignment f1 and the oracle ’ s smatch score . our model significantly outperforms the jamr and achieves the best performance on hand - alignments , which demonstrates the effectiveness of our model ."
    },
    {
        "table_id": "159",
        "table_info": {
            "table_caption": "Table 4: The parsing results.",
            "table_column_names": [
                "model",
                "newswire",
                "all"
            ],
            "table_content_values": [
                [
                    "JAMR parser: Word, POS, NER, DEP",
                    "JAMR parser: Word, POS, NER, DEP",
                    "[EMPTY]"
                ],
                [
                    "+ JAMR aligner",
                    "71.3",
                    "65.9"
                ],
                [
                    "+ Our aligner",
                    "73.1",
                    "67.6"
                ],
                [
                    "CAMR parser: Word, POS, NER, DEP",
                    "CAMR parser: Word, POS, NER, DEP",
                    "[EMPTY]"
                ],
                [
                    "+ JAMR aligner",
                    "68.4",
                    "64.6"
                ],
                [
                    "+ Our aligner",
                    "68.8",
                    "65.1"
                ]
            ]
        },
        "gold_description": "table 4 shows the results . from this table , we can see that our alignment consistently improves all the parsers by a margin ranging from 0 . 5 to 1 . 7 .",
        "generated_description": "the parsing results of jamr parser are listed in tab . our aligner improves the accuracy of parsing by 1 . 3 % on newswire set and 1 . 6 % on camr set , which confirms the effectiveness of our alignmenter . in addition , when our aligner is added to the parser , the accuracy further improves to 1 . 8 % and 2 . 1 % on the two datasets ."
    },
    {
        "table_id": "160",
        "table_info": {
            "table_caption": "Table 6: The parsing results. xn denotes the ensemble of n differently initialized parsers. The difference in rounding is due to previous works report differently rounded results. † BA17 represents the result of Ballesteros and Al-Onaizan (2017), ‡ Damonte et al. (2017)’s result is drawn from Ballesteros and Al-Onaizan (2017).",
            "table_column_names": [
                "model",
                "newswire",
                "all"
            ],
            "table_content_values": [
                [
                    "Our single parser: Word only",
                    "Our single parser: Word only",
                    "[EMPTY]"
                ],
                [
                    "+ JAMR aligner",
                    "68.6",
                    "63.9"
                ],
                [
                    "+ Our aligner",
                    "69.3",
                    "64.7"
                ],
                [
                    "Our single parser: Word, POS",
                    "Our single parser: Word, POS",
                    "[EMPTY]"
                ],
                [
                    "+ JAMR aligner",
                    "68.8",
                    "64.6"
                ],
                [
                    "+ Our aligner",
                    "69.8",
                    "65.2"
                ],
                [
                    "Our ensemble: Word only + Our aligner",
                    "Our ensemble: Word only + Our aligner",
                    "[EMPTY]"
                ],
                [
                    "x3",
                    "71.9",
                    "67.4"
                ],
                [
                    "x10",
                    "72.5",
                    "68.1"
                ],
                [
                    "Our ensemble: Word, POS + Our aligner",
                    "Our ensemble: Word, POS + Our aligner",
                    "[EMPTY]"
                ],
                [
                    "x3",
                    "72.5",
                    "67.7"
                ],
                [
                    "x10",
                    "73.3",
                    "[BOLD] 68.4"
                ],
                [
                    "BA17: Word only†",
                    "68",
                    "63"
                ],
                [
                    "+ POS",
                    "68",
                    "63"
                ],
                [
                    "+ POS, DEP",
                    "69",
                    "64"
                ],
                [
                    "Damonte et al. ( 2017 )‡",
                    "-",
                    "66"
                ],
                [
                    "Artzi et al. ( 2015 )",
                    "66.3",
                    "-"
                ],
                [
                    "Wang et al. ( 2015a )",
                    "70",
                    "66"
                ],
                [
                    "Pust et al. ( 2015 )",
                    "-",
                    "67.1"
                ],
                [
                    "Zhou et al. ( 2016 )",
                    "71",
                    "66"
                ],
                [
                    "Goodman et al. ( 2016 )",
                    "70",
                    "-"
                ],
                [
                    "Wang and Xue ( 2017 )",
                    "-",
                    "68.1"
                ]
            ]
        },
        "gold_description": "table 6 shows the performance of our transitionbased parser along with comparison to the parsers in the previous works . the second block in table 6 shows the results of our ensemble parser , in which ensemble significantly improves the performance and more parsers ensembled , more improvements are achieved .",
        "generated_description": "for our alignmenter , we use the same hyperparameters as \\ newcite damon et al . when using our aligner , the parser and the jamr aligner improve by 1 . 3 and 1 . 8 f1 points on the newswire set respectively when using the same batch size x3 , and the ensemble of the two parsers improves by another 1 . 4 and 0 . 7 f1 scores . for the parser , the pos tagger improves by 2 . 1 f1 score when using a batch size of 10 and the alignmenter improves by 0 . 8 and 1 point when using an embedding size of x10 . the ensemble results show that the parser benefits from having multiple parsers with different word embeddings ."
    },
    {
        "table_id": "161",
        "table_info": {
            "table_caption": "Table 1: SQA test results. † marks contextual models using the previous question or the answer to the previous question. * marks the models that use the table content. RA denotes an oracle model that has access to the previous reference answer at test time. ALL is the average question accuracy, SEQ the sequence accuracy, and POS X, the accuracy of the X’th question in a sequence.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] ALL",
                "[BOLD] SEQ",
                "[BOLD] POS1",
                "[BOLD] POS2",
                "[BOLD] POS3"
            ],
            "table_content_values": [
                [
                    "FP *",
                    "34.1",
                    "7.2",
                    "52.6",
                    "25.6",
                    "25.9"
                ],
                [
                    "NP *",
                    "39.4",
                    "10.8",
                    "58.9",
                    "35.9",
                    "24.6"
                ],
                [
                    "DynSp",
                    "42.0",
                    "10.2",
                    "[BOLD] 70.9",
                    "35.8",
                    "20.1"
                ],
                [
                    "FP † *",
                    "33.2",
                    "7.7",
                    "51.4",
                    "22.2",
                    "22.3"
                ],
                [
                    "NP † *",
                    "40.2",
                    "11.8",
                    "60.0",
                    "35.9",
                    "25.5"
                ],
                [
                    "DynSp †",
                    "44.7",
                    "12.8",
                    "70.4",
                    "41.1",
                    "23.6"
                ],
                [
                    "Camp † *",
                    "45.6",
                    "13.2",
                    "70.3",
                    "42.6",
                    "24.8"
                ],
                [
                    "Ours *",
                    "45.1",
                    "13.3",
                    "67.2",
                    "42.4",
                    "26.4"
                ],
                [
                    "Ours † *",
                    "[BOLD] 55.1",
                    "[BOLD] 28.1",
                    "67.2",
                    "[BOLD] 52.7",
                    "[BOLD] 46.8"
                ],
                [
                    "Ours † * (RA)",
                    "61.7",
                    "28.1",
                    "67.2",
                    "60.1",
                    "57.7"
                ]
            ]
        },
        "gold_description": "we compare our model to float parser ( fp ) ( pasupat and liang , 2015 ) , neural programmer ( np ) ( neelakantan et al . , 2016b ) , dynsp ( iyyer et al . , 2017 ) and camp ( sun et al . , 2018b ) in table 1 . we observe that our model improves the sota from 45 . 6 % by camp to 55 . 1 % in question accuracy ( all ) , reducing the relative error rate by 18 % . for the initial question ( pos1 ) , however , it is behind dynsp by 3 . 7 % . more interestingly , our model handles follow up questions especially well outperforming the previously best model fp by 20 % on pos3 , a 28 % relative error reduction . we observe that our model effectively leverages the context information by improving the average question accuracy from 45 . 1 % to 55 . 1 % in comparison to the use of context in dynsp yielding 2 . 7 % improvement . if we provide the previous reference answers , the average question accuracy jumps to 61 . 7 % , showing that 6 . 6 % of the errors are due to error propagation .",
        "generated_description": "we observe that our model outperforms the state - of - the - art in pos1 , pos2 and pos3 , while being competitive in seq . in addition , our model achieves the best overall performance across all the tasks ."
    },
    {
        "table_id": "162",
        "table_info": {
            "table_caption": "Table 2: ESE performance (p@k). Best performing combination is bold faced.",
            "table_column_names": [
                "[EMPTY]",
                "[EMPTY]",
                "[BOLD] Location Eq. 5",
                "[BOLD] Location Eq. 6",
                "[BOLD] Location Eq. 7",
                "[BOLD] Person Eq. 5",
                "[BOLD] Person Eq. 6",
                "[BOLD] Person Eq. 7",
                "[BOLD] Gene Eq. 5",
                "[BOLD] Gene Eq. 6",
                "[BOLD] Gene Eq. 7",
                "[BOLD] Protein Eq. 5",
                "[BOLD] Protein Eq. 6",
                "[BOLD] Protein Eq. 7",
                "[BOLD] Cell Type Eq. 5",
                "[BOLD] Cell Type Eq. 6",
                "[BOLD] Cell Type Eq. 7",
                "[BOLD] Virus Eq. 5",
                "[BOLD] Virus Eq. 6",
                "[BOLD] Virus Eq. 7"
            ],
            "table_content_values": [
                [
                    "[BOLD] Seed 1",
                    "Eq. 8",
                    "0.37",
                    "0.40",
                    "0.50",
                    "0.23",
                    "0.23",
                    "0.30",
                    "0.00",
                    "0.03",
                    "0.13",
                    "0.17",
                    "0.23",
                    "0.20",
                    "0.27",
                    "0.50",
                    "0.53",
                    "0.20",
                    "0.13",
                    "0.17"
                ],
                [
                    "[EMPTY]",
                    "Eq. 9",
                    "0.63",
                    "[BOLD] 0.73",
                    "0.73",
                    "0.03",
                    "[BOLD] 0.17",
                    "0.20",
                    "0.03",
                    "[BOLD] 0.07",
                    "0.07",
                    "0.43",
                    "[BOLD] 0.43",
                    "0.53",
                    "0.17",
                    "[BOLD] 0.23",
                    "0.23",
                    "0.07",
                    "[BOLD] 0.10",
                    "0.07"
                ],
                [
                    "[BOLD] Seed 2",
                    "Eq. 8",
                    "0.33",
                    "0.33",
                    "0.57",
                    "0.53",
                    "0.40",
                    "0.30",
                    "0.63",
                    "0.63",
                    "0.63",
                    "0.17",
                    "0.60",
                    "0.27",
                    "0.10",
                    "0.20",
                    "0.13",
                    "0.07",
                    "0.03",
                    "0.03"
                ],
                [
                    "[EMPTY]",
                    "Eq. 9",
                    "0.57",
                    "[BOLD] 0.70",
                    "0.63",
                    "0.47",
                    "[BOLD] 0.37",
                    "0.37",
                    "0.60",
                    "[BOLD] 0.57",
                    "0.60",
                    "0.07",
                    "[BOLD] 0.30",
                    "0.30",
                    "0.07",
                    "[BOLD] 0.07",
                    "0.07",
                    "0.03",
                    "[BOLD] 0.10",
                    "0.03"
                ]
            ]
        },
        "gold_description": "we designed ese to output thirty candidate entities ( nps ) ranked based on the similarity to the seed term . therefore , we calculated precision at k ( p @ k ) where k is always 30 . table 2 shows the best results when using the feature ensemble method which is more stable than the non - ensemble one ( due to lower standard deviation and non - zero precision ) . according to the results , the best combination in terms of the mean and standard deviation is obtained when using tfidf ( eq . 5 ) to weigh the edges and context - dependent similarity ( eq . 8 ) to rank nps . this shows that the uniqueness and the significant overlap of features between noun phrases were very important .",
        "generated_description": "from the table , we can observe the following conclusions : ( i ) for all three entity types , the supervised learning models outperform the unsupervised learning models in all three evaluation metrics . the results indicate that supervised learning is able to generalize well across different entity types . ( ii ) for instance , in the case of location , the results for cell type and protein are higher than that for the other two . ( iii ) for the person and cell type , supervised learning achieves the best results in three different evaluation metrics , i . e . , location , person and gene . ( iv ) the results for the virus are the lowest for all three metrics . we can observe that in most cases supervised learning outperforms supervised learning on all three similarity matrices . ( v ) for example , in case of seed 1 and seed 2 , the accuracy of supervised learning reaches 0 . 37 and 0 . 50 for the person and cell type eq . 8 and 9 respectively , while the accuracy is only 0 . 03 for the location and gene metrics . ( vi ) in case of seed 1 and seed 2 , we observe that the accuracies are close to each other in most of the cases ."
    },
    {
        "table_id": "163",
        "table_info": {
            "table_caption": "Table 3: Pipeline testing results of EAL and EAA annotation modes showing the model confidence (σ), F-Scores, and percentage cut from the pool of sentences.",
            "table_column_names": [
                "[BOLD] Dataset Name",
                "[BOLD] Entity Class",
                "[BOLD] EAL @ 1.0 F",
                "[BOLD] EAL @ 1.0 F",
                "[BOLD] EAA Annotation Mode  [BOLD] FA",
                "[BOLD] EAA Annotation Mode  [BOLD] HFA",
                "[BOLD] EAA Annotation Mode  [BOLD] UFA"
            ],
            "table_content_values": [
                [
                    "[BOLD] Dataset Name",
                    "[BOLD] Entity Class",
                    "[ITALIC] σ",
                    "% cut",
                    "[BOLD] F-Score (percentage cut)",
                    "[BOLD] F-Score (percentage cut)",
                    "[BOLD] F-Score (percentage cut)"
                ],
                [
                    "CoNLL-2003",
                    "Location",
                    "0.97",
                    "55%",
                    "0.99 (46%)",
                    "0.93 (83%)",
                    "0.82 (91%)"
                ],
                [
                    "CoNLL-2003",
                    "Person",
                    "0.97",
                    "59%",
                    "0.99 (48%)",
                    "0.95 (81%)",
                    "0.85 (90%)"
                ],
                [
                    "BioCreAtIvE II",
                    "Gene",
                    "0.94",
                    "35%",
                    "1.00 (35%)",
                    "0.96 (50%)",
                    "0.89 (69%)"
                ],
                [
                    "GENIA 3.02",
                    "Protein Molecule",
                    "0.99",
                    "33%",
                    "0.98 (36%)",
                    "0.87 (71%)",
                    "0.74 (85%)"
                ],
                [
                    "GENIA 3.02",
                    "Cell Type",
                    "0.99",
                    "62%",
                    "0.94 (70%)",
                    "0.82 (86%)",
                    "0.74 (91%)"
                ],
                [
                    "GENIA 3.02",
                    "Virus",
                    "0.94",
                    "24%",
                    "0.97 (79%)",
                    "0.89 (94%)",
                    "0.84 (96%)"
                ],
                [
                    "[BOLD] Average",
                    "[BOLD] Average",
                    "0.97",
                    "45%",
                    "0.98 (52%)",
                    "0.90 (78%)",
                    "0.81 (87%)"
                ]
            ]
        },
        "gold_description": "finally , for the last setting , we tested the system using the three auto - annotation modes ( i . e . , fa , hfa , and ufa ) as shown in table 3 . while the auto - annotation mode can allow us to reduce up to 87 % of the data pool , this drastic saving also reduces the accuracy of the learned model , achieving , on average , around 81 % f - score . overall , our framework presents a trade off between coverage and annotation cost . the hfa auto - annotation mode shows the benefit , especially in a realistic enterprise setting , when we need to annotate 33 % of the data to increase f - score by only 10 % ( when comparing the on average performance of hfa with esa ) is unreasonable . table 3 appears to show fa being inferior to eal in terms of the percentage cut for the location class , for example . in reality fa reduced sentence annotation by 65 % to reach 0 . 99 f - score . but as our testing criteria demanded that we either reach 1 . 0 f - score or finish all sentences from the pool , fa tried to finish the pool without any further performance improvement on the 0 . 99 f - score .",
        "generated_description": "for each dataset , we report the percentage of f - scores of the entity classes being added or removed from the test set after applying the proposed annotation scheme ( i . e . , eal ) . we observe that in most cases , we can see a significant decrease in the number of entity classes . for example , on the genia 3 . 02 and genia 2 . 9 datasets , a relative reduction of about 33 % and 27 % in f - score are observed for the location and protein molecule classes , respectively . on the conll - 2003 and biocreative ii datasets , the percentage reduction is more moderate , ranging from 10 % to 25 % . one interesting observation is that for some of the datasets ( e . g . , genia , ge ) , a large number of entities have been added to the gene and cell lines in the annotation of the first few rows of the table , whereas in other cases , the number has been reduced to less than 5 % . we observe a similar pattern in the human annotation of genia and ge ."
    },
    {
        "table_id": "164",
        "table_info": {
            "table_caption": "Table 1: Coarse text types in OntoNotes",
            "table_column_names": [
                "[BOLD] Spoken bc.conv",
                "[BOLD] Spoken 137,223",
                "[BOLD] Written news",
                "[BOLD] Written 68,6455"
            ],
            "table_content_values": [
                [
                    "bc.news",
                    "244,425",
                    "bible",
                    "243,040"
                ],
                [
                    "phone",
                    "110,132",
                    "trans.",
                    "98,143"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "web",
                    "71467"
                ],
                [
                    "[BOLD] total",
                    "491,780",
                    "[BOLD] total",
                    "1,099,105"
                ],
                [
                    "[BOLD] total 1,590,885",
                    "[BOLD] total 1,590,885",
                    "[BOLD] total 1,590,885",
                    "[BOLD] total 1,590,885"
                ]
            ]
        },
        "gold_description": "the coreference annotated portion of the corpus contains 1 . 59 million tokens from multiple genres , presented in table 1 . written data constitutes the large bulk of material , primarily from newswire ( wall street journal data ) , as well as some data from the web and the new testament , and some translations of news and online discussions in arabic and chinese . the translated data has been placed in its own category : it behaves more conservatively in preferring strict agreement than non - translated language ( see section 4 . 2 ) , perhaps due to translators ' editorial practices . the spoken data comes primarily from television broadcasts , including dialogue data from msnbc , phoenix and other broadcast sources ( bc . conv ) , or news , from cnn , abc and others ( bc . news ) , as well as phone conversations .",
        "generated_description": "ontonotes is one of the largest publicly available text datasets for spoken and written text . it contains 1 , 590 , 885 texts written and 1 , 099 , 105 texts for spoken text , and 1 . 4 million texts for written text , covering a wide range of text types . the spoken text types include newswire , telephone , faxes , and faxes . the written text types are mainly comprised of text books , newspaper articles , and encyclopedia texts . the size of the written text is higher than that of the spoken text because written news and telephone call transcripts are much larger ."
    },
    {
        "table_id": "165",
        "table_info": {
            "table_caption": "Table 2: Confusion matrix for test data classification",
            "table_column_names": [
                "[EMPTY]",
                "[EMPTY]",
                "Predicted Sg",
                "Predicted Pl",
                "Total"
            ],
            "table_content_values": [
                [
                    "Actual",
                    "Sg",
                    "222",
                    "39",
                    "261"
                ],
                [
                    "Actual",
                    "Pl",
                    "7",
                    "81",
                    "88"
                ],
                [
                    "[EMPTY]",
                    "Total",
                    "229",
                    "120",
                    "349"
                ]
            ]
        },
        "gold_description": "looking at the actual classifications obtained by the classifier produces the confusion matrix in table 2 . the matrix makes it clear that the classifier is very good at avoiding errors against the majority class : it almost never guesses ' notional ' when it shouldn ' t . conversely , about 1 / 3 of actual notional cases are misclassified , predicted to be ' strict ' . among the erroneous cases , only 6 belong to type iii ( about 15 % of errors ) , showing that the classifier largely handles this type quite well next to the other types , since type iii covers about 20 % of plural - to - singular agreement cases .",
        "generated_description": "we observe that there is a large number of ( 34 ) misclassification of generative adjectives ( sg ) and compound nouns ( pl ) and a small number ( 7 ) of misclassified prepositions ( 1 ) and postpositional verbs ( 2 ) misclassified pl and preposition ( 1 ) . since the precision and recall of these two classes are very low ( very low , in some cases less than 10 % ) we assume that the classifier has difficulty in distinguishing between the two classes and it is prone to over - generalization ."
    },
    {
        "table_id": "166",
        "table_info": {
            "table_caption": "Table 3: Agreement patterns across genres",
            "table_column_names": [
                "[BOLD] genre  [ITALIC] written",
                "[BOLD] agreement  [ITALIC] notional",
                "[BOLD] agreement  [ITALIC] strict",
                "[BOLD] agreement  [ITALIC] % notional"
            ],
            "table_content_values": [
                [
                    "bible",
                    "169",
                    "487",
                    "25.76"
                ],
                [
                    "newswire",
                    "344",
                    "843",
                    "28.98"
                ],
                [
                    "translations",
                    "55",
                    "210",
                    "20.75"
                ],
                [
                    "web",
                    "48",
                    "71",
                    "40.33"
                ],
                [
                    "[BOLD] total written",
                    "616",
                    "1611",
                    "27.66"
                ],
                [
                    "[ITALIC] spoken",
                    "[ITALIC] notional",
                    "[ITALIC] strict",
                    "[ITALIC] % notional"
                ],
                [
                    "bc.conv",
                    "237",
                    "201",
                    "54.11"
                ],
                [
                    "bc.news",
                    "296",
                    "378",
                    "43.91"
                ],
                [
                    "phone",
                    "60",
                    "89",
                    "40.26"
                ],
                [
                    "[BOLD] total spoken",
                    "593",
                    "668",
                    "47.02"
                ]
            ]
        },
        "gold_description": "next we can consider the effect of genre , and expectations that speech promotes notional agreement . this is confirmed in table 3 . however we note that individual genres do behave differently : data from the web is closer to spoken language . the most restrictive genre in avoiding notional agreement is translations . both of these facts may reflect a combination of modality , genre and editorial practice effects . however the strong differences suggest that genre is likely crucial to any model attempting to predict this phenomenon .",
        "generated_description": "the genres bible and newswire had the highest agreement with 25 . 76 % and 28 . 98 % of notional labels , respectively . the translations and web genres had the lowest agreement with 20 . 75 % and 40 . 33 % . the genres bc . conv and bc . news have the highest average agreement of 54 . 11 % and 43 . 91 % with the strict label . across genres spoken and written , the average agreement was 47 . 02 % notional and 27 . 66 % strict label with the average of 46 . 66 % . the average number of sentences per genre was written and spoken ."
    },
    {
        "table_id": "167",
        "table_info": {
            "table_caption": "Table 3: Number of propositions per type in AMPERE.",
            "table_column_names": [
                "Eval",
                "Req",
                "Fact",
                "Ref",
                "Quot",
                "Non-A",
                "Total"
            ],
            "table_content_values": [
                [
                    "3,982",
                    "1,911",
                    "3,786",
                    "207",
                    "161",
                    "339",
                    "10,386"
                ]
            ]
        },
        "gold_description": "we also show the number of propositions in each category in table 3 . the most frequent types are evaluation ( 38 . 3 % ) and fact ( 36 . 5 % ) .",
        "generated_description": "in total , ampere contains 10 , 386 propositions , comprising 3 , 982 evaluation propositions and 1 , 930 non - evaluation propositions . the majority of the propositions ( 91 . 6 % ) are referential and non - referring expressions . the balance of the dataset is approximately balanced in terms of the number of propositions per type : 1 , 786 propositions for req , 207 for fact and 207 for recall ."
    },
    {
        "table_id": "168",
        "table_info": {
            "table_caption": "Table 4: Proposition segmentation results. Result that is significantly better than all comparisons is marked with ∗ (p<10−6, McNemar test).",
            "table_column_names": [
                "[EMPTY]",
                "[BOLD] Prec.",
                "[BOLD]  Rec.",
                "[BOLD]  F1"
            ],
            "table_content_values": [
                [
                    "FullSent",
                    "73.68",
                    "56.00",
                    "63.64"
                ],
                [
                    "PDTB-conn",
                    "51.11",
                    "49.71",
                    "50.40"
                ],
                [
                    "RST-parser",
                    "30.28",
                    "43.00",
                    "35.54"
                ],
                [
                    "CRF",
                    "66.53",
                    "52.92",
                    "58.95"
                ],
                [
                    "BiLSTM-CRF",
                    "[BOLD] 82.25",
                    "[BOLD] 79.96",
                    "[BOLD] 81.09∗"
                ],
                [
                    "CRF-joint",
                    "74.99",
                    "63.33",
                    "68.67"
                ],
                [
                    "BiLSTM-CRF-joint",
                    "81.12",
                    "78.42",
                    "79.75"
                ]
            ]
        },
        "gold_description": "table 4 shows that bilstm - crf outperforms other methods in f1 . more importantly , the perfor mance on reviews is lower than those reached on existing datasets , e . g . , an f1 of 86 . 7 is obtained by crf for essays ( stab and gurevych , 2017 ) .",
        "generated_description": "we can see that our proposed bilstm - crf - joint model significantly outperforms all comparisons and achieves an f1 score of 79 . 75 % , which is an increase of 2 . 5 points over the previous state - of - the - art ."
    },
    {
        "table_id": "169",
        "table_info": {
            "table_caption": "Table 4: All 2-layer systems, and 1-layer systems if best in column. State-of-the-art results in bold; † indicates best in column (corresponding row shaded in purple). Key: L=linear, T=tree, H=hybrid, (1,2)=# layers, S=single-task specific, G=single-task general, +lexfeats=with all lexical features, MultiSimp=multi-task simple, MultiBal=multi-task balanced, MultiFoc=multi-task focused, w/UDS-IH2=trained on all data including UDS-IH2. All-3.0 is a constant baseline, always predicting 3.0.",
            "table_column_names": [
                "[EMPTY]",
                "FactBank MAE",
                "FactBank r",
                "UW MAE",
                "UW r",
                "Meantime MAE",
                "Meantime r",
                "UDS-IH2 MAE",
                "UDS-IH2 r"
            ],
            "table_content_values": [
                [
                    "All-3.0",
                    "0.8",
                    "NAN",
                    "0.78",
                    "NAN",
                    "0.31",
                    "NAN",
                    "2.255",
                    "NAN"
                ],
                [
                    "Lee et al. 2015",
                    "-",
                    "-",
                    "0.511",
                    "0.708",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "Stanovsky et al. 2017",
                    "0.59",
                    "0.71",
                    "[BOLD] 0.42†",
                    "0.66",
                    "0.34",
                    "0.47",
                    "-",
                    "-"
                ],
                [
                    "L-biLSTM(2)-S",
                    "[BOLD] 0.427",
                    "[BOLD] 0.826",
                    "0.508",
                    "[BOLD] 0.719",
                    "0.427",
                    "0.335",
                    "[BOLD] 0.960†",
                    "[BOLD] 0.768"
                ],
                [
                    "T-biLSTM(2)-S",
                    "[BOLD] 0.577",
                    "[BOLD] 0.752",
                    "0.600",
                    "0.645",
                    "0.428",
                    "0.094",
                    "[BOLD] 1.101",
                    "[BOLD] 0.704"
                ],
                [
                    "L-biLSTM(2)-G",
                    "[BOLD] 0.412",
                    "[BOLD] 0.812",
                    "0.523",
                    "0.703",
                    "0.409",
                    "0.462",
                    "-",
                    "-"
                ],
                [
                    "T-biLSTM(2)-G",
                    "[BOLD] 0.455",
                    "[BOLD] 0.809",
                    "0.567",
                    "0.688",
                    "0.396",
                    "0.368",
                    "-",
                    "-"
                ],
                [
                    "L-biLSTM(2)-S+lexfeats",
                    "[BOLD] 0.429",
                    "[BOLD] 0.796",
                    "0.495",
                    "[BOLD] 0.730",
                    "0.427",
                    "0.322",
                    "[BOLD] 1.000",
                    "[BOLD] 0.755"
                ],
                [
                    "T-biLSTM(2)-S+lexfeats",
                    "[BOLD] 0.542",
                    "[BOLD] 0.744",
                    "0.567",
                    "0.676",
                    "0.375",
                    "0.242",
                    "[BOLD] 1.087",
                    "[BOLD] 0.719"
                ],
                [
                    "L-biLSTM(2)-MultiSimp",
                    "[BOLD] 0.353",
                    "[BOLD] 0.843",
                    "0.503",
                    "[BOLD] 0.725",
                    "0.345",
                    "[BOLD] 0.540",
                    "-",
                    "-"
                ],
                [
                    "T-biLSTM(2)-MultiSimp",
                    "[BOLD] 0.482",
                    "[BOLD] 0.803",
                    "0.599",
                    "0.645",
                    "0.545",
                    "0.237",
                    "-",
                    "-"
                ],
                [
                    "L-biLSTM(2)-MultiBal",
                    "[BOLD] 0.391",
                    "[BOLD] 0.821",
                    "0.496",
                    "[BOLD] 0.724",
                    "[BOLD] 0.278",
                    "[BOLD] 0.613†",
                    "-",
                    "-"
                ],
                [
                    "T-biLSTM(2)-MultiBal",
                    "[BOLD] 0.517",
                    "[BOLD] 0.788",
                    "0.573",
                    "0.659",
                    "0.400",
                    "0.405",
                    "-",
                    "-"
                ],
                [
                    "L-biLSTM(1)-MultiFoc",
                    "[BOLD] 0.343",
                    "[BOLD] 0.823",
                    "0.516",
                    "0.698",
                    "[BOLD] 0.229†",
                    "[BOLD] 0.599",
                    "-",
                    "-"
                ],
                [
                    "L-biLSTM(2)-MultiFoc",
                    "[BOLD] 0.314",
                    "[BOLD] 0.846",
                    "0.502",
                    "[BOLD] 0.710",
                    "[BOLD] 0.305",
                    "0.377",
                    "-",
                    "-"
                ],
                [
                    "T-biLSTM(2)-MultiFoc",
                    "1.100",
                    "0.234",
                    "0.615",
                    "0.616",
                    "0.395",
                    "0.300",
                    "-",
                    "-"
                ],
                [
                    "L-biLSTM(2)-MultiSimp w/UDS-IH2",
                    "[BOLD] 0.377",
                    "[BOLD] 0.828",
                    "0.508",
                    "[BOLD] 0.722",
                    "0.367",
                    "0.469",
                    "[BOLD] 0.965",
                    "[BOLD] 0.771†"
                ],
                [
                    "T-biLSTM(2)-MultiSimp w/UDS-IH2",
                    "0.595",
                    "[BOLD] 0.716",
                    "0.598",
                    "0.609",
                    "0.467",
                    "0.345",
                    "[BOLD] 1.072",
                    "[BOLD] 0.723"
                ],
                [
                    "H-biLSTM(2)-S",
                    "0.488",
                    "[BOLD] 0.775",
                    "0.526",
                    "[BOLD] 0.714",
                    "0.442",
                    "0.255",
                    "[BOLD] 0.967",
                    "[BOLD] 0.768"
                ],
                [
                    "H-biLSTM(1)-MultiSimp",
                    "[BOLD] 0.313†",
                    "[BOLD] 0.857†",
                    "0.528",
                    "0.704",
                    "0.314",
                    "0.545",
                    "-",
                    "-"
                ],
                [
                    "H-biLSTM(2)-MultiSimp",
                    "[BOLD] 0.431",
                    "[BOLD] 0.808",
                    "0.514",
                    "[BOLD] 0.723",
                    "0.401",
                    "0.461",
                    "-",
                    "-"
                ],
                [
                    "H-biLSTM(2)-MultiBal",
                    "[BOLD] 0.386",
                    "[BOLD] 0.825",
                    "0.502",
                    "[BOLD] 0.713",
                    "0.352",
                    "[BOLD] 0.564",
                    "-",
                    "-"
                ],
                [
                    "H-biLSTM(2)-MultiSimp w/UDS-IH2",
                    "[BOLD] 0.393",
                    "[BOLD] 0.820",
                    "0.481",
                    "[BOLD] 0.749†",
                    "0.374",
                    "[BOLD] 0.495",
                    "[BOLD] 0.969",
                    "[BOLD] 0.760"
                ]
            ]
        },
        "gold_description": "table 4 reports the results for all of the 2 - layer l - , t - , and h - bilstms . 7 the best - performing system for each dataset and metric are highlighted in purple , and when the best - performing system for a particular dataset was a 1 - layer model , that system is included in table 4 . the highest - performing system for each is reported in table 4 . on its own , the bilstm with linear topology ( l - bilstm ) performs consistently better than the bilstm with tree topology ( t - bilstm ) . however , the hybrid topology ( h - bilstm ) , consisting of both a l - and tbilstm is the top - performing system on uw for correlation ( table 4 ) . though our methods achieve state of the art in the single - task setting , the best performing systems are mostly multi - task ( table 4 and supplementary materials ) .",
        "generated_description": "note that lee et al . 2015 is a re - implementation of a neural network based model ( nnan ) that uses a logistic regression classifier ( nnl ) to predict the probability of a given word being a sentiment term . we use mae and root mean squared error ( rmse ) as the performance metrics . we observe that the t - bilstm ( 2 ) - s model outperforms all the other models and achieves the best results on the three datasets ."
    },
    {
        "table_id": "170",
        "table_info": {
            "table_caption": "Table 5: Proposition classification F1 scores. Results that are significant better than other methods are marked with ∗ (p<10−6, McNemar test).",
            "table_column_names": [
                "[EMPTY]",
                "Overall",
                "Eval",
                "Req",
                "Fact",
                "Ref",
                "Quot"
            ],
            "table_content_values": [
                [
                    "[ITALIC] With Gold-Standard Segments",
                    "[ITALIC] With Gold-Standard Segments",
                    "[ITALIC] With Gold-Standard Segments",
                    "[ITALIC] With Gold-Standard Segments",
                    "[ITALIC] With Gold-Standard Segments",
                    "[ITALIC] With Gold-Standard Segments",
                    "[ITALIC] With Gold-Standard Segments"
                ],
                [
                    "Majority",
                    "40.75",
                    "57.90",
                    "–",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "PropLexicon",
                    "36.83",
                    "40.42",
                    "36.07",
                    "32.23",
                    "59.57",
                    "31.28"
                ],
                [
                    "SVM",
                    "60.98",
                    "63.88",
                    "[BOLD] 69.02",
                    "54.74",
                    "[BOLD] 69.47",
                    "7.69"
                ],
                [
                    "CNN",
                    "[BOLD] 66.56∗",
                    "[BOLD] 69.02",
                    "63.26",
                    "[BOLD] 66.17",
                    "67.44",
                    "[BOLD] 52.94"
                ],
                [
                    "[ITALIC] With Predicted Segments",
                    "[ITALIC] With Predicted Segments",
                    "[ITALIC] With Predicted Segments",
                    "[ITALIC] With Predicted Segments",
                    "[ITALIC] With Predicted Segments",
                    "[ITALIC] With Predicted Segments",
                    "[ITALIC] With Predicted Segments"
                ],
                [
                    "Majority",
                    "33.30",
                    "47.60",
                    "–",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "PropLexicon",
                    "23.21",
                    "22.45",
                    "23.97",
                    "23.73",
                    "35.96",
                    "16.67"
                ],
                [
                    "SVM",
                    "51.46",
                    "54.05",
                    "48.16",
                    "52.77",
                    "52.27",
                    "4.71"
                ],
                [
                    "CNN",
                    "55.48",
                    "57.75",
                    "53.71",
                    "55.19",
                    "48.78",
                    "33.33"
                ],
                [
                    "CRF-joint",
                    "50.69",
                    "46.78",
                    "55.74",
                    "52.27",
                    "[BOLD] 55.77",
                    "26.47"
                ],
                [
                    "BiLSTM-CRF-joint",
                    "[BOLD] 62.64∗",
                    "[BOLD] 62.36∗",
                    "[BOLD] 67.31∗",
                    "[BOLD] 61.86",
                    "54.74",
                    "[BOLD] 37.36"
                ]
            ]
        },
        "gold_description": "f1 scores for all propositions and each type are reported in table 5 . cnn performs better for types with significantly more training samples , i . e . , evaluation and fact , indicating the effect of data size on neural model ' s performance . joint models ( crf - joint and bilstm - crf - joint ) yield the best f1 scores for all categories when gold - standard segmentation is unavailable .",
        "generated_description": "the first column shows the results of our segmentation models with gold - standard segments . as shown in the second column , the cnn significantly outperforms the majority classifier and the lexicon - based classifier ( p - value < 0 . 05 ) on all the metrics . moreover , our predicted segmentation model outperforms both the svm and the cnn by a large margin on the referee and answerability metrics . the results show that our model is better at identifying referential and answerable questions than the baselines ."
    },
    {
        "table_id": "171",
        "table_info": {
            "table_caption": "Table 1: The model and its hyper-parameters.",
            "table_column_names": [
                "# encoders",
                "2"
            ],
            "table_content_values": [
                [
                    "encoder type",
                    "B-LSTM"
                ],
                [
                    "encoder layers",
                    "2"
                ],
                [
                    "encoder hidden dim",
                    "500"
                ],
                [
                    "# decoders",
                    "1"
                ],
                [
                    "decoder type",
                    "LSTM"
                ],
                [
                    "decoder layers",
                    "2"
                ],
                [
                    "decoder hidden dim",
                    "500"
                ],
                [
                    "word vector dim",
                    "300"
                ],
                [
                    "attention type",
                    "[ITALIC] general"
                ],
                [
                    "dropout",
                    "0.3"
                ],
                [
                    "beam size",
                    "5"
                ]
            ]
        },
        "gold_description": "table 1 lists all hyper - parameters which have all been chosen using only training and validation data . the two encoders have been implemented using a bidirectional long short - term memory ( b - lstm ) ( hochreiter and schmidhuber , 1997 ) while the decoder uses a unidirectional lstm . both the encoders and the decoder use two hidden layers . for the attention network , we have used the opennmt ' s general option ( luong et al . , 2015 ) .",
        "generated_description": "we use the cross - validation to tune the hyper - parameters and select the best model according to the validation performance on the development set . the number of training epochs and number of epochs is proportional to the number of target words in the training set . we apply dropout srivastava et al . the dropout rate is set to 0 . 3 in order to avoid over - fitting ."
    },
    {
        "table_id": "172",
        "table_info": {
            "table_caption": "Table 2: Results on the WMT17 IT domain English-German APE test set.",
            "table_column_names": [
                "Model",
                "TER",
                "BLEU"
            ],
            "table_content_values": [
                [
                    "MT Bojar et al. ( 2017 )",
                    "24.48",
                    "62.49"
                ],
                [
                    "SPE Bojar et al. ( 2017 )",
                    "24.69",
                    "62.97"
                ],
                [
                    "Varis and Bojar ( 2017 )",
                    "24.03",
                    "64.28"
                ],
                [
                    "Bérard et al. ( 2017 )",
                    "22.81",
                    "65.91"
                ],
                [
                    "train 11K",
                    "41.58",
                    "43.05"
                ],
                [
                    "train 23K",
                    "30.23",
                    "57.14"
                ],
                [
                    "train 23K + 500K",
                    "[BOLD] 22.60",
                    "[BOLD] 66.21"
                ]
            ]
        },
        "gold_description": "table 2 compares the accuracy of our model on the test data with two baselines and two state - of - theart comparable systems . the mt baseline simply consists of the accuracy of the mt sentences with respect to the pe ground truth . the other baseline is given by a statistical pe ( spe ) system ( simard et al . , 2007 ) chosen by the wmt17 organizers . table 2 shows that when our model is trained with only the 11k wmt17 official training sentences , it cannot even approach the baselines . even when the 12k wmt16 sentences are added , its accuracy is still well below that of the baselines . however , when the 500k artificial data are added , it reports a major improvement and it outperforms them both significantly . in addition , we have compared our model with two recent systems that have used our same training settings ( 500k artificial triplets + 23k manual triplets oversampled 10 times ) , reporting a slightly higher accuracy than both ( 1 . 43 ter and 1 . 93 bleu p . p . over ( varis and bojar , 2017 ) and 0 . 21 ter and 0 . 30 bleu p . p . over ( b ´ erard et al . , 2017 ) ) . since their models explicitly predicts edit operations rather than post - edited sentences , we speculate that these two tasks are of comparable intrinsic complexity .",
        "generated_description": "we train mt and spe on the wmt17 it domain with the same training data as in varis and bojar the mt model is a re - implementation of the multilingual spe model of bojar et al . as the size of the training data increases , the performance of spe steadily improves , reaching the best result of 62 . 97 bleu score in the case of the train 23k + 500k setting . our best result ( 22 . 60 ter ) outperforms the previous state - of - the - art ( spe ) by 1 . 8 ter points and achieves the new state of the art result on this task with only 11k training examples ."
    },
    {
        "table_id": "173",
        "table_info": {
            "table_caption": "Table 3: The performance of the best setting for each property.",
            "table_column_names": [
                "[BOLD] Feature",
                "[BOLD] Representation",
                "[BOLD] Embedding",
                "[BOLD] Window",
                "[BOLD] Dimension",
                "[BOLD] Precision",
                "[BOLD] Recall",
                "[BOLD] F1"
            ],
            "table_content_values": [
                [
                    "Used for transportation",
                    "Co-occurrence",
                    "word2vec SG",
                    "10",
                    "300",
                    "74.5",
                    "78.8",
                    "76.6"
                ],
                [
                    "Is a weapon",
                    "Backtranslation",
                    "word2vec CBOW",
                    "2",
                    "300",
                    "71.4",
                    "88.2",
                    "78.9"
                ],
                [
                    "Is round",
                    "Co-occurrence",
                    "word2vec CBOW",
                    "10",
                    "300",
                    "56.2",
                    "87.1",
                    "68.4"
                ],
                [
                    "Has various colors",
                    "Co-occurrence",
                    "GloVe",
                    "2",
                    "200",
                    "70.6",
                    "76.6",
                    "73.5"
                ],
                [
                    "Made of metal",
                    "Matrix",
                    "word2vec SG",
                    "5",
                    "300",
                    "78.6",
                    "61.1",
                    "68.8"
                ]
            ]
        },
        "gold_description": "table 3 displays the best performing dsmfor each property . there is a preference to word2vec and to a higher embedding dimension .",
        "generated_description": "the results show that the dimensionality of the embeddings is the most important feature to decide for each property . glove is the best performing embedding for most of the properties . however , for the word “ has various colors ” property , the window size of word2vec cbow is too small which results in a poor performance . for the backtranslation property , we can see that using a larger window size does not improve the results . we conjecture that this is due to the fact that the word vector space of backtranslation is much larger than that of word embedding , therefore a smaller window size is more suitable ."
    },
    {
        "table_id": "174",
        "table_info": {
            "table_caption": "Table 6: Mean predictions for linear (L-biLSTM-S(2)) and tree models (T-biLSTM-S(2)) on UDS-IH2-dev, grouped by governing dependency relation. Only the 10 most frequent governing dependency relations in UDS-IH2-dev are shown.",
            "table_column_names": [
                "Relation",
                "Mean Label",
                "L-biLSTM",
                "T-biLSTM",
                "#"
            ],
            "table_content_values": [
                [
                    "root",
                    "1.07",
                    "1.03",
                    "0.96",
                    "949"
                ],
                [
                    "conj",
                    "0.37",
                    "0.44",
                    "0.46",
                    "316"
                ],
                [
                    "advcl",
                    "0.46",
                    "0.53",
                    "0.45",
                    "303"
                ],
                [
                    "xcomp",
                    "-0.42",
                    "-0.57",
                    "-0.49",
                    "234"
                ],
                [
                    "acl:relcl",
                    "1.28",
                    "1.40",
                    "1.31",
                    "193"
                ],
                [
                    "ccomp",
                    "0.11",
                    "0.31",
                    "0.34",
                    "191"
                ],
                [
                    "acl",
                    "0.77",
                    "0.59",
                    "0.58",
                    "159"
                ],
                [
                    "parataxis",
                    "0.44",
                    "0.63",
                    "0.79",
                    "127"
                ],
                [
                    "amod",
                    "1.92",
                    "1.88",
                    "1.81",
                    "76"
                ],
                [
                    "csubj",
                    "0.36",
                    "0.38",
                    "0.27",
                    "37"
                ]
            ]
        },
        "gold_description": "evidence of this complementarity can be seen in table 6 , which contains a breakdown of system performance by governing dependency relation , for both linear and tree models , on uds - ih2 - dev . in most cases , the l - bilstm ' s mean prediction is closer to the true mean . this appears to arise in part because the t - bilstm is less confident in its predictions – i . e . its mean prediction tends to be closer to 0 . this results in the l - bilstm being too confident in certain cases – e . g . in the case of the xcomp governing relation , where the t - bilstm mean prediction is closer to the true mean .",
        "generated_description": "we can see that the t - bilstm - s ( 2 ) performs better than the linear ( l - bilcstm ) model on root , advcl , xcomp , conj , and acl : relcl . the tree model performs better on parataxis , amod and csubj , but worse on root and advcl . one possible reason for this is that root is one of the least frequent relations in the uds dataset ."
    },
    {
        "table_id": "175",
        "table_info": {
            "table_caption": "Table 1: The lexicons used as external knowledge.",
            "table_column_names": [
                "Lexicons",
                "Annotations",
                "# dim.",
                "# words"
            ],
            "table_content_values": [
                [
                    "LIWC",
                    "psycho-linguistic",
                    "73",
                    "18,504"
                ],
                [
                    "Bing Liu",
                    "valence",
                    "1",
                    "2,477"
                ],
                [
                    "AFINN",
                    "sentiment",
                    "1",
                    "6,786"
                ],
                [
                    "MPQA",
                    "sentiment",
                    "4",
                    "6,886"
                ],
                [
                    "SemEval15",
                    "sentiment",
                    "1",
                    "1,515"
                ],
                [
                    "Emolex",
                    "emotion",
                    "19",
                    "14,182"
                ]
            ]
        },
        "gold_description": "we augment our models with existing linguistic and affective knowledge from human experts . specifically , we leverage lexica containing psycho - linguistic , sentiment and emotion annotations . the word ' s annotations from the lexicons shown in table 1 . as prior knowledge , we leverage the lexicons presented in table 1 . we selected widely - used lexicons that represent different facets of affective and psycho - linguistic features , namely ;",
        "generated_description": "the liwc lexicon represents the knowledge of psycho - linguistic psychology and liwc is one of the most used lexicons for external knowledge . it contains 73 psychological terms as external knowledge while bing liu and afinn are used for valence and sentiment while semeval15 and mpqa lexicons are for emotion . in addition , emolex is a new lexicon for emotion which was proposed by \\ newcitedblp : journals / corr / abs - 1908 - 10683 . it covers 19 different emotions and has the highest number of annotations ."
    },
    {
        "table_id": "176",
        "table_info": {
            "table_caption": "Table 3: Comparison across benchmark datasets. Reported values are averaged across ten runs. All reported measures are F1 scores, apart from SST−5 which is evaluated with Accuracy.",
            "table_column_names": [
                "Model",
                "SST-5",
                "Sent17",
                "PhychExp",
                "Irony18",
                "SCv1",
                "SCv2"
            ],
            "table_content_values": [
                [
                    "baseline",
                    "43.5±0.5",
                    "68.3±0.2",
                    "53.2±0.8",
                    "46.3±1.4",
                    "64.1±0.5",
                    "74.0±0.7"
                ],
                [
                    "emb. conc.",
                    "43.3±0.6",
                    "68.4±0.2",
                    "57.1±1.2",
                    "48.1±1.2",
                    "64.2±0.7",
                    "74.2±0.7"
                ],
                [
                    "conc.",
                    "44.0±0.7",
                    "68.6±0.3",
                    "54.3±0.6",
                    "47.4±0.9",
                    "[BOLD] 65.1±0.6",
                    "74.3±1.2"
                ],
                [
                    "gate",
                    "44.2±0.4",
                    "68.7±0.3",
                    "53.4±1.0",
                    "[BOLD] 48.5±0.7",
                    "64.7±0.7",
                    "74.3±1.2"
                ],
                [
                    "affine",
                    "43.2±0.7",
                    "68.5±0.3",
                    "53.1±0.9",
                    "45.3±1.5",
                    "60.3±0.8",
                    "74.0±1.0"
                ],
                [
                    "gate+emb.conc.",
                    "[BOLD] 46.2±0.5",
                    "[BOLD] 68.9±0.3",
                    "[BOLD] 57.2±1.1",
                    "[BOLD] 48.4±1.0",
                    "[BOLD] 64.9±0.6",
                    "[BOLD] 74.4±0.9"
                ],
                [
                    "state-of-the-art",
                    "51.7",
                    "68.5",
                    "57.0",
                    "53.6",
                    "69.0",
                    "76.0"
                ],
                [
                    "state-of-the-art",
                    "Shen et al. ( 2018 )",
                    "Cliche ( 2017 )",
                    "Felbo et al. ( 2017 )",
                    "Baziotis et al. ( 2018 )",
                    "Felbo et al. ( 2017 )",
                    "Ilić et al. ( 2018 )"
                ]
            ]
        },
        "gold_description": "we present our results in section 5 ( table 3 ) in table 3 we use the abbreviations \" baseline \" and \" emb . conc . \" for the two baseline models respectively . we compare the performance of the three proposed conditioning methods with the two baselines and the state - of - the - art in table 3 . the results show that incorporating external knowledge in rnn - based architectures consistently improves performance over the baseline for all datasets . furthermore , feature - based gating improves upon baseline concatenation in the embedding layer across benchmarks , with the exception of psychexp dataset . for the sent17 dataset we achieve state - ofthe - art f1 score using the feature - based gating method ; we further improve performance when combining gating with the emb . conc . method . for sst - 5 , we observe a significant performance boost with combined attentional gating and embedding conditioning ( gate + emb . conc . ) . for psychexp , we marginally outperform the state - ofthe - art also with the combined method , while for irony18 , feature - based gating yields the best results . finally , concatenation based conditioning is the top method for scv1 , and the combination method for scv2 .",
        "generated_description": "the results show that concatenation , emb . conc . and affine are all significantly better than the baseline , with gate being slightly better than affine on most of the tasks but slightly worse on sst - 5 and phychexp ."
    },
    {
        "table_id": "177",
        "table_info": {
            "table_caption": "Table 5: Mean gold labels, counts, and MAE for L-biLSTM(2)-S and T-biLSTM(2)-S model predictions on UDS-IH2-dev, grouped by modals and negation.",
            "table_column_names": [
                "Modal",
                "Negated",
                "Mean Label",
                "Linear MAE",
                "Tree MAE",
                "#"
            ],
            "table_content_values": [
                [
                    "none",
                    "no",
                    "1.00",
                    "0.93",
                    "1.03",
                    "2244"
                ],
                [
                    "none",
                    "yes",
                    "-0.19",
                    "1.40",
                    "1.69",
                    "98"
                ],
                [
                    "may",
                    "no",
                    "-0.38",
                    "1.00",
                    "0.99",
                    "14"
                ],
                [
                    "would",
                    "no",
                    "-0.61",
                    "0.85",
                    "0.99",
                    "39"
                ],
                [
                    "ca(n’t)",
                    "yes",
                    "-0.72",
                    "1.28",
                    "1.55",
                    "11"
                ],
                [
                    "can",
                    "yes",
                    "-0.75",
                    "0.99",
                    "0.86",
                    "6"
                ],
                [
                    "(wi)’ll",
                    "no",
                    "-0.94",
                    "1.47",
                    "1.14",
                    "8"
                ],
                [
                    "could",
                    "no",
                    "-1.03",
                    "0.97",
                    "1.32",
                    "20"
                ],
                [
                    "can",
                    "no",
                    "-1.25",
                    "1.02",
                    "1.21",
                    "73"
                ],
                [
                    "might",
                    "no",
                    "-1.25",
                    "0.66",
                    "1.06",
                    "6"
                ],
                [
                    "would",
                    "yes",
                    "-1.27",
                    "0.40",
                    "0.86",
                    "5"
                ],
                [
                    "should",
                    "no",
                    "-1.31",
                    "1.20",
                    "1.01",
                    "22"
                ],
                [
                    "will",
                    "no",
                    "-1.88",
                    "0.75",
                    "0.86",
                    "75"
                ]
            ]
        },
        "gold_description": "table 5 illustrates the influence of modals and negation on the factuality of the events they have direct scope over . the context with the highest factuality on average is no direct modal and no negation ( first row ) ; all other modal contexts have varying degrees of negative mean factuality scores , with will as the most negative .",
        "generated_description": "the mean absolute difference in mean mae between the linear mae and tree mae for negated negations is 0 . 93 and 2244 negated terms , corresponding to a statistically significant difference of 0 . 03 and 1 . 69 ( p < 0 . 0001 ) ."
    },
    {
        "table_id": "178",
        "table_info": {
            "table_caption": "Table 2: Coefficient of determination (r2) between global metrics and crowdsourced topic-word matching annotations.",
            "table_column_names": [
                "[EMPTY]",
                "Metric",
                "Amazon",
                "Newsgroups",
                "New York Times"
            ],
            "table_content_values": [
                [
                    "Global",
                    "SigVac",
                    "0.6960",
                    "0.6081",
                    "0.6063"
                ],
                [
                    "Global",
                    "SigUni",
                    "0.6310",
                    "0.4839",
                    "0.4935"
                ],
                [
                    "Global",
                    "Coherence",
                    "0.4907",
                    "0.4463",
                    "0.3799"
                ]
            ]
        },
        "gold_description": "as seen in table 2 , we report the coefficient of determination ( r2 ) for each global metric and dataset . note that global metrics do correlate somewhat with human judgment of local topic quality . however , the correlation is moderate to poor , especially in the case of coherence",
        "generated_description": "topic - word matching annotations are an important aspect of automated topic categorization that has been neglected in recent years in this section , we compare the quality of topic - word mappings produced by the sigvac , siguni , and coherence metrics on amazon , newsgroups , and the new york times datasets . for each metric , we measure the coefficient of determination ( r2 ) between its performance and that of the human - annotated topic labels . we observe that the global coherence metric outperforms the other global metrics on all three datasets . this shows that the topic coherence of the automatically generated topics is better than the one produced by humans in the case of the amazon and newsgroups datasets . however , the contribution of the global metrics to the overall quality of the manually - generated topics is not as high as the individual metrics ."
    },
    {
        "table_id": "179",
        "table_info": {
            "table_caption": "Table 3: Coefficient of determination (r2) between automated metrics and crowdsourced topic-word matching annotations. We include metrics measuring both local topic quality and global topic quality.",
            "table_column_names": [
                "[EMPTY]",
                "Metric",
                "Amazon",
                "Newsgroups",
                "New York Times"
            ],
            "table_content_values": [
                [
                    "Local",
                    "SwitchP",
                    "0.9077",
                    "0.8737",
                    "0.7022"
                ],
                [
                    "Local",
                    "SwitchVI",
                    "0.8485",
                    "0.8181",
                    "0.6977"
                ],
                [
                    "Local",
                    "AvgRank",
                    "0.5103",
                    "0.5089",
                    "0.4473"
                ],
                [
                    "Local",
                    "Window",
                    "0.4884",
                    "0.3024",
                    "0.1127"
                ],
                [
                    "Local",
                    "WordDiv",
                    "0.3112",
                    "0.2197",
                    "0.0836"
                ],
                [
                    "Global",
                    "SigVac",
                    "0.6960",
                    "0.6081",
                    "0.6063"
                ],
                [
                    "Global",
                    "SigUni",
                    "0.6310",
                    "0.4839",
                    "0.4935"
                ],
                [
                    "Global",
                    "Coherence",
                    "0.4907",
                    "0.4463",
                    "0.3799"
                ]
            ]
        },
        "gold_description": "humans agree more often with models trained on amazon reviews than on new york times . as seen in table 3 , we report the coefficient of determination ( r2 ) for each metric and dataset . switchp most closely approximates human judgments of local topic quality , with an r2 which indicates a strong correlation . as evidenced by the lower r2 for switchvi , even switching between related topics does not seem to line up with human judgments of local topic quality .",
        "generated_description": "we include metrics measuring both local topic quality and global topic quality . for local topic evaluation , we measure the coefficient of determination ( r2 ) between the automated metrics and the crowdsourced topic - word matching annotations . we observe that the quality of the local topic models is better than that of the global topic models on both amazon and newsgroups . on newsgroups , the local metrics are competitive with the global metrics . on the other hand , on amazon , the global topics are significantly better than the local ones . for example , on the new york times topic , the average rank of local metrics is 0 . 5103 , as opposed to 0 . 8181 and 0 . 6977 for switchp and switchvi , respectively . this shows that while the metrics can measure topic quality in isolation , they are not able to measure topic coherence in isolation . we also observe that siguni is the best global topic model on both local and global metrics , which confirms that it is better to generate topics that reflect the topic of the entire document than to filter words that are not related to the topic in the conversation ."
    },
    {
        "table_id": "180",
        "table_info": {
            "table_caption": "Table 3: Event salience performance. (-E) and (-F) marks removing Entity information and Features from the full KCM model. The relative performance differences are computed against Frequency. W/T/L are the number of documents a method wins, ties, and loses compared to Frequency. † and ‡ mark the statistically significant improvements over Frequency†, LeToR‡ respectively.",
            "table_column_names": [
                "[BOLD] Method Location",
                "[BOLD] P@01 0.3555",
                "[BOLD] P@01 –",
                "[BOLD] P@05 0.3077",
                "[BOLD] P@05 –",
                "[BOLD] P@10 0.2505",
                "[BOLD] P@10 –",
                "[BOLD] AUC 0.5226",
                "[BOLD] AUC –"
            ],
            "table_content_values": [
                [
                    "PageRank",
                    "0.3628",
                    "–",
                    "0.3438",
                    "–",
                    "0.3007",
                    "–",
                    "0.5866",
                    "–"
                ],
                [
                    "Frequency",
                    "0.4542",
                    "–",
                    "0.4024",
                    "–",
                    "0.3445",
                    "–",
                    "0.5732",
                    "–"
                ],
                [
                    "LeToR",
                    "0.4753†",
                    "+4.64%",
                    "0.4099†",
                    "+1.87%",
                    "0.3517†",
                    "+2.10%",
                    "0.6373†",
                    "+11.19%"
                ],
                [
                    "KCE (-EF)",
                    "0.4420",
                    "−2.69%",
                    "0.4038",
                    "+0.34%",
                    "0.3464†",
                    "+0.54%",
                    "0.6089†",
                    "+6.23%"
                ],
                [
                    "KCE (-E)",
                    "0.4861†‡",
                    "+7.01%",
                    "0.4227†‡",
                    "+5.04%",
                    "0.3603†‡",
                    "+4.58%",
                    "0.6541†‡",
                    "+14.12%"
                ],
                [
                    "KCE",
                    "0.5049†‡",
                    "+11.14%",
                    "0.4277†‡",
                    "+6.29%",
                    "0.3638†‡",
                    "+5.61%",
                    "0.6557†‡",
                    "+14.41%"
                ],
                [
                    "[BOLD] Method",
                    "[BOLD] R@01",
                    "[BOLD] R@01",
                    "[BOLD] R@05",
                    "[BOLD] R@05",
                    "[BOLD] R@10",
                    "[BOLD] R@10",
                    "[BOLD] W/T/L",
                    "[BOLD] W/T/L"
                ],
                [
                    "Location",
                    "0.0807",
                    "–",
                    "0.2671",
                    "–",
                    "0.3792",
                    "–",
                    "–/–/–",
                    "–/–/–"
                ],
                [
                    "PageRank",
                    "0.0758",
                    "–",
                    "0.2760",
                    "–",
                    "0.4163",
                    "–",
                    "–/–/–",
                    "–/–/–"
                ],
                [
                    "Frequency",
                    "0.0792",
                    "–",
                    "0.2846",
                    "–",
                    "0.4270",
                    "–",
                    "–/–/–",
                    "–/–/–"
                ],
                [
                    "LeToR",
                    "0.0836†",
                    "+5.61%",
                    "0.2980†",
                    "+4.70%",
                    "0.4454†",
                    "+4.31%",
                    "8037 / 48493 / 6770",
                    "8037 / 48493 / 6770"
                ],
                [
                    "KCE (-EF)",
                    "0.0714",
                    "−9.77%",
                    "0.2812",
                    "−1.18%",
                    "0.4321†",
                    "+1.20%",
                    "6936 / 48811 / 7553",
                    "6936 / 48811 / 7553"
                ],
                [
                    "KCE (-E)",
                    "0.0925†‡",
                    "+16.78%",
                    "0.3172†‡",
                    "+11.46%",
                    "0.4672†‡",
                    "+9.41%",
                    "11676 / 43294 / 8330",
                    "11676 / 43294 / 8330"
                ],
                [
                    "KCE",
                    "0.0946†‡",
                    "+19.44%",
                    "0.3215†‡",
                    "+12.96%",
                    "0.4719†‡",
                    "+10.51%",
                    "12554 / 41461 / 9285",
                    "12554 / 41461 / 9285"
                ]
            ]
        },
        "gold_description": "we summarize the main results in table 3 . frequency is the best performing baseline . its precision at 1 and 5 are higher than 40 % . pagerank performs worse than frequency on all the precision and recall metrics . location performs the worst . letor outperforms the baselines significantly on all metrics . particularly , its p @ 1 value outperforms the frequency baseline the most ( 4 . 64 % ) , indicating a much better estimation on the most salient event . in terms of auc , letor outperforms frequency by a large margin ( 11 . 19 % relative gain ) . the kce model further beats letor significantly on all metrics , by around 5 % on auc and precision values , and by around 10 % on the recall values . notably , the p @ 1 score is much higher , reaching 50 % . the large relative gain on all the recall metrics and the high performance on precision show that kce works really well on the top of the rank list . to understand the source of performance gain of kce , we conduct an ablation study by removing its components : - e removes of entity kernels ; - ef removes the entity kernels and the features . we observe a performance drop in both cases .",
        "generated_description": "pagerank and frequency perform poorly in this evaluation . we believe this is due to the fact that they do not consider the importance of the first preceding and next sentence in selecting the best document . in contrast , letor and kce make full use of the preceding and subsequent sentences in a document , and thus are able to capture important information about the document that is not already contained in the document . as a result , the results of pagerank are significantly lower than those of the other baselines . frequency performs poorly because it is not able to identify important content in the first sentence and thus fails to aggregate information from the whole document ."
    },
    {
        "table_id": "181",
        "table_info": {
            "table_caption": "Table 4: Feature Ablation Results. + sign indicates the additional features to Frequency. Loc is the sentence location feature. Event is the event voting feature. Entity is the entity voting feature. Local is the local entity voting feature. † marks the statistically significant improvements over + Loc.",
            "table_column_names": [
                "[BOLD] Feature Groups",
                "P@1",
                "P@5",
                "P@10",
                "R@1",
                "R@5",
                "R@10",
                "AUC"
            ],
            "table_content_values": [
                [
                    "Loc",
                    "0.3548",
                    "0.3069",
                    "0.2497",
                    "0.0807",
                    "0.2671",
                    "0.3792",
                    "0.5226"
                ],
                [
                    "Frequency",
                    "0.4536",
                    "0.4018",
                    "0.3440",
                    "0.0792",
                    "0.2846",
                    "0.4270",
                    "0.5732"
                ],
                [
                    "+ Loc",
                    "0.4734",
                    "0.4097",
                    "0.3513",
                    "0.0835",
                    "0.2976",
                    "0.4436",
                    "0.6354"
                ],
                [
                    "+ Loc + Event",
                    "0.4726",
                    "0.4101†",
                    "0.3516",
                    "0.0831",
                    "0.2969",
                    "0.4431",
                    "0.6365†"
                ],
                [
                    "+ Loc + Entity",
                    "0.4739",
                    "0.4100",
                    "0.3518",
                    "0.0812",
                    "0.2955",
                    "0.4418",
                    "0.6374"
                ],
                [
                    "+ Loc + Entity + Event",
                    "0.4739",
                    "0.4100",
                    "0.3518†",
                    "0.0832",
                    "0.2974",
                    "0.4452†",
                    "0.6374†"
                ],
                [
                    "+ Loc + Entity + Event + Local",
                    "0.4754†",
                    "0.4100",
                    "0.3517†",
                    "0.0837",
                    "0.2981",
                    "0.4454†",
                    "0.6373†"
                ]
            ]
        },
        "gold_description": "we gradually add feature groups to the frequency baseline . the combination of location ( sentence location ) and frequency almost sets the performance for the whole model . adding each voting feature individually produces mixed results . however , adding all voting features improves all metrics . though the margin is small , 4 of them are statistically significant over frequency + location . to understand the contribution of individual features , we conduct an ablation study of various feature settings in table 4 .",
        "generated_description": "in general , the feature groups loc , frequency and loc + event perform better than the other features , which confirms the importance of local features for this task . however , when incorporating all features , the performance significantly improves , i . e . , 1 . 3 points in terms of p @ 1 , 1 . 5 points in p @ 5 , 0 . 8 points in r @ 5 and 0 . 6 points in auc . the best performance is achieved when all feature groups are combined ."
    },
    {
        "table_id": "182",
        "table_info": {
            "table_caption": "Table 7: Notable attributes of 50 instances from UDS-IH2-dev with highest absolute prediction error (using H-biLSTM(2)-MultiSim w/UDS-IH2).",
            "table_column_names": [
                "Attribute",
                "#"
            ],
            "table_content_values": [
                [
                    "Grammatical error present, incl. run-ons",
                    "16"
                ],
                [
                    "Is an auxiliary or light verb",
                    "14"
                ],
                [
                    "Annotation is incorrect",
                    "13"
                ],
                [
                    "Future event",
                    "12"
                ],
                [
                    "Is a question",
                    "5"
                ],
                [
                    "Is an imperative",
                    "3"
                ],
                [
                    "Is not an event or state",
                    "2"
                ],
                [
                    "One or more of the above",
                    "43"
                ]
            ]
        },
        "gold_description": "table 7 shows results from a manual error analysis on 50 events from uds - ih2 - dev with highest absolute prediction error ( using h - bilstm ( 2 ) multisim w / uds - ih2 ) . grammatical errors ( such as run - on sentences ) in the underlying text of uds - ih2 appear to pose a particular challenge for these models ;",
        "generated_description": "s5ss0sss0px2 notable attributes of uds - ih2 - dev in this section , we will discuss the top five attributes that are the ones that contribute to the highest absolute error . we manually selected 50 instances from the dev set that are identified as the ones with higher absolute error ( using the h - bilstm ( 2 ) - multisim w / uds - ih2 ) . in total , 43 of the 50 instances are not an event or state . the remaining two are either an auxiliary or light verb ."
    },
    {
        "table_id": "183",
        "table_info": {
            "table_caption": "Table 5: Similarities between event entity pairs. Word2vec shows the cosine similarity in pre-trained embeddings. KCE lists their closest kernel mean after training. (E) marks entities.",
            "table_column_names": [
                "attack",
                "kill",
                "[BOLD] Word2Vec 0.69",
                "[BOLD] KCE 0.3"
            ],
            "table_content_values": [
                [
                    "arrest",
                    "charge",
                    "0.53",
                    "0.3"
                ],
                [
                    "USA (E)",
                    "war",
                    "0.46",
                    "0.3"
                ],
                [
                    "911 attack (E)",
                    "attack",
                    "0.72",
                    "0.3"
                ],
                [
                    "attack",
                    "trade",
                    "0.42",
                    "0.9"
                ],
                [
                    "hotel (E)",
                    "travel",
                    "0.49",
                    "0.9"
                ],
                [
                    "charge",
                    "murder",
                    "0.49",
                    "0.7"
                ],
                [
                    "business (E)",
                    "increase",
                    "0.43",
                    "0.7"
                ],
                [
                    "attack",
                    "walk",
                    "0.44",
                    "-0.3"
                ],
                [
                    "people (E)",
                    "work",
                    "0.40",
                    "-0.3"
                ]
            ]
        },
        "gold_description": "we inspect some pairs of events and entities in different kernels and list some examples in table 5 . the pairs in table 3 exhibit interesting types of relations : e . g . , \" arrest - charge \" and \" attack - kill \" form script - like chains ; \" 911 attack \" forms a quasiidentity relation ( recasens et al . , 2010 ) with \" attack \" ; \" business \" and \" increase \" are candidates as frame - argument structure .",
        "generated_description": "word2vec shows the cosine similarity of event entity pairs with their word embeddings . kce lists their closest kernel mean after training . we can see that word2vec has a very high similarity score for event pairs . for example , the similarity score of ‘ kill ’ is 0 . 69 for the ‘ attack ’ event pair , while its closest word embedding similarity for the 908 attack event pair has a score of 0 . 72 . this shows that pre - trained embedding models are able to capture the semantic similarity between event pairs and their word vectors ."
    },
    {
        "table_id": "184",
        "table_info": {
            "table_caption": "Table 2: Self-BLEU and percent of generated n-grams that are unique relative to own generations (left) WikiText-103 test set (middle) a sample of 5000 sentences from Toronto Book Corpus (right). For the WT103 and TBC rows, we sample 1000 sentences from the respective datasets.",
            "table_column_names": [
                "Model",
                "Self-BLEU (↓)",
                "% Unique  [ITALIC] n-grams (↑) Self",
                "% Unique  [ITALIC] n-grams (↑) Self",
                "% Unique  [ITALIC] n-grams (↑) Self",
                "% Unique  [ITALIC] n-grams (↑) WT103",
                "% Unique  [ITALIC] n-grams (↑) WT103",
                "% Unique  [ITALIC] n-grams (↑) WT103",
                "% Unique  [ITALIC] n-grams (↑) TBC",
                "% Unique  [ITALIC] n-grams (↑) TBC",
                "% Unique  [ITALIC] n-grams (↑) TBC"
            ],
            "table_content_values": [
                [
                    "Model",
                    "Self-BLEU (↓)",
                    "n=2",
                    "n=3",
                    "n=4",
                    "n=2",
                    "n=3",
                    "n=4",
                    "n=2",
                    "n=3",
                    "n=4"
                ],
                [
                    "BERT (large)",
                    "9.43",
                    "63.15",
                    "92.38",
                    "98.01",
                    "59.91",
                    "91.86",
                    "98.43",
                    "64.59",
                    "93.27",
                    "98.59"
                ],
                [
                    "BERT (base)",
                    "10.06",
                    "60.76",
                    "91.76",
                    "98.14",
                    "57.90",
                    "91.72",
                    "98.55",
                    "60.94",
                    "92.04",
                    "98.56"
                ],
                [
                    "GPT",
                    "40.02",
                    "31.13",
                    "67.01",
                    "87.28",
                    "33.71",
                    "72.86",
                    "91.12",
                    "25.74",
                    "65.04",
                    "88.42"
                ],
                [
                    "WT103",
                    "9.80",
                    "70.29",
                    "94.36",
                    "99.05",
                    "56.19",
                    "88.05",
                    "97.44",
                    "68.35",
                    "94.20",
                    "99.23"
                ],
                [
                    "TBC",
                    "12.51",
                    "62.19",
                    "92.70",
                    "98.73",
                    "55.30",
                    "91.08",
                    "98.81",
                    "44.75",
                    "82.06",
                    "96.31"
                ]
            ]
        },
        "gold_description": "we present sample generations , quality results , and diversity results respectively in tables 1 , 2 , 3 . this observation is further bolstered by the fact that the gpt generations have a higher corpus - bleu with tbc than tbc has with itself . the corpusbleu between bert models and the datasets is low , particularly with wt103 . we find that bert generations are more diverse than gpt generations . gpt has high n - gram overlap ( smaller percent of unique n - grams ) with tbc , but surprisingly also with wikitext - 103 , despite being trained on different data . furthermore , gpt generations have greater n - gram overlap with these datasets than these datasets have with themselves , further suggesting that gpt is relying significantly on generic sentences . bert has lower n - gram overlap with both corpora , with similar degrees of n - gram overlap as the samples of the data .",
        "generated_description": "in order to better understand the effect of model size on generation quality , we investigate how the number of unique n - grams ( n ) used by the model affects the generation quality . we can see that the bert ( base ) and large ( large ) models have the highest percentage of unique unigrams and bigrams ( n = 2 ) on wt103 and tbc datasets , respectively . the large bert model outperforms the base model by a large margin ( 9 . 43 % vs . 9 . 06 % in terms of unigram bleu ) on the wt103 dataset , while the small bert models ( base , large ) have the lowest percentage ( 10 . 06 % ) on tbc dataset . this is consistent with our intuition that a larger model requires a larger number of training examples ( more training examples ) in order to have a better estimation of the probability of producing the generated text ."
    },
    {
        "table_id": "185",
        "table_info": {
            "table_caption": "Table 3: Quality metrics of model generations. Perplexity (PPL) is measured using an additional language model (Dauphin et al., 2016). For the WT103 and TBC rows, we sample 1000 sentences from the respective datasets.",
            "table_column_names": [
                "Model",
                "Corpus-BLEU (↑) WT103",
                "Corpus-BLEU (↑) TBC",
                "PPL (↓)"
            ],
            "table_content_values": [
                [
                    "BERT (large)",
                    "5.05",
                    "7.60",
                    "331.47"
                ],
                [
                    "BERT (base)",
                    "7.80",
                    "7.06",
                    "279.10"
                ],
                [
                    "GPT",
                    "10.81",
                    "30.75",
                    "154.29"
                ],
                [
                    "WT103",
                    "17.48",
                    "6.57",
                    "54.00"
                ],
                [
                    "TBC",
                    "10.05",
                    "23.05",
                    "314.28"
                ]
            ]
        },
        "gold_description": "we present sample generations , quality results , and diversity results respectively in tables 1 , 2 , 3 . we find that , compared to gpt , the bert generations are of worse quality , but are more diverse . surprisingly , the outside language model , which was trained on wikipedia , is less perplexed by the gpt generations than the bert generations , even though gpt was only trained on romance novels and bert was trained on romance novels and wikipedia . on actual data from tbc , the outside language model is about as perplexed as on the bert generations , which suggests that domain shift is an issue in using a trained language model for evaluating generations and that the gpt generations might have collapsed to fairly generic and simple sentences . the perplexity on bert samples is not absurdly high , and in reading the samples , we find that many are fairly coherent .",
        "generated_description": "wt103 and tbc . we sample 1000 sentences from each of the datasets for each evaluation , and sample the last 1000 sentences to construct the test sets . we find that bert ( large ) produces the highest quality generations on wt103 , followed by gpt and the bert base model . the bert large model also produces the lowest quality sentences on tbc , which is surprising given that this model was trained on much larger data than the gpt model . we note that the perplexity of the wt103 generated sentences is significantly higher than that of the tbc generated sentences , indicating that the language model is able to benefit from the additional capacity provided by the larger model ."
    },
    {
        "table_id": "186",
        "table_info": {
            "table_caption": "Table 1: F1 scores on six test sets. The last column, Avg, shows the average of F1 scores on MSNBC, AQUAINT, ACE2004, CWEB, and WIKI.",
            "table_column_names": [
                "Methods",
                "AIDA-B",
                "MSNBC",
                "AQUAINT",
                "ACE2004",
                "CWEB",
                "WIKI",
                "Avg"
            ],
            "table_content_values": [
                [
                    "[ITALIC] Wikipedia",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Milne and Witten ( 2008 )",
                    "-",
                    "78",
                    "85",
                    "81",
                    "64.1",
                    "[BOLD] 81.7",
                    "77.96"
                ],
                [
                    "Ratinov et al. ( 2011a )",
                    "-",
                    "75",
                    "83",
                    "82",
                    "56.2",
                    "67.2",
                    "72.68"
                ],
                [
                    "Hoffart et al. ( 2011 )",
                    "-",
                    "79",
                    "56",
                    "80",
                    "58.6",
                    "63",
                    "67.32"
                ],
                [
                    "Cheng and Roth ( 2013 )",
                    "-",
                    "90",
                    "90",
                    "86",
                    "67.5",
                    "73.4",
                    "81.38"
                ],
                [
                    "Chisholm and Hachey ( 2015 )",
                    "84.9",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "[ITALIC] Wiki + unlab",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Lazic et al. ( 2015 )",
                    "86.4",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "[EMPTY]"
                ],
                [
                    "Our model",
                    "[BOLD] 89.66 ±0.16",
                    "[BOLD] 92.2 ±0.2",
                    "[BOLD] 90.7 ±0.2",
                    "[BOLD] 88.1 ±0.0",
                    "[BOLD] 78.2 ±0.2",
                    "[BOLD] 81.7 ±0.1",
                    "[BOLD] 86.18"
                ],
                [
                    "[ITALIC] Wiki + Extra supervision",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Chisholm and Hachey ( 2015 )",
                    "88.7",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "[ITALIC] Fully-supervised (Wiki +",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[ITALIC] AIDA CoNLL train)",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Guo and Barbosa ( 2016 )",
                    "89.0",
                    "92",
                    "87",
                    "88",
                    "77",
                    "84.5",
                    "85.7"
                ],
                [
                    "Globerson et al. ( 2016 )",
                    "91.0",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "Yamada et al. ( 2016 )",
                    "91.5",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "Ganea and Hofmann ( 2017 )",
                    "92.22 ±0.14",
                    "93.7 ±0.1",
                    "88.5 ±0.4",
                    "88.5 ±0.3",
                    "77.9 ±0.1",
                    "77.5 ±0.1",
                    "85.22"
                ],
                [
                    "Le and Titov ( 2018 )",
                    "93.07 ±0.27",
                    "93.9 ±0.2",
                    "88.3 ±0.6",
                    "89.9 ±0.8",
                    "77.5 ±0.1",
                    "78.0 ±0.1",
                    "85.5"
                ]
            ]
        },
        "gold_description": "first , we compare to systems which relied on wikipedia and those which used wikipedia along with unlabeled data ( ' wikipedia + unlab ' ) , i . e . the top half of table 1 . these methods are comparable to ours , as they use the same type of information as supervision . our model outperformed all of them on all test sets . when evaluated on aida - b , their scores are still lower than ours , though significantly higher that those of the previous systems suggesting that web links are indeed valuable . second , we compare to fully - supervised systems , which were estimated on aida - conll documents . we distinguish results on a test set taken from aida - conll ( aida - b ) and the other standard test sets not directly corresponding to the aida - conll domain . when tested on the latter , our approach is very effective , on average outperforming fully - supervised techniques . as expected , on the in - domain test set ( aida - b ) , the majority of recent fully - supervised methods are more accurate than our model . however , even on this test set our model is not as far behind , for example , outperforming the system of guo and barbosa ( 2016 ) .",
        "generated_description": "our baseline is a bilstm - crf trained on the wikipedia split of the conll 2003 ner task . our model outperforms the previous state - of - the - art by a large margin on all the tasks . our full model ( with wikipedia embeddings as unlabelled data ) achieves the best performance on the aida - b , msnbc , aquaint , ace2004 , cweb , and wiki datasets , with an average of 77 . 96 % f1 ."
    },
    {
        "table_id": "187",
        "table_info": {
            "table_caption": "Table 2: F1 scores of our model when it is weakly-supervised and when it is fully-supervised on Wikipedia and on AIDA CoNLL. AIDA-A is our development set. Avg is the average of F1 scores on MSNBC, AQUAINT, ACE2004, CWEB, and WIKI. Each F1 is the mean of five runs.",
            "table_column_names": [
                "Our model",
                "AIDA-A",
                "AIDA-B",
                "Avg"
            ],
            "table_content_values": [
                [
                    "weakly-supervised",
                    "88.05",
                    "89.66",
                    "86.18"
                ],
                [
                    "fully-supervised",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "on Wikipedia",
                    "87.23",
                    "87.83",
                    "85.84"
                ],
                [
                    "on AIDA CoNLL",
                    "91.34",
                    "91.87",
                    "84.55"
                ]
            ]
        },
        "gold_description": "the results are shown in table 2 ( ' wikipedia ' ) . the resulting model is significantly less accurate than the one which used unlabeled documents . the score difference is larger for aida - conll test set than for the other 5 test sets . additionally we train our model on aida - conll , producing its fully - supervised version ( ' aida conll ' row in table 2 ) . though , as expected , this version is more accurate on aida test set , similarly to other fully - supervised methods , it overfits and does not perform that well on the 5 out - of - domain test sets .",
        "generated_description": "our model is weakly - supervised . in this section , we compare the performance of our model when it is supervised on wikipedia and on aida conll . we can see that our model performs much better on wikipedia than on cweb and wiki . on aida - a , our model achieves 87 . 23 and 87 . 83 f1 score on wikipedia , and 85 . 84 and 84 . 55 f1 scores on aida - b . our model also significantly outperforms the model that is only trained on wikipedia . this shows that our full model outperforms our model that only has access to the wikipedia and aida datasets ."
    },
    {
        "table_id": "188",
        "table_info": {
            "table_caption": "Table 3: Ablation study on AIDA CoNLL development set. Each F1 score is the mean of five runs.",
            "table_column_names": [
                "Model",
                "AIDA-A"
            ],
            "table_content_values": [
                [
                    "Our model",
                    "88.05"
                ],
                [
                    "without local",
                    "82.41"
                ],
                [
                    "without attention",
                    "86.82"
                ],
                [
                    "No disambiguation model ( [ITALIC] sc)",
                    "86.42"
                ]
            ]
        },
        "gold_description": "as we do not want to test multiple systems on the final test set , we report the remaining ablations on the development set ( aida - a ) , table 3 . we constructed a baseline which only relies on link statistics in wikipedia as well as string similarity ( we refereed to its scoring function as sc ) . it appears surprisingly strong , however , we still outperform it by 1 . 6 % ( see table 3 ) . when we use only global coherence ( i . e . only second term in expression ( 1 ) ) and drop any modeling of local context on the disambiguation stage , the performance drops very substantially ( to 82 . 4 % f1 , see table 3 ) . without using local scores the disambiguation model appears to be even less accurate than our ' no - statisticaldisambiguation ' baseline . it is also important to have an accurate global model : not using global attention results in a 1 . 2 % drop in performance .",
        "generated_description": "we conduct an ablation study on aida conll dataset to verify the effectiveness of each component of our model . specifically , we compare our model with the following variants : ( 1 ) local embedding : concatenate the word embeddings of ida and cider directly into a single vector , ( 2 ) no disambiguation : we only use the bag - of - words representation to find the sense of each word , and ( 3 ) no attention : we remove the attention mechanism , i . e . , we only sum the local and global embedding vectors to obtain the final word embedding vector . from the table , we can see that the local part contributes the most to the overall performance , which indicates that the global information is very important for aida - a . the attention mechanism is also effective to improve the performance . the reason is that local information can capture the global dependency between the word and its context , which is beneficial for the linking of related words ."
    },
    {
        "table_id": "189",
        "table_info": {
            "table_caption": "Table 4: Accuracy (%) by NER type on AIDA-A.",
            "table_column_names": [
                "Type",
                "Our model",
                "Fully-supervised learning"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "on AIDA CoNLL"
                ],
                [
                    "LOC",
                    "85.53",
                    "89.41"
                ],
                [
                    "MISC",
                    "75.71",
                    "83.27"
                ],
                [
                    "ORG",
                    "89.51",
                    "92.70"
                ],
                [
                    "PER",
                    "97.20",
                    "97.73"
                ]
            ]
        },
        "gold_description": "figure 4 shows the accuracy of two systems for different ner ( named entity recognition ) types . we consider four types : location ( loc ) , organization ( org ) , person ( per ) , and miscellany ( mics ) . these types are given in conll 2003 dataset , which was used as a basis for aida conll . our model is accurate for per , achieving accuracy of about 97 % , only 0 . 53 % lower than the supervised model .",
        "generated_description": "the fully - supervised model achieves the best performance for 8 of the 10 most common ner types , and the per type is the most accurate one . our model outperforms the supervised model for 7 of the 9 most common types , including per and misc ."
    },
    {
        "table_id": "190",
        "table_info": {
            "table_caption": "Table 1: Model performance (P, R and F) for development sets for all MWE and only discontinuous ones (%: proportion of discontinuous MWES)",
            "table_column_names": [
                "[EMPTY]",
                "[EMPTY]",
                "All Token-",
                "All MWE-",
                "Discontinuous",
                "Discontinuous",
                "Discontinuous",
                "Discontinuous"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "based",
                    "based",
                    "[EMPTY]",
                    "MWE-based",
                    "MWE-based",
                    "MWE-based"
                ],
                [
                    "L",
                    "model",
                    "F",
                    "F",
                    "%",
                    "P",
                    "R",
                    "F"
                ],
                [
                    "EN",
                    "baseline",
                    "41.37",
                    "35.38",
                    "32",
                    "24.44",
                    "10.48",
                    "14.67"
                ],
                [
                    "[EMPTY]",
                    "GCN-based",
                    "39.78",
                    "39.11",
                    "32",
                    "39.53",
                    "16.19",
                    "22.97"
                ],
                [
                    "[EMPTY]",
                    "Att-based",
                    "33.33",
                    "31.79",
                    "32",
                    "46.88",
                    "14.29",
                    "21.90"
                ],
                [
                    "[EMPTY]",
                    "H-combined",
                    "41.63",
                    "[BOLD] 40.76",
                    "32",
                    "63.33",
                    "18.10",
                    "[BOLD] 28.15"
                ],
                [
                    "DE",
                    "baseline",
                    "62.27",
                    "57.17",
                    "43",
                    "69.50",
                    "45.37",
                    "54.90"
                ],
                [
                    "[EMPTY]",
                    "GCN-based",
                    "65.48",
                    "[BOLD] 61.17",
                    "43",
                    "65.19",
                    "47.69",
                    "55.08"
                ],
                [
                    "[EMPTY]",
                    "Att-based",
                    "61.20",
                    "58.19",
                    "43",
                    "67.86",
                    "43.98",
                    "53.37"
                ],
                [
                    "[EMPTY]",
                    "H-combined",
                    "63.80",
                    "60.71",
                    "43",
                    "68.59",
                    "49.54",
                    "[BOLD] 57.53"
                ],
                [
                    "FR",
                    "baseline",
                    "76.62",
                    "72.16",
                    "43",
                    "75.27",
                    "52.04",
                    "61.54"
                ],
                [
                    "[EMPTY]",
                    "GCN-based",
                    "79.59",
                    "75.15",
                    "43",
                    "79.58",
                    "56.51",
                    "66.09"
                ],
                [
                    "[EMPTY]",
                    "Att-based",
                    "78.21",
                    "74.23",
                    "43",
                    "71.49",
                    "60.59",
                    "65.59"
                ],
                [
                    "[EMPTY]",
                    "H-combined",
                    "80.25",
                    "[BOLD] 76.56",
                    "43",
                    "77.94",
                    "59.11",
                    "[BOLD] 67.23"
                ],
                [
                    "FA",
                    "baseline",
                    "88.45",
                    "86.50",
                    "14",
                    "67.76",
                    "55.88",
                    "61.29"
                ],
                [
                    "[EMPTY]",
                    "GCN-based",
                    "87.78",
                    "86.42",
                    "14",
                    "78.72",
                    "54.41",
                    "64.35"
                ],
                [
                    "[EMPTY]",
                    "Att-based",
                    "87.55",
                    "84.20",
                    "14",
                    "62.32",
                    "63.24",
                    "62.77"
                ],
                [
                    "[EMPTY]",
                    "H-combined",
                    "88.76",
                    "[BOLD] 87.15",
                    "14",
                    "75.44",
                    "63.24",
                    "[BOLD] 68.80"
                ]
            ]
        },
        "gold_description": "we perform hyperparameter optimisation and make comparisons among our systems , including gcn + bi - lstm ( gcn - based ) , cnn + attention + bi - lstm ( attbased ) , and their combination using a highway layer ( h - combined ) in table 1 . systems are evaluated using two types of precision , recall and f - score measures : strict mwebased scores ( every component of an mwe should be correctly tagged to be considered as true positive ) , and token - based scores ( a partial match between a predicted and a gold mwe would be considered as true positive ) . we report results for all mwes as well as discontinuous ones specifically . gcn - based outperforms att - based and they both outperform the strong baseline in terms of mwe - based f - score in three out of four languages . combining gcn with attention using highway networks results in further improvements for en , fr and fa . the hcombined model consistently exceeds the baseline for all languages . gcn and h - combined models each show significant improvement with regard to discontinuous mwes , regardless of the proportion of such expressions . the overall results confirm our assumption that a hybrid architecture can mitigate errors of individual models and bolster their strengths .",
        "generated_description": "as can be seen from the table , the performance of the mwe - based models is significantly better than that of the non - mwe ones for all languages and all vocabularies , showing the effectiveness of the proposed models . in addition , the h - combined model achieves the best overall performance , outperforming both the baseline and the gcn - based ones in terms of f - score in all corpora . in particular , it outperforms the att - based model in the discontinuous corpora , showing that the complementation between the utterances of different languages benefits the detection of mwes ."
    },
    {
        "table_id": "191",
        "table_info": {
            "table_caption": "Table 9: MAE of L-biLSTM(2)-S and L-biLSTM(2)-S+lexfeats, for predictions on events in UDS-IH2-dev that are xcomp-governed by an infinitival-taking verb.",
            "table_column_names": [
                "Verb",
                "L-biLSTM(2)-S",
                "+lexfeats",
                "#"
            ],
            "table_content_values": [
                [
                    "decide to",
                    "3.28",
                    "2.66",
                    "2"
                ],
                [
                    "forget to",
                    "0.67",
                    "0.48",
                    "2"
                ],
                [
                    "get to",
                    "1.55",
                    "1.43",
                    "9"
                ],
                [
                    "hope to",
                    "1.35",
                    "1.23",
                    "5"
                ],
                [
                    "intend to",
                    "1.18",
                    "0.61",
                    "1"
                ],
                [
                    "promise to",
                    "0.40",
                    "0.49",
                    "1"
                ],
                [
                    "try to",
                    "1.14",
                    "1.42",
                    "12"
                ],
                [
                    "want to",
                    "1.22",
                    "1.17",
                    "24"
                ]
            ]
        },
        "gold_description": "despite the underperformance of these features overall , table 9 shows that they may still improve performance in the subset of instances where they appear .",
        "generated_description": "in uds - ih2 - dev , the model l - bilstm ( 2 ) - s has the same hyper - parameter settings as the model described in the previous section . however , it is xcomp - governed by an infinitival - taking verb , and as such is able to exploit lexical information in the target verb ’ s features . to test this , we compare the mae of the l - by - lexfeats model with that of the baseline model , given the same set of target verb features . as can be seen from the table , the benefit of using alexicalized features is evident for all events , with a mae reduction of 24 % for the want to event , from 24 % to 12 % , and from 3 % to 1 % for other events ."
    },
    {
        "table_id": "192",
        "table_info": {
            "table_caption": "Table 2: Comparing the performance of the systems on test data in terms of MWE-based F-score",
            "table_column_names": [
                "[EMPTY]",
                "All | Discontinuous EN",
                "All | Discontinuous DE",
                "All | Discontinuous FR",
                "All | Discontinuous FA"
            ],
            "table_content_values": [
                [
                    "baseline",
                    "33.01 | 16.53",
                    "54.12 | 53.94",
                    "67.66 | 58.70",
                    "[BOLD] 81.62 | 61.73"
                ],
                [
                    "GCN-based",
                    "36.27 |  [BOLD] 24.15",
                    "56.96 | 54.87",
                    "70.79 | 59.95",
                    "81.00 |  [BOLD] 62.35"
                ],
                [
                    "H-combined",
                    "[BOLD] 41.91 | 22.73",
                    "[BOLD] 59.29 |  [BOLD] 55.00",
                    "[BOLD] 70.97 |  [BOLD] 63.90",
                    "80.04 | 61.90"
                ],
                [
                    "ATILF-LLF",
                    "31.58 | 09.91",
                    "54.43 | 40.34",
                    "58.60 | 51.96",
                    "77.48 | 53.85"
                ],
                [
                    "SHOMA",
                    "26.42 | 01.90",
                    "48.71 | 40.12",
                    "62.00 | 51.43",
                    "78.35 | 56.10"
                ]
            ]
        },
        "gold_description": "we show the superior performance ( in terms of mwe - based f - score ) of our top systems on the test data compared to the baseline and stateof - the - art systems , namely , atilf - llf ( al saied et al . , 2017 ) and shoma ( taslimipoor and rohanian , 2018 ) . gcn works the best for discontinuous mwes in en and fa , while h - combined outperforms based on results for all mwes except for fa . the overall results confirm our assumption that a hybrid architecture can mitigate errors of individual models and bolster their strengths .",
        "generated_description": "in order to evaluate the performance of the systems , we compare their performance in terms of mwe - based f - score , which is the most commonly used evaluation metric in nlp research meng et al . the results show that the proposed h - combined system outperforms the baseline system by a large margin in all languages . the gcn - based and atilf - llf perform the worst among the systems . this is because that these two systems are designed for one - shot learning , which means that they do not take advantage of any training data or cross - entropy loss during training . the performance of shoma is lower than that of the other systems , which indicates that it is not suitable for multi - label mwes . the experimental results also indicate that the performance gap between the two systems is because of the mismatch between training data and test data . for the english language , the gap is much larger than that for the german , the fr and the fa languages , because the training data of these languages are more similar to the test data than that in the other two languages ."
    },
    {
        "table_id": "193",
        "table_info": {
            "table_caption": "Table 2: Results for pretraining experiments on development sets except where noted. Bold denotes best result overall. Underlining denotes an average score surpassing the Random baseline. See Section 6 for discussion of WNLI results (*).",
            "table_column_names": [
                "[BOLD] Pretr. Baselines",
                "[BOLD] Avg Baselines",
                "[BOLD] CoLA Baselines",
                "[BOLD] SST Baselines",
                "[BOLD] MRPC Baselines",
                "[BOLD] MRPC Baselines",
                "[BOLD] QQP Baselines",
                "[BOLD] QQP Baselines",
                "[BOLD] STS Baselines",
                "[BOLD] STS Baselines",
                "[BOLD] MNLI Baselines",
                "[BOLD] QNLI Baselines",
                "[BOLD] RTE Baselines",
                "[BOLD] WNLI Baselines"
            ],
            "table_content_values": [
                [
                    "[BOLD] Random",
                    "68.2",
                    "16.9",
                    "84.3",
                    "77.7/",
                    "85.6",
                    "83.0/",
                    "80.6",
                    "81.7/",
                    "82.6",
                    "73.9",
                    "[BOLD] 79.6",
                    "57.0",
                    "31.0*"
                ],
                [
                    "[BOLD] Single-Task",
                    "69.1",
                    "21.3",
                    "89.0",
                    "77.2/",
                    "84.7",
                    "84.7/",
                    "81.9",
                    "81.4/",
                    "82.2",
                    "74.8",
                    "78.8",
                    "56.0",
                    "11.3*"
                ],
                [
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks",
                    "GLUE Tasks as Pretraining Tasks"
                ],
                [
                    "[BOLD] CoLA",
                    "68.2",
                    "21.3",
                    "85.7",
                    "75.0/",
                    "83.7",
                    "85.7/",
                    "82.4",
                    "79.0/",
                    "80.3",
                    "72.7",
                    "78.4",
                    "56.3",
                    "15.5*"
                ],
                [
                    "[BOLD] SST",
                    "68.6",
                    "16.4",
                    "89.0",
                    "76.0/",
                    "84.2",
                    "84.4/",
                    "81.6",
                    "80.6/",
                    "81.4",
                    "73.9",
                    "78.5",
                    "58.8",
                    "19.7*"
                ],
                [
                    "[BOLD] MRPC",
                    "68.2",
                    "16.4",
                    "85.6",
                    "77.2/",
                    "84.7",
                    "84.4/",
                    "81.8",
                    "81.2/",
                    "82.2",
                    "73.6",
                    "79.3",
                    "56.7",
                    "22.5*"
                ],
                [
                    "[BOLD] QQP",
                    "68.0",
                    "14.7",
                    "86.1",
                    "77.2/",
                    "84.5",
                    "84.7/",
                    "81.9",
                    "81.1/",
                    "82.0",
                    "73.7",
                    "78.2",
                    "57.0",
                    "45.1*"
                ],
                [
                    "[BOLD] STS",
                    "67.7",
                    "14.1",
                    "84.6",
                    "77.9/",
                    "85.3",
                    "81.7/",
                    "79.2",
                    "81.4/",
                    "82.2",
                    "73.6",
                    "79.3",
                    "57.4",
                    "43.7*"
                ],
                [
                    "[BOLD] MNLI",
                    "69.1",
                    "16.7",
                    "88.2",
                    "78.9/",
                    "85.2",
                    "84.5/",
                    "81.5",
                    "81.8/",
                    "82.6",
                    "74.8",
                    "[BOLD] 79.6",
                    "58.8",
                    "36.6*"
                ],
                [
                    "[BOLD] QNLI",
                    "67.9",
                    "15.6",
                    "84.2",
                    "76.5/",
                    "84.2",
                    "84.3/",
                    "81.4",
                    "80.6/",
                    "81.8",
                    "73.4",
                    "78.8",
                    "58.8",
                    "[BOLD] 56.3"
                ],
                [
                    "[BOLD] RTE",
                    "68.1",
                    "18.1",
                    "83.9",
                    "77.5/",
                    "85.4",
                    "83.9/",
                    "81.2",
                    "81.2/",
                    "82.2",
                    "74.1",
                    "79.1",
                    "56.0",
                    "39.4*"
                ],
                [
                    "[BOLD] WNLI",
                    "68.0",
                    "16.3",
                    "84.3",
                    "76.5/",
                    "84.6",
                    "83.0/",
                    "80.5",
                    "81.6/",
                    "82.5",
                    "73.6",
                    "78.8",
                    "58.1",
                    "11.3*"
                ],
                [
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks",
                    "Non-GLUE Pretraining Tasks"
                ],
                [
                    "[BOLD] DisSent WT",
                    "68.6",
                    "18.3",
                    "86.6",
                    "79.9/",
                    "86.0",
                    "85.3/",
                    "82.0",
                    "79.5/",
                    "80.5",
                    "73.4",
                    "79.1",
                    "56.7",
                    "42.3*"
                ],
                [
                    "[BOLD] LM WT",
                    "70.1",
                    "30.8",
                    "85.7",
                    "76.2/",
                    "84.2",
                    "86.2/",
                    "82.9",
                    "79.2/",
                    "80.2",
                    "74.0",
                    "79.4",
                    "60.3",
                    "25.4*"
                ],
                [
                    "[BOLD] LM BWB",
                    "[BOLD] 70.4",
                    "30.7",
                    "86.8",
                    "79.9/",
                    "86.2",
                    "[BOLD] 86.3/",
                    "[BOLD] 83.2",
                    "80.7/",
                    "81.4",
                    "74.2",
                    "79.0",
                    "57.4",
                    "47.9*"
                ],
                [
                    "[BOLD] MT En-De",
                    "68.1",
                    "16.7",
                    "85.4",
                    "77.9/",
                    "84.9",
                    "83.8/",
                    "80.5",
                    "82.4/",
                    "82.9",
                    "73.5",
                    "[BOLD] 79.6",
                    "55.6",
                    "22.5*"
                ],
                [
                    "[BOLD] MT En-Ru",
                    "68.4",
                    "16.8",
                    "85.1",
                    "79.4/",
                    "86.2",
                    "84.1/",
                    "81.2",
                    "82.7/",
                    "83.2",
                    "74.1",
                    "79.1",
                    "56.0",
                    "26.8*"
                ],
                [
                    "[BOLD] Reddit",
                    "66.9",
                    "15.3",
                    "82.3",
                    "76.5/",
                    "84.6",
                    "81.9/",
                    "79.2",
                    "81.5/",
                    "81.9",
                    "72.7",
                    "76.8",
                    "55.6",
                    "53.5*"
                ],
                [
                    "[BOLD] SkipThought",
                    "68.7",
                    "16.0",
                    "84.9",
                    "77.5/",
                    "85.0",
                    "83.5/",
                    "80.7",
                    "81.1/",
                    "81.5",
                    "73.3",
                    "79.1",
                    "[BOLD] 63.9",
                    "49.3*"
                ],
                [
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining",
                    "Multitask Pretraining"
                ],
                [
                    "[BOLD] MTL GLUE",
                    "68.9",
                    "15.4",
                    "[BOLD] 89.9",
                    "78.9/",
                    "86.3",
                    "82.6/",
                    "79.9",
                    "[BOLD] 82.9/",
                    "[BOLD] 83.5",
                    "[BOLD] 74.9",
                    "78.9",
                    "57.8",
                    "38.0*"
                ],
                [
                    "[BOLD] MTL Non-GLUE",
                    "69.9",
                    "30.6",
                    "87.0",
                    "[BOLD] 81.1/",
                    "[BOLD] 87.6",
                    "86.0/",
                    "82.2",
                    "79.9/",
                    "80.6",
                    "72.8",
                    "78.9",
                    "54.9",
                    "22.5*"
                ],
                [
                    "[BOLD] MTL All",
                    "[BOLD] 70.4",
                    "[BOLD] 33.2",
                    "88.2",
                    "78.9/",
                    "85.9",
                    "85.5/",
                    "81.8",
                    "79.7/",
                    "80.0",
                    "73.9",
                    "78.7",
                    "57.4",
                    "33.8*"
                ],
                [
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results"
                ],
                [
                    "[BOLD] LM BWB",
                    "66.5",
                    "29.1",
                    "86.9",
                    "75.0/",
                    "82.1",
                    "82.7/",
                    "63.3",
                    "74.0/",
                    "73.1",
                    "73.4",
                    "68.0",
                    "51.3",
                    "65.1"
                ],
                [
                    "[BOLD] MTL All",
                    "68.5",
                    "36.3",
                    "88.9",
                    "77.7/",
                    "84.8",
                    "82.7/",
                    "63.6",
                    "77.8/",
                    "76.7",
                    "75.3",
                    "66.2",
                    "53.2",
                    "65.1"
                ]
            ]
        },
        "gold_description": "tables 2 and 3 respectively show results for our pretraining and intermediate training experiments . from table 2 , among target tasks , we find the grammar - related cola task benefits dramatically from lm pretraining : the results achieved with lm pretraining are significantly better than the results achieved without . in contrast , the meaningoriented sts sees good results with several kinds of pretraining , but does not benefit substantially from lm pretraining . among pretraining tasks , language modeling performs best , followed by mnli . the remaining pretraining tasks yield performance near that of the random baseline . even our single - task baseline gets less than a one point gain over this simple baseline . the multitask models are tied or outperformed by models trained on one of their constituent tasks , suggesting that our approach to multitask learning does not reliably produce models that productively combine the knowledge taught by each task . however , of the two models that perform best on the development data , the multitask model generalizes better than the single - task model on test data for tasks like sts and mnli where the test set contains out - of - domain data .",
        "generated_description": "for sts , mnli , rte , and wnli , the model pretrained on glue achieves the best performance . for cola and mrpc , pretraining on mnli outperforms the random baseline by a large margin ( 6 . 7 % and 2 . 8 % absolute ) . for the other tasks , the performance of pretraining using glue tasks as pretraining tasks is comparable to that of random pretraining ( 1 . 2 % and 1 . 5 % absolute ) or even slightly better ( 0 . 4 % and 0 . 7 % ) ."
    },
    {
        "table_id": "194",
        "table_info": {
            "table_caption": "Table 3: Results for intermediate training experiments on development sets except where noted. E and B respectively denote ELMo and BERT experiments. Bold denotes best scores by section. Underlining denotes average scores better than the single-task baseline. See Section 6 for discussion of WNLI results (*). BERT Base numbers are from Devlin et al. (2019).",
            "table_column_names": [
                "[BOLD] Intermediate Task ELMo with Intermediate Task Training",
                "[BOLD] Avg ELMo with Intermediate Task Training",
                "[BOLD] CoLA ELMo with Intermediate Task Training",
                "[BOLD] SST ELMo with Intermediate Task Training",
                "[BOLD] MRPC ELMo with Intermediate Task Training",
                "[BOLD] MRPC ELMo with Intermediate Task Training",
                "[BOLD] QQP ELMo with Intermediate Task Training",
                "[BOLD] QQP ELMo with Intermediate Task Training",
                "[BOLD] STS ELMo with Intermediate Task Training",
                "[BOLD] STS ELMo with Intermediate Task Training",
                "[BOLD] MNLI ELMo with Intermediate Task Training",
                "[BOLD] QNLI ELMo with Intermediate Task Training",
                "[BOLD] RTE ELMo with Intermediate Task Training",
                "[BOLD] WNLI ELMo with Intermediate Task Training"
            ],
            "table_content_values": [
                [
                    "[BOLD] Random [ITALIC] E",
                    "70.5",
                    "38.5",
                    "87.7",
                    "79.9/",
                    "86.5",
                    "86.7/",
                    "83.4",
                    "80.8/",
                    "82.1",
                    "75.6",
                    "79.6",
                    "[BOLD] 61.7",
                    "33.8*"
                ],
                [
                    "[BOLD] Single-Task [ITALIC] E",
                    "71.2",
                    "39.4",
                    "[BOLD] 90.6",
                    "77.5/",
                    "84.4",
                    "86.4/",
                    "82.4",
                    "79.9/",
                    "80.6",
                    "75.6",
                    "78.0",
                    "55.6",
                    "11.3*"
                ],
                [
                    "[BOLD] CoLA [ITALIC] E",
                    "71.1",
                    "39.4",
                    "87.3",
                    "77.5/",
                    "85.2",
                    "86.5/",
                    "83.0",
                    "78.8/",
                    "80.2",
                    "74.2",
                    "78.2",
                    "59.2",
                    "33.8*"
                ],
                [
                    "[BOLD] SST [ITALIC] E",
                    "71.2",
                    "38.8",
                    "[BOLD] 90.6",
                    "80.4/",
                    "86.8",
                    "87.0/",
                    "83.5",
                    "79.4/",
                    "81.0",
                    "74.3",
                    "77.8",
                    "53.8",
                    "43.7*"
                ],
                [
                    "[BOLD] MRPC [ITALIC] E",
                    "71.3",
                    "40.0",
                    "88.4",
                    "77.5/",
                    "84.4",
                    "86.4/",
                    "82.7",
                    "79.5/",
                    "80.6",
                    "74.9",
                    "78.4",
                    "58.1",
                    "[BOLD] 54.9*"
                ],
                [
                    "[BOLD] QQP [ITALIC] E",
                    "70.8",
                    "34.3",
                    "88.6",
                    "79.4/",
                    "85.7",
                    "86.4/",
                    "82.4",
                    "81.1/",
                    "82.1",
                    "74.3",
                    "78.1",
                    "56.7",
                    "38.0*"
                ],
                [
                    "[BOLD] STS [ITALIC] E",
                    "71.6",
                    "39.9",
                    "88.4",
                    "79.9/",
                    "86.4",
                    "86.7/",
                    "83.3",
                    "79.9/",
                    "80.6",
                    "74.3",
                    "78.6",
                    "58.5",
                    "26.8*"
                ],
                [
                    "[BOLD] MNLI [ITALIC] E",
                    "72.1",
                    "38.9",
                    "89.0",
                    "80.9/",
                    "86.9",
                    "86.1/",
                    "82.7",
                    "81.3/",
                    "82.5",
                    "75.6",
                    "79.7",
                    "58.8",
                    "16.9*"
                ],
                [
                    "[BOLD] QNLI [ITALIC] E",
                    "71.2",
                    "37.2",
                    "88.3",
                    "81.1/",
                    "86.9",
                    "85.5/",
                    "81.7",
                    "78.9/",
                    "80.1",
                    "74.7",
                    "78.0",
                    "58.8",
                    "22.5*"
                ],
                [
                    "[BOLD] RTE [ITALIC] E",
                    "71.2",
                    "38.5",
                    "87.7",
                    "81.1/",
                    "87.3",
                    "86.6/",
                    "83.2",
                    "80.1/",
                    "81.1",
                    "74.6",
                    "78.0",
                    "55.6",
                    "32.4*"
                ],
                [
                    "[BOLD] WNLI [ITALIC] E",
                    "70.9",
                    "38.4",
                    "88.6",
                    "78.4/",
                    "85.9",
                    "86.3/",
                    "82.8",
                    "79.1/",
                    "80.0",
                    "73.9",
                    "77.9",
                    "57.0",
                    "11.3*"
                ],
                [
                    "[BOLD] DisSent WT [ITALIC] E",
                    "71.9",
                    "39.9",
                    "87.6",
                    "[BOLD] 81.9/",
                    "[BOLD] 87.2",
                    "85.8/",
                    "82.3",
                    "79.0/",
                    "80.7",
                    "74.6",
                    "79.1",
                    "61.4",
                    "23.9*"
                ],
                [
                    "[BOLD] MT En-De [ITALIC] E",
                    "72.1",
                    "40.1",
                    "87.8",
                    "79.9/",
                    "86.6",
                    "86.4/",
                    "83.2",
                    "81.8/",
                    "82.4",
                    "75.9",
                    "79.4",
                    "58.8",
                    "31.0*"
                ],
                [
                    "[BOLD] MT En-Ru [ITALIC] E",
                    "70.4",
                    "[BOLD] 41.0",
                    "86.8",
                    "76.5/",
                    "85.0",
                    "82.5/",
                    "76.3",
                    "81.4/",
                    "81.5",
                    "70.1",
                    "77.3",
                    "60.3",
                    "45.1*"
                ],
                [
                    "[BOLD] Reddit [ITALIC] E",
                    "71.0",
                    "38.5",
                    "87.7",
                    "77.2/",
                    "85.0",
                    "85.4/",
                    "82.1",
                    "80.9/",
                    "81.7",
                    "74.2",
                    "79.3",
                    "56.7",
                    "21.1*"
                ],
                [
                    "[BOLD] SkipThought [ITALIC] E",
                    "71.7",
                    "40.6",
                    "87.7",
                    "79.7/",
                    "86.5",
                    "85.2/",
                    "82.1",
                    "81.0/",
                    "81.7",
                    "75.0",
                    "79.1",
                    "58.1",
                    "52.1*"
                ],
                [
                    "[BOLD] MTL GLUE [ITALIC] E",
                    "72.1",
                    "33.8",
                    "90.5",
                    "81.1/",
                    "87.4",
                    "86.6/",
                    "83.0",
                    "82.1/",
                    "83.3",
                    "[BOLD] 76.2",
                    "79.2",
                    "61.4",
                    "42.3*"
                ],
                [
                    "[BOLD] MTL Non-GLUE [ITALIC] E",
                    "[BOLD] 72.4",
                    "39.4",
                    "88.8",
                    "80.6/",
                    "86.8",
                    "[BOLD] 87.1/",
                    "[BOLD] 84.1",
                    "[BOLD] 83.2/",
                    "[BOLD] 83.9",
                    "75.9",
                    "[BOLD] 80.9",
                    "57.8",
                    "22.5*"
                ],
                [
                    "[BOLD] MTL All [ITALIC] E",
                    "72.2",
                    "37.9",
                    "89.6",
                    "79.2/",
                    "86.4",
                    "86.0/",
                    "82.8",
                    "81.6/",
                    "82.5",
                    "76.1",
                    "80.2",
                    "60.3",
                    "31.0*"
                ],
                [
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training",
                    "BERT with Intermediate Task Training"
                ],
                [
                    "[BOLD] Single-Task [ITALIC] B",
                    "78.8",
                    "56.6",
                    "90.9",
                    "88.5/",
                    "91.8",
                    "89.9/",
                    "86.4",
                    "86.1/",
                    "86.0",
                    "83.5",
                    "[BOLD] 87.9",
                    "69.7",
                    "[BOLD] 56.3"
                ],
                [
                    "[BOLD] CoLA [ITALIC] B",
                    "78.3",
                    "[BOLD] 61.3",
                    "91.1",
                    "87.7/",
                    "91.4",
                    "89.7/",
                    "86.3",
                    "85.0/",
                    "85.0",
                    "83.3",
                    "85.9",
                    "64.3",
                    "43.7*"
                ],
                [
                    "[BOLD] SST [ITALIC] B",
                    "78.4",
                    "57.4",
                    "[BOLD] 92.2",
                    "86.3/",
                    "90.0",
                    "89.6/",
                    "86.1",
                    "85.3/",
                    "85.1",
                    "83.2",
                    "87.4",
                    "67.5",
                    "43.7*"
                ],
                [
                    "[BOLD] MRPC [ITALIC] B",
                    "78.3",
                    "60.3",
                    "90.8",
                    "87.0/",
                    "91.1",
                    "89.7/",
                    "86.3",
                    "86.6/",
                    "86.4",
                    "[BOLD] 83.8",
                    "83.9",
                    "66.4",
                    "[BOLD] 56.3"
                ],
                [
                    "[BOLD] QQP [ITALIC] B",
                    "79.1",
                    "56.8",
                    "91.3",
                    "88.5/",
                    "91.7",
                    "[BOLD] 90.5/",
                    "[BOLD] 87.3",
                    "88.1/",
                    "87.8",
                    "83.4",
                    "87.2",
                    "69.7",
                    "[BOLD] 56.3"
                ],
                [
                    "[BOLD] STS [ITALIC] B",
                    "79.4",
                    "61.1",
                    "92.3",
                    "88.0/",
                    "91.5",
                    "89.3/",
                    "85.5",
                    "86.2/",
                    "86.0",
                    "82.9",
                    "87.0",
                    "71.5",
                    "50.7*"
                ],
                [
                    "[BOLD] MNLI [ITALIC] B",
                    "[BOLD] 79.6",
                    "56.0",
                    "91.3",
                    "88.0/",
                    "91.3",
                    "90.0/",
                    "86.7",
                    "87.8/",
                    "87.7",
                    "82.9",
                    "87.0",
                    "[BOLD] 76.9",
                    "[BOLD] 56.3"
                ],
                [
                    "[BOLD] QNLI [ITALIC] B",
                    "78.4",
                    "55.4",
                    "91.2",
                    "[BOLD] 88.7/",
                    "[BOLD] 92.1",
                    "89.9/",
                    "86.4",
                    "86.5/",
                    "86.3",
                    "82.9",
                    "86.8",
                    "68.2",
                    "[BOLD] 56.3"
                ],
                [
                    "[BOLD] RTE [ITALIC] B",
                    "77.7",
                    "59.3",
                    "91.2",
                    "86.0/",
                    "90.4",
                    "89.2/",
                    "85.9",
                    "85.9/",
                    "85.7",
                    "82.0",
                    "83.3",
                    "65.3",
                    "[BOLD] 56.3"
                ],
                [
                    "[BOLD] WNLI [ITALIC] B",
                    "76.2",
                    "53.2",
                    "92.1",
                    "85.5/",
                    "90.0",
                    "89.1/",
                    "85.5",
                    "85.6/",
                    "85.4",
                    "82.4",
                    "82.5",
                    "58.5",
                    "[BOLD] 56.3"
                ],
                [
                    "[BOLD] DisSent WT [ITALIC] B",
                    "78.1",
                    "58.1",
                    "91.9",
                    "87.7/",
                    "91.2",
                    "89.2/",
                    "85.9",
                    "84.2/",
                    "84.1",
                    "82.5",
                    "85.5",
                    "67.5",
                    "43.7*"
                ],
                [
                    "[BOLD] MT En-De [ITALIC] B",
                    "73.9",
                    "47.0",
                    "90.5",
                    "75.0/",
                    "83.4",
                    "89.6/",
                    "86.1",
                    "84.1/",
                    "83.9",
                    "81.8",
                    "83.8",
                    "54.9",
                    "[BOLD] 56.3"
                ],
                [
                    "[BOLD] MT En-Ru [ITALIC] B",
                    "74.3",
                    "52.4",
                    "89.9",
                    "71.8/",
                    "81.3",
                    "89.4/",
                    "85.6",
                    "82.8/",
                    "82.8",
                    "81.5",
                    "83.1",
                    "58.5",
                    "43.7*"
                ],
                [
                    "[BOLD] Reddit [ITALIC] B",
                    "75.6",
                    "49.5",
                    "91.7",
                    "84.6/",
                    "89.2",
                    "89.4/",
                    "85.8",
                    "83.8/",
                    "83.6",
                    "81.8",
                    "84.4",
                    "58.1",
                    "[BOLD] 56.3"
                ],
                [
                    "[BOLD] SkipThought [ITALIC] B",
                    "75.2",
                    "53.9",
                    "90.8",
                    "78.7/",
                    "85.2",
                    "89.7/",
                    "86.3",
                    "81.2/",
                    "81.5",
                    "82.2",
                    "84.6",
                    "57.4",
                    "43.7*"
                ],
                [
                    "[BOLD] MTL GLUE [ITALIC] B",
                    "[BOLD] 79.6",
                    "56.8",
                    "91.3",
                    "88.0/",
                    "91.4",
                    "90.3/",
                    "86.9",
                    "[BOLD] 89.2/",
                    "[BOLD] 89.0",
                    "83.0",
                    "86.8",
                    "74.7",
                    "43.7*"
                ],
                [
                    "[BOLD] MTL Non-GLUE [ITALIC] B",
                    "76.7",
                    "54.8",
                    "91.1",
                    "83.6/",
                    "88.7",
                    "89.2/",
                    "85.6",
                    "83.2/",
                    "83.2",
                    "82.4",
                    "84.4",
                    "64.3",
                    "43.7*"
                ],
                [
                    "[BOLD] MTL All [ITALIC] B",
                    "79.3",
                    "53.1",
                    "91.7",
                    "88.0/",
                    "91.3",
                    "90.4/",
                    "87.0",
                    "88.1/",
                    "87.9",
                    "83.5",
                    "87.6",
                    "75.1",
                    "45.1*"
                ],
                [
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results",
                    "[ITALIC] Test Set Results"
                ],
                [
                    "[BOLD] Non-GLUE [ITALIC] E",
                    "69.7",
                    "34.5",
                    "89.5",
                    "78.2/",
                    "84.8",
                    "83.6/",
                    "64.3",
                    "77.5/",
                    "76.0",
                    "75.4",
                    "74.8",
                    "55.6",
                    "65.1"
                ],
                [
                    "[BOLD] MNLI [ITALIC] B",
                    "77.1",
                    "49.6",
                    "93.2",
                    "88.5/",
                    "84.7",
                    "70.6/",
                    "88.3",
                    "86.0/",
                    "85.5",
                    "82.7",
                    "78.7",
                    "72.6",
                    "65.1"
                ],
                [
                    "[BOLD] GLUE [ITALIC] B",
                    "77.3",
                    "49.0",
                    "93.5",
                    "89.0/",
                    "85.3",
                    "70.6/",
                    "88.6",
                    "85.8/",
                    "84.9",
                    "82.9",
                    "81.0",
                    "71.7",
                    "34.9"
                ],
                [
                    "[BOLD] BERT Base",
                    "78.4",
                    "52.1",
                    "93.5",
                    "88.9/",
                    "84.8",
                    "71.2/",
                    "89.2",
                    "87.1/",
                    "85.8",
                    "84.0",
                    "91.1",
                    "66.4",
                    "65.1"
                ]
            ]
        },
        "gold_description": "tables 2 and 3 respectively show results for our pretraining and intermediate training experiments . looking to table 3 , using elmo uniformly improves over training the encoder from scratch . the elmo - augmented random baseline is strong , lagging behind the single - task baseline by less than a point . most intermediate tasks beat the random baseline , but several fail to significantly outperform the single - task baseline . mnli and english – german translation perform best with elmo , with skipthought and dissent also beating the single - task baseline . intermediate multitask training on all the non - glue tasks produces our best - performing elmo model . using bert consistently outperforms elmo and pretraining from scratch . we find that intermediate training on each of mnli , qqp , and sts leads to improvements over no intermediate training , while intermediate training on the other tasks harms transfer performance . the improve ments gained via sts , a small - data task , versus the negative impact of fairly large - data tasks ( e . g . qnli ) , suggests that the benefit of intermediate training is not solely due to additional training , but that the signal provided by the intermediate task complements the original language modeling objective . intermediate training on generation tasks such as mt and skipthought significantly impairs bert ' s transfer ability . we speculate that this degradation may be due to catastrophic forgetting in fine - tuning for a task substantially different from the tasks bert was originally trained on . this phenomenon might be mitigated in our elmo models via the frozen encoder and skip connection . on the test set , we lag slightly behind the bert base results from devlin et al . ( 2019 ) , likely due in part to our limited hyperparameter tuning .",
        "generated_description": "intermediate task performance . we first discuss the performance of elmo after fine - tuning on the respective training sets for each of the eight tasks . we observe that performance on the majority of the tasks improves as the training sets are more similar to those seen during training ( e . g . , single - task accuracy for sts and mnli improves from 77 . 5 % to 84 . 4 % and from 80 . 8 % to 90 . 6 % for qnli ) . performance on cola and mrpc is particularly poor ( 61 . 7 % and 61 . 7 % , respectively ) . we hypothesize that this is due to the fact that these tasks usually contain a large number of samples which makes the model less confident towards the majority class label . on the other hand , performance on wnli and rte improves slightly ( 1 . 2 % and 1 . 1 % relative ) when moving from random e to single - task e , suggesting that the model is more confident towards majority class labels . performance on the sts , mnli , rte , and qqp tasks also improves ( 0 . 6 % , 0 . 8 % , 1 . 5 % , 2 . 3 % relative ) , however , the gains are not as large as for the other four tasks ."
    },
    {
        "table_id": "195",
        "table_info": {
            "table_caption": "Table 4: Pearson correlations between performances on a subset of all pairs of target tasks, measured over all runs reported in Table 2. The Avg column shows the correlation between performance on a target task and the overall GLUE score. For QQP and STS, the correlations are computed respectively using F1 and Pearson correlation. Negative correlations are underlined.",
            "table_column_names": [
                "[BOLD] Task",
                "[BOLD] Avg",
                "[BOLD] CoLA",
                "[BOLD] SST",
                "[BOLD] STS",
                "[BOLD] QQP",
                "[BOLD] MNLI",
                "[BOLD] QNLI"
            ],
            "table_content_values": [
                [
                    "[BOLD] CoLA",
                    "0.86",
                    "1.00",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[BOLD] SST",
                    "0.60",
                    "0.25",
                    "1.00",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[BOLD] MRPC",
                    "0.39",
                    "0.21",
                    "0.34",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[BOLD] STS",
                    "-0.36",
                    "-0.60",
                    "0.01",
                    "1.00",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[BOLD] QQP",
                    "0.61",
                    "0.61",
                    "0.27",
                    "-0.58",
                    "1.00",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[BOLD] MNLI",
                    "0.54",
                    "0.16",
                    "0.66",
                    "0.40",
                    "0.08",
                    "1.00",
                    "[EMPTY]"
                ],
                [
                    "[BOLD] QNLI",
                    "0.43",
                    "0.13",
                    "0.26",
                    "0.04",
                    "0.27",
                    "0.56",
                    "1.00"
                ],
                [
                    "[BOLD] RTE",
                    "0.34",
                    "0.08",
                    "0.16",
                    "-0.10",
                    "0.04",
                    "0.14",
                    "0.32"
                ],
                [
                    "[BOLD] WNLI",
                    "-0.21",
                    "-0.21",
                    "-0.37",
                    "0.31",
                    "-0.37",
                    "-0.07",
                    "-0.26"
                ]
            ]
        },
        "gold_description": "table 4 presents an alternative view of the results of the pretraining experiment ( table 2 ) : the table shows correlations between pairs of target tasks over the space of pretrained encoders . the correlations reflect the degree to which the performance on one target task with some encoder predicts performance on another target task with the same encoder . see appendix d for the full table and similar tables for intermediate elmo and bert experiments . many correlations are low , suggesting that different tasks benefit from different forms of pretraining to a substantial degree , and bolstering the observation that no single pretraining task yields good performance on all target tasks . for reasons noted earlier , the models that tended to perform best overall also tended to overfit the wnli training set most , leading to a negative correlation between wnli and overall glue score . sts also shows a negative correlation , likely due to the observation that it does not benefit from lm pretraining . in contrast , cola shows a strong correlation with the overall glue scores , but has weak or negative correlations with many tasks : the use of lm pretraining dramatically improves cola performance , but most other forms of pretraining have little effect .",
        "generated_description": "for each task , we report the mean and standard deviation of the micro f1 score across 10 runs . we also report the average of the f1 scores across all 10 runs , as well as the standard deviation for each individual run . we can see that the average performance of the individual models is very close to zero on cola and sst , and close to one on mnli and qnli . on the other hand , the performance on sts and mnli is very different from that of the other tasks . the reason is that the sts task is much more difficult than the mnli task . the mnli dataset is much simpler than the sst and cola datasets , and thus the generalization ability of the models is better ."
    },
    {
        "table_id": "196",
        "table_info": {
            "table_caption": "Table 1: NE-tags of numbers in Wikipedia.",
            "table_column_names": [
                "NE Tag",
                "Frequency"
            ],
            "table_content_values": [
                [
                    "date, time, duration, set (temporal)",
                    "54.28%"
                ],
                [
                    "number",
                    "40.13%"
                ],
                [
                    "[ITALIC]  Relation cardinality",
                    "[ITALIC] 18.86%"
                ],
                [
                    "percent",
                    "2.92%"
                ],
                [
                    "money",
                    "2.25%"
                ],
                [
                    "person, location, organization",
                    "0.26%"
                ],
                [
                    "ordinal",
                    "0.16%"
                ]
            ]
        },
        "gold_description": "the distribution of their named - entity ( ne ) tags , according to stanford ne - tagger , is shown in table 1 . while temporal - related numbers are the most frequent , around 40 % are classified only as unspecific number . by manually checking 100 random numbers , we observed that 47 are relation cardinalities , 1 i . e . , approximately 18 . 86 % of all numbers in wikipedia are relation cardinalities .",
        "generated_description": "ne tags are defined as follows : date , time , duration , set ( temporal ) , number , cardinality , percent , money , ordinal are defined in wikipedia . ne - tags are defined by assigning a special tag to each of these entities . for example , “ number ” and “ percent ” are the two most common ne tags in wikipedia with frequency around 40 % and 2 . 92 % , respectively . the most frequent ne tag in wikipedia is “ money ” with frequency 2 . 25 % . other commonly used ne tags include “ person , location , organization ” ( 0 . 26 % ) , “ organization ” , etc ."
    },
    {
        "table_id": "197",
        "table_info": {
            "table_caption": "Table 2: Number of Wikidata entities as subjects (#s) of each predicate (p), and evaluation results on manually annotated randomly selected subjects that have at least an object.",
            "table_column_names": [
                "[ITALIC] p",
                "# [ITALIC] s",
                "[ITALIC] baseline P",
                "[ITALIC] vanilla P",
                "[ITALIC] vanilla R",
                "[ITALIC] vanilla F1",
                "[ITALIC] only-nummod P",
                "[ITALIC] only-nummod R",
                "[ITALIC] only-nummod F1"
            ],
            "table_content_values": [
                [
                    "has part (creative work series)",
                    "261",
                    ".050",
                    ".333",
                    ".316",
                    ".324",
                    ".353",
                    ".316",
                    ".333"
                ],
                [
                    "contains admin. terr. entity",
                    "18,000",
                    ".034",
                    ".390",
                    ".188",
                    ".254",
                    ".548",
                    ".200",
                    ".293"
                ],
                [
                    "spouse",
                    "45,917",
                    "0",
                    ".014",
                    ".011",
                    ".013",
                    ".028",
                    ".017",
                    ".021"
                ],
                [
                    "child",
                    "35,057",
                    ".112",
                    ".151",
                    ".129",
                    ".139",
                    ".320",
                    ".219",
                    ".260"
                ],
                [
                    "child (manual ground truth)",
                    "6,408",
                    "[EMPTY]",
                    "0.374",
                    "0.309",
                    "0.338",
                    "0.452",
                    "0.315",
                    "0.371"
                ]
            ]
        },
        "gold_description": "table 2 shows the performance of our crf - based method in finding the correct relation cardinality , evaluated on manually annotated 20 ( has part ) , 100 ( admin . terr . entity ) and 200 ( child and spouse ) randomly selected subjects that have at least one object . the random - number baseline achieves a precision of 5 % ( has part ) , 3 . 5 % ( admin . territ . entity ) , 0 % ( spouse ) and 11 . 2 % ( child ) . compared to that , especially using only - nummod , our method gives encouraging results for has part , admin . territ . entity and child , with 30 - 50 % precision and around 30 % f1 - score . as shown by the last row of table 2 , higher quality of training data can considerably boost the performance of cardinality extraction .",
        "generated_description": "the only - nummod model outperforms the baseline in terms of both precision and recall for has part ( creative work series ) and contains admin . entity , but underperforms on has part for the other entities . for example , the f1 - score of has part drops from . 333 to . 333 when using the vanilla model , and from . 353 to . 249 when using only the nummod model . the results suggest that the only modality accounting for a large portion of the over - representation of entities in the entity set is the entity modality that has part ."
    },
    {
        "table_id": "198",
        "table_info": {
            "table_caption": "Table 1: LAS on the test sets, the best LAS in each group is marked in bold face.",
            "table_column_names": [
                "Model",
                "Model",
                "Ara",
                "Baq",
                "Fre",
                "Ger",
                "Heb",
                "Hun",
                "Kor",
                "Pol",
                "Swe",
                "Avg"
            ],
            "table_content_values": [
                [
                    "Int",
                    "WORD",
                    "84.50",
                    "77.87",
                    "82.20",
                    "85.35",
                    "[BOLD] 74.68",
                    "76.17",
                    "84.62",
                    "80.71",
                    "79.14",
                    "80.58"
                ],
                [
                    "Int",
                    "W2V",
                    "[BOLD] 85.11",
                    "79.07",
                    "[BOLD] 82.73",
                    "[BOLD] 86.60",
                    "74.55",
                    "78.21",
                    "85.30",
                    "82.37",
                    "79.67",
                    "81.51"
                ],
                [
                    "Int",
                    "LSTM",
                    "83.42",
                    "82.97",
                    "81.35",
                    "85.34",
                    "74.03",
                    "83.06",
                    "86.56",
                    "80.13",
                    "77.44",
                    "81.48"
                ],
                [
                    "Int",
                    "CNN",
                    "84.65",
                    "[BOLD] 83.91",
                    "82.41",
                    "85.61",
                    "74.23",
                    "[BOLD] 83.68",
                    "[BOLD] 86.99",
                    "[BOLD] 83.28",
                    "[BOLD] 80.00",
                    "[BOLD] 82.75"
                ],
                [
                    "Int",
                    "LSTM+WORD",
                    "[BOLD] 84.75",
                    "83.43",
                    "[BOLD] 82.25",
                    "85.56",
                    "74.62",
                    "83.43",
                    "86.85",
                    "82.30",
                    "79.85",
                    "82.56"
                ],
                [
                    "Int",
                    "CNN+WORD",
                    "84.58",
                    "[BOLD] 84.22",
                    "81.79",
                    "[BOLD] 85.85",
                    "[BOLD] 74.79",
                    "[BOLD] 83.51",
                    "[BOLD] 87.21",
                    "[BOLD] 83.66",
                    "[BOLD] 80.52",
                    "[BOLD] 82.90"
                ],
                [
                    "Int",
                    "LSTM+W2V",
                    "85.35",
                    "83.94",
                    "83.04",
                    "86.38",
                    "[BOLD] 75.15",
                    "83.30",
                    "87.35",
                    "83.00",
                    "79.38",
                    "82.99"
                ],
                [
                    "Int",
                    "CNN+W2V",
                    "[BOLD] 85.67",
                    "[BOLD] 84.37",
                    "[BOLD] 83.09",
                    "[BOLD] 86.81",
                    "74.95",
                    "[BOLD] 84.08",
                    "[BOLD] 87.72",
                    "[BOLD] 84.44",
                    "[BOLD] 80.35",
                    "[BOLD] 83.50"
                ],
                [
                    "Ext",
                    "B15-WORD",
                    "[BOLD] 83.46",
                    "73.56",
                    "[BOLD] 82.03",
                    "[BOLD] 84.62",
                    "[BOLD] 72.70",
                    "69.31",
                    "83.37",
                    "[BOLD] 79.83",
                    "[BOLD] 76.40",
                    "78.36"
                ],
                [
                    "Ext",
                    "B15-LSTM",
                    "83.40",
                    "[BOLD] 78.61",
                    "81.08",
                    "84.49",
                    "72.26",
                    "[BOLD] 76.34",
                    "[BOLD] 86.21",
                    "78.24",
                    "74.47",
                    "[BOLD] 79.46"
                ],
                [
                    "Ext",
                    "BestPub",
                    "86.21",
                    "85.70",
                    "85.66",
                    "89.65",
                    "81.65",
                    "86.13",
                    "87.27",
                    "87.07",
                    "82.75",
                    "85.79"
                ]
            ]
        },
        "gold_description": "the experimental results are shown in table 1 , with int denoting internal comparisons ( with three groups ) and ext denoting external comparisons , the highest las in each group is marked in bold face . in the first group , we compare the las of the four single models word , w2v , lstm , and cnn . in macro average of all languages , the cnn model performs 2 . 17 % higher than the word model , and 1 . 24 % higher than the w2v model . the lstm model , however , performs only 0 . 9 % higher than the word model and 1 . 27 % lower than the cnn model . in the second group , we observe that the additional word - lookup model does not significantly improve the cnn moodel ( from 82 . 75 % in cnn to 82 . 90 % in cnn + word on average ) while the lstm model is improved by a much larger margin ( from 81 . 48 % in lstm to 82 . 56 % in lstm + word on average ) . this suggests that the cnn model has already learned the most important information from the the word forms , while the lstm model has not . also , the combined cnn + word model is still better than the lstm + word model , despite the large improvement in the latter . while comparing to the best published results ( björkelund et al . , 2013 , 2014 ) , we have to note that their approach uses explicit morphological features , ensemble , ranking , etc . , which all can boost parsing performance . we only use a greedy parser with much fewer features , but bridge the 6 points gap between the previous best greedy parser and the best published result by more than one half . on average , the b15 - lstm model improves their own baseline by 1 . 1 % , similar to the",
        "generated_description": "we can see that the word2vec and word - embedding based models outperform the lstm and cnn baselines by a large margin in all the languages . the w2v model outperforms the word embedding based model in 5 out of 7 languages and the cnn model performs the best in 5 languages ."
    },
    {
        "table_id": "199",
        "table_info": {
            "table_caption": "Table 2: LAS improvements by CNN and LSTM in the IV and OOV cases on the development sets.",
            "table_column_names": [
                "Model",
                "Case",
                "Ara",
                "Baq",
                "Fre",
                "Ger",
                "Heb",
                "Hun",
                "Kor",
                "Pol",
                "Swe",
                "Avg"
            ],
            "table_content_values": [
                [
                    "CNN",
                    "ΔIV",
                    "0.12",
                    "2.72",
                    "-0.44",
                    "0.13",
                    "-0.35",
                    "1.48",
                    "1.30",
                    "0.98",
                    "1.39",
                    "0.81"
                ],
                [
                    "CNN",
                    "ΔOOV",
                    "0.03",
                    "5.78",
                    "0.33",
                    "0.10",
                    "-1.04",
                    "5.04",
                    "2.17",
                    "2.34",
                    "0.95",
                    "1.74"
                ],
                [
                    "LSTM",
                    "ΔIV",
                    "-0.58",
                    "1.98",
                    "-0.55",
                    "-0.08",
                    "-1.23",
                    "1.62",
                    "1.12",
                    "-0.49",
                    "0.21",
                    "0.22"
                ],
                [
                    "LSTM",
                    "ΔOOV",
                    "-0.32",
                    "5.09",
                    "0.12",
                    "-0.21",
                    "-1.99",
                    "4.74",
                    "1.51",
                    "0.10",
                    "0.38",
                    "1.05"
                ]
            ]
        },
        "gold_description": "table 2 shows the results , where the two cases are denoted as δiv and δoov . the general trend in the results is that the improvements of both models in the oov case are larger than in the iv case , which means that the character composition models indeed alleviates the oov problem . also , cnn improves on seven languages in the iv case and eight languages in the oov case , and it performs consistently better than lstm in both cases .",
        "generated_description": "from the table , we can see that the δiv values of cnn and lstm in the iv and oov cases are very close to each other and the average δoov is close to zero . this indicates that both models are able to successfully filter out the out - of - vocabulary words ( oov ) and insert words into the iv cases ."
    },
    {
        "table_id": "200",
        "table_info": {
            "table_caption": "Table 3: Accuracy scores × 1000 for different train and test dataset combinations",
            "table_column_names": [
                "train dataset",
                "test dataset Belarusian",
                "test dataset Russian",
                "test dataset Ukrai-nian"
            ],
            "table_content_values": [
                [
                    "Belarusian",
                    "647",
                    "326",
                    "373"
                ],
                [
                    "Russian",
                    "495",
                    "738",
                    "516"
                ],
                [
                    "Ukrainian",
                    "556",
                    "553",
                    "683"
                ],
                [
                    "Ukrainian, Belarusian",
                    "769",
                    "597",
                    "701"
                ],
                [
                    "Russian, Belarusian",
                    "740",
                    "740",
                    "563"
                ],
                [
                    "Russian, Ukrainian",
                    "627",
                    "756",
                    "[BOLD] 700"
                ],
                [
                    "Russian, Ukrainian, Belarusian",
                    "[BOLD] 772",
                    "[BOLD] 760",
                    "698"
                ]
            ]
        },
        "gold_description": "averaged . the table 3 presents the results of these experiments . the table 3 shows , that : 1 . in monolingual setting , we can get highquality results . the scores are significantly lower than the scores of the same model on the standard dataset , due to the smaller sizes of the training datasets . nevertheless , one can see , that our approach to word stress detection applies not only to the russian language data , but also to the data in the belarusian and ukrainian languages ; 2 . cross - lingual setting ( 1 ) : the belarusian training dataset , being the smallest one among the three datasets , is not a good source for training word stress detection models in other languages , while the ukrainian dataset stands out as a good source for training word stress detection systems both for the russian and belarusian languages ; 3 . cross - lingual setting ( 2 ) : adding one or two datasets to the other languages improves the quality . for example , around 10 % of accuracy is gained by adding the russian training dataset to the belarusian training dataset , while testing on belarusian .",
        "generated_description": "for different train and test sets , we report the accuracy scores × 1000 for different combinations of the train and the test sets . as expected , there is a large performance gain when using the same training and test set for both the russian and ukrainian languages . however , when we use different training and testing sets for the different language pairs , there are different performance boosts in terms of accuracy scores . for example , when the training data is russian and the ukrainian language pairs are combined , the accuracy of the russian language is increased from 495 to 738 and from 516 to 683 , respectively . similarly , when using a training and a test set of the ukrai - nian language pair , the accuracies of the belarusian and russian language are increased from 373 and 516 respectively . in general , we observe that using different training / test sets for different languages can improve the performance of the model ."
    },
    {
        "table_id": "201",
        "table_info": {
            "table_caption": "Table 1: BLEU and exact-match scores over held-out test set",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] BLEU  [BOLD] (All)",
                "[BOLD] BLEU  [BOLD] (WSJ)",
                "[BOLD] BLEU  [BOLD] (All overlap)",
                "[BOLD] Exact  [BOLD] Match%",
                "[BOLD] Coverage%"
            ],
            "table_content_values": [
                [
                    "Neural MRS (gold)",
                    "66.11",
                    "73.12",
                    "69.27",
                    "24.09",
                    "100"
                ],
                [
                    "Neural MRS (silver)",
                    "75.43",
                    "81.76",
                    "77.13",
                    "25.82",
                    "100"
                ],
                [
                    "Neural MRS (gold + silver)",
                    "77.17",
                    "83.37",
                    "79.15",
                    "32.07",
                    "100"
                ],
                [
                    "ACE (ERG)",
                    "–",
                    "–",
                    "62.05",
                    "15.08",
                    "78"
                ],
                [
                    "DAG transducer Ye et al. ( 2018 )",
                    "–",
                    "68.07",
                    "–",
                    "–",
                    "100"
                ]
            ]
        },
        "gold_description": "we compare the performance of our neural generator when trained on either gold , silver , or gold and silver data ( table 1 ) . generation quality is primarily evaluated with bleu semi - supervised training leads to an 11 bleu point improvement compar train in addition to bleu , we also report exact match accuracy on the overlapping subset . results show that our neural models outperform the grammar - based generator by a large margin .",
        "generated_description": "finally , neural mrs outperforms the state - of - the - art dag transducer ye et al . we report bleu and exact - match scores over the held - out test set for the wsj portion of the gigaword corpus as well as over the in - domain test set ( all overlap ) . we report the gold and silver version ( gold + silver ) and the coverage ( gold and silver ) . the coverage is defined as the percentage of the test set that exactly matches the gold triples in the training set ."
    },
    {
        "table_id": "202",
        "table_info": {
            "table_caption": "Table 3: Hyper-parameter values.",
            "table_column_names": [
                "[BOLD] Parameter char emb size",
                "[BOLD] Value 50",
                "[BOLD] Parameter bigram emb size",
                "[BOLD] Value 50"
            ],
            "table_content_values": [
                [
                    "lattice emb size",
                    "50",
                    "LSTM hidden",
                    "200"
                ],
                [
                    "char dropout",
                    "0.5",
                    "lattice dropout",
                    "0.5"
                ],
                [
                    "LSTM layer",
                    "1",
                    "regularization  [ITALIC] λ",
                    "1e-8"
                ],
                [
                    "learning rate  [ITALIC] lr",
                    "0.015",
                    "[ITALIC] lr decay",
                    "0.05"
                ]
            ]
        },
        "gold_description": "table 3 shows the values of hyper - parameters for our models , in particular , the embedding sizes are set to 50 and the hidden size of lstm models to 200 . dropout ( srivastava et al . , 2014 ) is applied to both word and character embeddings with a rate of 0 . 5 . stochastic gradient descent ( sgd ) is used for optimization , with an initial learning rate of 0 . 015 and a decay rate of 0 . 05 .",
        "generated_description": "we use the same hyper - parameter values for all the experiments as \\ newciteglorot2010globally . the size of the embeddings is set to 50 and the dimension of bigrams is set as 50 . all the models are trained using adam optimizer kingma and ba the initial learning rate is lr = 0 . 015 and the decay rate is decayed to 0 . 05 in order to make the training procedure as stable as possible . for regularization λ , we set it to 1e − 0 . 5 and set the number of epochs to one ."
    },
    {
        "table_id": "203",
        "table_info": {
            "table_caption": "Table 4: Development results.",
            "table_column_names": [
                "[BOLD] Input",
                "[BOLD] Models",
                "[BOLD] P",
                "[BOLD] R",
                "[BOLD] F1"
            ],
            "table_content_values": [
                [
                    "Auto seg",
                    "Word baseline",
                    "73.20",
                    "57.05",
                    "64.12"
                ],
                [
                    "Auto seg",
                    "+char LSTM",
                    "71.98",
                    "65.41",
                    "68.54"
                ],
                [
                    "Auto seg",
                    "+char LSTM′",
                    "71.08",
                    "65.83",
                    "68.35"
                ],
                [
                    "Auto seg",
                    "+char+bichar LSTM",
                    "72.63",
                    "67.60",
                    "70.03"
                ],
                [
                    "Auto seg",
                    "+char CNN",
                    "73.06",
                    "66.29",
                    "69.51"
                ],
                [
                    "Auto seg",
                    "+char+bichar CNN",
                    "72.01",
                    "65.50",
                    "68.60"
                ],
                [
                    "No seg",
                    "Char baseline",
                    "67.12",
                    "58.42",
                    "62.47"
                ],
                [
                    "No seg",
                    "+softword",
                    "69.30",
                    "62.47",
                    "65.71"
                ],
                [
                    "No seg",
                    "+bichar",
                    "71.67",
                    "64.02",
                    "67.63"
                ],
                [
                    "No seg",
                    "+bichar+softword",
                    "72.64",
                    "66.89",
                    "69.64"
                ],
                [
                    "No seg",
                    "Lattice",
                    "[BOLD] 74.64",
                    "[BOLD] 68.83",
                    "[BOLD] 71.62"
                ]
            ]
        },
        "gold_description": "as shown in table 4 , without using word segmentation , a characterbased lstm - crf model gives a development f1score of 62 . 47 % . adding character - bigram and softword representations as described in section 3 . 1 increases the f1 - score to 67 . 63 % and 65 . 71 % , respectively , demonstrating the usefulness of both sources of information . in addition , a combination of both gives a 69 . 64 % f1 - score , which is the best among various character representations . table 4 shows a variety of different settings for word - based chinese ner . with automatic segmentation , a word - based lstm crf baseline gives a 64 . 12 % f1 - score , which is higher compared to the character - based baseline . this demonstrates that both word information and character information are useful for chinese ner . the two methods of word + char lstm and word + char lstm ( cid : 48 ) , lead to similar improvements . a cnn representation of character sequences gives a slightly higher f1 - score compared to lstm character representations . on the other hand , further using character bigram information leads to increased f1 - score over word + char lstm , but decreased f1 - score over word + char cnn . as shown in table 4 , the lattice lstm - crf model gives a development f1 - score of 71 . 62 % , which is significantly7 higher compared with both the word - based and character - based methods , despite that it does not use character bigrams or word segmentation information .",
        "generated_description": "firstly , we compare the performance of the word - based segmenter with the auto - segmented segmenter and the character - level lstm ′ model with no segmentation ( i . e . , without the use of bichar ) , and with and without the subword segmentation . we can observe that the improvement of the auto seg model is more significant when the segmentation is done manually . adding character lstms or cnn layers on top of word embeddings can further improve the performance . however , the improvement is not as significant as that of character cnns or bichars . this is probably because that the auto segmentation needs to be done manually by manually segmenting the words into subword units , which is much more difficult than segmenting by simply using a character embedding . secondly , we can see that the character cnn based segmenter achieves better performance than the word embedding based one . it indicates that character - based representation can capture the rich contextual information for word segmentation which is beyond the scope of a simple character - lstm based segmentation model . thirdly , the character and bichar based segmenters achieve better performances than the other two models , which proves the effectiveness of our method . finally , we find that the sub - word information is more important than the character representation ."
    },
    {
        "table_id": "204",
        "table_info": {
            "table_caption": "Table 5: Main results on OntoNotes.",
            "table_column_names": [
                "[BOLD] Input",
                "[BOLD] Models",
                "[BOLD] P",
                "[BOLD] R",
                "[BOLD] F1"
            ],
            "table_content_values": [
                [
                    "Gold seg",
                    "Yang et al. ( 2016 )",
                    "65.59",
                    "71.84",
                    "68.57"
                ],
                [
                    "Gold seg",
                    "Yang et al. ( 2016 )*†",
                    "72.98",
                    "[BOLD] 80.15",
                    "[BOLD] 76.40"
                ],
                [
                    "Gold seg",
                    "Che et al. ( 2013 )*",
                    "77.71",
                    "72.51",
                    "75.02"
                ],
                [
                    "Gold seg",
                    "Wang et al. ( 2013 )*",
                    "76.43",
                    "72.32",
                    "74.32"
                ],
                [
                    "Gold seg",
                    "Word baseline",
                    "76.66",
                    "63.60",
                    "69.52"
                ],
                [
                    "Gold seg",
                    "+char+bichar LSTM",
                    "[BOLD] 78.62",
                    "73.13",
                    "75.77"
                ],
                [
                    "Auto seg",
                    "Word baseline",
                    "72.84",
                    "59.72",
                    "65.63"
                ],
                [
                    "Auto seg",
                    "+char+bichar LSTM",
                    "73.36",
                    "70.12",
                    "71.70"
                ],
                [
                    "No seg",
                    "Char baseline",
                    "68.79",
                    "60.35",
                    "64.30"
                ],
                [
                    "No seg",
                    "+bichar+softword",
                    "74.36",
                    "69.43",
                    "71.81"
                ],
                [
                    "No seg",
                    "Lattice",
                    "[BOLD] 76.35",
                    "[BOLD] 71.56",
                    "[BOLD] 73.88"
                ]
            ]
        },
        "gold_description": "the ontonotes test results are shown in table 5 . with gold - standard segmentation , our word - based methods give competitive results to the state - of - the - art on the dataset ( che et al . , 2013 ; wang et al . , 2013 ) , in addition , the results show that our word - based models can serve as highly competitive baselines . with automatic segmentation , the f1 - score of word + char + bichar lstm decreases from 75 . 77 % to 71 . 70 % , showing the influence of segmentation to ner . consistent with observations on the development set , adding lattice word information leads to an 88 . 81 % → 93 . 18 % increasement of f1 - score over the character baseline , as compared with 88 . 81 % → 91 . 87 % by adding bichar + softword . the lattice model gives significantly the best f1 - score on automatic segmentation .",
        "generated_description": "the “ no seg ” row refers to the model without any segmentation , i . e . , the model is only trained with the word embeddings . as can be seen from the table , with the help of character and bichar lstm representations , the f1 score of our proposed model with gold segmentation inputs increases from 74 . 40 % to 78 . 62 % and from 73 . 13 % to 73 . 36 % , which are 2 . 5 and 1 . 5 points higher than the previous state - of - the - art results , respectively . with auto - segmentation inputs , the performance of the model also improves from 63 . 52 % to 74 . 32 % and 76 . 40 % , which is 1 . 6 and 2 . 2 points higher compared to the previous best results . the character and bichar representations are complementary to each other ."
    },
    {
        "table_id": "205",
        "table_info": {
            "table_caption": "Table 6: Main results on MSRA.",
            "table_column_names": [
                "[BOLD] Models",
                "[BOLD] P",
                "[BOLD] R",
                "[BOLD] F1"
            ],
            "table_content_values": [
                [
                    "Chen et al. ( 2006a )",
                    "91.22",
                    "81.71",
                    "86.20"
                ],
                [
                    "Zhang et al. ( 2006 )*",
                    "92.20",
                    "90.18",
                    "91.18"
                ],
                [
                    "Zhou et al. ( 2013 )",
                    "91.86",
                    "88.75",
                    "90.28"
                ],
                [
                    "Lu et al. ( 2016 )",
                    "–",
                    "–",
                    "87.94"
                ],
                [
                    "Dong et al. ( 2016 )",
                    "91.28",
                    "90.62",
                    "90.95"
                ],
                [
                    "Word baseline",
                    "90.57",
                    "83.06",
                    "86.65"
                ],
                [
                    "+char+bichar LSTM",
                    "91.05",
                    "89.53",
                    "90.28"
                ],
                [
                    "Char baseline",
                    "90.74",
                    "86.96",
                    "88.81"
                ],
                [
                    "+bichar+softword",
                    "92.97",
                    "90.80",
                    "91.87"
                ],
                [
                    "Lattice",
                    "[BOLD] 93.57",
                    "[BOLD] 92.79",
                    "[BOLD] 93.18"
                ]
            ]
        },
        "gold_description": "results on the msra dataset are shown in table 6 . our chosen segmentor gives 95 . 93 % accuracy on 5 - fold cross - validated training set . compared with the existing methods , our wordbased and character - based lstm - crf models give competitive accuracies . the lattice model significantly outperforms both the best characterbased and word - based models ( p < 0 . 01 ) , achieving the best result on this standard benchmark .",
        "generated_description": "the “ + char + bichar lstm ” model uses character - level character embeddings to represent bichar characters in a word - level model , and the ” + chichar + softword ” adds a softword word embedding to the baseline model . the lattice model achieves the best performance in precision , recall , and f1 - score ."
    },
    {
        "table_id": "206",
        "table_info": {
            "table_caption": "Table 8: Main results on resume NER.",
            "table_column_names": [
                "[BOLD] Models",
                "[BOLD] P",
                "[BOLD] R",
                "[BOLD] F1"
            ],
            "table_content_values": [
                [
                    "Word baseline",
                    "93.72",
                    "93.44",
                    "93.58"
                ],
                [
                    "+char+bichar LSTM",
                    "94.07",
                    "94.42",
                    "94.24"
                ],
                [
                    "Char baseline",
                    "93.66",
                    "93.31",
                    "93.48"
                ],
                [
                    "+bichar+softword",
                    "94.53",
                    "[BOLD] 94.29",
                    "94.41"
                ],
                [
                    "Lattice",
                    "[BOLD] 94.81",
                    "94.11",
                    "[BOLD] 94.46"
                ]
            ]
        },
        "gold_description": "results on the resume ner test data are shown in table 8 . the lattice model significantly outperforms both the word - based mode and the character - based model for weibo and resume ( p < 0 . 01 ) , giving state - of - the - art results .",
        "generated_description": "the “ + char + bichar lstm ” row shows the results of adding bichar ’ s character embeddings to the baseline model , and the ” + chichar + softword ” shows the same results as that of the original paper . lattice obtains the best performance in precision , recall and f1 - score . it should be noted that the f1 score of the word - level baseline is 93 . 58 % , which is lower than the one obtained by the lattice model ."
    },
    {
        "table_id": "207",
        "table_info": {
            "table_caption": "Table 3: Development set results for English–Estonian. character-F and BLEU scores in percentages. +/− stands for adding/removing a component. Multiple modifications are indicated by increasing the indentation.",
            "table_column_names": [
                "[EMPTY]",
                "chrF-1.0",
                "BLEU%"
            ],
            "table_content_values": [
                [
                    "en-et",
                    "dev",
                    "dev"
                ],
                [
                    "BPE",
                    "56.52",
                    "17.93"
                ],
                [
                    "monolingual",
                    "53.44",
                    "15.82"
                ],
                [
                    "Cognate Morfessor",
                    "57.05",
                    "18.40"
                ],
                [
                    "+finetuned",
                    "57.23",
                    "18.45"
                ],
                [
                    "+ensemble-of-5",
                    "[BOLD] 57.75",
                    "[BOLD] 19.09"
                ],
                [
                    "+ensemble-of-3",
                    "57.64",
                    "18.96"
                ],
                [
                    "+linked embeddings",
                    "56.20",
                    "17.48"
                ],
                [
                    "−LM filtering",
                    "52.94",
                    "14.65"
                ],
                [
                    "6+6 layers",
                    "57.35",
                    "18.84"
                ]
            ]
        },
        "gold_description": "table 3 shows the development set results for estonian . cognate morfessor outperforms the comparable bpe system according to both measures for estonian . the cross - lingual segmentation is particularly beneficial for estonian . for estonian , we have two ensemble configurations one combining 3 monolingually finetuned independent runs , and one combining 5 monolingually finetuned savepoints from 4 independent runs .",
        "generated_description": "for english – estonian , our baseline is a bpe - based monolingual model trained with cross - entropy loss . cognate morfessor is a morpheme - based model that uses a concatenation of the concatenated bpe output and the cognate output of the bpe model as an additional input to the decoder . we see that adding / removing components leads to a significant performance drop in both chrf - 1 . 0 and bleu . however , we see that an ensemble - of - 5 model with 5 bpe layers gives the best results . this is in line with our hypothesis that the model benefits from being finetuned for each language pair . finetuning gives an absolute improvement of 0 . 8 – 0 . 9 character - f and 0 . 5 – 1 . 1 bleus over using bpe without any finetuning , and an ensemble of 3 to 5 layers gives a slight improvement of 1 . 0 – 2 . 0 characters - f . we also observe that using a single layer of 512 - dimensional linked embeddings leads to the lowest performance . we suspect that this is due to the small amount of training data for english – estonian ."
    },
    {
        "table_id": "208",
        "table_info": {
            "table_caption": "Table 1: Results on UNCorpus-1.",
            "table_column_names": [
                "[EMPTY]",
                "Sestorain et al. ( 2018 )† PBSMT",
                "Sestorain et al. ( 2018 )† NMT-0",
                "Sestorain et al. ( 2018 )† Dual-0",
                "Our baselines Basic",
                "Our baselines Pivot",
                "Agree"
            ],
            "table_content_values": [
                [
                    "\\En → \\Es",
                    "61.26",
                    "51.93",
                    "—",
                    "56.58",
                    "56.58",
                    "56.36"
                ],
                [
                    "\\En → \\Fr",
                    "50.09",
                    "40.56",
                    "—",
                    "44.27",
                    "44.27",
                    "44.80"
                ],
                [
                    "\\Es → \\En",
                    "59.89",
                    "51.58",
                    "—",
                    "55.70",
                    "55.70",
                    "55.24"
                ],
                [
                    "\\Fr → \\En",
                    "52.22",
                    "43.33",
                    "—",
                    "46.46",
                    "46.46",
                    "46.17"
                ],
                [
                    "Supervised (avg.)",
                    "55.87",
                    "46.85",
                    "—",
                    "50.75",
                    "50.75",
                    "50.64"
                ],
                [
                    "\\Es → \\Fr",
                    "52.44",
                    "20.29",
                    "36.68",
                    "34.75",
                    "[BOLD] 38.10",
                    "37.54"
                ],
                [
                    "\\Fr → \\Es",
                    "49.79",
                    "19.01",
                    "39.19",
                    "37.67",
                    "[BOLD] 40.84",
                    "40.02"
                ],
                [
                    "Zero-shot (avg.)",
                    "51.11",
                    "19.69",
                    "37.93",
                    "36.21",
                    "[BOLD] 39.47",
                    "38.78"
                ]
            ]
        },
        "gold_description": "tables 1 and 2 show results on the uncorpus datasets . our approach consistently outperforms basic and dual - 0 , despite the latter being trained with additional monolingual data ( sestorain et al . , 2018 ) .",
        "generated_description": "sestorain et al . as can be seen from the table , our basic pivot model ( pivot ) outperforms the pbsmt baseline by a large margin when translating from english to french and from french to english , and is on par with the state - of - the - art nmt - 0 model . the improved performance of the dual - encoder model ( dual - 0 ) is a testament to its capability of exploiting cross - lingual transfer , which is beyond the scope of this work . our final model ( agreeableness ) is close to the best pbsmt result ( 37 . 54 bleu points ) and is significantly better than the supervised baseline ( avg . ) by a significant margin ( 2 points ) . this shows that the agreement between the two languages is not only implicitly learned but is also implicitly learned ."
    },
    {
        "table_id": "209",
        "table_info": {
            "table_caption": "Table 3: Results of semantic feature ablation, model trained with gold data only",
            "table_column_names": [
                "[BOLD] Ablation",
                "[BOLD] BLEU"
            ],
            "table_content_values": [
                [
                    "All attributes",
                    "72.06"
                ],
                [
                    "No node attributes",
                    "59.37"
                ],
                [
                    "No node attr except num, tense",
                    "67.34"
                ],
                [
                    "No edge features",
                    "71.27"
                ]
            ]
        },
        "gold_description": "edge properties are removed , which table 3 shows has an effect of less than 1 bleu point ) . we ablate node ( predicate ) and edge attributes number and tense have the largest effect on the reported bleu score . has only a small impact on performance .",
        "generated_description": "semantic feature ablation . we observe that including all features is the most effective one , achieving a bleu score of 72 . 06 . ablating only the node attributes results in a performance drop from 59 . 37 to 67 . 34 , which shows the usefulness of these features . removing any of the other features results in even worse performance , showing the importance of edge features for the verb tense and number of nodes ."
    },
    {
        "table_id": "210",
        "table_info": {
            "table_caption": "Table 2: Results on UNCorpus-2.",
            "table_column_names": [
                "[EMPTY]",
                "Sestorain et al. ( 2018 ) PBSMT",
                "Sestorain et al. ( 2018 ) NMT-0",
                "Sestorain et al. ( 2018 ) Dual-0",
                "Our baselines Basic",
                "Our baselines Pivot",
                "Agree"
            ],
            "table_content_values": [
                [
                    "\\En → \\Es",
                    "61.26",
                    "47.51",
                    "44.30",
                    "55.15",
                    "55.15",
                    "54.30"
                ],
                [
                    "\\En → \\Fr",
                    "50.09",
                    "36.70",
                    "34.34",
                    "43.42",
                    "43.42",
                    "42.57"
                ],
                [
                    "\\En → \\Ru",
                    "43.25",
                    "30.45",
                    "29.47",
                    "36.26",
                    "36.26",
                    "35.89"
                ],
                [
                    "\\Es → \\En",
                    "59.89",
                    "48.56",
                    "45.55",
                    "54.35",
                    "54.35",
                    "54.33"
                ],
                [
                    "\\Fr → \\En",
                    "52.22",
                    "40.75",
                    "37.75",
                    "45.55",
                    "45.55",
                    "45.87"
                ],
                [
                    "\\Ru → \\En",
                    "52.59",
                    "39.35",
                    "37.96",
                    "45.52",
                    "45.52",
                    "44.67"
                ],
                [
                    "Supervised (avg.)",
                    "53.22",
                    "40.55",
                    "36.74",
                    "46.71",
                    "46.71",
                    "46.27"
                ],
                [
                    "\\Es → \\Fr",
                    "52.44",
                    "25.85",
                    "34.51",
                    "34.73",
                    "35.93",
                    "[BOLD] 36.02"
                ],
                [
                    "\\Fr → \\Es",
                    "49.79",
                    "22.68",
                    "37.71",
                    "38.20",
                    "39.51",
                    "[BOLD] 39.94"
                ],
                [
                    "\\Es → \\Ru",
                    "39.69",
                    "9.36",
                    "24.55",
                    "26.29",
                    "27.15",
                    "[BOLD] 28.08"
                ],
                [
                    "\\Ru → \\Es",
                    "49.61",
                    "26.26",
                    "33.23",
                    "33.43",
                    "[BOLD] 37.17",
                    "35.01"
                ],
                [
                    "\\Fr → \\Ru",
                    "36.48",
                    "9.35",
                    "22.76",
                    "23.88",
                    "24.99",
                    "[BOLD] 25.13"
                ],
                [
                    "\\Ru → \\Fr",
                    "43.37",
                    "22.43",
                    "26.49",
                    "28.52",
                    "[BOLD] 30.06",
                    "29.53"
                ],
                [
                    "Zero-shot (avg.)",
                    "45.23",
                    "26.26",
                    "29.88",
                    "30.84",
                    "[BOLD] 32.47",
                    "32.29"
                ]
            ]
        },
        "gold_description": "we see that models trained with agreement perform comparably to pivot , outperforming it in some cases , e . g . , when the target is russian , perhaps because it is quite different linguistically from the english pivot . unlike dual - 0 , agree maintains high performance in the supervised directions ( within 1 bleu point compared to basic ) ,",
        "generated_description": "the accuracies reported in sestorain et al . as can be seen from the table , our basic pivot model outperforms pbsmt and nmt - 0 by a large margin and is close to the performance of dual - 0 , which is the current state - of - the - art pbsmt model . this is due to the fact that our pivot model is pre - trained on a large amount of monolingual data and is able to generalize well across different language pairs . moreover , our agreeableness ( avg . ) baseline outperforms the previous best pbsmt by a small margin but is still far from being as good as our best model ( 53 . 22 vs . 52 . 59 ) ."
    },
    {
        "table_id": "211",
        "table_info": {
            "table_caption": "Table 3: Zero-shot results on Europarl. Note that Soft and Distill are not multilingual systems.",
            "table_column_names": [
                "[EMPTY]",
                "Previous work Soft‡",
                "Previous work Distill†",
                "Our baselines Basic",
                "Our baselines Pivot",
                "Agree"
            ],
            "table_content_values": [
                [
                    "\\En → \\Es",
                    "—",
                    "—",
                    "34.69",
                    "34.69",
                    "33.80"
                ],
                [
                    "\\En → \\De",
                    "—",
                    "—",
                    "23.06",
                    "23.06",
                    "22.44"
                ],
                [
                    "\\En → \\Fr",
                    "31.40",
                    "—",
                    "33.87",
                    "33.87",
                    "32.55"
                ],
                [
                    "\\Es → \\En",
                    "31.96",
                    "—",
                    "34.77",
                    "34.77",
                    "34.53"
                ],
                [
                    "\\De → \\En",
                    "26.55",
                    "—",
                    "29.06",
                    "29.06",
                    "29.07"
                ],
                [
                    "\\Fr → \\En",
                    "—",
                    "—",
                    "33.67",
                    "33.67",
                    "33.30"
                ],
                [
                    "Supervised (avg.)",
                    "—",
                    "—",
                    "31.52",
                    "31.52",
                    "30.95"
                ],
                [
                    "\\Es → \\De",
                    "—",
                    "—",
                    "18.23",
                    "20.14",
                    "[BOLD] 20.70"
                ],
                [
                    "\\De → \\Es",
                    "—",
                    "—",
                    "20.28",
                    "[BOLD] 26.50",
                    "22.45"
                ],
                [
                    "\\Es → \\Fr",
                    "30.57",
                    "[BOLD] 33.86",
                    "27.99",
                    "32.56",
                    "30.94"
                ],
                [
                    "\\Fr → \\Es",
                    "—",
                    "—",
                    "27.12",
                    "[BOLD] 32.96",
                    "29.91"
                ],
                [
                    "\\De → \\Fr",
                    "23.79",
                    "[BOLD] 27.03",
                    "21.36",
                    "25.67",
                    "24.45"
                ],
                [
                    "\\Fr → \\De",
                    "—",
                    "—",
                    "18.57",
                    "[BOLD] 19.86",
                    "19.15"
                ],
                [
                    "Zero-shot (avg.)",
                    "—",
                    "—",
                    "22.25",
                    "26.28",
                    "24.60"
                ]
            ]
        },
        "gold_description": "table 3 shows the results on the europarl corpus . our approach consistently outperforms basic by 2 - 3 bleu points but lags a bit behind pivot on average ( except on de where it is better ) . es",
        "generated_description": "our distillation model outperforms the basic and pivot baselines by a large margin and is close to the performance of the best previous work agreeableness . the distilled model is able to improve over the basic model by + 1 . 5 bleu points when translating english to german and + 0 . 8 bleus when translating french to english , and + 2 . 5 and + 3 . 1 bleu points when decoding english to arabic . this shows that the distilled model has learned a better alignment with respect to the target language ."
    },
    {
        "table_id": "212",
        "table_info": {
            "table_caption": "Table 4: Results on the official IWSLT17 multilingual task.",
            "table_column_names": [
                "[EMPTY]",
                "Previous work SOTA†",
                "Previous work CPG‡",
                "Our baselines Basic",
                "Our baselines Pivot",
                "Agree"
            ],
            "table_content_values": [
                [
                    "Supervised (avg.)",
                    "24.10",
                    "19.75",
                    "24.63",
                    "24.63",
                    "23.97"
                ],
                [
                    "Zero-shot (avg.)",
                    "20.55",
                    "11.69",
                    "19.86",
                    "19.26",
                    "[BOLD] 20.58"
                ]
            ]
        },
        "gold_description": "table 4 presents results on the original iwslt17 task . the vanilla training method ( johnson et al . , 2016 ) achieves very high zero shot performance , even outperforming pivot .",
        "generated_description": "zero - shot learning outperforms the supervised baseline by a large margin and is close to matching the performance of the best previous work on this task . this shows that a pre - trained model can already be applied to a new task that is different from the one used to train the pivot model ."
    },
    {
        "table_id": "213",
        "table_info": {
            "table_caption": "Table 5: Results on our proposed IWSLT17?",
            "table_column_names": [
                "[EMPTY]",
                "Basic",
                "Pivot",
                "Agree"
            ],
            "table_content_values": [
                [
                    "Supervised (avg.)",
                    "28.72",
                    "28.72",
                    "[BOLD] 29.17"
                ],
                [
                    "Zero-shot (avg.)",
                    "12.61",
                    "[BOLD] 17.68",
                    "15.23"
                ]
            ]
        },
        "gold_description": "on our proposed preprocessed iwslt17 ( cid : 63 ) that eliminates the overlap and reduces the number of supervised directions ( 8 ) , there is a considerable gap between the supervised and zeroshot performance of basic . agree performs better than basic and is slightly worse than pivot .",
        "generated_description": "zero - shot learning does not perform well on our proposed iwslt17 dataset . this may be due to the fact that the training data is small and we do not have a large amount of unlabeled data . on the other hand , the supervised learning seems to help more on the agreeableness metric ."
    },
    {
        "table_id": "214",
        "table_info": {
            "table_caption": "Table 2: BLEU scores for the bilingual test sets. Here all contexts are incorporated as InitDec for Europarl and InitDec+AddDec for Subtitles unless otherwise specified. bold: Best performance, †: Statistically significantly better than the base model, based on bootstrap resampling Clark et al. (2011) with p < 0.05.",
            "table_column_names": [
                "[EMPTY]",
                "[BOLD] Europarl  [BOLD] En-Fr",
                "[BOLD] Europarl  [BOLD] En-Fr",
                "[BOLD] Europarl  [BOLD] En-Fr",
                "[BOLD] Europarl  [BOLD] En-Et",
                "[BOLD] Europarl  [BOLD] En-Et",
                "[BOLD] Europarl  [BOLD] En-Et",
                "[BOLD] Europarl  [BOLD] En-De",
                "[BOLD] Europarl  [BOLD] En-De",
                "[BOLD] Europarl  [BOLD] En-De",
                "[BOLD] Subtitles  [BOLD] En-Ru",
                "[BOLD] Subtitles  [BOLD] En-Ru",
                "[BOLD] Subtitles  [BOLD] En-Ru"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "Overall",
                    "En→Fr",
                    "Fr→En",
                    "Overall",
                    "En→Et",
                    "Et→En",
                    "Overall",
                    "En→De",
                    "De→En",
                    "Overall",
                    "En→Ru",
                    "Ru→En"
                ],
                [
                    "[ITALIC] Base Model",
                    "37.36",
                    "38.13",
                    "36.03",
                    "20.68",
                    "18.64",
                    "26.65",
                    "24.74",
                    "21.80",
                    "27.74",
                    "19.05",
                    "14.90",
                    "23.04"
                ],
                [
                    "+ [ITALIC] Source Context as Lang-Specific Attention via",
                    "+ [ITALIC] Source Context as Lang-Specific Attention via",
                    "+ [ITALIC] Source Context as Lang-Specific Attention via",
                    "+ [ITALIC] Source Context as Lang-Specific Attention via",
                    "+ [ITALIC] Source Context as Lang-Specific Attention via",
                    "+ [ITALIC] Source Context as Lang-Specific Attention via",
                    "+ [ITALIC] Source Context as Lang-Specific Attention via",
                    "+ [ITALIC] Source Context as Lang-Specific Attention via",
                    "+ [ITALIC] Source Context as Lang-Specific Attention via",
                    "+ [ITALIC] Source Context as Lang-Specific Attention via",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "InitDec",
                    "38.40†",
                    "39.19†",
                    "36.86†",
                    "[BOLD] 21.79†",
                    "19.54†",
                    "[BOLD] 28.33†",
                    "[BOLD] 26.34†",
                    "[BOLD] 23.31†",
                    "29.39†",
                    "18.88",
                    "14.89",
                    "22.56"
                ],
                [
                    "AddDec",
                    "38.50†",
                    "[BOLD] 39.35†",
                    "36.98†",
                    "21.65†",
                    "[BOLD] 19.66†",
                    "27.48†",
                    "26.30†",
                    "23.09†",
                    "[BOLD] 29.52†",
                    "19.34",
                    "15.16",
                    "23.12"
                ],
                [
                    "InitDec+AddDec",
                    "[BOLD] 38.55†",
                    "39.34†",
                    "[BOLD] 37.14†",
                    "21.49†",
                    "19.43†",
                    "27.55†",
                    "26.25†",
                    "23.18†",
                    "29.30†",
                    "[BOLD] 19.35",
                    "[BOLD] 15.16",
                    "[BOLD] 23.14"
                ],
                [
                    "+ [ITALIC] Source Context via",
                    "+ [ITALIC] Source Context via",
                    "+ [ITALIC] Source Context via",
                    "+ [ITALIC] Source Context via",
                    "+ [ITALIC] Source Context via",
                    "+ [ITALIC] Source Context via",
                    "+ [ITALIC] Source Context via",
                    "+ [ITALIC] Source Context via",
                    "+ [ITALIC] Source Context via",
                    "+ [ITALIC] Source Context via",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Direct Tranformation",
                    "38.35†",
                    "39.13†",
                    "36.96†",
                    "21.75†",
                    "[BOLD] 19.59†",
                    "28.07†",
                    "26.29†",
                    "23.34†",
                    "29.22†",
                    "19.09",
                    "14.89",
                    "22.76"
                ],
                [
                    "Hierarchical Gating",
                    "38.33†",
                    "39.14†",
                    "36.89†",
                    "21.62†",
                    "19.55†",
                    "27.64†",
                    "26.31†",
                    "23.17†",
                    "29.45†",
                    "19.20",
                    "15.10",
                    "22.73"
                ],
                [
                    "Lang-Specific Attention",
                    "38.40†",
                    "39.19†",
                    "36.86†",
                    "21.79†",
                    "19.54†",
                    "28.33†",
                    "26.34†",
                    "23.31†",
                    "29.39†",
                    "[BOLD] 19.35",
                    "[BOLD] 15.16",
                    "[BOLD] 23.14"
                ],
                [
                    "Combined Attention",
                    "[BOLD] 38.50†",
                    "[BOLD] 39.36†",
                    "36.94†",
                    "21.66†",
                    "19.52†",
                    "27.90†",
                    "26.38†",
                    "23.31†",
                    "29.44†",
                    "18.96",
                    "14.82",
                    "22.92"
                ],
                [
                    "Lang-Specific S-Attention",
                    "38.46†",
                    "39.24†",
                    "[BOLD] 37.06†",
                    "[BOLD] 21.84†",
                    "19.58†",
                    "[BOLD] 28.43†",
                    "[BOLD] 26.49†",
                    "[BOLD] 23.49†",
                    "[BOLD] 29.49†",
                    "19.09",
                    "14.59",
                    "22.98"
                ],
                [
                    "+ [ITALIC] Lang-Specific S-Attention using",
                    "+ [ITALIC] Lang-Specific S-Attention using",
                    "+ [ITALIC] Lang-Specific S-Attention using",
                    "+ [ITALIC] Lang-Specific S-Attention using",
                    "+ [ITALIC] Lang-Specific S-Attention using",
                    "+ [ITALIC] Lang-Specific S-Attention using",
                    "+ [ITALIC] Lang-Specific S-Attention using",
                    "+ [ITALIC] Lang-Specific S-Attention using",
                    "+ [ITALIC] Lang-Specific S-Attention using",
                    "+ [ITALIC] Lang-Specific S-Attention using",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Source Context",
                    "38.46†",
                    "39.24†",
                    "37.06†",
                    "[BOLD] 21.84†",
                    "19.58†",
                    "[BOLD] 28.43†",
                    "[BOLD] 26.49†",
                    "[BOLD] 23.49†",
                    "29.49†",
                    "19.09",
                    "14.59",
                    "22.98"
                ],
                [
                    "Target Context",
                    "38.76†",
                    "[BOLD] 39.57†",
                    "37.35†",
                    "21.77†",
                    "[BOLD] 19.68†",
                    "27.86†",
                    "26.21†",
                    "23.16†",
                    "29.26†",
                    "19.23",
                    "14.77",
                    "[BOLD] 23.23"
                ],
                [
                    "Dual Context Src-Tgt",
                    "[BOLD] 38.80†",
                    "39.51†",
                    "[BOLD] 37.50†",
                    "21.74†",
                    "19.60†",
                    "27.98†",
                    "26.39†",
                    "23.28†",
                    "[BOLD] 29.50†",
                    "18.89",
                    "14.52",
                    "23.06"
                ],
                [
                    "Dual Context Src-Tgt-Mix",
                    "38.76†",
                    "39.52†",
                    "37.43†",
                    "21.68†",
                    "19.63†",
                    "27.71†",
                    "26.37†",
                    "23.26†",
                    "29.48†",
                    "[BOLD] 19.26",
                    "[BOLD] 14.86",
                    "23.01"
                ]
            ]
        },
        "gold_description": "for the europarl data , we see decent improvements with initdec for en - et ( + 1 . 11 bleu ) and en - de ( + 1 . 60 bleu ) , and with initdec + adddec for en - fr ( + 1 . 19 bleu ) . we also observe that , for all language - pairs , both translation directions benefit from context , on the other hand , for the subtitles data , we see a maximum improvement of + 0 . 30 bleu for initdec + adddec . the next set of experiments evaluates the five different approaches for computing the sourceside context . from table 2 that for english - estonian and english - german , our model indeed benefits from using is evident finally , our results with source , target and dual contexts are reported . interestingly , just using the source context is sufficient for english - estonian and english - german . for english - french , on the other hand , we see significant improvements for the models using the target - side conversation history over using only the source - side . unlike europarl , for subtitles , we see improvements for our src - tgtmix dual context variant over the src - tgt one for en → ru , to summarise , for majority of the cases our language - specific sentence - level attention is a winner or a close second . using the target context is useful when the base model generates reasonable - quality translations ; otherwise , using the source context should suffice .",
        "generated_description": "in this section , we present the results of our experiments on the wmt en - fr , en - de and en - et tasks , and the results on the subtitles of the english - russian and russian - english tasks . for the first group of experiments , we focus on the base and large models . we observe that the base model obtains the best results in terms of bleu scores for all language pairs . the base model outperforms the large model by a large margin in the majority of the cases . in particular , we observe that for the fr - en and de - en tasks , the large models outperform the base models by a big margin . the results show that the pre - trained embeddings learned from unlabeled data are beneficial for the supervised learning of multilingual bpe models . in addition , we also notice that the quality of the subtitle translations is affected by the size of the training data . for example , for the en - ru task , the subtitles provided in the europarl dataset are smaller in size than those provided by wmt , but the quality is still much better ."
    },
    {
        "table_id": "215",
        "table_info": {
            "table_caption": "Table 3: BLEU scores for the bilingual test sets. bold: Best performance, †: Statistically significantly better than the contextual baseline.",
            "table_column_names": [
                "[EMPTY]",
                "[BOLD] Europarl  [BOLD] En-Fr",
                "[BOLD] Europarl  [BOLD] En-Et",
                "[BOLD] Europarl  [BOLD] En-De",
                "[BOLD] Subtitles  [BOLD] En-Ru"
            ],
            "table_content_values": [
                [
                    "[ITALIC] Prev Sent",
                    "38.15",
                    "21.70",
                    "26.09",
                    "[BOLD] 19.13"
                ],
                [
                    "Our Model",
                    "[BOLD] 38.46†",
                    "[BOLD] 21.84",
                    "[BOLD] 26.49†",
                    "19.09"
                ]
            ]
        },
        "gold_description": "from table 3 , it can be seen that our model surpasses the local - context baseline for europarl showing that the wider context is indeed beneficial for en - ru , it can be seen that using previous sentence is sufficient",
        "generated_description": "our model consistently outperforms the prev sent baseline in terms of bleu scores for all language pairs and datasets , and is statistically significantly better than the contextual baseline in all cases . for the europarl en - fr and en - et translation tasks , our model outperforms prev sent by a significant margin ( 1 . 2 b @ 10 points ) , and is only slightly worse than the supervised baseline in the case of the subtitles task ( 0 . 1 b @ 5 points ) . for the subtitles task , we do not observe significant differences between our model and the contextual baselines ."
    },
    {
        "table_id": "216",
        "table_info": {
            "table_caption": "Table 4: BLEU scores for En-De bilingual test set.",
            "table_column_names": [
                "[BOLD] Type of Context",
                "[BOLD] BLEU"
            ],
            "table_content_values": [
                [
                    "No context (Base Model)",
                    "24.74"
                ],
                [
                    "Current Turn",
                    "26.39"
                ],
                [
                    "Current Language from Previous Turns",
                    "26.21"
                ],
                [
                    "Other Language from Previous Turns",
                    "26.32"
                ],
                [
                    "Complete Context",
                    "[BOLD] 26.49"
                ]
            ]
        },
        "gold_description": "we conduct an ablation study to validate our hypothesis of using the complete context versus using only one of the three types of contexts in a bilingual multi - speaker conversation : ( i ) current turn , ( ii ) previous turns in current language , and ( iii ) previous turns in the other language . the results for en - de are reported in table 4 . we see decrease in bleu for all types of contexts with significant decrease when considering only current language from previous turns . the results show that the current turn has the most influence on translating a sentence ,",
        "generated_description": "complete context refers to the model that uses all the information in the current dialogue history as input to the decoder instead of only the information of the current turn . we can see that the model with complete context obtains the best bleu score of 26 . 49 , which is 0 . 2 points higher than the no context model ( 24 . 74 ) ."
    },
    {
        "table_id": "217",
        "table_info": {
            "table_caption": "Table 1: Performance comparison between different state-of-the-art approaches on SCWS, in terms of Spearman’s correlation. UTDSM refers to the projected cross-topic representation, UTDSM Random refers to the case when random words served as anchors and GMM (c) corresponds to GMM smoothing with c components.",
            "table_column_names": [
                "Method",
                "AvgSimC",
                "MaxSimC"
            ],
            "table_content_values": [
                [
                    "Liu et al. ( 2015a )",
                    "67.3",
                    "68.1"
                ],
                [
                    "Liu et al. ( 2015b )",
                    "69.5",
                    "67.9"
                ],
                [
                    "Amiri et al. ( 2016 )",
                    "70.9",
                    "-"
                ],
                [
                    "Lee and Chen ( 2017 )",
                    "68.7",
                    "67.9"
                ],
                [
                    "Guo et al. ( 2018 )",
                    "69.3",
                    "68.2"
                ],
                [
                    "[ITALIC] 300-dimensions",
                    "[ITALIC] 300-dimensions",
                    "[ITALIC] 300-dimensions"
                ],
                [
                    "Global-DSM",
                    "67.1",
                    "67.1"
                ],
                [
                    "UTDSM Random",
                    "69.1±0.1",
                    "66.4±0.2"
                ],
                [
                    "UTDSM",
                    "[BOLD] 69.6",
                    "67.1"
                ],
                [
                    "UTDSM + GMM (1)",
                    "67.4",
                    "67.4"
                ],
                [
                    "UTDSM + GMM (2)",
                    "68.4",
                    "[BOLD] 68.3"
                ],
                [
                    "UTDSM + GMM (3)",
                    "68.9",
                    "[BOLD] 68.3"
                ],
                [
                    "UTDSM + GMM (8)",
                    "69.1",
                    "68.0"
                ],
                [
                    "UTDSM + GMM (10)",
                    "69.0",
                    "67.8"
                ],
                [
                    "[ITALIC] 500-dimensions",
                    "[ITALIC] 500-dimensions",
                    "[ITALIC] 500-dimensions"
                ],
                [
                    "Global-DSM",
                    "67.6",
                    "67.6"
                ],
                [
                    "UTDSM Random",
                    "69.4±0.1",
                    "66.5±0.3"
                ],
                [
                    "UTDSM",
                    "[BOLD] 70.2",
                    "68.0"
                ],
                [
                    "UTDSM + GMM (1)",
                    "67.6",
                    "67.6"
                ],
                [
                    "UTDSM + GMM (2)",
                    "68.8",
                    "[BOLD] 68.6"
                ],
                [
                    "UTDSM + GMM (3)",
                    "69.0",
                    "68.5"
                ],
                [
                    "UTDSM + GMM (8)",
                    "69.5",
                    "68.5"
                ],
                [
                    "UTDSM + GMM (10)",
                    "69.2",
                    "68.0"
                ]
            ]
        },
        "gold_description": "in table 1 we compare our model ( utdsm ) with our baseline ( global - dsm ) and other state - ofthe - art multi - prototype approaches for the contextual semantic similarity task . it is clear that all different setups of utdsm perform better than the baseline for both contextual semantic similarity metrics . using a single gaussian distribution ( utdsm + gmm ( 1 ) ) at the smoothing step of our method produces similar results to the baseline model . we also observe that random anchoring performs slightly worse than utdsm with respect to avgsimc . furthermore , we observe that gmm smoothing has a different effect on the maxsimc and avgsimc metrics . specifically , for avgsimc we consistently report lower results when gmm smoothing is applied for different number of components . at the same time , our smoothing technique highly improves the performance of maxsimc for all possible configurations . overall , the performance of our model is highly competitive to the state - of - the - art models in terms of avgsimc , for 500 - dimensional topic embeddings . we also achieve state - of - the - art performance for the maxsimc metric , using smoothed topic embeddings of 300 or 500 dimensions with 2 or 3 gaussian components .",
        "generated_description": "we observe that both the utdsm and the global - dsm perform better than the previous state - of - the - art methods on both the datasets . for the 300 - dimensional model , we observe that the best performance is achieved using a single random embedding ( row 1 ) , while for the 200 - dimensional models , the best performances are achieved using multiple random embeddings ( rows 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 ) and ( 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 , 24 , 25 ) . we also observe that when using the gmm , the performance of the proposed method is consistently and significantly higher than the other baselines . for example , the average score of the random model ( row 3 ) is 69 . 1 , while the best score for the 300 dimensional model is 68 . 3 . we also notice that the performance can be further improved when using more gmm layers . for instance , when increasing the number of hidden layers from 2 to 3 ( row 5 and 6 ) , the performance on the avgsimc and maxsimc metrics increases by 0 . 3 and 0 . 5 , respectively ."
    },
    {
        "table_id": "218",
        "table_info": {
            "table_caption": "Table 2: BLEU scores for domain match experiments",
            "table_column_names": [
                "[BOLD] Test domain",
                "[BOLD] Training Data  [BOLD] WSJ",
                "[BOLD] Training Data  [BOLD] WSJ + Giga"
            ],
            "table_content_values": [
                [
                    "WSJ",
                    "65.78",
                    "83.42"
                ],
                [
                    "Brown",
                    "45.00",
                    "76.99"
                ],
                [
                    "Wikipedia",
                    "35.90",
                    "62.26"
                ]
            ]
        },
        "gold_description": "we evaluate the in - and out - of - domain performance of our approach by training models on either wsj gold data only , or both wsj gold data and gigaword silver data , and evaluating on different domains . the results in table 2 show that while the generator performs best on test data which matches the training domain ( news ) , semisupervised training leads to substantial out - ofdomain improvements on the wikipedia and the brown corpus portions of the test set .",
        "generated_description": "for the first experiment , we train a domain match model on the wsj portion of the gigaword corpus and test it on the brown corpus . for the second experiment we use the same training data as in the previous experiment . we observe that the domain match experiments are very close to the ones performed by \\ newciteneelakantan - etal - 2018 - gigaword : the bleu score on the wikipedia test set is 62 . 26 compared to the 65 . 78 score obtained by the model trained on wsj alone ."
    },
    {
        "table_id": "219",
        "table_info": {
            "table_caption": "Table 2: Evaluation results of multi-class text classification.",
            "table_column_names": [
                "Method",
                "Precision",
                "Recall",
                "F1-score",
                "Accuracy"
            ],
            "table_content_values": [
                [
                    "LDA",
                    "39.7",
                    "41.8",
                    "38.8",
                    "41.8"
                ],
                [
                    "Global-DSM",
                    "62.9",
                    "63.3",
                    "62.9",
                    "63.3"
                ],
                [
                    "MaxCD",
                    "61.9",
                    "63.0",
                    "62.0",
                    "63.0"
                ],
                [
                    "AvgD",
                    "63.5",
                    "64.6",
                    "63.3",
                    "64.3"
                ],
                [
                    "AvgCD",
                    "[BOLD] 64.6",
                    "[BOLD] 65.5",
                    "[BOLD] 64.5",
                    "[BOLD] 65.5"
                ]
            ]
        },
        "gold_description": "evaluation results on text classification are presented in table 2 . we observe that our model performs better than the baseline across all metrics for both averaging approaches ( avgcd , avgd ) , while the usage of dominant topics appears to have lower performance ( maxcd ) . specifically , we get an improvement of 2 − 2 . 5 % on topic - based average and 0 . 5 − 1 % on simple average combination compared to using global - dsm .",
        "generated_description": "from the table , we can see that : ( 1 ) the proposed avgcd achieves the best performance in terms of precision , recall and f1 - score , which significantly outperforms the baselines by a large margin . ( 2 ) compared with the baseline methods lda and global - dsm , the performance of avgcd is much higher than these two methods , which indicates that the representation of multi - class text is better captured by avgcd . ( 3 ) compared to maxcd , avgd and avgcd also achieve better performance , which shows that the distributional decomposition is more suitable for multi - label text classification . ( 4 ) our proposed maxcd and avgd perform slightly worse than the original ddsm . we attribute this to the fact that the dimensionality of max - pooling layer is much smaller than that of the embedding layer , which may lead to the overfitting ."
    },
    {
        "table_id": "220",
        "table_info": {
            "table_caption": "Table 3: Evaluation results on paraphrase detection task.",
            "table_column_names": [
                "Method",
                "Precision",
                "Recall",
                "F1-score",
                "Accuracy"
            ],
            "table_content_values": [
                [
                    "Global-DSM",
                    "68.6",
                    "69.2",
                    "62.0",
                    "69.2"
                ],
                [
                    "MaxCD",
                    "[BOLD] 69.0",
                    "69.3",
                    "62.1",
                    "69.3"
                ],
                [
                    "AvgD",
                    "67.7",
                    "[BOLD] 69.4",
                    "[BOLD] 64.0",
                    "[BOLD] 69.4"
                ],
                [
                    "AvgCD",
                    "68.8",
                    "69.4",
                    "62.6",
                    "69.4"
                ]
            ]
        },
        "gold_description": "results for the paraphrase identification task are presented in table 3 . avgd yields the best results especially in f1 metric showing that cross - topic representations are semantically richer than single embeddings baseline ( global - dsm ) .",
        "generated_description": "we find that our proposed avgd and avgcd outperform the baselines by a large margin in terms of all the evaluation metrics . the global - dsm obtains the worst performance , which is consistent with the results on the sentiment analysis task . the maxcd method performs slightly better than the global - ddsm , but it is still worse than the other two methods . the reason may be that the max - pooling strategy used by maxcd makes the representation of the target sentence closer to that of the source sentence , which may reduce the discriminative power of the model . on the other hand , our proposed averaging method ( avgd ) does not suffer from the problem of overfitting ."
    },
    {
        "table_id": "221",
        "table_info": {
            "table_caption": "Table 2: Macro f1-score of unknown intent detection with different proportion (25%, 50% and 75%) of classes are treated as known intents on SNIPS and ATIS dataset.",
            "table_column_names": [
                "% of known intents",
                "SNIPS 25%",
                "50%",
                "75%",
                "ATIS 25%",
                "50%",
                "75%"
            ],
            "table_content_values": [
                [
                    "MSP",
                    "0.0",
                    "6.2",
                    "8.3",
                    "8.1",
                    "15.3",
                    "17.2"
                ],
                [
                    "DOC",
                    "72.5",
                    "67.9",
                    "63.9",
                    "61.6",
                    "62.8",
                    "37.7"
                ],
                [
                    "DOC (Softmax)",
                    "72.8",
                    "65.7",
                    "61.8",
                    "63.6",
                    "63.3",
                    "38.7"
                ],
                [
                    "LOF (Softmax)",
                    "76.0",
                    "69.4",
                    "65.8",
                    "67.3",
                    "61.8",
                    "38.9"
                ],
                [
                    "LOF (LMCL)",
                    "[BOLD] 79.2",
                    "[BOLD] 84.1",
                    "[BOLD] 78.8",
                    "[BOLD] 69.6",
                    "[BOLD] 63.4",
                    "[BOLD] 39.6"
                ]
            ]
        },
        "gold_description": "we show the experiment results in table 2 . our method consistently performs better than all baselines in all settings . our method is also better than lof ( softmax ) . we observe that on the atis dataset , the performance of unknown intent detection dramatically drops as the known intent increases .",
        "generated_description": "in this section , we investigate the performance of unknown intent detection when different proportions ( 25 % , 50 % , 75 % ) of the total number of classes are treated as known intents . for snips dataset , we vary the proportion of known intent detection classes to 25 % , 50 % and 75 % , and observe that the proposed method consistently outperforms msp , doc and lof with a macro - f1 - score of 79 . 2 % , 78 . 8 % and 63 . 4 % , respectively . for atis dataset , lof achieves the best performance with 25 % and 50 % reduction in classification rate , and outperforms doc and msp with 75 % reduction rate . in addition , we also observe that our method outperforms softmax - based method ( doc ) when less than 25 % of all classes are considered as known intent . we conjecture that this is due to the fact that the amount of negative samples in the negative sample is larger than that in the positive sample so that the negative samples can be more discriminative than that of positive samples ."
    },
    {
        "table_id": "222",
        "table_info": {
            "table_caption": "Table 1: Comparing AL with and without its truncated average, tracking time-indexed lag ALi=gi−i−1γ when |x|=|y|=4 for a wait-3 system.",
            "table_column_names": [
                "Statistics  [ITALIC] i",
                "Statistics 1",
                "Statistics 2",
                "Statistics 3",
                "Statistics 4",
                "Scores  [ITALIC] τ=2",
                "Scores  [ITALIC] τ=| [BOLD] y|"
            ],
            "table_content_values": [
                [
                    "[ITALIC] gi",
                    "3",
                    "4",
                    "4",
                    "4",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "AL [ITALIC] i",
                    "3",
                    "3",
                    "2",
                    "1",
                    "AL = 3",
                    "AL = 2.25"
                ]
            ]
        },
        "gold_description": "table 1 shows the time - indexed lags that are averaged to calculate al for a wait - 3 system . the lags make the problem clear : each position beyond the point where all source tokens have been read ( gi = | x | ) has its lag reduced by , pulling the average lag below k .",
        "generated_description": "we truncate the average of the four statistics when | x | = | y | = 4 for a wait - 3 system . the results show that the lag time - indexed ( al ) is very sensitive to the number of statistics ( statistics 1 , 2 , 3 and 4 ) , with a large lag ( al = 2 . 25 ) in most cases . however , this is not the case for statistics 1 and 2 , where the lag is very small ( al = 1 . 25 ) . this is because these two statistics have very different distributions in the data ."
    },
    {
        "table_id": "223",
        "table_info": {
            "table_caption": "Table 4: BLEU scores for evaluating AMR and DMRS generators on an AMR test set",
            "table_column_names": [
                "[BOLD] Representation",
                "[BOLD] Train on  [BOLD] Gold",
                "[BOLD] Train on  [BOLD] Gold+Silver"
            ],
            "table_content_values": [
                [
                    "AMR",
                    "22.0",
                    "33.8"
                ],
                [
                    "DMRS - no attributes",
                    "40.1",
                    "63.6"
                ],
                [
                    "DMRS - all attributes",
                    "56.9",
                    "75.8"
                ]
            ]
        },
        "gold_description": "we compare our approach to amr - to - text generation by evaluating our generator on a standard amr test set for models trained on gold as well as gold plus silver data . 9 we evaluate dmrs models both with and without predicate and edge attributes , the results in table 4 show that our mrs generator performs better than the amr generator by a large margin , even when the additional mrs attributes are excluded .",
        "generated_description": "dmrs outperforms amr in terms of bleu score when trained and evaluated on the gold amr and silver amr test set . the difference in performance between dmrs and amr is larger when attributes are removed from the representation ( i . e . , dmrs - no attributes ) rather than when all attributes are used ( amr - all attributes ) . this is due to the fact that the dmrs generator is able to generate more realistic dialogs by considering all attributes when generating responses ."
    },
    {
        "table_id": "224",
        "table_info": {
            "table_caption": "Table 3: Accuracy of the model trained only on MultiNLI on our datasets, which are used as probing datasets. The Complexity column refers to the syntactic complexity of the sentences.",
            "table_column_names": [
                "[BOLD] Complexity",
                "[BOLD] Ent.",
                "[BOLD] Neutral",
                "[BOLD] Cont.",
                "[BOLD] All"
            ],
            "table_content_values": [
                [
                    "MultiNLI Matched Dev Set",
                    "MultiNLI Matched Dev Set",
                    "MultiNLI Matched Dev Set",
                    "MultiNLI Matched Dev Set",
                    "MultiNLI Matched Dev Set"
                ],
                [
                    "[BOLD] All",
                    "83.56",
                    "84.12",
                    "86.37",
                    "[BOLD] 84.66"
                ],
                [
                    "Dative Alternation",
                    "Dative Alternation",
                    "Dative Alternation",
                    "Dative Alternation",
                    "Dative Alternation"
                ],
                [
                    "[BOLD] Simple",
                    "100",
                    "-",
                    "4.22",
                    "[BOLD] 52.63"
                ],
                [
                    "[BOLD] Medium",
                    "100",
                    "-",
                    "2.16",
                    "[BOLD] 49.27"
                ],
                [
                    "[BOLD] Complex",
                    "99.77",
                    "-",
                    "0.36",
                    "[BOLD] 50.45"
                ],
                [
                    "[BOLD] All",
                    "99.92",
                    "-",
                    "2.25",
                    "[BOLD] 50.78"
                ],
                [
                    "Numerical Reasoning",
                    "Numerical Reasoning",
                    "Numerical Reasoning",
                    "Numerical Reasoning",
                    "Numerical Reasoning"
                ],
                [
                    "[BOLD] Simple",
                    "38.14",
                    "0.66",
                    "69.53",
                    "[BOLD] 45.04"
                ],
                [
                    "[BOLD] Medium",
                    "57.14",
                    "1.36",
                    "50.14",
                    "[BOLD] 38.11"
                ],
                [
                    "[BOLD] Complex",
                    "55.48",
                    "3.04",
                    "46.26",
                    "[BOLD] 36.15"
                ],
                [
                    "[BOLD] All",
                    "50.25",
                    "1.69",
                    "55.31",
                    "[BOLD] 39.77"
                ]
            ]
        },
        "gold_description": "the model has relatively low accuracy on the entailment and contradiction examples while close to zero accuracy on the neutral ones . the accuracy of the model on each test set . on our dative alternation dataset , the accuracy on our test sets is substantially lower than on the multinli development set ( 50 . 78 % versus 84 . 66 % respectively ) , the model has very high accuracy on the entailment examples , while close to zero on the contradiction ones . on the numerical reasoning dataset , the model also seems to fail on this inference type with test set accuracy much lower than on the multinli development set ,",
        "generated_description": "s4ss3sss0px3 performance on multinli . we evaluate the model ’ s ability to solve the multi - numerical reasoning problems using both the scored entry for each entity type as well as the score for all the entities present in the test set . this allows us to see how the model performs for different types of entity types . we report the accuracy for simple , medium , and complex entity types in both the development and test sets . as expected , models that have access to more complex entities perform better than those with simple and medium complexity . surprisingly , models with large complexity outperform those with small complexity . we hypothesize that this is due to the fact that the model has access to a larger number of hyperparameters during training , which allows the model to learn more complex features ."
    },
    {
        "table_id": "225",
        "table_info": {
            "table_caption": "Table 2: BLEU and METEOR scores for the sentence-level baseline (S-NMT) vs. variants of our Document NMT model. bold: Best performance, †: Statistically significantly better than the baseline.",
            "table_column_names": [
                "[EMPTY]",
                "[BOLD] Memory-to-Context BLEU",
                "[BOLD] Memory-to-Context BLEU",
                "[BOLD] Memory-to-Context BLEU",
                "[BOLD] Memory-to-Context BLEU",
                "[BOLD] Memory-to-Context METEOR",
                "[BOLD] Memory-to-Context METEOR",
                "[BOLD] Memory-to-Context METEOR",
                "[BOLD] Memory-to-Context METEOR",
                "[BOLD] Memory-to-Output BLEU",
                "[BOLD] Memory-to-Output BLEU",
                "[BOLD] Memory-to-Output BLEU",
                "[BOLD] Memory-to-Output BLEU",
                "[BOLD] Memory-to-Output METEOR",
                "[BOLD] Memory-to-Output METEOR",
                "[BOLD] Memory-to-Output METEOR",
                "[BOLD] Memory-to-Output METEOR"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "Fr→En",
                    "De→En",
                    "De→En",
                    "Et→En",
                    "Fr→En",
                    "De→En",
                    "De→En",
                    "Et→En",
                    "Fr→En",
                    "De→En",
                    "De→En",
                    "Et→En",
                    "Fr→En",
                    "De→En",
                    "De→En",
                    "Et→En"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "NC-11",
                    "NC-16",
                    "[EMPTY]",
                    "[EMPTY]",
                    "NC-11",
                    "NC-16",
                    "[EMPTY]",
                    "[EMPTY]",
                    "NC-11",
                    "NC-16",
                    "[EMPTY]",
                    "[EMPTY]",
                    "NC-11",
                    "NC-16",
                    "[EMPTY]"
                ],
                [
                    "[ITALIC] S-NMT",
                    "20.85",
                    "5.24",
                    "9.18",
                    "20.42",
                    "23.27",
                    "10.90",
                    "14.35",
                    "24.65",
                    "20.85",
                    "5.24",
                    "9.18",
                    "20.42",
                    "23.27",
                    "10.90",
                    "14.35",
                    "24.65"
                ],
                [
                    "[ITALIC]  +src",
                    "21.91†",
                    "6.26†",
                    "10.20†",
                    "22.10†",
                    "24.04†",
                    "11.52†",
                    "15.45†",
                    "25.92†",
                    "[BOLD] 21.80†",
                    "6.10†",
                    "9.98†",
                    "21.50†",
                    "23.99†",
                    "11.53†",
                    "15.29†",
                    "25.44†"
                ],
                [
                    "[ITALIC]  +trg",
                    "21.74†",
                    "6.24†",
                    "9.97†",
                    "21.94†",
                    "23.98†",
                    "11.58†",
                    "15.32†",
                    "25.89†",
                    "21.76†",
                    "[BOLD] 6.31†",
                    "10.04†",
                    "21.82†",
                    "24.06†",
                    "[BOLD] 12.10†",
                    "15.75†",
                    "25.93†"
                ],
                [
                    "[ITALIC]  +both",
                    "[BOLD] 22.00†",
                    "[BOLD] 6.57†",
                    "[BOLD] 10.54†",
                    "[BOLD] 22.32†",
                    "[BOLD] 24.40†",
                    "[BOLD] 12.24†",
                    "[BOLD] 16.18†",
                    "[BOLD] 26.34†",
                    "21.77†",
                    "6.20†",
                    "[BOLD] 10.23†",
                    "[BOLD] 22.20†",
                    "[BOLD] 24.27†",
                    "11.84†",
                    "[BOLD] 15.82†",
                    "[BOLD] 26.10†"
                ]
            ]
        },
        "gold_description": "we consistently observe + 1 . 15 / + 1 . 13 bleu / meteor score improvements across the three language pairs upon comparing our best model to s - nmt ( see table 2 ) . overall , our document nmt model with both memories has been the most effective variant for all of the three language pairs . from table 2 , we consistently see + . 95 / + 1 . 00 bleu / meteor improvements between the best variants of our model and the sentence - level baseline across the three lan for german → english , guage pairs . for french → english , all variants of document nmt model show comparable performance when using bleu ; however , when evaluated using meteor , the dual memory model is the best . the target memory variants give comparable results , whereas for estonian → english , the dual memory variant proves to be the best . overall , the memory - tocontext model variants perform better than their memory - to - output counterparts .",
        "generated_description": "in this section , we compare the performance of our memory - to - context model with the baselines . the results show that our model consistently outperforms the nc - 11 and nc - 10 models in terms of bleu and meteor scores , especially in the memory retrieval task . this is because our model can retrieve more semantic information from the context and then use this information to improve the translation performance ."
    },
    {
        "table_id": "226",
        "table_info": {
            "table_caption": "Table 2: Probing tasks performance of vector averaging (AVG) and max pooling (MAX) vs. DCT embeddings with various K. Majority (baseline), Human (human-bound), and a linear classifier with sentence length as sole feature (Length) as reported in Conneau et al. (2018), respectively.",
            "table_column_names": [
                "Model",
                "[BOLD] Surface SentLen",
                "[BOLD] Surface WC",
                "[BOLD] Syntactic TreeDepth",
                "[BOLD] Syntactic TopConst",
                "[BOLD] Syntactic BShift",
                "[BOLD] Semantic Tense",
                "[BOLD] Semantic SubjNum",
                "[BOLD] Semantic ObjNum",
                "[BOLD] Semantic SOMO",
                "[BOLD] Semantic CoordInv"
            ],
            "table_content_values": [
                [
                    "Majority",
                    "20.0",
                    "0.5",
                    "17.9",
                    "5.0",
                    "50.0",
                    "50.0",
                    "50.0",
                    "50.0",
                    "50.0",
                    "50.0"
                ],
                [
                    "Human",
                    "100",
                    "100",
                    "84.0",
                    "84.0",
                    "98.0",
                    "85.0",
                    "88.0",
                    "86.5",
                    "81.2",
                    "85.0"
                ],
                [
                    "Length",
                    "100",
                    "0.2",
                    "18.1",
                    "9.3",
                    "50.6",
                    "56.5",
                    "50.3",
                    "50.1",
                    "50.2",
                    "50.0"
                ],
                [
                    "AVG",
                    "64.12",
                    "82.1",
                    "36.38",
                    "68.04",
                    "50.16",
                    "87.9",
                    "80.89",
                    "80.24",
                    "50.39",
                    "51.95"
                ],
                [
                    "MAX",
                    "62.67",
                    "88.97",
                    "33.02",
                    "62.63",
                    "50.31",
                    "85.66",
                    "77.11",
                    "76.04",
                    "51.86",
                    "52.33"
                ],
                [
                    "[ITALIC] c[0]",
                    "[BOLD] 98.67",
                    "[BOLD] 91.11",
                    "38.6",
                    "70.54",
                    "50.42",
                    "88.25",
                    "80.88",
                    "80.56",
                    "[BOLD] 55.6",
                    "55"
                ],
                [
                    "[ITALIC] c[0:1]",
                    "97.18",
                    "89.16",
                    "40.41",
                    "78.34",
                    "52.25",
                    "88.58",
                    "86.59",
                    "84.36",
                    "54.62",
                    "70.42"
                ],
                [
                    "[ITALIC] c[0:2]",
                    "95.84",
                    "86.77",
                    "43.01",
                    "80.41",
                    "54.84",
                    "88.87",
                    "88.06",
                    "86.26",
                    "53.07",
                    "71.87"
                ],
                [
                    "[ITALIC] c[0:3]",
                    "94.63",
                    "84.96",
                    "[BOLD] 43.35",
                    "81.01",
                    "57.29",
                    "88.88",
                    "88.36",
                    "86.51",
                    "53.79",
                    "[BOLD] 72.01"
                ],
                [
                    "[ITALIC] c[0:4]",
                    "93.25",
                    "83.24",
                    "43.26",
                    "81.49",
                    "60.31",
                    "[BOLD] 88.91",
                    "[BOLD] 88.65",
                    "87.15",
                    "52.77",
                    "71.91"
                ],
                [
                    "[ITALIC] c[0:5]",
                    "92.29",
                    "81.84",
                    "42.75",
                    "[BOLD] 81.60",
                    "62.01",
                    "88.82",
                    "88.44",
                    "87.98",
                    "52.38",
                    "70.96"
                ],
                [
                    "[ITALIC] c[0:6]",
                    "91.56",
                    "79.83",
                    "43.05",
                    "81.41",
                    "[BOLD] 62.59",
                    "88.87",
                    "[BOLD] 88.65",
                    "[BOLD] 88.28",
                    "52.07",
                    "70.63"
                ]
            ]
        },
        "gold_description": "we report the performance in probing tasks in table 2 . in general , dct yields better performance compared to averaging on all tasks , and larger k often yields improved performance in syntactic and semantic tasks . for the surface information tasks , sentlen and word content ( wc ) , c significantly outperforms avg . the performance decreased with increasing k in c [ 0 : k ] , while increasing k has no positive effect on surface information tasks , syntactic and semantic tasks demonstrate performance gains with larger k . this trend is clearly observed in all syntactic tasks and three of the semantic tasks , where dct performs well above avg and the performance improves with increasing k . the only exception is somo , where increasing k actually results in lower performance , although all dct results are still higher than avg .",
        "generated_description": "the results show that humans achieve 100 % accuracy on sentlen , 100 % , 100 % , and 100 % on treedepth , topconst , bshift , subjnum , somo , coordinv , and somo . the average accuracy of length and average number of words per sentence ( avg ) are 50 . 2 % and 50 . 0 % , respectively . we can see that humans perform better than the greedy parser and greedy parser on most tasks , which shows that greedy parser is better at finding the correct length of a sentence than the other two parsers ."
    },
    {
        "table_id": "227",
        "table_info": {
            "table_caption": "Table 3: DCT embedding Performance in SentEval downstream tasks compared to vector averaging (AVG) and max pooling (MAX).",
            "table_column_names": [
                "Model",
                "[BOLD] Sentiment Analysis MR",
                "[BOLD] Sentiment Analysis SST2",
                "[BOLD] Sentiment Analysis SST5",
                "[BOLD] Sentiment Analysis CR",
                "[BOLD] Sentiment Analysis MPQA",
                "[BOLD] SUBJ",
                "[BOLD] Relatedness/Paraphrase SICK-R",
                "[BOLD] Relatedness/Paraphrase STSB",
                "[BOLD] Relatedness/Paraphrase MRPC",
                "[BOLD] Inference SICK-E",
                "[BOLD] TREC"
            ],
            "table_content_values": [
                [
                    "AVG",
                    "78.3",
                    "[BOLD] 84.13",
                    "44.16",
                    "79.6",
                    "87.94",
                    "92.33",
                    "81.95",
                    "69.26",
                    "74.43",
                    "79.5",
                    "83.2"
                ],
                [
                    "MAX",
                    "73.31",
                    "79.24",
                    "41.86",
                    "73.35",
                    "86.54",
                    "88.02",
                    "81.93",
                    "[BOLD] 71.57",
                    "72.5",
                    "77.98",
                    "76.2"
                ],
                [
                    "[ITALIC] c[0]",
                    "[BOLD] 78.45",
                    "83.53",
                    "44.57",
                    "79.81",
                    "[BOLD] 88.36",
                    "[BOLD] 92.79",
                    "82.61",
                    "71.11",
                    "72.93",
                    "78.91",
                    "84.8"
                ],
                [
                    "[ITALIC] c[0:1]",
                    "78.15",
                    "83.47",
                    "[BOLD] 46.06",
                    "79.84",
                    "87.76",
                    "92.61",
                    "82.73",
                    "70.82",
                    "72.81",
                    "79.64",
                    "88.2"
                ],
                [
                    "[ITALIC] c[0:2]",
                    "78.02",
                    "82.98",
                    "45.16",
                    "79.68",
                    "87.62",
                    "92.5",
                    "[BOLD] 82.95",
                    "70.36",
                    "72.87",
                    "79.76",
                    "[BOLD] 89.8"
                ],
                [
                    "[ITALIC] c[0:3]",
                    "77.81",
                    "83.8",
                    "45.79",
                    "79.66",
                    "87.54",
                    "92.4",
                    "82.93",
                    "69.79",
                    "73.57",
                    "[BOLD] 80.56",
                    "88.2"
                ],
                [
                    "[ITALIC] c[0:4]",
                    "77.72",
                    "83.75",
                    "44.03",
                    "[BOLD] 80.08",
                    "87.4",
                    "92.61",
                    "82.53",
                    "69.31",
                    "72.35",
                    "79.72",
                    "[BOLD] 89.8"
                ],
                [
                    "[ITALIC] c[0:5]",
                    "77.42",
                    "82.43",
                    "43.3",
                    "78.6",
                    "87.21",
                    "92.19",
                    "82.36",
                    "68.9",
                    "73.91",
                    "79.89",
                    "88.8"
                ],
                [
                    "[ITALIC] c[0:6]",
                    "77.47",
                    "82.81",
                    "42.99",
                    "78.78",
                    "87.06",
                    "92.15",
                    "81.86",
                    "68.17",
                    "[BOLD] 75.07",
                    "79.76",
                    "86.4"
                ]
            ]
        },
        "gold_description": "our results in table 3 are consistent with these observations , where we see improvements in most tasks , but the difference is not as significant as the probing tasks , except in trec question classification where increasing k leads to much better performance .",
        "generated_description": "for sentiment analysis , our proposed c [ 0 , c [ 0 ] achieves the best performance on 7 out of 10 tasks . for the semantic relatedness / paraphrase task , our model outperforms all other models and the baselines on average . the improvements are statistically significant ( t - test with p - value < 0 . 01 ) on five tasks and two tasks , with mcnemar ’ s test indicating statistical significance ."
    },
    {
        "table_id": "228",
        "table_info": {
            "table_caption": "Table 4: Performance in text classification (20-NG, R-8) and sentiment (SST-5) tasks of various models as reported in Kayal and Tsatsaronis (2019), where DCT* refers to the implementation in Kayal and Tsatsaronis (2019). Our DCT embeddings are denoted as c[k] in the bottom row. Bold indicates the best result, and italic indicates second-best.",
            "table_column_names": [
                "Model",
                "[BOLD] 20-NG P",
                "[BOLD] 20-NG R",
                "[BOLD] 20-NG F1",
                "[BOLD] R-8 P",
                "[BOLD] R-8 R",
                "[BOLD] R-8 F1",
                "[BOLD] SST-5 P",
                "[BOLD] SST-5 R",
                "[BOLD] SST-5 F1"
            ],
            "table_content_values": [
                [
                    "PCA",
                    "55.43",
                    "54.67",
                    "54.77",
                    "83.83",
                    "83.42",
                    "83.41",
                    "26.47",
                    "25.08",
                    "25.23"
                ],
                [
                    "DCT*",
                    "61.07",
                    "59.16",
                    "59.78",
                    "90.41",
                    "90.78",
                    "90.38",
                    "30.11",
                    "30.09",
                    "29.53"
                ],
                [
                    "Avg. vec.",
                    "68.72",
                    "68.19",
                    "68.25",
                    "96.34",
                    "96.30",
                    "96.27",
                    "27.88",
                    "26.44",
                    "24.81"
                ],
                [
                    "p-means",
                    "[ITALIC] 72.20",
                    "[ITALIC] 71.65",
                    "[BOLD] 71.79",
                    "96.69",
                    "96.67",
                    "96.65",
                    "33.77",
                    "33.41",
                    "33.26"
                ],
                [
                    "ELMo",
                    "71.20",
                    "[BOLD] 71.79",
                    "71.36",
                    "94.54",
                    "91.32",
                    "91.32",
                    "[ITALIC] 42.35",
                    "[ITALIC] 41.51",
                    "[ITALIC] 41.54"
                ],
                [
                    "BERT",
                    "70.89",
                    "70.79",
                    "70.88",
                    "95.52",
                    "95.39",
                    "95.39",
                    "39.92",
                    "39.38",
                    "39.35"
                ],
                [
                    "EigenSent",
                    "66.98",
                    "66.40",
                    "66.54",
                    "95.91",
                    "95.80",
                    "95.76",
                    "35.32",
                    "33.69",
                    "33.91"
                ],
                [
                    "EigenSent⊕Avg",
                    "[BOLD] 72.24",
                    "71.62",
                    "[ITALIC] 71.78",
                    "[BOLD] 97.18",
                    "[BOLD] 97.13",
                    "[BOLD] 97.14",
                    "[BOLD] 42.77",
                    "[BOLD] 41.67",
                    "[BOLD] 41.81"
                ],
                [
                    "c[k]",
                    "[ITALIC] 72.20",
                    "71.58",
                    "71.73",
                    "[ITALIC] 96.98",
                    "[ITALIC] 96.98",
                    "[ITALIC] 96.94",
                    "37.67",
                    "34.47",
                    "34.54"
                ]
            ]
        },
        "gold_description": "table 4 shows the best results for the various models as reported in kayal and tsat saronis ( 2019 ) , in addition to the best performance of our model denoted as c [ k ] . note that the dct - based model , dct * , described in kayal and tsatsaronis ( 2019 ) performed relatively poorly in all tasks , while our model achieved close to state - of - the - art performance in both the 20 - ng and r - 8 tasks . our model outperformed eignsent on all tasks and generally performed better than or on par with p - means , elmo , bert , and eigensent ⊕ avg on both the 20 - ng and r - 8 . on the other hand , both eigensent ⊕ avg and elmo performed better than all other models on sst - 5 .",
        "generated_description": "our p - means model outperforms the baselines by a large margin in terms of all the evaluation metrics on both 20 - ng and sst - 5 datasets . our model also achieves the best recall and f1 scores among all the baseline models , which verifies the effectiveness of our proposed statistical weighting scheme . in particular , our p - meanans model achieves a recall of 72 . 20 % and an f1 score of 71 . 79 % on the 20 and 30 - ng datasets , respectively ."
    },
    {
        "table_id": "229",
        "table_info": {
            "table_caption": "Table 1: Coverage of words from the manual transcripts in the DSTC2 development set of different batch ASR output types (%). In the pruned cnet interjections and hypotheses with scores below 0.001 were removed.",
            "table_column_names": [
                "[EMPTY]",
                "1-best",
                "10-best",
                "cnet",
                "pruned cnet"
            ],
            "table_content_values": [
                [
                    "all words",
                    "69.3",
                    "78.6",
                    "85.7",
                    "83.1"
                ],
                [
                    "slots/values",
                    "69.8",
                    "75.6",
                    "82.4",
                    "80.6"
                ]
            ]
        },
        "gold_description": "as shown in table 1 , this does not discard too many correct hypotheses but markedly reduces the size of the cnet to an average of seven timesteps with two hypotheses .",
        "generated_description": "in the pruned cnet model , all words but slots and values were removed , and the interjections and hypotheses with scores below 0 . 001 were removed . the coverage of the 1 - best and 10 - best subsets of the dstc2 development set was 69 . 3 % and 75 . 6 % , respectively . the pruned model outperforms the original cnet in terms of coverage of all words and slots , but loses in coverage of slots / values . this suggests that the positional information plays an important role in the asr output , which is beyond the scope of this work ."
    },
    {
        "table_id": "230",
        "table_info": {
            "table_caption": "Table 3: DSTC2 test set accuracy of ten runs with different random seeds in the format average maximumminimum. ⋆ denotes a statistically significant higher result than the baseline (p<0.05, Wilcoxon signed-rank test with Bonferroni correction for ten repeated comparisons). The cnet ensemble corresponds to the best cnet model with pruning threshold 0.001 and weighted pooling.",
            "table_column_names": [
                "[BOLD] method",
                "[BOLD] goals",
                "[BOLD] requests"
            ],
            "table_content_values": [
                [
                    "1-best baseline",
                    "63.6 66.658.7",
                    "96.8 97.196.5"
                ],
                [
                    "[ITALIC] cnet - no pruning",
                    "[ITALIC] cnet - no pruning",
                    "[ITALIC] cnet - no pruning"
                ],
                [
                    "weighted pooling",
                    "63.7 65.661.6",
                    "96.7 97.096.3"
                ],
                [
                    "[ITALIC] cnet - score threshold 0.001",
                    "[ITALIC] cnet - score threshold 0.001",
                    "[ITALIC] cnet - score threshold 0.001"
                ],
                [
                    "average pooling",
                    "63.7 66.460.0",
                    "96.6 96.896.0"
                ],
                [
                    "weighted pooling",
                    "[BOLD] 65.2 68.559.1",
                    "97.0 97.496.6"
                ],
                [
                    "[ITALIC] cnet - score threshold 0.01",
                    "[ITALIC] cnet - score threshold 0.01",
                    "[ITALIC] cnet - score threshold 0.01"
                ],
                [
                    "average pooling",
                    "64.6 67.959.7",
                    "96.9⋆ 97.296.5"
                ],
                [
                    "weighted pooling",
                    "64.7 68.462.2",
                    "[BOLD] 97.1⋆ 97.396.9"
                ],
                [
                    "[ITALIC] ensemble models",
                    "[ITALIC] ensemble models",
                    "[ITALIC] ensemble models"
                ],
                [
                    "baseline",
                    "69.7",
                    "96.7"
                ],
                [
                    "cnet",
                    "[BOLD] 71.4",
                    "[BOLD] 97.2"
                ],
                [
                    "[ITALIC] results from related work",
                    "[ITALIC] results from related work",
                    "[ITALIC] results from related work"
                ],
                [
                    "Vodolán et al. ( 2017 )",
                    "80.0",
                    "-"
                ],
                [
                    "Williams ( 2014 )",
                    "78.4",
                    "98.0"
                ],
                [
                    "Mrksic et al. ( 2017 )",
                    "73.4",
                    "96.5"
                ]
            ]
        },
        "gold_description": "table 3 displays the results for our model evaluated on cnets for increasingly aggressive pruning levels ( discarding only interjections , additionally discarding hypotheses with scores below 0 . 001 and 0 . 01 , respectively ) . as can be seen , using the full cnet except for interjections does not improve over the baseline . however , when pruning low - probability hypotheses both pooling strategies improve over the baseline . yet , average pooling performs worse for the lower pruning threshold , which shows that the model is still affected by noise among the hypotheses . weighted pooling performs better for the lower pruning threshold of 0 . 001 with which we obtain the highest result overall , improving the joint goals accuracy by 1 . 6 percentage points compared to the baseline . moreover , we see that an ensemble model that averages the predictions of ten cnet models trained with different random seeds also outperforms an ensemble of ten baseline models . our ensemble models outperform mrksic et al . ( 2017 ) for the joint requests but are a bit worse for the joint goals .",
        "generated_description": "we can see that weighted pooling and average pooling both achieve better performance than the 1 - best baseline . we can also see that the score threshold pruning is more important for cnet - based pooling , as the performance degrades significantly ( 1 . 6 % goals and 3 . 3 % requests ) ."
    },
    {
        "table_id": "231",
        "table_info": {
            "table_caption": "Table 2: DSTC2 test set accuracy for 1-best ASR outputs of ten runs with different random seeds in the format average maximumminimum.",
            "table_column_names": [
                "[BOLD] test data",
                "[BOLD] goals",
                "[BOLD] requests"
            ],
            "table_content_values": [
                [
                    "[ITALIC] train on transcripts + batch ASR (baseline)",
                    "[ITALIC] train on transcripts + batch ASR (baseline)",
                    "[ITALIC] train on transcripts + batch ASR (baseline)"
                ],
                [
                    "[ITALIC] batch ASR",
                    "63.6 66.658.7",
                    "96.8 97.196.5"
                ],
                [
                    "[ITALIC] train on transcripts + live ASR",
                    "[ITALIC] train on transcripts + live ASR",
                    "[ITALIC] train on transcripts + live ASR"
                ],
                [
                    "[ITALIC] live ASR",
                    "63.8 67.060.2",
                    "97.5 97.797.2"
                ],
                [
                    "transcripts",
                    "78.3 82.474.3",
                    "98.7 99.098.0"
                ]
            ]
        },
        "gold_description": "as can be seen from table 2 , the dst accuracy slightly increases for the higher - quality live asr outputs . more importantly , the dst performance drastically increases , when we evaluate on the manual transcripts that reflect the true user utterances nearly perfectly .",
        "generated_description": "in the first experiment , we train the baseline asr model using a small amount of in - domain training data and apply it to a large amount of out - of - domain asr training data . in the second experiment , the asr models are trained on a live asr output of the test set instead of a generated transcript from the training set . the results show that the models trained on in - house transcripts perform much better than the ones trained on out - domain data . this is in line with our hypothesis that asr errors are more a result of asr error rather than asr difficulty . the third experiment simulates the scenario where the system is presented with asr outputs generated from a large number of different asr runs with different random seeds in the target language . the asr system trained on the live training set performs the best and achieves the highest accuracy for all goals and requests ."
    },
    {
        "table_id": "232",
        "table_info": {
            "table_caption": "Table 2: Sentences/clauses after sentence splitting.",
            "table_column_names": [
                "[BOLD] Gold Class",
                "[BOLD] Train",
                "[BOLD] Dev",
                "[BOLD] Test"
            ],
            "table_content_values": [
                [
                    "None",
                    "15,401",
                    "3,905",
                    "4,141"
                ],
                [
                    "Obligation",
                    "11,005",
                    "2,860",
                    "970"
                ],
                [
                    "Prohibition",
                    "1,172",
                    "314",
                    "108"
                ],
                [
                    "Obligation List Intro",
                    "828",
                    "203",
                    "70"
                ],
                [
                    "Obligation List Item",
                    "2888",
                    "726",
                    "255"
                ],
                [
                    "Prohibition List Item",
                    "251",
                    "28",
                    "19"
                ],
                [
                    "[BOLD] Total",
                    "[BOLD] 31,545",
                    "[BOLD] 8,036",
                    "[BOLD] 5,563"
                ]
            ]
        },
        "gold_description": "fourth , we introduce finer classes ( tables 1 – 2 ) , which fit better the target task , where nested clauses are frequent . the splitter produced 31 , 545 training , 8 , 036 development , and 5 , 563 test sentences / clauses . 3 table 2 shows their distribution in the six gold ( correct ) classes . cf . table 2 ) ,",
        "generated_description": "after sentence splitting , the dataset is split into training , development , and test sets , with the development set used as the test set . the labels of the first sentence of each document are used to construct the labels for the next sentence . during the sentence splitting process , the labels are merged back into the gold class labels to form the final sentence . this process is repeated for all sentences of the gold labels . the final sentence is the one with the highest cosine similarity with the gold label ."
    },
    {
        "table_id": "233",
        "table_info": {
            "table_caption": "Table 1: Manually aligned News Commentary data.",
            "table_column_names": [
                "Ru",
                "Ja",
                "En",
                "#sent.",
                "Usage test",
                "Usage development"
            ],
            "table_content_values": [
                [
                    "✓",
                    "✓",
                    "✓",
                    "913",
                    "600",
                    "313"
                ],
                [
                    "✓",
                    "✓",
                    "[EMPTY]",
                    "173",
                    "-",
                    "173"
                ],
                [
                    "[EMPTY]",
                    "✓",
                    "✓",
                    "276",
                    "-",
                    "276"
                ],
                [
                    "✓",
                    "[EMPTY]",
                    "✓",
                    "0",
                    "-",
                    "-"
                ],
                [
                    "✓",
                    "[EMPTY]",
                    "[EMPTY]",
                    "4",
                    "-",
                    "-"
                ],
                [
                    "[EMPTY]",
                    "✓",
                    "[EMPTY]",
                    "287",
                    "-",
                    "-"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "✓",
                    "1",
                    "-",
                    "-"
                ],
                [
                    "Total",
                    "Total",
                    "Total",
                    "1,654",
                    "-",
                    "-"
                ]
            ]
        },
        "gold_description": "as a result , we obtained 1 , 654 lines of data comprising trilingual , bilingual , and monolingual segments ( mainly sentences ) as summarized in table 1 . finally , for the sake of comparability , we randomly chose 600 trilingual sentences to create a test set , and concatenated the rest of them and bilingual sentences to form development sets .",
        "generated_description": "the first set of columns shows the amount of manually aligned data that was used for training the models . the second set shows the number of sentences that were used during test and development . the third set shows that the models were trained using only the manually aligned news commentary data . the last column shows the performance of the models trained using the automatically aligned data without any cross - lingual parameter tuning ."
    },
    {
        "table_id": "234",
        "table_info": {
            "table_caption": "Table 3: Precision, recall, f1, and auc scores, with the best results in bold and gray background.",
            "table_column_names": [
                "[BOLD] Gold Class",
                "[BOLD] bilstm P",
                "[BOLD] bilstm R",
                "[BOLD] bilstm F1",
                "[BOLD] bilstm AUC",
                "[BOLD] bilstm-att P",
                "[BOLD] bilstm-att R",
                "[BOLD] bilstm-att F1",
                "[BOLD] bilstm-att AUC",
                "[BOLD] x-bilstm-att P",
                "[BOLD] x-bilstm-att R",
                "[BOLD] x-bilstm-att F1",
                "[BOLD] x-bilstm-att AUC",
                "[BOLD] h-bilstm-att P",
                "[BOLD] h-bilstm-att R",
                "[BOLD] h-bilstm-att F1",
                "[BOLD] h-bilstm-att AUC"
            ],
            "table_content_values": [
                [
                    "None",
                    "0.95",
                    "0.91",
                    "0.93",
                    "0.98",
                    "0.97",
                    "0.90",
                    "0.93",
                    "[BOLD] 0.99",
                    "0.96",
                    "0.90",
                    "0.93",
                    "0.98",
                    "[BOLD] 0.98",
                    "[BOLD] 0.96",
                    "[BOLD] 0.97",
                    "[BOLD] 0.99"
                ],
                [
                    "Obligation",
                    "0.75",
                    "0.85",
                    "0.79",
                    "0.86",
                    "0.75",
                    "0.88",
                    "0.81",
                    "0.86",
                    "0.75",
                    "0.87",
                    "0.81",
                    "0.88",
                    "[BOLD] 0.87",
                    "[BOLD] 0.92",
                    "[BOLD] 0.90",
                    "[BOLD] 0.96"
                ],
                [
                    "Prohibition",
                    "0.67",
                    "0.62",
                    "0.64",
                    "0.75",
                    "0.74",
                    "0.75",
                    "0.74",
                    "0.80",
                    "0.65",
                    "0.75",
                    "0.70",
                    "0.74",
                    "[BOLD] 0.84",
                    "[BOLD] 0.83",
                    "[BOLD] 0.84",
                    "[BOLD] 0.90"
                ],
                [
                    "Obl. List Begin",
                    "0.70",
                    "0.86",
                    "0.77",
                    "0.81",
                    "0.71",
                    "0.85",
                    "0.77",
                    "0.83",
                    "0.72",
                    "0.75",
                    "0.74",
                    "0.80",
                    "[BOLD] 0.90",
                    "[BOLD] 0.89",
                    "[BOLD] 0.89",
                    "[BOLD] 0.93"
                ],
                [
                    "Obl. List Item",
                    "0.53",
                    "0.66",
                    "0.59",
                    "0.64",
                    "0.48",
                    "0.70",
                    "0.57",
                    "0.60",
                    "0.49",
                    "0.78",
                    "0.60",
                    "0.66",
                    "[BOLD] 0.85",
                    "[BOLD] 0.94",
                    "[BOLD] 0.89",
                    "[BOLD] 0.94"
                ],
                [
                    "Proh. List Item",
                    "0.59",
                    "0.35",
                    "0.43",
                    "0.50",
                    "0.61",
                    "0.55",
                    "0.59",
                    "0.62",
                    "[BOLD] 0.83",
                    "0.50",
                    "0.62",
                    "0.67",
                    "0.80",
                    "[BOLD] 0.84",
                    "[BOLD] 0.82",
                    "[BOLD] 0.92"
                ],
                [
                    "[BOLD] Macro-average",
                    "0.70",
                    "0.70",
                    "0.70",
                    "0.74",
                    "0.73",
                    "0.78",
                    "0.74",
                    "0.78",
                    "0.73",
                    "0.76",
                    "0.73",
                    "0.79",
                    "[BOLD] 0.87",
                    "[BOLD] 0.90",
                    "[BOLD] 0.89",
                    "[BOLD] 0.94"
                ],
                [
                    "[BOLD] Micro-average",
                    "0.90",
                    "0.88",
                    "0.88",
                    "0.94",
                    "0.90",
                    "0.88",
                    "0.89",
                    "0.96",
                    "0.90",
                    "0.88",
                    "0.89",
                    "0.94",
                    "[BOLD] 0.95",
                    "[BOLD] 0.95",
                    "[BOLD] 0.95",
                    "[BOLD] 0.98"
                ]
            ]
        },
        "gold_description": "table 3 reports the precision , recall , f1 score , area under the precision - recall curve ( auc ) per class , as well as micro - and macro - averages . the self - attention mechanism ( bilstm - att ) leads to clear overall improvements ( in macro and micro f1 and auc , table 3 ) comparing to the plain bilstm , supporting the hypothesis that selfattention allows the classifier to focus on indicative tokens . allowing the bilstm to consider tokens of neighboring sentences ( x - bilstm - att ) does not lead to any clear overall improvements . the hierarchical h - bilstm - att clearly outperforms the other three methods , supporting the hypothesis that considering entire sections and allowing the sentence embeddings to interact in the upper bilstm ( fig . 3 ) is beneficial .",
        "generated_description": "forilstm - att the best performance is reached with 0 . 98 f1 and 0 . 93 auc . however , the attention - based approach outperforms the monolithic one by a large margin in terms of both precision and recall . the best performing x - bilstm approach achieves 0 . 96 f1 , 0 . 97 auc , and a perfect auc of 0 . 99 . this highlights the importance of contextual information in the multi - task learning setting ."
    },
    {
        "table_id": "235",
        "table_info": {
            "table_caption": "Table 8: BLEU scores of M2M Transformer NMT systems trained on the mixture of given parallel corpus and pseudo-parallel data generated by back-translation using (b3). Six “X∗→Y” columns show whether the pseudo-parallel data for each translation direction is involved. Bold indicates the scores higher than (b3) and “∙” indicates statistical significance of the improvement.",
            "table_column_names": [
                "ID",
                "Pseudo-parallel data involved Ja∗→Ru",
                "Pseudo-parallel data involved Ru∗→Ja",
                "Pseudo-parallel data involved Ja∗→En",
                "Pseudo-parallel data involved En∗→Ja",
                "Pseudo-parallel data involved Ru∗→En",
                "Pseudo-parallel data involved En∗→Ru",
                "BLEU score Ja→Ru",
                "BLEU score Ru→Ja",
                "BLEU score Ja→En",
                "BLEU score En→Ja",
                "BLEU score Ru→En",
                "BLEU score En→Ru"
            ],
            "table_content_values": [
                [
                    "(b3)",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "3.72",
                    "8.35",
                    "10.24",
                    "12.43",
                    "22.10",
                    "16.92"
                ],
                [
                    "#1",
                    "✓",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "∙ [BOLD] 4.59",
                    "[BOLD] 8.63",
                    "[BOLD] 10.64",
                    "[BOLD] 12.94",
                    "[BOLD] 22.21",
                    "[BOLD] 17.30"
                ],
                [
                    "#2",
                    "-",
                    "✓",
                    "-",
                    "-",
                    "-",
                    "-",
                    "[BOLD] 3.74",
                    "∙ [BOLD] 8.85",
                    "10.13",
                    "[BOLD] 13.05",
                    "[BOLD] 22.48",
                    "[BOLD] 17.20"
                ],
                [
                    "#3",
                    "✓",
                    "✓",
                    "-",
                    "-",
                    "-",
                    "-",
                    "∙ [BOLD] 4.56",
                    "∙ [BOLD] 9.09",
                    "[BOLD] 10.57",
                    "∙ [BOLD] 13.23",
                    "[BOLD] 22.48",
                    "∙ [BOLD] 17.89"
                ],
                [
                    "2-13 #4",
                    "-",
                    "-",
                    "✓",
                    "-",
                    "-",
                    "-",
                    "3.71",
                    "8.05",
                    "∙ [BOLD] 11.00",
                    "[BOLD] 12.66",
                    "[BOLD] 22.17",
                    "16.76"
                ],
                [
                    "#5",
                    "-",
                    "-",
                    "-",
                    "✓",
                    "-",
                    "-",
                    "3.62",
                    "8.10",
                    "9.92",
                    "∙ [BOLD] 14.06",
                    "21.66",
                    "16.68"
                ],
                [
                    "#6",
                    "-",
                    "-",
                    "✓",
                    "✓",
                    "-",
                    "-",
                    "3.61",
                    "7.94",
                    "∙ [BOLD] 11.51",
                    "∙ [BOLD] 14.38",
                    "[BOLD] 22.22",
                    "16.80"
                ],
                [
                    "2-13 #7",
                    "-",
                    "-",
                    "-",
                    "-",
                    "✓",
                    "-",
                    "[BOLD] 3.80",
                    "[BOLD] 8.37",
                    "[BOLD] 10.67",
                    "[BOLD] 13.00",
                    "[BOLD] 22.51",
                    "∙ [BOLD] 17.73"
                ],
                [
                    "#8",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "✓",
                    "[BOLD] 3.77",
                    "8.04",
                    "[BOLD] 10.52",
                    "12.43",
                    "∙ [BOLD] 22.85",
                    "[BOLD] 17.13"
                ],
                [
                    "#9",
                    "-",
                    "-",
                    "-",
                    "-",
                    "✓",
                    "✓",
                    "3.37",
                    "8.03",
                    "10.19",
                    "[BOLD] 12.79",
                    "[BOLD] 22.77",
                    "[BOLD] 17.26"
                ],
                [
                    "2-13 #10",
                    "✓",
                    "✓",
                    "✓",
                    "✓",
                    "✓",
                    "✓",
                    "∙ [BOLD] 4.43",
                    "∙ [BOLD] 9.38",
                    "∙ [BOLD] 12.06",
                    "∙ [BOLD] 14.43",
                    "∙ [BOLD] 23.09",
                    "[BOLD] 17.30"
                ]
            ]
        },
        "gold_description": "table 8 shows the bleu scores achieved by several reasonable combinations of six - way pseudo - parallel data . we observed that the use of all six - way pseudo - parallel data ( # 10 ) significantly improved the base model for all the translation directions , except en → ru . a translation direction often benefited when the pseudo - parallel data for that specific direction was used . however , the resulting bleu scores for ja → ru and ru → ja tasks do not exceed 10 bleu points , implying the inherent limitation of the in - domain data as well as the difficulty of these translation directions . models of our multistage fine - tuning , i . e . , v and vii , achieved significantly higher bleu scores than ( b3 ) in table 5 , a weak baseline without using any monolingual data , and # 10 in table 8 , a strong baseline established with monolingual data .",
        "generated_description": "in this section , we show the bleu scores of the proposed approach when there is sufficient parallel data involved and when the target language is different from the source language . first , we can see that our approach significantly outperforms the other approaches in terms of ru → ja and en → ja translation directions , and achieves the best results in ru → en direction as well as in en → ru direction . second , our approach improves the quality of the parallel data when there are two languages involved . when there is one language involved and the other language does not have enough parallel data , the performance of the approach is worse than that of the one using parallel data only . third , the proposed method is better than the alternative approaches when the source and target languages are different from each other . the reason is that our method can better exploit the similarity between the target and source languages ."
    },
    {
        "table_id": "236",
        "table_info": {
            "table_caption": "Table 2: Accuracies (%) on few-shot DA. “On 1.0” represents the results on the original FewRel dataset and “On 2.0” represents the results on the new test set. The models with “-ADV” use adversarial training described in Section 3.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] 5-Way 1-Shot  [BOLD] On 1.0",
                "[BOLD] 5-Way 1-Shot  [BOLD] On 2.0",
                "[BOLD] 5-Way 5-Shot  [BOLD] On 1.0",
                "[BOLD] 5-Way 5-Shot  [BOLD] On 2.0"
            ],
            "table_content_values": [
                [
                    "GNN (CNN)",
                    "66.23±0.75",
                    "27.94±0.03",
                    "81.28±0.62",
                    "29.33±0.11"
                ],
                [
                    "Proto (CNN)",
                    "74.52±0.07",
                    "35.09±0.10",
                    "88.40±0.06",
                    "49.37±0.10"
                ],
                [
                    "Proto-ADV (CNN)",
                    "70.28±0.15",
                    "42.21±0.09",
                    "84.63±0.07",
                    "58.71±0.06"
                ],
                [
                    "Proto (BERT)",
                    "80.68±0.28",
                    "40.12±0.19",
                    "89.60±0.09",
                    "51.50±0.29"
                ],
                [
                    "Proto-ADV (BERT)",
                    "73.35±0.95",
                    "41.90±0.44",
                    "82.30±0.53",
                    "54.74±0.22"
                ],
                [
                    "BERT-PAIR",
                    "88.32±0.64",
                    "56.25±0.40",
                    "93.22±0.13",
                    "67.44±0.54"
                ],
                [
                    "[BOLD] Model",
                    "[BOLD] 10-Way 1-Shot",
                    "[BOLD] 10-Way 1-Shot",
                    "[BOLD] 10-Way 5-Shot",
                    "[BOLD] 10-Way 5-Shot"
                ],
                [
                    "[BOLD] Model",
                    "[BOLD] On 1.0",
                    "[BOLD] On 2.0",
                    "[BOLD] On 1.0",
                    "[BOLD] On 2.0"
                ],
                [
                    "GNN (CNN)",
                    "46.27±0.80",
                    "16.44±0.04",
                    "64.02±0.77",
                    "18.26±0.03"
                ],
                [
                    "Proto (CNN)",
                    "62.38±0.06",
                    "22.98±0.05",
                    "80.45±0.08",
                    "35.22±0.06"
                ],
                [
                    "Proto-ADV (CNN)",
                    "56.34±0.08",
                    "28.91±0.10",
                    "74.67±0.12",
                    "44.35±0.09"
                ],
                [
                    "Proto (BERT)",
                    "71.48±0.15",
                    "26.45±0.10",
                    "82.89±0.11",
                    "36.93±0.01"
                ],
                [
                    "Proto-ADV (BERT)",
                    "61.49±0.69",
                    "27.36±0.50",
                    "72.60±0.38",
                    "37.40±0.36"
                ],
                [
                    "BERT-PAIR",
                    "80.63±0.17",
                    "43.64±0.46",
                    "87.02±0.12",
                    "53.17±0.09"
                ]
            ]
        },
        "gold_description": "table 2 demonstrates the evaluation results of few - shot da on the existing fewrel test set and the new test set . ( 1 ) all few - shot models suffer dramatic perfor mance falls when tested on a different domain . ( 2 ) adversarial training does improve the results on the new test domain , yet still has large space for growth . ( 3 ) bert - pair outperforms all other few - shot models on both 1 . 0 and 2 . 0 test set .",
        "generated_description": "from the table , we can see that : ( 1 ) compared to the baselines ( proto and proto - adv ) , the cnn - based proto and adv models outperform the bert - based models on 1 - shot and 5 - way 5 - shot learning , which confirms the effectiveness of our proposed representation learning method . ( 2 ) compared with the original bert model , our proposed adv model outperforms the original proto model and the gnn - based model by a large margin , which shows the advantage of explicitly modeling the interaction between the target and the source domain knowledge . ( 3 ) our proposed method achieves the best performance on 2 . 0 - way setting , which indicates that our representation learning framework can generalize well to new domains and new types of dialogues that are unseen in the training data ."
    },
    {
        "table_id": "237",
        "table_info": {
            "table_caption": "Table 3: Accuracies (%) on few-shot NOTA. Models with * simply ignore the NOTA setting and assume all queries can be classified as one of the given relations.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] 5-Way-1-Shot  [BOLD] 0% NOTA",
                "[BOLD] 5-Way-1-Shot  [BOLD] 15% NOTA",
                "[BOLD] 5-Way-1-Shot  [BOLD] 30% NOTA",
                "[BOLD] 5-Way-1-Shot  [BOLD] 50% NOTA"
            ],
            "table_content_values": [
                [
                    "Proto (CNN)*",
                    "74.52±0.07",
                    "62.18±0.22",
                    "53.38±0.14",
                    "37.26±0.04"
                ],
                [
                    "Proto (CNN)",
                    "69.17±0.07",
                    "60.59±0.05",
                    "53.18±0.12",
                    "40.00±0.10"
                ],
                [
                    "Proto (BERT)*",
                    "80.68±0.28",
                    "67.92±0.31",
                    "58.22±0.20",
                    "40.64±0.14"
                ],
                [
                    "Proto (BERT)",
                    "81.65±0.97",
                    "70.02±0.23",
                    "61.08±0.28",
                    "45.94±0.50"
                ],
                [
                    "BERT-PAIR*",
                    "88.32±0.64",
                    "73.60±0.51",
                    "63.00±0.47",
                    "43.99±0.09"
                ],
                [
                    "BERT-PAIR",
                    "76.73±0.55",
                    "77.67±0.14",
                    "78.49±0.21",
                    "80.31±0.12"
                ]
            ]
        },
        "gold_description": "all models are trained given 50 % nota queries and tested under four different nota rates : 0 % , 15 % , 30 % , 50 % . for detailed numbers of results on fewshot nota , please refer to table 3 . ( 1 ) treating nota as the + 1 relation is beneficial for handling few - shot nota , though the results still fall fast when the nota rate increases . ( 2 ) bert - pair works better under the nota setting for its binary - classification style model , and stays stable with rising nota rate . ( 3 ) though bert - pair achieves promising results , huge gaps still exist between the conventional ( 0 % nota rate ) and nota settings ( gaps of 8 points for 5 - way 1 - shot and 7 points for 5way 5 - shot with 50 % nota rate ) , which calls for further research to address the challenge .",
        "generated_description": "the results show that proto ( cnn ) and proto ( bert ) significantly outperform bert - pair on the 5 - way - 1 - shot dataset in all the three settings . furthermore , the performance of proto with bert embeddings ( with or without cnn ) is close to that of bert ( with and without bert ) , which demonstrates the effectiveness of our proposed embedding pre - training framework ."
    },
    {
        "table_id": "238",
        "table_info": {
            "table_caption": "Table 2: Performance analysis of entity classification methods on the three datasets.",
            "table_column_names": [
                "Typing methods",
                "Wiki/Figer(gold) Acc.",
                "Wiki/Figer(gold) Ma-F1",
                "Wiki/Figer(gold) Mi-F1",
                "OntoNotes Acc.",
                "OntoNotes Ma-F1",
                "OntoNotes Mi-F1",
                "BBN Acc.",
                "BBN Ma-F1",
                "BBN Mi-F1"
            ],
            "table_content_values": [
                [
                    "[BOLD] FIGER* ",
                    "0.474",
                    "0.692",
                    "0.655",
                    "0.369",
                    "0.578",
                    "0.516",
                    "0.467",
                    "0.672",
                    "0.612"
                ],
                [
                    "[BOLD] HYENA* ",
                    "0.288",
                    "0.528",
                    "0.506",
                    "0.249",
                    "0.497",
                    "0.446",
                    "0.523",
                    "0.576",
                    "0.587"
                ],
                [
                    "[BOLD] AFET-NoCo* ",
                    "0.526",
                    "0.693",
                    "0.654",
                    "0.486",
                    "0.652",
                    "0.594",
                    "0.655",
                    "0.711",
                    "0.716"
                ],
                [
                    "[BOLD] AFET-CoH* ",
                    "0.433",
                    "0.583",
                    "0.551",
                    "0.521",
                    "0.680",
                    "0.609",
                    "0.657",
                    "0.703",
                    "0.712"
                ],
                [
                    "[BOLD] AFET* ",
                    "0.533",
                    "0.693",
                    "0.664",
                    "0.551",
                    "0.711",
                    "0.647",
                    "0.670",
                    "0.727",
                    "0.735"
                ],
                [
                    "[BOLD] AFET†‡ ",
                    "0.509",
                    "0.689",
                    "0.653",
                    "[BOLD] 0.553",
                    "[BOLD] 0.712",
                    "[BOLD] 0.646",
                    "0.683",
                    "0.744",
                    "0.747"
                ],
                [
                    "[BOLD] Attentive† ",
                    "0.581",
                    "0.780",
                    "0.744",
                    "0.473",
                    "0.655",
                    "0.586",
                    "0.484",
                    "0.732",
                    "0.724"
                ],
                [
                    "[BOLD] our-AllC†",
                    "[BOLD] 0.662",
                    "0.805",
                    "0.770",
                    "0.514",
                    "0.672",
                    "0.626",
                    "0.655",
                    "0.736",
                    "0.752"
                ],
                [
                    "[BOLD] our-NoM†",
                    "0.646",
                    "0.808",
                    "0.768",
                    "0.521",
                    "0.683",
                    "0.626",
                    "0.615",
                    "0.742",
                    "0.755"
                ],
                [
                    "[BOLD] our†",
                    "0.658",
                    "[BOLD] 0.812",
                    "[BOLD] 0.774",
                    "0.522",
                    "0.685",
                    "0.633",
                    "0.604",
                    "0.741",
                    "0.757"
                ],
                [
                    "[BOLD] model level transfer-learning†",
                    "-",
                    "-",
                    "-",
                    "0.531",
                    "0.684",
                    "0.637",
                    "0.645",
                    "0.784",
                    "[BOLD] 0.795"
                ],
                [
                    "[BOLD] feature level transfer-learning†",
                    "-",
                    "-",
                    "-",
                    "0.471",
                    "0.689",
                    "0.635",
                    "[BOLD] 0.733",
                    "[BOLD] 0.791",
                    "0.792"
                ]
            ]
        },
        "gold_description": "these results are shown in table 2 as feature level transfer - learning . these results are shown in table 2 as model level transfer learning . table 2 shows the results of the proposed method , its variants and the baseline methods .",
        "generated_description": "we can see that our proposed method outperforms all the existing typing methods on all the three datasets . our model achieves the best accuracy and micro - f1 scores on wiki / figer ( gold ) and ontonotes , and outperforms the state - of - the - art on bbn by a large margin ( + 1 . 8 % and + 1 . 3 % in terms of ma and + 0 . 7 % and - 0 . 2 % on micro - f1 over the best performing method on the ontonote dataset . on the bbn dataset , our model achieves a much smaller performance improvement ( + 0 . 1 % and 0 . 0 % on ma and - 1 . 2 % , - 00 . 3 % ) compared to the best finetuning method ( i . e . , fiet - noco ) , which shows that our method is much more efficient than existing methods ."
    },
    {
        "table_id": "239",
        "table_info": {
            "table_caption": "Table 4: Performance analysis of the proposed model and AFET on top 10 (in terms of type frequency) types present in OntoNotes dataset.",
            "table_column_names": [
                "Label type",
                "Support",
                "our Prec.",
                "our Rec.",
                "our F-1",
                "AFET Prec.",
                "AFET Rec.",
                "AFET F-1"
            ],
            "table_content_values": [
                [
                    "[ITALIC] /other",
                    "42.6%",
                    "0.838",
                    "0.809",
                    "0.823",
                    "0.774",
                    "0.962",
                    "[BOLD] 0.858"
                ],
                [
                    "[ITALIC] /organization",
                    "11.0%",
                    "0.588",
                    "0.490",
                    "[BOLD] 0.534",
                    "0.903",
                    "0.273",
                    "0.419"
                ],
                [
                    "[ITALIC] /person",
                    "9.9%",
                    "0.559",
                    "0.467",
                    "[BOLD] 0.508",
                    "0.669",
                    "0.352",
                    "0.461"
                ],
                [
                    "[ITALIC] /organization/company",
                    "7.8%",
                    "0.932",
                    "0.166",
                    "[BOLD] 0.282",
                    "1.0",
                    "0.127",
                    "0.225"
                ],
                [
                    "[ITALIC] /location",
                    "7.5%",
                    "0.687",
                    "0.796",
                    "[BOLD] 0.737",
                    "0.787",
                    "0.609",
                    "0.687"
                ],
                [
                    "[ITALIC] /organization/government",
                    "2.1%",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "[ITALIC] /location/country",
                    "2.0%",
                    "0.783",
                    "0.614",
                    "[BOLD] 0.688",
                    "0.838",
                    "0.498",
                    "0.625"
                ],
                [
                    "[ITALIC] /other/legal",
                    "1.8%",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "[ITALIC] /location/city",
                    "1.8%",
                    "0.919",
                    "0.610",
                    "[BOLD] 0.733",
                    "0.816",
                    "0.637",
                    "0.715"
                ],
                [
                    "[ITALIC] /person/political_figure",
                    "1.6%",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0",
                    "0"
                ]
            ]
        },
        "gold_description": "results are shown in table 4 . compared to afet , the proposed model performs better in all types except other in the top - 10 frequent types .",
        "generated_description": "in general , we observe that afet outperforms the baselines in terms of precision , recall , and f1 - measure for different label types . however , it is worth noting that the precision of some labels ( e . g . , “ organization ” , “ person ” and “ location ” ) is significantly higher than recall , which is expected as these labels often refer to organizations or entities with close ties to the given ground truth ."
    },
    {
        "table_id": "240",
        "table_info": {
            "table_caption": "Table 2: Performance of different approaches in relation to the average cosine similarity of words associated with a property (cos). The last row shows the Spearman Rank correlation between f1-scores and average cosine similarity. Property types are listed under type (p = part, vp = visual-perceptual, op = other-perceptual, e = encyclopaedic, f = functional, t = taxonomic).",
            "table_column_names": [
                "feature",
                "cos",
                "f1-neigh",
                "f1-lr",
                "f1-net",
                "type"
            ],
            "table_content_values": [
                [
                    "is_heavy",
                    "0.15",
                    "0.15",
                    "0.17",
                    "0.21",
                    "op"
                ],
                [
                    "is_strong",
                    "0.15",
                    "0.13",
                    "0.13",
                    "0.34",
                    "e"
                ],
                [
                    "is_thin",
                    "0.16",
                    "0",
                    "0.05",
                    "0.1",
                    "vp"
                ],
                [
                    "is_hard",
                    "0.16",
                    "0.15",
                    "0.08",
                    "0.26",
                    "op"
                ],
                [
                    "is_expensive",
                    "0.16",
                    "0",
                    "0.28",
                    "0.37",
                    "e"
                ],
                [
                    "…",
                    "…",
                    "…",
                    "…",
                    "…",
                    "[EMPTY]"
                ],
                [
                    "is_black",
                    "0.2",
                    "0.29",
                    "0.23",
                    "0.24",
                    "vp"
                ],
                [
                    "is_electric",
                    "0.21",
                    "0.48",
                    "0.5",
                    "0.69",
                    "vp"
                ],
                [
                    "is_dangerous",
                    "0.21",
                    "0.53",
                    "0.57",
                    "0.59",
                    "e"
                ],
                [
                    "is_colourful",
                    "0.21",
                    "0.14",
                    "0.25",
                    "0.32",
                    "vp"
                ],
                [
                    "is_brown",
                    "0.21",
                    "0.13",
                    "0.22",
                    "0.33",
                    "vp"
                ],
                [
                    "has_a_handle _handles",
                    "0.22",
                    "0.44",
                    "0.57",
                    "0.58",
                    "p"
                ],
                [
                    "has_a_seat _seats",
                    "0.22",
                    "0.43",
                    "0.3",
                    "0.48",
                    "p"
                ],
                [
                    "does_smell _is_smelly",
                    "0.22",
                    "0.08",
                    "0.15",
                    "0.37",
                    "op"
                ],
                [
                    "made_of_glass",
                    "0.22",
                    "0.29",
                    "0",
                    "0.28",
                    "vp"
                ],
                [
                    "has_a_point",
                    "0.23",
                    "0.38",
                    "0.23",
                    "0.47",
                    "p"
                ],
                [
                    "does_protect",
                    "0.24",
                    "0.38",
                    "0.26",
                    "0.37",
                    "f"
                ],
                [
                    "is_yellow",
                    "0.24",
                    "0.22",
                    "0",
                    "0.23",
                    "vp"
                ],
                [
                    "is_soft",
                    "0.24",
                    "0.12",
                    "0",
                    "0.16",
                    "op"
                ],
                [
                    "is_red",
                    "0.25",
                    "0.34",
                    "0.13",
                    "0.27",
                    "vp"
                ],
                [
                    "is_fast",
                    "0.25",
                    "0.3",
                    "0.31",
                    "0.48",
                    "vp"
                ],
                [
                    "is_tall",
                    "0.25",
                    "0.43",
                    "0.57",
                    "0.65",
                    "vp"
                ],
                [
                    "is_a_tool",
                    "0.26",
                    "0.5",
                    "0.51",
                    "0.47",
                    "t"
                ],
                [
                    "…",
                    "…",
                    "…",
                    "…",
                    "…",
                    "[EMPTY]"
                ],
                [
                    "is_a_weapon",
                    "0.3",
                    "0.74",
                    "0.56",
                    "0.63",
                    "t"
                ],
                [
                    "is_green",
                    "0.31",
                    "0.45",
                    "0.45",
                    "0.45",
                    "vp"
                ],
                [
                    "has_a_ blade_blades",
                    "0.32",
                    "0.68",
                    "0.65",
                    "0.74",
                    "p"
                ],
                [
                    "is_worn",
                    "0.32",
                    "0.47",
                    "0.86",
                    "0.9",
                    "f"
                ],
                [
                    "has_wheels",
                    "0.32",
                    "0.82",
                    "0.83",
                    "0.87",
                    "p"
                ],
                [
                    "is_found _in_kitchens",
                    "0.33",
                    "0.56",
                    "0.73",
                    "0.76",
                    "e"
                ],
                [
                    "does_fly",
                    "0.33",
                    "0.57",
                    "0.76",
                    "0.76",
                    "f"
                ],
                [
                    "has_a_tail",
                    "0.33",
                    "0.53",
                    "0.68",
                    "0.69",
                    "p"
                ],
                [
                    "is_an_animal",
                    "0.33",
                    "0.64",
                    "0.76",
                    "0.78",
                    "t"
                ],
                [
                    "is_eaten_edible",
                    "0.33",
                    "0.37",
                    "0.88",
                    "0.85",
                    "f"
                ],
                [
                    "has_four_legs",
                    "0.34",
                    "0.67",
                    "0.66",
                    "0.66",
                    "p"
                ],
                [
                    "is_a_vehicle",
                    "0.34",
                    "0.76",
                    "0.69",
                    "0.79",
                    "t"
                ],
                [
                    "does_eat",
                    "0.34",
                    "0.68",
                    "0.71",
                    "0.68",
                    "f"
                ],
                [
                    "…",
                    "…",
                    "…",
                    "…",
                    "…",
                    "[EMPTY]"
                ],
                [
                    "has_a_beak",
                    "0.37",
                    "0.63",
                    "0.83",
                    "0.87",
                    "p"
                ],
                [
                    "made_of_cotton",
                    "0.37",
                    "0.68",
                    "0.56",
                    "0.64",
                    "vp"
                ],
                [
                    "has_roots",
                    "0.37",
                    "0.3",
                    "0.65",
                    "0.72",
                    "p"
                ],
                [
                    "is_a_mammal",
                    "0.37",
                    "0.69",
                    "0.85",
                    "0.86",
                    "t"
                ],
                [
                    "does_grow",
                    "0.37",
                    "0.52",
                    "0.81",
                    "0.81",
                    "e"
                ],
                [
                    "is_a_plant",
                    "0.37",
                    "0.43",
                    "0.63",
                    "0.64",
                    "t"
                ],
                [
                    "has_leaves",
                    "0.37",
                    "0.41",
                    "0.71",
                    "0.78",
                    "p"
                ],
                [
                    "…",
                    "…",
                    "…",
                    "…",
                    "…",
                    "[EMPTY]"
                ],
                [
                    "has_pips_seeds",
                    "0.47",
                    "0.5",
                    "0.08",
                    "0.46",
                    "p"
                ],
                [
                    "is_juicy",
                    "0.5",
                    "0.71",
                    "0.48",
                    "0.56",
                    "op"
                ],
                [
                    "is_a_vegetable",
                    "0.52",
                    "0.78",
                    "0.75",
                    "0.81",
                    "t"
                ],
                [
                    "is_played _does_play",
                    "0.53",
                    "0.9",
                    "0.98",
                    "0.98",
                    "f"
                ],
                [
                    "does_make_music",
                    "0.55",
                    "0.89",
                    "0.95",
                    "0.92",
                    "f"
                ],
                [
                    "spearman-r",
                    "[EMPTY]",
                    "0.72",
                    "0.52",
                    "0.59",
                    "[EMPTY]"
                ]
            ]
        },
        "gold_description": "table 2 shows a selection of the f1 - scores achieved on properties in the cslb dataset in relation to the average cosine similarity of the associated words . a high average cosine similarity means that the concepts overall have similar vector representations and can thus be seen as having a low diversity . the results of the spearman rank correlation clearly indicate that scores achieved by nearest neighbors correlate more strongly with the average cosine than the two supervised classification approaches .",
        "generated_description": "is the is - a - priori feature a good indicator of strength or irreducibility of an attack ? is the proportion of op that is identified as a potential danger compared to the other features ? we show the precision , recall , and f1 - measure of the three measures for each feature as well as the overall accuracy of the classifier . observe that the precision is high for is_strong , is_colourful , and is_elephant . this is due to the fact that these features strongly suggest that an attack is highly likely to be successful if the language of the propaganda is strong enough . for example , “ is_black ” is a strong indicator of black - neural attack , which has a precision of 0 . 8 , and the recall is 0 . 2 . another example is is_farsi , which is a proof of the effectiveness of the radical left - wing classification method ."
    },
    {
        "table_id": "241",
        "table_info": {
            "table_caption": "Table 4: Training times and parameters to learn.",
            "table_column_names": [
                "[BOLD] Network",
                "[BOLD] Training Time",
                "[BOLD] Parameters"
            ],
            "table_content_values": [
                [
                    "[BOLD] bilstm",
                    "5h 30m",
                    "1,278M"
                ],
                [
                    "[BOLD] bilstm-att",
                    "8h 30m",
                    "1,279M"
                ],
                [
                    "[BOLD] x-bilstm-att",
                    "25h 40m",
                    "1,279M"
                ],
                [
                    "[BOLD] h-bilstm-att",
                    "2h 30m",
                    "1,837M"
                ]
            ]
        },
        "gold_description": "h - bilstm - att is also much faster to train than bilstm and bilstm - att ( table 4 ) , even though it has more parameters , because it converges faster ( 5 - 7 epochs vs . 12 - 15 ) .",
        "generated_description": "in all experiments , we use bidirectional lstms with lstm - based encoders and cnns . for the training of the attention - based models we use 5h 30m and 1 , 278m of parameters for the lmm and lmm - attention models , respectively . we use 8h and 8h of training time for the bilstm and x - bilstm - att networks , respectively , and 25h and 40m of training times for the hierarchical and hierarchical hierarchical lsmms . the network is trained using stochastic gradient descent ( sgd ) algorithm with a learning rate of 1e - 5 and a decay rate of 0 . 01 . the learning rate is decayed by factorization after the first 20 epochs of training and the network is stopped using the best performing model according to the validation loss ."
    },
    {
        "table_id": "242",
        "table_info": {
            "table_caption": "Table 4: F1 scores achieved by logistic regression (lr) two runs of a neural net classifier (net1 and net2 and the n-best nearest neighbors evaluated with leave-one-out on the full datasets (marked as full_ and the crow-only sets (marked as crowd_).",
            "table_column_names": [
                "property",
                "av-cos",
                "neigh",
                "lr",
                "net1",
                "net2"
            ],
            "table_content_values": [
                [
                    "full_is_yellow",
                    "0.23",
                    "0.19",
                    "0.47",
                    "0.64",
                    "0.64"
                ],
                [
                    "full_is_used_in _cooking",
                    "0.37",
                    "0.29",
                    "0.98",
                    "0.98",
                    "0.98"
                ],
                [
                    "full_is_black",
                    "0.19",
                    "0.35",
                    "0.75",
                    "0.77",
                    "0.77"
                ],
                [
                    "full_is_red",
                    "0.23",
                    "0.36",
                    "0.51",
                    "0.54",
                    "0.52"
                ],
                [
                    "full_is_dangerous",
                    "0.24",
                    "0.58",
                    "0.88",
                    "0.88",
                    "0.87"
                ],
                [
                    "crowd_is_dangerous",
                    "0.26",
                    "0.61",
                    "0.86",
                    "0.86",
                    "0.86"
                ],
                [
                    "full_has_wheels",
                    "0.38",
                    "0.90",
                    "0.96",
                    "0.96",
                    "0.95"
                ],
                [
                    "full_is_found_in_seas",
                    "0.44",
                    "0.87",
                    "0.97",
                    "0.98",
                    "0.98"
                ],
                [
                    "crowd_is_found _in_seas",
                    "0.50",
                    "0.87",
                    "0.94",
                    "0.96",
                    "0.96"
                ],
                [
                    "full_does_kill",
                    "0.27",
                    "0.67",
                    "0.83",
                    "0.86",
                    "0.82"
                ],
                [
                    "crowd_does_kill",
                    "0.30",
                    "0.70",
                    "0.82",
                    "0.84",
                    "0.80"
                ],
                [
                    "full_made_of_wood",
                    "0.17",
                    "0.14",
                    "0.84",
                    "0.85",
                    "0.85"
                ],
                [
                    "full_is_food_test",
                    "0.37",
                    "0.00",
                    "0.36",
                    "0.36",
                    "0.36"
                ],
                [
                    "full_is_an _animal_test",
                    "0.37",
                    "0.52",
                    "0.88",
                    "0.88",
                    "0.88"
                ]
            ]
        },
        "gold_description": "table 4 shows the f1 - scores on the full clean datasets . for polysemy between food and animals ( table 4 ) , we observe that when trained on pure animal and food words and tested on polysemous animal and food words , the classifiers perform highly with a large difference to nearest neighbors .",
        "generated_description": "property . we show the correlation between the cosine similarity of the property vector and the human - annotated relatedness scores for each property . we can see that full_is_yellow is the most related property , which has the highest correlation with the human intuition . the other properties are fairly similar , with the difference being less pronounced . property . when we zoom into the full list of properties , we see that the correlation is very close to 0 . 5 , which is quite high for most of the properties . this indicates that the trained embeddings are able to capture the relatedness between the property and the hyper - parameters ."
    },
    {
        "table_id": "243",
        "table_info": {
            "table_caption": "Table 1: Word-level evaluation results. Each value corresponds to average Pearson correlation of 7 identical models initialized with different random seeds. Correlations were scaled to the [−100;100] range for easier reading. Bold values represent the best method per training dataset, per task; underlined values represent the best-performing method per task, independent of training dataset. For each task and dataset, every best-performing method was significantly different to other methods (p<0.05), except for w trained in SNLI at the MTurk287 task. Statistical significance was obtained with a two-sided Welch’s t-test for two independent samples without assuming equal variance (Welch, 1947).",
            "table_column_names": [
                "SNLI",
                "w",
                "[BOLD] MEN 71.78",
                "[BOLD] MTurk287 35.40",
                "[BOLD] MTurk771 49.05",
                "[BOLD] RG65 61.80",
                "[BOLD] RW 18.43",
                "[BOLD] SimLex999 19.17",
                "[BOLD] SimVerb3500 10.32",
                "[BOLD] WS353 39.27",
                "[BOLD] WS353R 28.01",
                "[BOLD] WS353S 53.42"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "c",
                    "9.85",
                    "-5.65",
                    "0.82",
                    "-5.28",
                    "17.81",
                    "0.86",
                    "2.76",
                    "-2.20",
                    "0.20",
                    "-3.87"
                ],
                [
                    "[EMPTY]",
                    "cat",
                    "71.91",
                    "[BOLD] 35.52",
                    "48.84",
                    "62.12",
                    "18.46",
                    "19.10",
                    "10.21",
                    "39.35",
                    "28.16",
                    "53.40"
                ],
                [
                    "[EMPTY]",
                    "sg",
                    "70.49",
                    "34.49",
                    "46.15",
                    "59.75",
                    "18.24",
                    "17.20",
                    "8.73",
                    "35.86",
                    "23.48",
                    "50.83"
                ],
                [
                    "[EMPTY]",
                    "vg",
                    "[BOLD] 80.00",
                    "32.54",
                    "[BOLD] 62.09",
                    "[BOLD] 68.90",
                    "[BOLD] 20.76",
                    "[BOLD] 37.70",
                    "[BOLD] 20.45",
                    "[BOLD] 54.72",
                    "[BOLD] 47.24",
                    "[BOLD] 65.60"
                ],
                [
                    "MNLI",
                    "w",
                    "68.76",
                    "50.15",
                    "68.81",
                    "65.83",
                    "18.43",
                    "42.21",
                    "25.18",
                    "61.10",
                    "58.21",
                    "70.17"
                ],
                [
                    "[EMPTY]",
                    "c",
                    "4.84",
                    "0.06",
                    "1.95",
                    "-0.06",
                    "12.18",
                    "3.01",
                    "1.52",
                    "-4.68",
                    "-3.63",
                    "-3.65"
                ],
                [
                    "[EMPTY]",
                    "cat",
                    "68.77",
                    "50.40",
                    "68.77",
                    "65.92",
                    "18.35",
                    "42.22",
                    "25.12",
                    "61.15",
                    "58.26",
                    "70.21"
                ],
                [
                    "[EMPTY]",
                    "sg",
                    "67.66",
                    "49.58",
                    "68.29",
                    "64.84",
                    "18.36",
                    "41.81",
                    "24.57",
                    "60.13",
                    "57.09",
                    "69.41"
                ],
                [
                    "[EMPTY]",
                    "vg",
                    "[BOLD] 76.69",
                    "[BOLD] 56.06",
                    "[BOLD] 70.13",
                    "[BOLD] 69.00",
                    "[BOLD] 25.35",
                    "[BOLD] 48.40",
                    "[BOLD] 35.12",
                    "[BOLD] 68.91",
                    "[BOLD] 64.70",
                    "[BOLD] 77.23"
                ]
            ]
        },
        "gold_description": "table 1 shows the quality of word representations in terms of the correlation between word similarity scores obtained by the proposed models and word similarity scores defined by humans . for that can see first , we each task , character only models had significantly worse performance than every other model trained on the same dataset . further , bold results show the overall trend that vector gates outperformed the other methods regardless of training dataset . additionally , results from the mnli row in general , and underlined results in particular , show that training on multinli produces word representations better at capturing word similarity . exceptions to the previous rule are models evaluated in men and rw . more notably , in the rarewords dataset ( luthe word only , concat , ong et al . , 2013 ) , and scalar gate methods performed equally , despite having been trained in different datasets ( p > 0 . 1 ) , and the char only method performed significantly worse when trained in multinli . the vector gate , however , performed significantly better than its counterpart trained in snli .",
        "generated_description": "for each word w , we report the spearman ’ s correlation of its cosine similarity with the human relatedness scores in the corresponding test datasets . we also report the relatedness score of the corresponding embedding - based similarity scores obtained by the corresponding bert models . we can see that bert outperforms most of the other embedding models on the men , rg65 , rw , simverb3500 , mturk287 , and ws353 datasets , except for the case of simlex999 . one of the main reasons for the superior performance of bert is due to the fact that these datasets contain a large number of pairs of words which are related to the given word , e . g . , “ dog ” , “ cup ” and “ coffee_kettle ” . in this case , bert may assign high similarity scores to infrequent words which tend to be semantically related to a given word ."
    },
    {
        "table_id": "244",
        "table_info": {
            "table_caption": "Table 2: Experimental results. Each value shown in the table is the average result of 7 identical models initialized with different random seeds. Values represent accuracy (%) unless indicated by †, in which case they represent Pearson correlation scaled to the range [−100,100] for easier reading. Bold values represent the best method per training dataset, per task; underlined values represent the best-performing method per task, independent of training dataset. Values marked with an asterisk (∗) are significantly different to the average performance of the best model trained on the same dataset (p<0.05). Results for every best-performing method trained on one dataset are significantly different to the best-performing method trained on the other. Statistical significance was obtained in the same way as described in table 1.",
            "table_column_names": [
                "[EMPTY]",
                "[EMPTY]",
                "Classification  [BOLD] CR",
                "Classification  [BOLD] MPQA",
                "Classification  [BOLD] MR",
                "Classification  [BOLD] SST2",
                "Classification  [BOLD] SST5",
                "Classification  [BOLD] SUBJ",
                "Classification  [BOLD] TREC",
                "Entailment  [BOLD] SICKE",
                "Relatedness  [BOLD] SICKR†",
                "Semantic Textual Similarity  [BOLD] STS16†",
                "Semantic Textual Similarity  [BOLD] STSB†"
            ],
            "table_content_values": [
                [
                    "SNLI",
                    "w",
                    "80.50",
                    "84.59",
                    "74.18",
                    "78.86",
                    "42.33",
                    "[BOLD] 90.38",
                    "[BOLD] 86.83",
                    "86.37",
                    "88.52",
                    "59.90∗",
                    "71.29∗"
                ],
                [
                    "[EMPTY]",
                    "c",
                    "74.90∗",
                    "78.86∗",
                    "65.93∗",
                    "69.42∗",
                    "35.56∗",
                    "82.97∗",
                    "83.31∗",
                    "84.13∗",
                    "83.89∗",
                    "59.33∗",
                    "67.20∗"
                ],
                [
                    "[EMPTY]",
                    "cat",
                    "80.44",
                    "84.66",
                    "74.31",
                    "78.37",
                    "41.34∗",
                    "90.28",
                    "85.80∗",
                    "[BOLD] 86.40",
                    "88.44",
                    "59.90∗",
                    "71.24∗"
                ],
                [
                    "[EMPTY]",
                    "sg",
                    "[BOLD] 80.59",
                    "84.60",
                    "[BOLD] 74.49",
                    "[BOLD] 79.04",
                    "41.63∗",
                    "90.16",
                    "86.00",
                    "86.10∗",
                    "[BOLD] 88.57",
                    "60.05∗",
                    "71.34∗"
                ],
                [
                    "[EMPTY]",
                    "vg",
                    "80.42",
                    "[BOLD] 84.66",
                    "74.26",
                    "78.87",
                    "[BOLD] 42.38",
                    "90.07",
                    "85.97",
                    "85.67",
                    "88.31∗",
                    "[BOLD] 60.92",
                    "[BOLD] 71.99"
                ],
                [
                    "MNLI",
                    "w",
                    "83.80",
                    "[BOLD] 89.13",
                    "79.05",
                    "83.38",
                    "45.21",
                    "91.79",
                    "89.23",
                    "84.92",
                    "86.33",
                    "66.08",
                    "71.96∗"
                ],
                [
                    "[EMPTY]",
                    "c",
                    "70.23∗",
                    "72.19∗",
                    "62.83∗",
                    "64.55∗",
                    "32.47∗",
                    "79.49∗",
                    "74.74∗",
                    "81.53∗",
                    "75.92∗",
                    "51.47∗",
                    "61.74∗"
                ],
                [
                    "[EMPTY]",
                    "cat",
                    "[BOLD] 83.96",
                    "89.12",
                    "[BOLD] 79.23",
                    "83.70",
                    "45.08∗",
                    "[BOLD] 91.92",
                    "[BOLD] 90.03",
                    "[BOLD] 85.06",
                    "86.45",
                    "[BOLD] 66.17",
                    "71.82∗"
                ],
                [
                    "[EMPTY]",
                    "sg",
                    "83.88",
                    "89.06",
                    "79.22",
                    "83.71",
                    "45.26",
                    "91.66∗",
                    "88.83∗",
                    "84.96",
                    "86.40",
                    "65.49∗",
                    "71.87∗"
                ],
                [
                    "[EMPTY]",
                    "vg",
                    "83.45∗",
                    "89.05",
                    "79.13",
                    "[BOLD] 83.87",
                    "[BOLD] 45.88",
                    "91.55∗",
                    "89.49",
                    "84.82",
                    "[BOLD] 86.50",
                    "65.75",
                    "[BOLD] 72.82"
                ]
            ]
        },
        "gold_description": "table 2 shows the impact that different methods for combining character and word - level word representations have in the quality of the sentence representations produced by our models . we can observe the same trend mentioned in section 4 . 1 , and highlighted by the difference between bold values , that models trained in multinli performed better than those trained in snli at a statistically significant level , the two exceptions to the previous trend , sicke and sickr , benefited more from models trained on snli . additionally , there was no method that significantly outperformed the word only baseline in classification tasks . on the other hand , the vector gate significantly outperformed every other method in the stsb task when trained in both datasets , and in the sts16 task when trained in snli .",
        "generated_description": "we observe that our model achieves the highest accuracies for cr , mpqa , mr , sst2 , subj , trec , and trec . for sts16 and sicke , our model outperforms the baselines in most cases . for the semantic textual similarity task ( stsb ) , our model is competitive with the state - of - the - art results reported by \\ newcitedblp : journals / corr / abs - 1911 - 10683 and outperforms \\ newciteluong - etal : 2015 : acl - ijcnlp . the results for the relatedness task ( sickr ) are obtained by training our model on the semantic textual similarity ( sts16 ) task and using the stsb task - specific embeddings obtained from the wikitext - 103 corpus . our model performs the best in terms of sickr and stsb compared to the other models ."
    },
    {
        "table_id": "245",
        "table_info": {
            "table_caption": "Table 2: System accuracy (%) bucketed by gender and difficulty (so-called “gotchas,” shaded in purple). For female pronouns, a “gotcha” sentence is one where either (1) the correct answer is occupation but the occupation is <50% female (according to BLS); or (2) the occupation is ≥50% female but the correct answer is participant; this is reversed for male pronouns. Systems do uniformly worse on “gotchas.”",
            "table_column_names": [
                "System",
                "“Gotcha”?",
                "Female",
                "Male"
            ],
            "table_content_values": [
                [
                    "Rule",
                    "no",
                    "38.3",
                    "51.7"
                ],
                [
                    "Rule",
                    "[HTML]CBCEFByes",
                    "[HTML]CBCEFB10.0",
                    "[HTML]CBCEFB37.5"
                ],
                [
                    "Stat",
                    "no",
                    "50.8",
                    "61.7"
                ],
                [
                    "Stat",
                    "[HTML]CBCEFByes",
                    "[HTML]CBCEFB45.8",
                    "[HTML]CBCEFB40.0"
                ],
                [
                    "Neural",
                    "no",
                    "50.8",
                    "49.2"
                ],
                [
                    "Neural",
                    "[HTML]CBCEFByes",
                    "[HTML]CBCEFB36.7",
                    "[HTML]CBCEFB46.7"
                ]
            ]
        },
        "gold_description": "we also identify so - called \" gotcha \" sentences in which pronoun gender does not match the occupation ' s majority gender ( bls ) if occupation is the correct answer ; all systems perform worse on these \" gotchas . \" 8 ( see table 2 . )",
        "generated_description": "we find that the three systems do uniformly worse on “ gotchas . ” for example , the rule - based system gets the correct answer 51 . 7 % of the time , as opposed to 50 . 8 % for the stat - based and neural models , and 49 . 2 % for participant . this shows that these sentences are harder for both systems to answer than they are for the gotchas described above . interestingly , the neural model performs better on occupation sentences than on stat sentences . we speculate that this is because the occupation sentences tend to be more gender - coherent , making them easier to learn ."
    },
    {
        "table_id": "246",
        "table_info": {
            "table_caption": "Table 1: Automatic evaluation results of different models. For a detailed explanation of the baseline models, please refer to Section 3.2. The best performing model for each column is highlighted in boldface.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] B-1",
                "[BOLD] B-2",
                "[BOLD] B-3",
                "[BOLD] B-4",
                "[BOLD] ROUGE-L",
                "[BOLD] METEOR",
                "[BOLD] CIDEr"
            ],
            "table_content_values": [
                [
                    "Facts-to-seq",
                    "0.404",
                    "0.324",
                    "0.274",
                    "0.242",
                    "0.433",
                    "0.214",
                    "1.627"
                ],
                [
                    "Facts-to-seq w. Attention",
                    "0.491",
                    "0.414",
                    "0.366",
                    "0.335",
                    "0.512",
                    "0.257",
                    "2.207"
                ],
                [
                    "Static Memory",
                    "0.374",
                    "0.298",
                    "0.255",
                    "0.223",
                    "0.383",
                    "0.185",
                    "1.328"
                ],
                [
                    "DMN+",
                    "0.281",
                    "0.234",
                    "0.236",
                    "0.234",
                    "0.275",
                    "0.139",
                    "0.912"
                ],
                [
                    "Our Model",
                    "[BOLD] 0.611",
                    "[BOLD] 0.535",
                    "[BOLD] 0.485",
                    "[BOLD] 0.461",
                    "[BOLD] 0.641",
                    "[BOLD] 0.353",
                    "[BOLD] 3.295"
                ]
            ]
        },
        "gold_description": "the results of the experiments are reported in table 1 . a facts - to - seq model exploiting our positional fact encoding performs adequately . with an additional attention mechanism ( facts - to - seq w . attention ) , the results are even better . the analysis of the static memory approach amounts to an ablation study , the dmn + is even outperformed by our facts - to - seq baseline .",
        "generated_description": "our model significantly outperforms the facts - to - seq baseline and other baselines in terms of all automatic evaluation metrics , which indicates that our model is able to generate more relevant and comprehensive summaries for videos with factual correctness . moreover , the results of dmn + and static memory indicate that they can store the history of videos in a dynamic memory and dynamically retrieve relevant information from the video to improve the video captioning performance . our model achieves the best performance in bleu - 4 , rouge - l , meteor and cider scores , which means that it is capable of preserving the most salient information from videos ."
    },
    {
        "table_id": "247",
        "table_info": {
            "table_caption": "Table 2: Accuracies for the approaches. Baseline refers to the unmodified, non-adversarial InferSent.",
            "table_column_names": [
                "Model",
                "Val",
                "Test",
                "Hard"
            ],
            "table_content_values": [
                [
                    "Baseline",
                    "84.25",
                    "84.22",
                    "68.02"
                ],
                [
                    "AdvCls",
                    "84.58",
                    "83.56",
                    "66.27"
                ],
                [
                    "AdvDat",
                    "78.45",
                    "78.30",
                    "55.60"
                ]
            ]
        },
        "gold_description": "table 2 reports the results on snli , with the configurations that performed best on the validation set for each of the adversarial methods . both training methods perform worse than our unmodified , non - adversarial infersent baseline on snli ' s test set , since they remove biases that may be useful for performing this task . the difference for advcls is minimal , and it even slightly outperforms infersent on the validation set . while advdat ' s results are noticeably lower than the non - adversarial infersent , the drops are still less than 6 % points .",
        "generated_description": "for the baseline model , we report the accuracies of the unmodified , non - adversarial infersent model . we observe that advcls and advdat both outperform the baseline on both val and test sets , however advdat slightly underperforms on the hard test set . we hypothesize that this might be due to the dataset being too small to train a strong adversarial sentence - classifier for the adversarial examples . on the other hand , given that the models ’ accuracies on the val set are already high , we expect these approaches to perform well on the test set as well ."
    },
    {
        "table_id": "248",
        "table_info": {
            "table_caption": "Table 3: Indicator words and how correlated they are with CONTRADICTION predictions. The parentheses indicate hyper-parameter values: (λLoss,λEnc) for AdvCls and (λRand,λEnc) for AdvDat. Baseline refers to the unmodified InferSent.",
            "table_column_names": [
                "Word",
                "Count",
                "Score ^ [ITALIC] p( [ITALIC] l| [ITALIC] w)",
                "Score Baseline",
                "Percentage decrease from baseline AdvCls (1,1)",
                "Percentage decrease from baseline AdvDat (0.4,1)",
                "Percentage decrease from baseline AdvDat (1,1)"
            ],
            "table_content_values": [
                [
                    "sleeping",
                    "108",
                    "0.88",
                    "0.24",
                    "15.63",
                    "53.13",
                    "-81.25"
                ],
                [
                    "driving",
                    "53",
                    "0.81",
                    "0.32",
                    "-8.33",
                    "50",
                    "-66.67"
                ],
                [
                    "Nobody",
                    "52",
                    "1",
                    "0.42",
                    "14.29",
                    "42.86",
                    "14.29"
                ],
                [
                    "alone",
                    "50",
                    "0.9",
                    "0.32",
                    "0",
                    "83.33",
                    "0"
                ],
                [
                    "cat",
                    "49",
                    "0.84",
                    "0.31",
                    "7.14",
                    "57.14",
                    "-85.71"
                ],
                [
                    "asleep",
                    "43",
                    "0.91",
                    "0.39",
                    "-18.75",
                    "50",
                    "12.5"
                ],
                [
                    "no",
                    "31",
                    "0.84",
                    "0.36",
                    "0",
                    "52.94",
                    "-52.94"
                ],
                [
                    "empty",
                    "28",
                    "0.93",
                    "0.3",
                    "-16.67",
                    "83.33",
                    "-16.67"
                ],
                [
                    "eats",
                    "24",
                    "0.83",
                    "0.3",
                    "37.5",
                    "87.5",
                    "-25"
                ],
                [
                    "naked",
                    "20",
                    "0.95",
                    "0.46",
                    "0",
                    "83.33",
                    "-33.33"
                ]
            ]
        },
        "gold_description": "for each of the most biased words in snli associated with the contradiction label , we computed the probability that a model predicts an example as a contradiction , given that the hypothesis contains the word . table 3 shows the top 10 examples in the training set . for each word w , we give its frequency in snli , its empirical correlation with the label and with infersent ' s prediction , and the percentage decrease in correlations with contradiction predictions by three configurations of our methods . generally , the baseline correlations are more uniform than the empirical ones ( ˆp ( l | w ) ) , however , we still observed small skews towards contradiction . thus , we investigate whether our methods reduce the probability of predicting contradiction when a hypothesis contains an indicator word . the model trained with advdat ( where λrand = 0 . 4 , λenc = 1 ) predicts contradiction much less frequently than infersent on examples with these words .",
        "generated_description": "for example , the word “ no ” has a relatively low score of 1 . 42 , which can be explained by the fact that this word is rarely used in the context of the first sentence and is usually used as a negation word . similarly , the score of “ passage ” is also relatively low , which is explained by its low frequency in the corpus as compared to other words ."
    },
    {
        "table_id": "249",
        "table_info": {
            "table_caption": "Table 1: NEI F1 scores. There are two language models: one for entity, and one for non-entity. All LMs are trained on the standard Train split. At test time, the tag associated with the language model which gives the lowest perplexity is used as prediction. Consecutive tokens with the same tag are joined into a phrase. Scores are Phrase-level F1.",
            "table_column_names": [
                "Model",
                "amh",
                "ara",
                "ben",
                "fas",
                "hin",
                "hun",
                "orm",
                "rus",
                "som",
                "tgl",
                "tir",
                "uig",
                "yor",
                "avg"
            ],
            "table_content_values": [
                [
                    "Cap.",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "SRILM",
                    "43.9",
                    "30.9",
                    "59.8",
                    "38.2",
                    "50.1",
                    "64.3",
                    "50.1",
                    "53.1",
                    "68.3",
                    "68.0",
                    "62.5",
                    "35.5",
                    "66.5",
                    "53.1"
                ],
                [
                    "Skip-gram",
                    "25.9",
                    "10.5",
                    "20.4",
                    "15.4",
                    "16.1",
                    "46.8",
                    "31.5",
                    "36.4",
                    "36.2",
                    "44.3",
                    "35.3",
                    "13.4",
                    "43.0",
                    "28.9"
                ],
                [
                    "CBOW",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "BiLSTM",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ]
            ]
        },
        "gold_description": "our datasets vary in size of entity and non - entity tokens , as shown in table 1 . the smallest , farsi , has 4 . 5k entity and 50k non - entity tokens ; the largest , english , has 29k entity and 170k nonentity tokens .",
        "generated_description": "as can be seen from the table , cbow outperforms srilm and cap . bag - of - words and skip - gram models by a large margin , and is competitive with the neural network models ."
    },
    {
        "table_id": "250",
        "table_info": {
            "table_caption": "Table 2: Comparison with existing datasets. (D documents or news clusters; L labeled documents or news clusters; T event types; S slots.)",
            "table_column_names": [
                "[BOLD] Dataset",
                "[BOLD] #D",
                "[BOLD] #L",
                "[BOLD] #T",
                "[BOLD] #S"
            ],
            "table_content_values": [
                [
                    "MUC 4",
                    "1700",
                    "400",
                    "4",
                    "4"
                ],
                [
                    "ACE 2005",
                    "599",
                    "599",
                    "33",
                    "36"
                ],
                [
                    "ERE",
                    "562",
                    "562",
                    "38",
                    "27"
                ],
                [
                    "ASTRE",
                    "1038",
                    "100",
                    "12",
                    "18"
                ],
                [
                    "[BOLD] GNBusiness",
                    "12,985",
                    "680",
                    "–",
                    "8"
                ]
            ]
        },
        "gold_description": "a comparison with existing event extraction and event schema induction datasets , including astre ( nguyen et al . , 2016a ) , muc 4 , ace 20052 and ere3 , is shown in table 2 . compared with the other datasets , gnbusiness has a much larger number of documents ( i . e . , news clusters in gnbusiness ) , and a comparable number of labeled documents .",
        "generated_description": "the gnbusiness dataset is a large document set of 12 , 985 wikipedia articles and news clusters , containing 11 , 985 documents ( 1 , 985 labeled documents ) and 680 news clusters ( 840 labeled news clusters ) . the size of each of these datasets ranges from 1 , 000 to 20 , 000 documents , and most of them are multi - label ( d ) , i . e . , 10 , 000 or more ) . the number of slots in gnbusiness is relatively small compared to other datasets ( 8 vs . 27 ) , so we do not compare it with other existing datasets that focus on entity linking ( e . g . , ace 2005 ) ."
    },
    {
        "table_id": "251",
        "table_info": {
            "table_caption": "Table 3: Varying MILk’s λ with and without mass preservation on the DeEn development set.",
            "table_column_names": [
                "[ITALIC] λ",
                "unpreserved BLEU",
                "unpreserved DAL",
                "preserved BLEU",
                "preserved DAL"
            ],
            "table_content_values": [
                [
                    "0.0",
                    "27.7",
                    "21.0",
                    "27.7",
                    "27.9"
                ],
                [
                    "0.1",
                    "27.0",
                    "13.6",
                    "27.6",
                    "10.5"
                ],
                [
                    "0.2",
                    "25.7",
                    "11.6",
                    "27.5",
                    "8.7"
                ]
            ]
        },
        "gold_description": "before preservation , milk with a latency weight λ = 0 still showed a substantial reduction in latency from the maximum value of 27 . 9 , indicating an intrinsic latency incentive . furthermore , training quickly destabilized , resulting in very poor trade - offs for λs as low as 0 . 2 .",
        "generated_description": "λ controls the trade - off between the quality and the size of milk ’ s λ . to test this , we vary λ from 0 . 0 to 1 . 0 and investigate the effect of mass preservation on the deen development set . when λ = 0 . 1 , the model produces a bleu score of 21 . 0 on the unpreserved set , while when λ is preserved , the score degrades to 10 . 5 . this suggests that the quality of the mt system benefits from the use of a large λ when producing a high - quality mt system ."
    },
    {
        "table_id": "252",
        "table_info": {
            "table_caption": "Table 4: Overall performance of schema matching.",
            "table_column_names": [
                "[BOLD] Method",
                "[BOLD] Schema Matching (%)  [ITALIC] P",
                "[BOLD] Schema Matching (%)  [ITALIC] R",
                "[BOLD] Schema Matching (%)  [ITALIC] F1"
            ],
            "table_content_values": [
                [
                    "DBLP:conf/acl/NguyenTFB15",
                    "41.5",
                    "53.4",
                    "46.7"
                ],
                [
                    "Clustering",
                    "41.2",
                    "50.6",
                    "45.4"
                ],
                [
                    "ODEE-F",
                    "41.7",
                    "53.2",
                    "46.8"
                ],
                [
                    "ODEE-FE",
                    "42.4",
                    "56.1",
                    "48.3"
                ],
                [
                    "ODEE-FER",
                    "[BOLD] 43.4",
                    "[BOLD] 58.3",
                    "[BOLD] 49.8"
                ]
            ]
        },
        "gold_description": "table 4 shows the overall performance of schema matching on gnbusinesstest . from the table , we can see that odee - fer achieves the best f1 scores among all the methods . by comparing nguyen et al . ( 2015 ) and odeef ( p = 0 . 01 ) , we can see that using continuous contextual features gives better performance than discrete features . among odee models , odee - fe gives a 2 % gain in f1 score against odee - f , there is a 1 % gain in f",
        "generated_description": "from the table , we can see that odee - fer achieves the best performance in terms of precision , recall and f1 - score , which significantly outperforms the baseline dblp : conf / acl / nguyentfb15 and the clustering approach by a large margin , which demonstrates the effectiveness of our approach ."
    },
    {
        "table_id": "253",
        "table_info": {
            "table_caption": "Table 5: Averaged slot coherence results.",
            "table_column_names": [
                "[BOLD] Method",
                "[BOLD] Ave Slot Coherence"
            ],
            "table_content_values": [
                [
                    "DBLP:conf/acl/NguyenTFB15",
                    "0.10"
                ],
                [
                    "ODEE-F",
                    "0.10"
                ],
                [
                    "ODEE-FE",
                    "0.16"
                ],
                [
                    "ODEE-FER",
                    "[BOLD] 0.18"
                ]
            ]
        },
        "gold_description": "table 5 shows the comparison of averaged slot coherence results over all the slots in the schemas . the averaged slot coherence of odee - fer is the highest , the averaged slot coherence of odee - f is comparable to that of nguyen et al . ( 2015 ) ( p = 0 . 3415 ) , the scores of odee - fe ( p = 0 . 06 ) and odee - fer ( p = 10 − 5 ) are both higher than that of odee - f ,",
        "generated_description": "we can see that odee - fer obtains the best slot coherence compared with the baseline dblp : conf / acl / nguyentfb15 and obtains close to the state - of - the - art performance ( 0 . 18 ) , which verifies the effectiveness of our framework ."
    },
    {
        "table_id": "254",
        "table_info": {
            "table_caption": "Table 2: Method Comparison (%).",
            "table_column_names": [
                "[BOLD] Method",
                "[BOLD] METEOR",
                "[BOLD] ROUGE-L",
                "[BOLD] HUMAN  [BOLD] PREFER-"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[BOLD] ENCE"
                ],
                [
                    "LSTM-LM",
                    "8.7",
                    "15.1",
                    "0"
                ],
                [
                    "Seq2seq",
                    "13.5",
                    "19.2",
                    "22"
                ],
                [
                    "ED(1)",
                    "13.3",
                    "[BOLD] 20.3",
                    "30"
                ],
                [
                    "ED(2)",
                    "[BOLD] 14.0",
                    "19.8",
                    "[BOLD] 48"
                ]
            ]
        },
        "gold_description": "table 2 presents meteor ( denkowski and lavie , 2014 ) and rouge - l ( lin , 2004 ) scores for each method , where we can see score gains on both metrics from the editing mechanism . table 2 shows that the human judges strongly favor the abstracts from our ed ( 2 ) method .",
        "generated_description": "from the table , we can see that our proposed ed ( 1 ) and ed ( 2 ) significantly outperform other methods in terms of both meteor and rouge - l , while the human preference score is close to that of the original seq2seq model . this indicates that the proposed method is able to generate more natural and informative responses ."
    },
    {
        "table_id": "255",
        "table_info": {
            "table_caption": "Table 3: Plagiarism Check: Percentage (%) of n-grams in test abstracts generated by system/human which appeared in training data.",
            "table_column_names": [
                "[ITALIC] n",
                "[BOLD] 1",
                "[BOLD] 2",
                "[BOLD] 3",
                "[BOLD] 4",
                "[BOLD] 5",
                "[BOLD] 6"
            ],
            "table_content_values": [
                [
                    "[BOLD] System",
                    "100",
                    "94.4",
                    "67.3",
                    "35.0",
                    "15.9",
                    "6.6"
                ],
                [
                    "[BOLD] Human",
                    "98.2",
                    "78.5",
                    "42.2",
                    "17.9",
                    "7.7",
                    "4.1"
                ]
            ]
        },
        "gold_description": "we also conduct a plagiarism check in table 3 , which shows that 93 . 4 % of 6 - grams generated by ed ( 2 ) did not appear in the training data , indicating that our model is not simply copying .",
        "generated_description": "in order to check the percentage of genuinely bad or grammatically incorrect n - grams in test abstracts generated by system and human which appeared in training data , we checked the incidence of them in test sets . as shown in table 3 , there is no significant difference between the percentages of bad or good ngrams produced by the system and humans ."
    },
    {
        "table_id": "256",
        "table_info": {
            "table_caption": "Table 5: Iteration comparison (%)",
            "table_column_names": [
                "[ITALIC] n",
                "[BOLD] 1",
                "[BOLD] 2",
                "[BOLD] 3",
                "[BOLD] 4",
                "[BOLD] 5",
                "[BOLD] 6"
            ],
            "table_content_values": [
                [
                    "METEOR",
                    "13.3",
                    "[BOLD] 14.0",
                    "13.6",
                    "13.9",
                    "13.8",
                    "13.5"
                ],
                [
                    "ROUGE-L",
                    "[BOLD] 20.3",
                    "19.8",
                    "18.6",
                    "19.2",
                    "18.9",
                    "18.8"
                ]
            ]
        },
        "gold_description": "we trained and evaluated our editing approach with 1 - 6 iterations and the experimental results ( table 5 ) showed that the second iteration produced the best results .",
        "generated_description": "we observe that the rouge - l score increases with n and the meteor score decreases with n . we conjecture that this is because the length of the reference summaries grows as n increases and the model has to iterate over the original summary to find the appropriate number of words to copy from the input summary ."
    },
    {
        "table_id": "257",
        "table_info": {
            "table_caption": "Table 4: Turing Test Passing Rates.",
            "table_column_names": [
                "[EMPTY]",
                "[BOLD] # Tests",
                "[BOLD] # Choices  [BOLD] per Test",
                "[BOLD] Non-expert  [BOLD] Non-CS",
                "[BOLD] Non-expert  [BOLD] CS",
                "[BOLD] NLP Expert  [BOLD] Junior",
                "[BOLD] NLP Expert  [BOLD] Senior"
            ],
            "table_content_values": [
                [
                    "Different Titles",
                    "50",
                    "2",
                    "30%",
                    "15%",
                    "12%",
                    "0%"
                ],
                [
                    "Different Titles",
                    "20",
                    "5",
                    "60%",
                    "20%",
                    "30%",
                    "20%"
                ],
                [
                    "Different Titles",
                    "10",
                    "10",
                    "80%",
                    "30%",
                    "30%",
                    "20%"
                ],
                [
                    "Same Title",
                    "50",
                    "2",
                    "54%",
                    "10%",
                    "4%",
                    "0%"
                ],
                [
                    "Same Title",
                    "20",
                    "5",
                    "75%",
                    "25%",
                    "5%",
                    "5%"
                ]
            ]
        },
        "gold_description": "as expected , table 4 shows that people with less domain knowledge are more easily deceived . specifically , non - cs human judges fail at more than half of the 1 - to - 1 sets for the same titles , which suggests that most of our system generated abstracts follow correct grammar and consistent writing style . domain experts fail on 1 or 2 sets , mostly because the human written abstracts in those sets don ' t seem very topically relevant . additionally , the more abstracts that we provided to human judges , the easier it is to conceal the system generated abstract amongst human generated ones .",
        "generated_description": "turing test performance . the number of tests and number of choices per test are also important to note . non - expert annotators pass the turing test 50 % of the time , while the nlp experts pass the test 50 % and the senior level annotator passes the test 20 % and 5 % times per test . the nlp expert outperforms the non - expert category in terms of the number of choosing of a text to annotate ( i . e . , choosing the same or different titles for the input text ) at the cost of a higher number of failed turing test choices . in general , the performance of the experts on the same text is better than that of non - experts ( e . g . , 10 % for the senior level ) . this is because the annotators tend to trust the experts more when they have a good title for the test text ."
    },
    {
        "table_id": "258",
        "table_info": {
            "table_caption": "Table 2: Unimodal sentiment analysis results on the CMU-MOSI test set. Numbers in bold are the best results on each modality.",
            "table_column_names": [
                "[BOLD] CC",
                "[BOLD] S",
                "[BOLD] S+P",
                "[BOLD] S+I",
                "[BOLD] S+P+I"
            ],
            "table_content_values": [
                [
                    "Random",
                    "–",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "Vocal",
                    "0.125",
                    "0.149",
                    "0.119",
                    "[BOLD] 0.153"
                ],
                [
                    "Visual",
                    "0.092",
                    "0.109",
                    "[BOLD] 0.116",
                    "0.106"
                ],
                [
                    "Verbal",
                    "0.404",
                    "[BOLD] 0.455",
                    "0.434",
                    "0.417"
                ],
                [
                    "Human",
                    "0.820",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[BOLD] MAE",
                    "[BOLD] S",
                    "[BOLD] S+P",
                    "[BOLD] S+I",
                    "[BOLD] S+P+I"
                ],
                [
                    "Random",
                    "1.880",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "Vocal",
                    "1.456",
                    "1.471",
                    "1.444",
                    "[BOLD] 1.431"
                ],
                [
                    "Visual",
                    "1.442",
                    "[BOLD] 1.439",
                    "1.453",
                    "1.460"
                ],
                [
                    "Verbal",
                    "1.196",
                    "[BOLD] 1.156",
                    "1.181",
                    "1.206"
                ],
                [
                    "Human",
                    "0.710",
                    "–",
                    "–",
                    "–"
                ]
            ]
        },
        "gold_description": "the results of unimodal sentiment prediction experiments are shown in table 2 . the verbal models have the best performance here , on each modality , the best performance is achieved by a multi - task learning model . all unimodal models have significantly different performance . p = 0 . 009 for s + p and s + p + i visual models , p < < 0 . 001 for visual and vocal s + i models . in multi - task learning , the main task gains additional information from the auxillary tasks . compared to the s model , the s + p model has increased focus on the polarity of sentiment , while the s + i model has increased focus on the intensity of sentiment . on the verbal modality , the s + p model achieved the best performance , while on the visual modality the s + i model achieved the best performance . for the vocal modality , the s + p + i model achieved the best performance , and the s + p model yielded improved performance over that of the s model .",
        "generated_description": "we report the mean average error ( mae ) , the standard deviation ( s ) , and the pearson correlation ( s + p ) of the five human annotators when performing the task in the same way as in the previous section ( i . e . , s + p + i ) . it can be seen that the proposed i + i method outperforms the baselines for both the verbal and the visual aspects , with the performance for the visual aspect being slightly better than for the acoustic one . this is due to the fact that the majority of the utterances in the test set of the avsd dataset contain only one or two categories of utterances , which makes the task easier ."
    },
    {
        "table_id": "259",
        "table_info": {
            "table_caption": "Table 3: Multimodal sentiment analysis results on the CMU-MOSI test set. Numbers in bold are the best results for each fusion strategy in each row.",
            "table_column_names": [
                "[BOLD] CC",
                "[BOLD] S",
                "[BOLD] S+P",
                "[BOLD] S+I",
                "[BOLD] S+P+I"
            ],
            "table_content_values": [
                [
                    "Random",
                    "–",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "EF",
                    "0.471",
                    "0.472",
                    "0.476",
                    "[BOLD] 0.482"
                ],
                [
                    "TFN",
                    "0.448",
                    "[BOLD] 0.461",
                    "0.446",
                    "0.429"
                ],
                [
                    "LF",
                    "[BOLD] 0.454",
                    "0.413",
                    "0.428",
                    "0.428"
                ],
                [
                    "HF",
                    "[BOLD] 0.469",
                    "0.424",
                    "0.458",
                    "0.432"
                ],
                [
                    "Human",
                    "0.820",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[BOLD] MAE",
                    "[BOLD] S",
                    "[BOLD] S+P",
                    "[BOLD] S+I",
                    "[BOLD] S+P+I"
                ],
                [
                    "Random",
                    "1.880",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "EF",
                    "1.197",
                    "1.181",
                    "1.193",
                    "[BOLD] 1.172"
                ],
                [
                    "TFN",
                    "1.186",
                    "1.181",
                    "[BOLD] 1.178",
                    "1.205"
                ],
                [
                    "LF",
                    "[BOLD] 1.179",
                    "1.211",
                    "1.204",
                    "1.201"
                ],
                [
                    "HF",
                    "[BOLD] 1.155",
                    "1.211",
                    "1.164",
                    "1.187"
                ],
                [
                    "Human",
                    "0.710",
                    "–",
                    "–",
                    "–"
                ]
            ]
        },
        "gold_description": "of results the multimodal the experiments are shown in table 3 . we find that ef > hf > tfn > lf . 4 unlike zadeh et al . ( 2017 ) , here the ef model outperforms the tfn model . however , the tfn model achieved the best performance on the training and validation sets . compared to the feature concatenation used in ef , the cartesian product used in tfn results in higher dimensionality of the multimodal input vector , 5 which in turn increases the complexity of the model . similarly , the hf model has worse performance than the ef model here , unlike in tian et al . ( 2016 ) . in general , the multimodal models have better performance than the unimodal models . in fact , the hf and lf models have better performance using single - task learning . for the tfn models , only the s + p model outperforms the s model , although the improvement is not significant . 7 for the ef models , multi - task learning results in better performance . dimension of the ef input is 420 , for tfn is 65 , 536 . 6except that the lf models often have worse performance than the verbal s + p model . p < < 0 . 001 for tfn s + p and verbal s + p , p = 0 . 017 for verbal s + p and lf s . 7p = 0 . 105 for s tfn and s + p tfn . 8p = 0 . 888 for s ef and s + p ef , p = 0 . 029 for s ef and s + i ef , p = 0 . 009 for s ef and s + p + i ef .",
        "generated_description": "we report the mean average error ( mae ) and the fisher ’ s exact match ( s ) as well as the pearson correlation between the annotators ’ judgments and the random forest ( rae ) we also report the pearson correlations in the last column of the table , as a way to measure the robustness of the classifiers against confounding factors . the results show that the macro average of the s + p and s + i models is 0 . 475 and 0 . 476 , respectively . the best results are obtained using the tfn model with an average mae of 1 . 181 and 1 . 193 , and the best results for the other models with the hf and tfn models are 0 . 469 ."
    },
    {
        "table_id": "260",
        "table_info": {
            "table_caption": "Table 2: The BioBERT performance on the MedNLI task. Each model is trained on three different combinations of PMC and PubMed datasets (top score marked as bold).",
            "table_column_names": [
                "[BOLD] Dataset",
                "[BOLD] Accuracy dev",
                "[BOLD] Accuracy test"
            ],
            "table_content_values": [
                [
                    "+PMC",
                    "80.50",
                    "78.97"
                ],
                [
                    "+PubMedd",
                    "81.14",
                    "78.83"
                ],
                [
                    "+PubMed+PMC",
                    "[BOLD] 82.15",
                    "[BOLD] 79.04"
                ]
            ]
        },
        "gold_description": "as shown in table 2 , biobert trained on pubmed + pmc performs the best .",
        "generated_description": "note that the biobert model is trained on three different combinations of pmc and pubmedd datasets . the top score on the mednli test set is achieved by using the combined pubmed and pmc datasets ."
    },
    {
        "table_id": "261",
        "table_info": {
            "table_caption": "Table 2: Classification performance of the compared methods on various topics, measured by the averaged macro F1-score over ten runs on the test data.",
            "table_column_names": [
                "Topic",
                "BiLSTM",
                "DeAT",
                "BiMPM",
                "RCN (Our)"
            ],
            "table_content_values": [
                [
                    "CC",
                    "68.1±0.6",
                    "70.9±0.7",
                    "71.5±0.6",
                    "[BOLD] 73.0±0.5∗"
                ],
                [
                    "HC",
                    "52.5±0.6",
                    "56.9±0.4",
                    "56.4±0.7",
                    "[BOLD] 58.6±0.4∗∗"
                ],
                [
                    "FM",
                    "58.3±0.6",
                    "60.6±0,7",
                    "59.8±0.7",
                    "[BOLD] 64.4±0.5∗∗"
                ],
                [
                    "AT",
                    "67.5±0.4",
                    "69.5±0.5",
                    "70.3±0.6",
                    "[BOLD] 72.2±0.4∗"
                ],
                [
                    "LA",
                    "61.3±0.3",
                    "63.2±0.6",
                    "62.4±0.4",
                    "[BOLD] 64.5±0.4∗∗"
                ],
                [
                    "Two tailed t-test: ∗∗  [ITALIC] p<0.01; ∗  [ITALIC] p<0.05",
                    "Two tailed t-test: ∗∗  [ITALIC] p<0.01; ∗  [ITALIC] p<0.05",
                    "Two tailed t-test: ∗∗  [ITALIC] p<0.01; ∗  [ITALIC] p<0.05",
                    "Two tailed t-test: ∗∗  [ITALIC] p<0.01; ∗  [ITALIC] p<0.05",
                    "Two tailed t-test: ∗∗  [ITALIC] p<0.01; ∗  [ITALIC] p<0.05"
                ]
            ]
        },
        "gold_description": "table 2 shows the results of our method and all the baselines on tasks with different topics . we can first observe that the proposed rcn consistently outperformed all the baselines across all topics . despite being modest , all the improvements of rcn over the baselines are statistically significant at p < 0 . 05 with a two - tailed t - test . bilstm performed the worst , showing that only using the rnn encoder for sequence encoding is not sufficient for obtaining optimal results . deat and bimpm performed similarly well ; rcn performed the best , with relative improvements from 2 . 1 % to 10 . 4 % over the second best .",
        "generated_description": "our rcn model outperforms the bilstm , deat , and bimpm models on nine of the ten topics and achieves the highest accuracy on four of them : cc , fm , hc , at , la , and fr . the results show that rcn is better than the other models on all topics . note that the results of rcn are obtained without tuning on the development set , and thus are directly comparable to the evaluation results ."
    },
    {
        "table_id": "262",
        "table_info": {
            "table_caption": "Table 3: Accuracy on Test set.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Accuracy (±0.01)"
            ],
            "table_content_values": [
                [
                    "LSTM+Feed-Forward",
                    "0.518"
                ],
                [
                    "LSTM+Gated CNN+Feed-Forward",
                    "0.524"
                ],
                [
                    "BERT Features(512 tokens)+Feed-Forward",
                    "0.639"
                ],
                [
                    "BERT Classifier(30 tokens / 15 tokens from each paragraph)",
                    "0.681"
                ],
                [
                    "BERT Classifier(128 tokens / 64 tokens from each paragraph)",
                    "0.717"
                ],
                [
                    "BERT Classifier(256 tokens / 128 tokens from each paragraph)",
                    "0.843"
                ]
            ]
        },
        "gold_description": "different approaches have been used to solve this task . the best result belongs to classifying order of paragraphs using pre - trained bert model . it achieves around 84 % accuracy on test set which outperforms other models significantly . first , each paragraph is encoded with lstm . the hidden state at the end of each sentence is extracted , and the resulting matrix is going through gated cnn ( dauphin et al . , 2017 ) for extraction of single encoding for each paragraph . the accuracy is barely above 50 % , which depicts that this method is not very promising . we have used a pre - trained bert in two different ways . first , as a feature extractor without fine - tuning , and second , by fine - tuning the weights during training . in the case of fine - tuning , we have used different numbers for maximum sequence length to test the capability of bert in this task . we increased the number of tokens and accuracy respectively increases . we found this method very promising and the accuracy significantly increases with respect to previous methods ( table 3 ) . this result reveals fine - tuning pre - trained bert can approximately learn the order of the paragraphs and arrow of the time in the stories .",
        "generated_description": "we can see that the accuracy of the lstm + gated cnn + feed - forward baseline is 0 . 518 , which is lower than the previous state - of - the - art result of 0 . 524 . the bert features can further improve the accuracy to 0 . 639 . however , the accuracy is still lower than that of the bert classifier ( 0 . 717 ) , which is due to the small number of training examples ."
    },
    {
        "table_id": "263",
        "table_info": {
            "table_caption": "Table 2: Average performance across all models depending on the window position.",
            "table_column_names": [
                "[BOLD] Window position",
                "[BOLD] SimLex999",
                "[BOLD] Analogies"
            ],
            "table_content_values": [
                [
                    "OS left",
                    "0.40",
                    "0.35"
                ],
                [
                    "OS right",
                    "0.43",
                    "0.35"
                ],
                [
                    "OS symmetric",
                    "0.43",
                    "[BOLD] 0.45"
                ],
                [
                    "GW left",
                    "0.43",
                    "0.64"
                ],
                [
                    "GW right",
                    "0.44",
                    "0.65"
                ],
                [
                    "GW symmetric",
                    "0.45",
                    "[BOLD] 0.68"
                ]
            ]
        },
        "gold_description": "table 2 shows how the position of the context window influences the average model performance . note that symmetric windows of , for instance , 10 are in fact 2 times larger than the ' left ' or ' right ' windows of the same size , as they consider 10 words both to the left and to the right of the focus word . this is most likely why symmetric windows consistently outperform ' single - sided ' ones on the analogy task , as they are able to include twice as much contextual input . however , the average performance on the semantic similarity task ( as indicated by the spearman correlation with the simlex999 test set ) does not exhibit the same trend . ' left ' windows are indeed worse than symmetric ones , but ' right ' windows are on par with the symmetric windows for opensubtitles and only one percent point behind them for gigaword . it means that in many cases ( at least with english texts ) taking into account only n context words to the right of the focus word is sufficient to achieve the same performance with simlex999 as by using a model which additionally considers n words to the left , and thus requires significantly more training time .",
        "generated_description": "we observe that the best performance is achieved when the window size is at the height of the window ( i . e . , around the window on the right ) . however , as we move away from the symmetric window , the performance starts to decline . we hypothesize that this is due to the fact that the word embeddings are now closer to the embedding space of the semantic space , making it less able to capture the asymmetry of the word ."
    },
    {
        "table_id": "264",
        "table_info": {
            "table_caption": "Table 3: Average performance across all models with and without cross-sentential contexts.",
            "table_column_names": [
                "[BOLD] Cross-sentential",
                "[BOLD] SimLex999",
                "[BOLD] Analogies"
            ],
            "table_content_values": [
                [
                    "OS False",
                    "[BOLD] 0.44",
                    "0.34"
                ],
                [
                    "OS True",
                    "0.40",
                    "[BOLD] 0.43"
                ],
                [
                    "GW False",
                    "0.44",
                    "0.66"
                ],
                [
                    "GW True",
                    "0.44",
                    "0.65"
                ]
            ]
        },
        "gold_description": "for similarity tasks , cross - sentential contexts do not seem useful , and can even be detrimental for large window sizes . however , for analogy tasks , crosssentential contexts lead to improved results thanks to the increased window it provides . this is especially pronounced for corpora with short sentences such as opensubtitles ( see table 3 ) .",
        "generated_description": "we observe that simlex999 and analogies are sensitive to the presence of cross - sentential contexts . this is not the case for semantic analogies , where the overlap between the semantic and syntactic contexts does not have a significant effect . in simlex999 , the number of analogies that can be drawn from the semantic contexts is reduced from 1 to 0 . 44 for both false and true labels . on the other hand , on the analogies dataset , this number drops from 0 . 66 to a much smaller 0 . 43 . this shows that the semantic context plays an important role in both datasets , as it allows the model to learn to better represent the semantic relations between words ."
    },
    {
        "table_id": "265",
        "table_info": {
            "table_caption": "Table 4: Average performance across all models depending on the removal of stop words.",
            "table_column_names": [
                "[BOLD] Stop words removal",
                "[BOLD] SimLex999",
                "[BOLD] Analogies"
            ],
            "table_content_values": [
                [
                    "OS no removal",
                    "0.41",
                    "0.34"
                ],
                [
                    "OS with removal",
                    "0.42",
                    "[BOLD] 0.43"
                ],
                [
                    "GW no removal",
                    "0.44",
                    "0.64"
                ],
                [
                    "GW with removal",
                    "0.44",
                    "[BOLD] 0.68"
                ]
            ]
        },
        "gold_description": "as shown in table 4 , the removal of stop words does not really influence the average model performance for the semantic similarity task . the analogy task , however , benefits substantially from this filtering , for both corpora .",
        "generated_description": "we observe that the removal of stop words significantly improves the performance of the analogy task , with the numbers increasing from 0 . 44 for the os dataset to 0 . 42 for the gw dataset . we also see that the effect is more significant for the simlex999 dataset , where the number of analogies increases from 1 . 64 to 2 . 68 ."
    },
    {
        "table_id": "266",
        "table_info": {
            "table_caption": "Table 1: Mean matched validation accuracies (%) broken down by type of pooling method and presence or absence of character embeddings. Confidence intervals are calculated at 95% confidence over 10 runs for each method.",
            "table_column_names": [
                "[BOLD] Method",
                "[BOLD] w/o. chars",
                "[BOLD] w. chars"
            ],
            "table_content_values": [
                [
                    "[ITALIC] mean",
                    "71.3 ± 1.2",
                    "71.3 ± 0.7"
                ],
                [
                    "[ITALIC] sum",
                    "70.7 ± 1.0",
                    "70.9 ± 0.8"
                ],
                [
                    "[ITALIC] last",
                    "70.9 ± 0.6",
                    "71.0 ± 1.2"
                ],
                [
                    "[ITALIC] max",
                    "70.6 ± 1.1",
                    "71.0 ± 1.1"
                ]
            ]
        },
        "gold_description": "table 1 presents the results of using different pooling strategies for generating a raw sentence representation vector from the word vectors . we can observe that that both the mean method , and picking the last hidden state for both directions performed slightly better than the two other strategies , however at 95 % confidence we cannot assert that any of these methods is statistically different from one another . another interesting result , as shown by table 1 and table 2 , is that the model seemed to be insensitive to the usage of character embeddings ,",
        "generated_description": "character embeddings are an important aspect of the model that we investigate in this work . we first compare different pooling methods to see which type of pooling works best . for example , sum pooling and max pooling both outperform mean and last pooling , but the former is more robust to the presence or absence of character embedddings . we also observe that sum and max perform better than the other methods ."
    },
    {
        "table_id": "267",
        "table_info": {
            "table_caption": "Table 3: Validation accuracies (%) for our best model broken down by genre. Both CBOW and ESIM results are reported as in (Williams et al., 2017).",
            "table_column_names": [
                "[BOLD] Genre",
                "[BOLD] CBOW",
                "[BOLD] ESIM",
                "[BOLD] InnerAtt"
            ],
            "table_content_values": [
                [
                    "Fiction",
                    "67.5",
                    "73.0",
                    "73.2"
                ],
                [
                    "Government",
                    "67.5",
                    "74.8",
                    "75.2"
                ],
                [
                    "Slate",
                    "60.6",
                    "67.9",
                    "67.2"
                ],
                [
                    "Telephone",
                    "63.7",
                    "72.2",
                    "73.0"
                ],
                [
                    "Travel",
                    "64.6",
                    "73.7",
                    "72.8"
                ],
                [
                    "9/11",
                    "63.2",
                    "71.9",
                    "70.5"
                ],
                [
                    "Face-to-face",
                    "66.3",
                    "71.2",
                    "74.5"
                ],
                [
                    "Letters",
                    "68.3",
                    "74.7",
                    "75.4"
                ],
                [
                    "Oup",
                    "62.8",
                    "71.7",
                    "71.5"
                ],
                [
                    "Verbatim",
                    "62.7",
                    "71.9",
                    "69.5"
                ],
                [
                    "[BOLD] MultiNLI Overall",
                    "[BOLD] 64.7",
                    "[BOLD] 72.2",
                    "[BOLD] 72.3"
                ]
            ]
        },
        "gold_description": "in table 3 we report the accuracies obtained by our best model in both matched ( first 5 genres ) and mismatched ( last 5 genres ) development sets . we can observe that our implementation performed like esim overall ,",
        "generated_description": "all models perform better on multinli overall , with esim and inneratt outperforming cbow by a large margin in the face - to - face genre . however , the performance of esim on the other genres , especially in the telephone and travel genres , drops notably ."
    },
    {
        "table_id": "268",
        "table_info": {
            "table_caption": "Table 1: GLUE task performance of BERT models with different initialization. We report the scores on the validation, rather than test data, so these results differ from the original BERT paper.",
            "table_column_names": [
                "[BOLD] Dataset",
                "[BOLD] Pre-trained",
                "[BOLD] Fine-tuned, initialized with  [BOLD] normal distr.",
                "[BOLD] Fine-tuned, initialized with  [BOLD] pre-trained",
                "[BOLD] Metric",
                "[BOLD] Size"
            ],
            "table_content_values": [
                [
                    "MRPC",
                    "0/31.6",
                    "81.2/68.3",
                    "87.9/82.3",
                    "F1/Acc",
                    "5.8K"
                ],
                [
                    "STS-B",
                    "33.1",
                    "2.9",
                    "82.7",
                    "Acc",
                    "8.6K"
                ],
                [
                    "SST-2",
                    "49.1",
                    "80.5",
                    "92",
                    "Acc",
                    "70K"
                ],
                [
                    "QQP",
                    "0/60.9",
                    "0/63.2",
                    "65.2/78.6",
                    "F1/Acc",
                    "400K"
                ],
                [
                    "RTE",
                    "52.7",
                    "52.7",
                    "64.6",
                    "Acc",
                    "2.7K"
                ],
                [
                    "QNLI",
                    "52.8",
                    "49.5",
                    "84.4",
                    "Acc",
                    "130K"
                ],
                [
                    "MNLI-m",
                    "31.7",
                    "61.0",
                    "78.6",
                    "Acc",
                    "440K"
                ]
            ]
        },
        "gold_description": "table 1 shows that finetuned bert outperforms pre - trained bert by a significant margin on all the tasks ( with an average of 35 . 9 points of absolute difference ) . bert with weights initialized from normal distribution and further fine - tuned for a given task consistently produces lower scores than the ones achieved with pre - trained bert . in fact , for some tasks ( sts - b and qnli ) , initialization with random weights yields worse performance than pre - trained bert without fine - tuning .",
        "generated_description": "the performance of bert models fine - tuned with normal distillation is shown in the first two columns . in the last column , we fine - tune bert on the glue task of mnli and mnli - m . the accuracy of these two datasets is very close to each other , which indicates that the dataset is not difficult for bert to perform well even when the dataset size is small ."
    },
    {
        "table_id": "269",
        "table_info": {
            "table_caption": "Table 3: Comparisons with some recent points in the literature. Scores are tokenized BLEU.",
            "table_column_names": [
                "Comparison Point Chen2018",
                "Comparison Point BPE",
                "Comparison Point EnFr",
                "Ref 41.0",
                "Ours 38.8"
            ],
            "table_content_values": [
                [
                    "Wu2016",
                    "BPE",
                    "EnFr",
                    "39.0",
                    "38.8"
                ],
                [
                    "Lee2017",
                    "Char",
                    "CsEn",
                    "22.5",
                    "25.9"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "DeEn",
                    "25.8",
                    "31.6"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "FiEn",
                    "13.1",
                    "19.3"
                ]
            ]
        },
        "gold_description": "first , from the enfr results in table 3 , we are in line with gnmt ( wu et al . , 2016 ) , and within 2 bleu points of the rnn and transformer models investigated by chen et al . ( 2018 ) . we compare quite favorably with lee et al . ( 2017 ) , exceeding their reported scores by 3 - 6 points ,",
        "generated_description": "we can see that our model outperforms all the other recent models and achieves the best scores for all language pairs . for example , our model improves the bpe score from 41 . 0 to 38 . 8 for english to french and from 31 . 6 to 33 . 1 for chinese to french , which are + 2 . 8 and + 1 . 8 bleu points absolute higher than the previous best score ."
    },
    {
        "table_id": "270",
        "table_info": {
            "table_caption": "Table 2: Character versus BPE translation.",
            "table_column_names": [
                "Language",
                "Tokenized BLEU BPE",
                "Tokenized BLEU Char",
                "Tokenized BLEU Delta",
                "SacreBLEU Char"
            ],
            "table_content_values": [
                [
                    "EnFr",
                    "38.8",
                    "39.2",
                    "0.4",
                    "38.1"
                ],
                [
                    "CsEn",
                    "24.8",
                    "25.9",
                    "1.1",
                    "25.6"
                ],
                [
                    "DeEn",
                    "29.7",
                    "31.6",
                    "1.9",
                    "31.6"
                ],
                [
                    "FiEn",
                    "17.5",
                    "19.3",
                    "1.8",
                    "19.5"
                ]
            ]
        },
        "gold_description": "table 2 clearly shows the characterlevel systems outperforming bpe for all language pairs .",
        "generated_description": "character vs bpe translation . in this section , we investigate the effectiveness of using character - based rather than bpe - based translation models for the mt task . we train character - and subword - based mt models for all language pairs and compare them with bpe models . for example , for the english to french ( enfr ) and czech to czech ( csen ) , using bpe leads to a drop from 38 . 1 bleu to 17 . 5 , while using character embeddings leads to only a drop of 1 . 8 bbleu . for the other languages ( deen , fien , and deen ) , the drop is less pronounced , only 0 . 3 and 0 . 5 bleus . this shows that character embedding models are more effective than either bpe or the delta - based models ."
    },
    {
        "table_id": "271",
        "table_info": {
            "table_caption": "Table 4: Error counts out of 100 randomly sampled examples from the DeEn test set.",
            "table_column_names": [
                "Error Type",
                "BPE",
                "Char"
            ],
            "table_content_values": [
                [
                    "Lexical Choice",
                    "19",
                    "8"
                ],
                [
                    "Compounds",
                    "13",
                    "1"
                ],
                [
                    "Proper Names",
                    "2",
                    "1"
                ],
                [
                    "Morphological",
                    "2",
                    "2"
                ],
                [
                    "Other lexical",
                    "2",
                    "4"
                ],
                [
                    "Dropped Content",
                    "7",
                    "0"
                ]
            ]
        },
        "gold_description": "bpe and character sys tems differ most in the number of lexical choice errors , and in the extent to which they drop content . regarding lexical choice , the two systems differ not only in the number of errors , but in the nature of those errors . in particular , the bpe model had more trouble handling german compound nouns . we also found that both systems occasionally mistranslate proper names .",
        "generated_description": "we find that the most common error types are lexical choice ( 19 ) , proper names ( 1 ) , and morphological and other lexical errors ( 2 ) , and dropped content ( 7 ) . dropped content is a relatively rare error type where the model fails to recognize that the target is no longer a noun but a column ( e . g . , “ what is the number of children in this tree ? ” ) ."
    },
    {
        "table_id": "272",
        "table_info": {
            "table_caption": "Table 6: Compression results on WMT15 DeEn. The Comp. column shows the ratio of total computations carried out in the encoder.",
            "table_column_names": [
                "Encoder",
                "BPE Size",
                "BLEU",
                "Comp."
            ],
            "table_content_values": [
                [
                    "BiLSTM",
                    "Char",
                    "31.6",
                    "1.00"
                ],
                [
                    "BiLSTM",
                    "1k",
                    "30.5",
                    "0.44"
                ],
                [
                    "BiLSTM",
                    "2k",
                    "30.4",
                    "0.35"
                ],
                [
                    "BiLSTM",
                    "4k",
                    "30.0",
                    "0.29"
                ],
                [
                    "BiLSTM",
                    "8k",
                    "29.6",
                    "0.25"
                ],
                [
                    "BiLSTM",
                    "16k",
                    "30.0",
                    "0.22"
                ],
                [
                    "BiLSTM",
                    "32k",
                    "29.7",
                    "0.20"
                ],
                [
                    "Lee et. al. reimpl",
                    "Char",
                    "28.0",
                    "0.20"
                ],
                [
                    "BiLSTM + pooling",
                    "Char",
                    "30.0",
                    "0.47"
                ],
                [
                    "HM, 3-layer",
                    "Char",
                    "31.2",
                    "0.77"
                ],
                [
                    "HM, 2-layer",
                    "Char",
                    "30.9",
                    "0.89"
                ]
            ]
        },
        "gold_description": "unfortunately , even at just 1k vocabulary items , bpe has already lost a bleu point with respect to the character model . comparing the performance of our pooled bilstm model against bpe , we notice that for a comparable level of compression ( bpe size of 1k ) , bpe out - performs the pooled model by around 0 . 5 bleu points . at a similar level of performance ( bpe size of 4k ) , bpe has significantly shorter sequences . as shown in table 6 , the 3 - hm configuration achieves much better compression even when this is accounted for , and also gives slightly better performance than 2 - hm . in general , hm gating results in less compression but better performance than the fixed - stride techniques .",
        "generated_description": "as shown in the table , a simple encoder with 1k bpe achieves 30 . 5 bleu score on the wmt15 deen benchmark . our reimplementation of lee et al . ’ s reimpl , which uses a much larger character n - gram model , results in 28 . 0 on the standard evaluation metric . we can see that the hm - encoder outperforms the reimplemented reimpl by a large margin . for instance , the hm with 16k bpe matches the performance of the character - level bilstm encoder while using only 1 / 32th of the number of computations needed by the standard encoder ."
    },
    {
        "table_id": "273",
        "table_info": {
            "table_caption": "Table 2: micro-F1 (μF1) and ensembled F1 (eF1) performance of each system on the PHEME dataset. Performance is averaged across the five splits of Zubiaga et al. (2017). Results show the mean, standard deviation, and ensembled score across 15 seeds. Bold indicates best performance, underline indicates second best.",
            "table_column_names": [
                "Method",
                "[ITALIC] μP",
                "[ITALIC] μR",
                "[ITALIC] μF1",
                "eP",
                "eR",
                "eF1"
            ],
            "table_content_values": [
                [
                    "Zubiaga et al.  2017",
                    "0.667",
                    "0.556",
                    "0.607",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "BiLSTM",
                    "0.623",
                    "0.564",
                    "0.590",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "BERT",
                    "0.699 ± 0.0165",
                    "0.608 ± 0.0257",
                    "0.650 ± 0.0134",
                    "0.713",
                    "0.619",
                    "0.663"
                ],
                [
                    "BERT + Wiki",
                    "0.693 ± 0.0159",
                    "0.614 ± 0.0263",
                    "0.651 ± 0.0121",
                    "0.707",
                    "0.622",
                    "0.662"
                ],
                [
                    "BERT + Wiki + PU",
                    "0.699 ± 0.0133",
                    "[BOLD] 0.625 ± 0.0164",
                    "[BOLD] 0.660 ± 0.0109",
                    "[BOLD] 0.722",
                    "[BOLD] 0.646",
                    "[BOLD] 0.682"
                ],
                [
                    "BERT + Wiki +  [ITALIC] PUC",
                    "[BOLD] 0.701 ± 0.0108",
                    "0.618 ± 0.0176",
                    "0.657 ± 0.0097",
                    "0.715",
                    "0.627",
                    "0.668"
                ]
            ]
        },
        "gold_description": "the results for the tested systems are given in table 2 . again we saw large gains from the bert based models over the baseline from ( zubiaga et al . , 2017 ) and the 2 - layer bilstm . compared to training solely on pheme , fine tuning from basic citation needed detection saw very little improvement ( 0 . 1 f1 points ) . however , fine tuning with a model trained using pu learning led to an increase of 1 f1 point over the non - pu learning model , indicating that pu learning enables the wikipedia data to be useful for transferring to rumour detection . for puc , we saw an improvement of 0 . 7 f1 points over the baseline and lower overall variance than vanilla pu learning , meaning that the results with puc are more consistent across runs . when models are ensembled , pretraining with vanilla pu learning improved over no pretraining by almost 2 f1 points .",
        "generated_description": "for each method , we report the mean average precision ( μp ) , mean recall ( μr ) , mean mean f1 score ( measured by micro - average precision ( ef1 ) , and the official evaluation metric of the conll shared task zubiaga et al . as can be seen , the proposed puc method achieves the best performance in terms of all metrics and achieves the new state - of - the - art performance across all the four metrics ."
    },
    {
        "table_id": "274",
        "table_info": {
            "table_caption": "Table 3: Mean average precision (MAP) of models on political speeches. Bold indicates best performance, underline indicates second best.",
            "table_column_names": [
                "Method",
                "MAP"
            ],
            "table_content_values": [
                [
                    "Konstantinovskiy et al.  2018",
                    "0.267"
                ],
                [
                    "Hansen et al.  2019",
                    "0.302"
                ],
                [
                    "BERT",
                    "[BOLD] 0.346 ± 0.024"
                ],
                [
                    "BERT + Wiki",
                    "0.339 ± 0.025"
                ],
                [
                    "BERT + Wiki + PU",
                    "0.328 ± 0.027"
                ],
                [
                    "BERT + Wiki +  [ITALIC] PUC",
                    "0.321 ± 0.031"
                ]
            ]
        },
        "gold_description": "the results for political speech check - worthiness detection are given in table 3 . we found that the vanilla bert model performed the best of all models . as we added transfer learning and pu learning , the performance steadily dropped , with the worst performing model being the one using puc .",
        "generated_description": "the results show that bert is the best performing pre - trained language model for political speeches , with a map of 0 . 346 , exceeding the previous state - of - the - art by 0 . 024 . bert with wikipedia embeddings further improves map to 0 . 339 , which is a significant improvement of 2 . 3 % over the best previously reported result ( 0 . 267 by konstantinovskiy et al . this result shows that pre - training a large amount of wikipedia data can improve the performance of a ner model , and that the results obtained using bert are competitive with the results reported in recent literature ."
    },
    {
        "table_id": "275",
        "table_info": {
            "table_caption": "Table 4: F1 score comparing manual relabelling of the top 100 predictions by PUC model with the original labels in each dataset by two different annotators. Italics are average value between the two annotators.",
            "table_column_names": [
                "Dataset",
                "P",
                "R",
                "F1"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "0.817",
                    "0.870",
                    "0.843"
                ],
                [
                    "Wikipedia",
                    "0.848",
                    "0.870",
                    "0.859"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] 0.833",
                    "[ITALIC] 0.870",
                    "[ITALIC] 0.851"
                ],
                [
                    "[EMPTY]",
                    "0.875",
                    "0.824",
                    "0.848"
                ],
                [
                    "Twitter",
                    "0.863",
                    "0.812",
                    "0.836"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] 0.869",
                    "[ITALIC] 0.818",
                    "[ITALIC] 0.842"
                ],
                [
                    "[EMPTY]",
                    "0.338",
                    "0.893",
                    "0.490"
                ],
                [
                    "Politics",
                    "0.311",
                    "1.0",
                    "0.475"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] 0.325",
                    "[ITALIC] 0.947",
                    "[ITALIC] 0.483"
                ]
            ]
        },
        "gold_description": "our results are given in table 4 . we found that the wikipedia and twitter datasets contained labels which were more general , evidenced by similar high f1 scores from both annotators ( > 0 . 8 ) . for political speeches , we observed that the human annotators both found many more examples to be check - worthy than were labelled in the dataset .",
        "generated_description": "finally , we compare the results of the puc model with the labels provided by two different annotators , who manually labelled the top 100 labels in each dataset with the original labels . the results show that the proposed model significantly outperforms the baselines in terms of precision , recall , and f1 - score in all three datasets . in particular , the f1 score increases from 0 . 475 to 0 . 845 for the politics dataset , from 1 . 0 to 2 . 0 for the wikipedia dataset , and from 1 to 3 . 1 for the twitter dataset . this result shows that our proposed model is able to achieve high recall , but low precision ."
    },
    {
        "table_id": "276",
        "table_info": {
            "table_caption": "TABLE I: The comparison result of the number of corrected prediction, precision, recall, and f1-score",
            "table_column_names": [
                "[BOLD] dataset",
                "[BOLD] tool",
                "[BOLD] # correct prediction",
                "[BOLD] positive  [BOLD] precision",
                "[BOLD] positive  [BOLD] recall",
                "[BOLD] positive  [BOLD] F1",
                "[BOLD] neutral  [BOLD] precision",
                "[BOLD] neutral  [BOLD] recall",
                "[BOLD] neutral  [BOLD] F1",
                "[BOLD] negative  [BOLD] precision",
                "[BOLD] negative  [BOLD] recall",
                "[BOLD] negative  [BOLD] F1"
            ],
            "table_content_values": [
                [
                    "[BOLD] Stack Overflow",
                    "SentiStrength",
                    "1043",
                    "0.200",
                    "[BOLD] 0.359",
                    "0.257",
                    "0.858",
                    "0.772",
                    "0.813",
                    "0.397",
                    "0.433",
                    "0.414"
                ],
                [
                    "positive: 178",
                    "NLTK",
                    "1168",
                    "0.317",
                    "0.244",
                    "0.276",
                    "0.815",
                    "[BOLD] 0.941",
                    "0.873",
                    "[BOLD] 0.625",
                    "0.084",
                    "0.148"
                ],
                [
                    "neutral: 1,191",
                    "Standford CoreNLP",
                    "604",
                    "0.231",
                    "0.344",
                    "0.276",
                    "[BOLD] 0.884",
                    "0.344",
                    "0.495",
                    "0.177",
                    "[BOLD] 0.837",
                    "0.292"
                ],
                [
                    "negative: 131",
                    "SentiStrength-SE",
                    "1170",
                    "0.312",
                    "0.221",
                    "0.259",
                    "0.826",
                    "0.930",
                    "0.875",
                    "0.500",
                    "0.185",
                    "0.270"
                ],
                [
                    "sum: 1,500",
                    "Stanford CoreNLP SO",
                    "1139",
                    "0.317",
                    "0.145",
                    "0.199",
                    "0.836",
                    "0.886",
                    "0.860",
                    "0.365",
                    "0.365",
                    "0.365"
                ],
                [
                    "[EMPTY]",
                    "N-gram auto-sklearn",
                    "[BOLD] 1317",
                    "[BOLD] 0.667",
                    "0.316",
                    "[BOLD] 0.418",
                    "0.871",
                    "0.939",
                    "[BOLD] 0.904",
                    "0.600",
                    "0.472",
                    "[BOLD] 0.514"
                ],
                [
                    "[EMPTY]",
                    "N-gram auto-sklearn with SMOTE†",
                    "-",
                    "0.680",
                    "0.005",
                    "0.009",
                    "0.344",
                    "0.930",
                    "0.499",
                    "0.657",
                    "0.160",
                    "0.251"
                ],
                [
                    "[BOLD] App reviews",
                    "SentiStrength",
                    "213",
                    "0.745",
                    "0.866",
                    "0.801",
                    "0.113",
                    "0.320",
                    "0.167",
                    "0.815",
                    "0.338",
                    "0.478"
                ],
                [
                    "positive: 186",
                    "NLTK",
                    "184",
                    "0.751",
                    "0.812",
                    "0.780",
                    "0.093",
                    "[BOLD] 0.440",
                    "0.154",
                    "[BOLD] 1.000",
                    "0.169",
                    "0.289"
                ],
                [
                    "neutral: 25",
                    "Standford CoreNLP",
                    "237",
                    "0.831",
                    "0.715",
                    "0.769",
                    "[BOLD] 0.176",
                    "0.240",
                    "[BOLD] 0.203",
                    "0.667",
                    "0.754",
                    "0.708"
                ],
                [
                    "negative: 130",
                    "SentiStrength-SE",
                    "201",
                    "0.741",
                    "0.817",
                    "0.777",
                    "0.106",
                    "0.400",
                    "0.168",
                    "0.929",
                    "0.300",
                    "0.454"
                ],
                [
                    "sum: 341",
                    "Stanford CoreNLP SO",
                    "142",
                    "0.770",
                    "0.253",
                    "0.381",
                    "0.084",
                    "0.320",
                    "0.133",
                    "0.470",
                    "0.669",
                    "0.552"
                ],
                [
                    "[EMPTY]",
                    "N-gram auto-sklearn",
                    "[BOLD] 293",
                    "[BOLD] 0.822",
                    "[BOLD] 0.894",
                    "[BOLD] 0.853",
                    "0.083",
                    "0.066",
                    "0.073",
                    "0.823",
                    "[BOLD] 0.808",
                    "[BOLD] 0.807"
                ],
                [
                    "[EMPTY]",
                    "N-gram auto-sklearn with SMOTE†",
                    "-",
                    "0.520",
                    "0.885",
                    "0.641",
                    "0.100",
                    "0.058",
                    "0.073",
                    "0.648",
                    "0.622",
                    "0.607"
                ],
                [
                    "[BOLD] Jira issues",
                    "SentiStrength",
                    "714",
                    "0.850",
                    "[BOLD] 0.921",
                    "0.884",
                    "-",
                    "-",
                    "-",
                    "0.993",
                    "0.703",
                    "0.823"
                ],
                [
                    "positive: 290",
                    "NLTK",
                    "276",
                    "0.840",
                    "0.362",
                    "0.506",
                    "-",
                    "-",
                    "-",
                    "[BOLD] 1.000",
                    "0.269",
                    "0.424"
                ],
                [
                    "neutral: 0",
                    "Standford CoreNLP",
                    "626",
                    "0.726",
                    "0.621",
                    "0.669",
                    "-",
                    "-",
                    "-",
                    "0.945",
                    "0.701",
                    "0.805"
                ],
                [
                    "negative: 636",
                    "SentiStrength-SE",
                    "704",
                    "0.948",
                    "0.883",
                    "0.914",
                    "-",
                    "-",
                    "-",
                    "0.996",
                    "0.704",
                    "0.825"
                ],
                [
                    "sum: 926",
                    "Stanford CoreNLP SO",
                    "333",
                    "0.635",
                    "0.252",
                    "0.361",
                    "-",
                    "-",
                    "-",
                    "0.724",
                    "0.409",
                    "0.523"
                ],
                [
                    "[EMPTY]",
                    "N-gram auto-sklearn",
                    "[BOLD] 884",
                    "[BOLD] 0.960",
                    "0.839",
                    "[BOLD] 0.893",
                    "-",
                    "-",
                    "-",
                    "0.932",
                    "[BOLD] 0.982",
                    "[BOLD] 0.956"
                ],
                [
                    "[EMPTY]",
                    "N-gram auto-sklearn with SMOTE†",
                    "-",
                    "0.986",
                    "0.704",
                    "0.809",
                    "-",
                    "-",
                    "-",
                    "0.781",
                    "0.988",
                    "0.872"
                ],
                [
                    "† Applying SMOTE, a oversampling technique, for our method.",
                    "† Applying SMOTE, a oversampling technique, for our method.",
                    "† Applying SMOTE, a oversampling technique, for our method.",
                    "† Applying SMOTE, a oversampling technique, for our method.",
                    "† Applying SMOTE, a oversampling technique, for our method.",
                    "† Applying SMOTE, a oversampling technique, for our method.",
                    "† Applying SMOTE, a oversampling technique, for our method.",
                    "† Applying SMOTE, a oversampling technique, for our method.",
                    "† Applying SMOTE, a oversampling technique, for our method.",
                    "† Applying SMOTE, a oversampling technique, for our method.",
                    "† Applying SMOTE, a oversampling technique, for our method.",
                    "† Applying SMOTE, a oversampling technique, for our method."
                ]
            ]
        },
        "gold_description": "table shows the number of correct predictions , precision , recall , and f1 values with all tools including our method ( n - gram auto - sklearn ) . we can see that the number of correct predictions are higher with our method in all three datasets , and our method achieved the highest f1 values for all three positive , all three negative , and one neutral . in summary , our method using n - gram idf and automated machine learning ( auto - sklearn ) largely outperformed existing sentiment analysis tools .",
        "generated_description": "for stack overflow , nltk and standford corenlp perform better than sentistrength on the positive and negative labels , but worse on the negative label . on the other hand , nnlp performs better than the other two tools on the majority of the negative labels . this indicates that nlp is better at detecting negative labels than sentiment analysis tools ."
    },
    {
        "table_id": "277",
        "table_info": {
            "table_caption": "Table 3: Correlation results for different configurations of our proposed approach and a competitor baseline based on cosine similarity of word embeddings.",
            "table_column_names": [
                "[EMPTY]",
                "rg  [BOLD] p",
                "rg  [BOLD] s",
                "rg  [BOLD] avg.",
                "wordsim  [BOLD] p",
                "wordsim  [BOLD] s",
                "wordsim  [BOLD] avg.",
                "mc  [BOLD] p",
                "mc  [BOLD] s",
                "mc  [BOLD] avg.",
                "semeval17  [BOLD] p",
                "semeval17  [BOLD] s",
                "semeval17  [BOLD] avg."
            ],
            "table_content_values": [
                [
                    "cosine",
                    "77.2",
                    "76.0",
                    "76.6",
                    "64.9",
                    "69.4",
                    "67.1",
                    "79.2",
                    "80.0",
                    "79.6",
                    "69.4",
                    "70.0",
                    "69.7"
                ],
                [
                    "10rv [ITALIC] w",
                    "78.1",
                    "77.0",
                    "77.5",
                    "66.0",
                    "69.6",
                    "67.8",
                    "79.7",
                    "80.7",
                    "[BOLD] 80.2",
                    "70.2",
                    "70.8",
                    "70.5"
                ],
                [
                    "10rv [ITALIC] r",
                    "77.4",
                    "75.5",
                    "76.4",
                    "65.8",
                    "69.5",
                    "67.6",
                    "78.8",
                    "77.9",
                    "78.3",
                    "70.0",
                    "70.7",
                    "70.3"
                ],
                [
                    "1800rv [ITALIC] w",
                    "79.5",
                    "80.6",
                    "[BOLD] 80.0",
                    "67.4",
                    "69.8",
                    "68.6",
                    "79.4",
                    "79.0",
                    "79.2",
                    "71.4",
                    "71.8",
                    "71.6"
                ],
                [
                    "1800rv [ITALIC] r",
                    "78.9",
                    "80.2",
                    "79.5",
                    "68.1",
                    "70.1",
                    "[BOLD] 69.1",
                    "79.2",
                    "79.7",
                    "79.4",
                    "72.2",
                    "73.0",
                    "[BOLD] 72.6"
                ]
            ]
        },
        "gold_description": "table 3 shows that the 10rvw variant consistently outperforms the word - level baseline .",
        "generated_description": "as can be seen , 10rvw outperforms the other two baselines by a large margin in all cases , and the improvement is statistically significant ( α = 0 . 05 , mcnemar ’ s test , p < 0 . 01 ) . the improvements are also statistically significant in the p and s tests for wordsim and mc ."
    },
    {
        "table_id": "278",
        "table_info": {
            "table_caption": "Table 1: Results on the NIST Chinese-English translation task. “Params” denotes the number of model parameters. “Emb.” represents the number of parameters used for word representation. “Red.” represents the reduction rate of the standard size. The results of SMT* and RNNsearch* are reported by Kuang et al. Kuang et al. (2018) with the same datasets and vocabulary settings. “↑” indicates the result is significantly better than that of the vanilla Transformer (p<0.01), while “⇑” indicates the result is significantly better than that of all other Transformer models (p<0.01). All significance tests are measured by paired bootstrap resampling Koehn (2004).",
            "table_column_names": [
                "Architecture",
                "Zh⇒En",
                "Params",
                "Emb.",
                "Red.",
                "Dev.",
                "MT02",
                "MT03",
                "MT04",
                "MT08",
                "All"
            ],
            "table_content_values": [
                [
                    "SMT*",
                    "-",
                    "-",
                    "-",
                    "-",
                    "34.00",
                    "35.81",
                    "34.70",
                    "37.15",
                    "25.28",
                    "33.39"
                ],
                [
                    "RNNsearch*",
                    "Vanilla",
                    "74.8M",
                    "55.8M",
                    "0%",
                    "35.92",
                    "37.88",
                    "36.21",
                    "38.83",
                    "26.30",
                    "34.81"
                ],
                [
                    "RNNsearch*",
                    "Source bridging",
                    "78.5M",
                    "55.8M",
                    "0%",
                    "36.79",
                    "38.71",
                    "37.24",
                    "40.28",
                    "27.40",
                    "35.91"
                ],
                [
                    "RNNsearch*",
                    "Target bridging",
                    "76.6M",
                    "55.8M",
                    "0%",
                    "36.69",
                    "39.04",
                    "37.63",
                    "40.41",
                    "27.98",
                    "36.27"
                ],
                [
                    "RNNsearch*",
                    "Direct bridging",
                    "78.9M",
                    "55.8M",
                    "0%",
                    "36.97",
                    "39.77",
                    "38.02",
                    "40.83",
                    "27.85",
                    "36.62"
                ],
                [
                    "Transformer",
                    "Vanilla",
                    "90.2M",
                    "46.1M",
                    "0%",
                    "41.37",
                    "42.53",
                    "40.25",
                    "43.58",
                    "32.89",
                    "40.33"
                ],
                [
                    "Transformer",
                    "Direct bridging",
                    "90.5M",
                    "46.1M",
                    "0%",
                    "41.67",
                    "42.89",
                    "41.34",
                    "43.56",
                    "32.69",
                    "40.54"
                ],
                [
                    "Transformer",
                    "Decoder WT",
                    "74.9M",
                    "30.7M",
                    "33.4%",
                    "41.90",
                    "43.02",
                    "41.89",
                    "43.87",
                    "32.62",
                    "40.82"
                ],
                [
                    "Transformer",
                    "[ITALIC] Shared-private",
                    "62.8M",
                    "18.7M",
                    "59.4%",
                    "42.57↑",
                    "43.73↑",
                    "41.99↑",
                    "44.53↑",
                    "33.81⇑",
                    "41.61⇑"
                ]
            ]
        },
        "gold_description": "table 1 : results on the nist chinese - english translation task . \" params \" denotes the number of model parameters . \" emb . \" represents the number of parameters used for word representation . \" red . \" represents the reduction rate of the standard size . the results of smt * and rnnsearch * are reported by kuang et al . ( 2018 ) with the same datasets and vocabulary settings . \" ↑ \" indicates the result is significantly better than that of the vanilla transformer ( p < 0 . 01 ) , while \" ⇑ \" indicates the result is significantly better than that of all other transformer models ( p < 0 . 01 ) . all significance tests are measured by paired bootstrap resampling ( koehn , 2004 ) . table 1 reports the results on the nist chineseenglish test sets . it is observed that the transformer models significantly outperform smt and rnnsearch models . therefore , we decide to implement all of our experiments based on transformer architecture . the direct bridging model can further improve the translation quality of the transformer baseline . the decoder wt model improves the translation quality while reducing the number of parameters for the word representation . this improved performance happens because there are fewer model parameters , which prevents over - fitting ( press and wolf , 2017 ) . finally , the performance is further improved by the proposed method while using even fewer parameters than other models .",
        "generated_description": "vanilla is simply a plain nmt model with the embedding layer set to the same dimension as the training data , i . e . , the encoder and decoder layers are set to 64k and 64k , respectively . we can see that the proposed rnnsearch model with source bridging achieves the best performance on the zh ⇒ en test sets , which confirms the effectiveness of our proposed model . we also find that the direct bridging of the source and target sides outperforms the other two bridging approaches , which indicates that the source - target relationship can benefit the target side more from bridging ."
    },
    {
        "table_id": "279",
        "table_info": {
            "table_caption": "Table 2: Results on the WMT English-German translation task. “‡” indicates the result is significantly better than the vanilla Transformer model (p<0.05).",
            "table_column_names": [
                "En⇒De",
                "Params",
                "Emb.",
                "Red.",
                "BLEU"
            ],
            "table_content_values": [
                [
                    "Vanilla",
                    "98.7M",
                    "54.5M",
                    "0%",
                    "27.62"
                ],
                [
                    "Direct bridging",
                    "98.9M",
                    "54.5M",
                    "0%",
                    "27.79"
                ],
                [
                    "Decoder WT",
                    "80.4M",
                    "36.2M",
                    "33.6%",
                    "27.51"
                ],
                [
                    "Three-way WT",
                    "63.1M",
                    "18.9M",
                    "65.3%",
                    "27.39"
                ],
                [
                    "[ITALIC] Shared-private",
                    "65.0M",
                    "20.9M",
                    "63.1%",
                    "28.06‡"
                ]
            ]
        },
        "gold_description": "table 2 : results on the wmt english - german translation task . \" ‡ \" indicates the result is significantly better than the vanilla transformer model ( p < 0 . 05 ) . similar observations are obtained on the english - german translation task , as shown in table 2 .",
        "generated_description": "the shared - private model achieves the best result with a bleu score of 28 . 06 , which is significantly better than the vanilla transformer ( p < 0 . 05 ) and outperforms the three - way wt baseline by a large margin ( p < 0 . 05 ) . the result shows that the proposed model can benefit from the shared encoder and decoder layers to improve the translation quality ."
    },
    {
        "table_id": "280",
        "table_info": {
            "table_caption": "Table 3: Results on the IWSLT {Ar, Ja, Ko, Zh}-to-En translation tasks. These distant language pairs belonging to 5 different language families and written in 5 different alphabets.“↑” indicates the result is significantly better than that of the vanilla Transformer (p<0.01).",
            "table_column_names": [
                "[EMPTY]",
                "Model",
                "Emb.",
                "Red.",
                "BLEU"
            ],
            "table_content_values": [
                [
                    "Ar⇒ En",
                    "Vanilla",
                    "23.6M",
                    "0%",
                    "28.36"
                ],
                [
                    "Ar⇒ En",
                    "[ITALIC] Shared-private",
                    "11.8M",
                    "50%",
                    "29.71↑"
                ],
                [
                    "Ja⇒ En",
                    "Vanilla",
                    "25.6M",
                    "0%",
                    "10.94"
                ],
                [
                    "Ja⇒ En",
                    "[ITALIC] Shared-private",
                    "13.3M",
                    "48.0%",
                    "12.35↑"
                ],
                [
                    "Ko⇒ En",
                    "Vanilla",
                    "25.1M",
                    "0%",
                    "16.48"
                ],
                [
                    "Ko⇒ En",
                    "[ITALIC] Shared-private",
                    "13.2M",
                    "47.4%",
                    "17.84↑"
                ],
                [
                    "Zh⇒ En",
                    "Vanilla",
                    "27.4M",
                    "0%",
                    "19.36"
                ],
                [
                    "Zh⇒ En",
                    "[ITALIC] Shared-private",
                    "13.8M",
                    "49.6%",
                    "21.00↑"
                ]
            ]
        },
        "gold_description": "table 3 : results on the iwslt { ar , ja , ko , zh } - to - en translation tasks . these distant language pairs belonging to 5 different language families and written in 5 different alphabets . \" ↑ \" indicates the result is significantly better than that of the vanilla transformer ( p < 0 . 01 ) . table 3 shows the results on the small - scale iwslt translation tasks . we observe that the proposed method stays consistently better than the vanilla model on these distant language pairs .",
        "generated_description": "these distant language pairs belong to 5 different language families ( ar , ja , ko , zh , and ar + ja ) and are written in different alphabets ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ) . we use embeddings of size 100 , 000 and 200 , 000 for training and testing . the bleu score of the shared - private model is lower than the vanilla model when using 50 % of the training data ( 28 . 36 vs . 23 . 6 , 29 . 71 , 10 . 94 , 28 . 36 ) . when using 0 % or 50 % training data , the model underperforms the vanilla model by a large margin ( 10 . 94 vs . 21 . 00 , 17 . 84 vs . 19 . 36 ) , indicating that the model suffers from the data sparsity issue when training data is limited ."
    },
    {
        "table_id": "281",
        "table_info": {
            "table_caption": "Table 4: Performance of models using different sharing coefficients on the validation set of the NIST Chinese-English translation task.",
            "table_column_names": [
                "Zh-En",
                "[ITALIC] λlm",
                "[ITALIC] λwf",
                "[ITALIC] λur",
                "Emb.",
                "BLEU"
            ],
            "table_content_values": [
                [
                    "Vanilla",
                    "-",
                    "-",
                    "-",
                    "46.1M",
                    "41.37"
                ],
                [
                    "Decoder WT",
                    "0",
                    "0",
                    "0",
                    "30.7M",
                    "41.90"
                ],
                [
                    "[ITALIC] Shared-private",
                    "0.5",
                    "0.7",
                    "0.9",
                    "21.2M",
                    "41.98"
                ],
                [
                    "[ITALIC] Shared-private",
                    "0.5",
                    "0.5",
                    "0.5",
                    "23.0M",
                    "42.26"
                ],
                [
                    "[ITALIC] Shared-private",
                    "0.9",
                    "0.7",
                    "0",
                    "21.0M",
                    "42.27"
                ],
                [
                    "[ITALIC] Shared-private",
                    "1",
                    "1",
                    "1",
                    "15.3M",
                    "42.36"
                ],
                [
                    "[ITALIC] Shared-private",
                    "0.9",
                    "0.7",
                    "0.5",
                    "18.7M",
                    "42.57"
                ]
            ]
        },
        "gold_description": "table 4 : performance of models using different sharing coefficients on the validation set of the nist chineseenglish translation task . as shown in table 4 , the decoder wt model can be seen as a kind of shared - private method where zero features are shared between the source and target word embeddings . for the proposed method , λ = ( 0 . 5 , 0 . 5 , 0 . 5 ) and λ = ( 1 , 1 , 1 ) are , respectively , used for sharing half and all features between the embeddings of all categories of words . this allows the model to significantly reduce the number of parameters and also improve the translation quality . for comparison purpose , we also consider sharing a large part of the features among the unrelated words by setting s3 to 0 . 9 , i . e . λ = ( 0 . 5 , 0 . 7 , 0 . 9 ) . we argue that it is hard for",
        "generated_description": "in this section , we study the effect of sharing coefficients of the parameter sharing coefficients λlm , λwf and λur on the performance of the shared - private zh - en model on the validation set of the nist chinese - english translation task . we can see that sharing the parameters of the encoder and decoder is beneficial for the model , but hurts the bleu score . we conjecture that this is because the sharing of parameters is private and the decoder and embedding layer are not exposed to the word embeddings during training . in addition , we can find that the embedding size is also important to the model . when sharing the weight matrix wf , the model achieves the best performance , which shows that word embedding dimensionality is an important factor ."
    },
    {
        "table_id": "282",
        "table_info": {
            "table_caption": "Table 2: Average time for users to set up the tool and identify verbs in a 623 word news article. Only one participant managed to install and use brat, taking 18 minutes on Ubuntu. The differences between GATE and either slate or YEDDA are significant at the 0.01 level according to a t-test.",
            "table_column_names": [
                "Tool",
                "Time (minutes) Ubuntu",
                "Time (minutes) macOS"
            ],
            "table_content_values": [
                [
                    "slate",
                    "10",
                    "16"
                ],
                [
                    "YEDDA",
                    "14",
                    "14"
                ],
                [
                    "GATE",
                    "21",
                    "22"
                ],
                [
                    "brat",
                    "-",
                    "-"
                ]
            ]
        },
        "gold_description": "table 2 presents the time required to install each tool and complete the first annotation task . slate and yedda are comparable in effort , which fits with their common design as simple tools with minimal dependencies . participants had great difficulty with brat , with only two managing to install it , one just as their time finished .",
        "generated_description": "the differences between gate and yedda are significant at the 0 . 01 level according to a t - test . time is measured in minutes on ubuntu and minutes on the macaroni and switchboard test set . for simplicity , we focus on the time taken by users to set up the tool and identify verbs in a 623 word news article on ubuntu . as can be seen in table 2 , the time for gate is significantly faster than either of the other two tools , while it is significantly slower than the time to initialize the slate and yedda tools . on the other hand , the differences between the runtime of gate ( 21 minutes versus 14 minutes for ubuntu and 22 minutes for macos ) are not significant ."
    },
    {
        "table_id": "283",
        "table_info": {
            "table_caption": "Table 1: Perplexity Scores",
            "table_column_names": [
                "No. of sentences",
                "node2vec",
                "syntree2vec",
                "word2vec"
            ],
            "table_content_values": [
                [
                    "0.01MB : 73",
                    "24.93",
                    "20.11",
                    "22"
                ],
                [
                    "0.03MB : 220",
                    "28.44",
                    "28.24",
                    "28.44"
                ],
                [
                    "0.07MB : 508",
                    "43.59",
                    "43.41",
                    "43.41"
                ],
                [
                    "0.15MB : 1070",
                    "71.20",
                    "71.01",
                    "71.01"
                ]
            ]
        },
        "gold_description": "since the loss , perplexity of syntree2vec is lower than word2vec , node2vec over most of the data sizes given below we say that the syntree2vec performs slightly better than both of them . there is a clear margin of difference between the perplexity scores of node2vec and syntree2vec .",
        "generated_description": "in order to evaluate the impact of the size of the training data on the performance of the embedding models , we use the perplexity scores of the corresponding models on a held - out test set . in particular , for a given sentence with a few hundred nodes , we run the word2vec and the syntree2vec embeddings on the training set using the same amount of data as in the evaluation setting . we can see that the two embedding methods outperform the original node2vec by a large margin when the training corpus is small ( e . g . , 0 . 03mb or 0 . 07mb of sentences ) , but the performance degrades significantly when the corpus is large , e . g . when the dataset contains more than 1 . 1m nodes ."
    },
    {
        "table_id": "284",
        "table_info": {
            "table_caption": "Table 1: Label description for ASNQ. Here S, LA, SA refer to answer sentence, long answer passage and short answer phrase respectively.",
            "table_column_names": [
                "Label",
                "S ∈ LA",
                "SA ∈ S",
                "# Train",
                "# Dev"
            ],
            "table_content_values": [
                [
                    "1",
                    "No",
                    "No",
                    "19,446,120",
                    "870,404"
                ],
                [
                    "2",
                    "No",
                    "Yes",
                    "428,122",
                    "25,814"
                ],
                [
                    "3",
                    "Yes",
                    "No",
                    "442,140",
                    "29,558"
                ],
                [
                    "4",
                    "Yes",
                    "Yes",
                    "61,186",
                    "4,286"
                ]
            ]
        },
        "gold_description": "for each question in asnq , the positive candidate answers are those sentences that occur in the long answer paragraphs in nq and contain annotated short answers . the remaining sentences from the document are labeled as negative for the target question . while the asnq statistics are reported in table 1 . asnq contains 57 , 242 distinct questions in the training set and 2 , 672 distinct questions in the dev . set ,",
        "generated_description": "the dataset for asnq is divided into training set , development set and test set . the size of training set is set to be equal to the size of short answer passage and short answer phrase respectively . the test set is used to evaluate the performance of the model on answer sentence and long answer passage separately . the model is trained on the training set and tested on the dev set . we use the official evaluation scripts provided by the organizers to compute the label number for each answer sentence . the label number refers to the number of questions for which the model was able to answer the question correctly compared to the ground truth answer . the dataset consists of four labels : 1 ) no , no , 1 , 2 , 3 , 4 and 5 . it is clear that the first label is the majority class ( class 1 ) and the second label ( class 2 ) is the most common class ( majority class ) . the third and fourth labels ( class 3 and class 4 ) are the least common classes ( minimally common class ) . finally , we use the last label ( last class ) to show that the model can answer the questions correctly compared with the first and second labels ."
    },
    {
        "table_id": "285",
        "table_info": {
            "table_caption": "Table 3: Performance of different models on WikiQA dataset. Here Comp-Agg + LM + LC refers to a Compare-Aggregate model with Language Modeling and Latent Clustering as proposed by Yoon et al. DBLP:journals/corr/abs-1905-12897. TL(QNLI) refers to Transfer Learning from the QNLI corpus. L and B stand for Large and Base, respectively.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] MAP",
                "[BOLD] MRR"
            ],
            "table_content_values": [
                [
                    "Comp-Agg + LM + LC",
                    "0.764",
                    "0.784"
                ],
                [
                    "Comp-Agg + LM + LC+ TL(QNLI)",
                    "0.834",
                    "0.848"
                ],
                [
                    "BERT-B FT WikiQA",
                    "0.813",
                    "0.828"
                ],
                [
                    "BERT-B FT ASNQ",
                    "0.884",
                    "0.898"
                ],
                [
                    "BERT-B TandA (ASNQ → WikiQA )",
                    "0.893",
                    "0.903"
                ],
                [
                    "BERT-L FT WikiQA",
                    "0.836",
                    "0.853"
                ],
                [
                    "BERT-L FT ASNQ",
                    "0.892",
                    "0.904"
                ],
                [
                    "BERT-L TandA (ASNQ → WikiQA)",
                    "0.904",
                    "0.912"
                ],
                [
                    "RoBERTa-B FT ASNQ",
                    "0.882",
                    "0.894"
                ],
                [
                    "RoBERTa-B TandA (ASNQ → WikiQA)",
                    "0.889",
                    "0.901"
                ],
                [
                    "RoBERTa-L FT ASNQ",
                    "0.910",
                    "0.919"
                ],
                [
                    "RoBERTa-L TandA (ASNQ → WikiQA )",
                    "[BOLD] 0.920",
                    "[BOLD] 0.933"
                ]
            ]
        },
        "gold_description": "table 3 : performance of different models on wikiqa dataset . here comp - agg + lm + lc refers to a compareaggregate model with language modeling and latent clustering as proposed by yoon et al . ( 2019 ) . tl ( qnli ) refers to transfer learning from the qnli corpus . l and b stand for large and base , respectively . table 3 reports the map and mrr of different pre - trained transformers models for two methods : standard fine - tuning ( ft ) and tanda . the latter takes two arguments that we indicate as transfer dataset → adapt dataset .",
        "generated_description": "comp - agg + lm + lc + lc refers to the models fine - tuned on qnli dataset using the asnq dataset and the wikiqa dataset . bert - b and roberta - l refer to the bert + lstm + task - lstm models . we can see that the performance of these two models is very close , and even outperforms the original qnli dataset . we attribute this to the fact that bert is pre - trained on a large amount of unlabeled data ( millions of examples ) while the task - specific pre - training data ( asnq ) is relatively small ( only about 5k examples ) for the tanda task . the performance of bert models trained on the semeval - 2018 dataset is not as good as that of the original dataset , which is due to the size of the training data ( only around 1 . 5k examples ) . however , the performance can still be improved , especially when the target dataset is larger than the source dataset . in this section , we analyze the effect of fine - tuneing bert on the task of tanda . we notice that fine - training bert with a target dataset of the same source dataset as the target data improves the results of the model trained on that dataset . for instance , the map and mrr increase from 0 . 828 ( bert - b ft ) to 0 . 903 and 0 . 894 ( roberta - l ft ) when the model is trained on asnsq and the target datasets are wikitexta , respectively . the improvement is even more significant ( 0 . 904 map + 0 . 910 mrr ) ."
    },
    {
        "table_id": "286",
        "table_info": {
            "table_caption": "Table 4: Performance of different models on TREC-QA dataset. Here Comp-Agg + LM + LC refers to a Compare-Aggregate model with Language Modeling and Latent Clustering as proposed in [25]. TL(QNLI) refers to Transfer Learning from the QNLI corpus. L and B stand for Large and Base, respectively.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] MAP",
                "[BOLD] MRR"
            ],
            "table_content_values": [
                [
                    "Comp-Agg + LM + LC",
                    "0.868",
                    "0.928"
                ],
                [
                    "Comp-Agg + LM + LC + TL(QNLI)",
                    "0.875",
                    "0.940"
                ],
                [
                    "BERT-B FT TREC-QA",
                    "0.857",
                    "0.937"
                ],
                [
                    "BERT-B FT ASNQ",
                    "0.823",
                    "0.872"
                ],
                [
                    "BERT-B TandA (ASNQ → TREC-QA)",
                    "0.912",
                    "0.951"
                ],
                [
                    "BERT-L FT TREC-QA",
                    "0.904",
                    "0.946"
                ],
                [
                    "BERT-L FT ASNQ",
                    "0.824",
                    "0.872"
                ],
                [
                    "BERT-L TandA (ASNQ → TREC-QA )",
                    "0.912",
                    "0.967"
                ],
                [
                    "RoBERTa-B FT ASNQ",
                    "0.849",
                    "0.907"
                ],
                [
                    "RoBERTa-B TandA (ASNQ →TREC-QA )",
                    "0.914",
                    "0.952"
                ],
                [
                    "RoBERTa-L FT ASNQ",
                    "0.880",
                    "0.928"
                ],
                [
                    "RoBERTa-L TandA (ASNQ → TREC-QA)",
                    "[BOLD] 0.943",
                    "[BOLD] 0.974"
                ]
            ]
        },
        "gold_description": "table 4 : performance of different models on trec - qa dataset . here comp - agg + lm + lc refers to a compareaggregate model with language modeling and latent clustering as proposed in ( yoon et al . 2019 ) . tl ( qnli ) refers to transfer learning from the qnli corpus . l and b stand for large and base , respectively . table 4 reports the results of our experiments with trec - qa .",
        "generated_description": "comp - agg + lm + lc + lc refers to the same model using bert - b and roberta - l fine - tuned with asnq and trec - qa datasets , respectively . we can see that the performance of these two models is worse than that of the original qnli models . this is due to the nature of the dataset that the questions and answers are collected from the same source ( asnq ) and the context of the questions are the same . bert is pre - trained on the wikiqa dataset while the trec dataset is collected from different sources ( i . e . web - videos and ascnq ) . in this case , the pre - training data of bert may not be the same as the target dataset since the source dataset is different from the target one . in this experiment , we fine - tune the bert models on the asnsq dataset and evaluate them on trec . - qa . the results show that the tanda performance of the models trained with bert are worse than those trained with the original dataset ( compared to the first group of experiments ) , which indicates that fine - training bert does not help to improve the performances of the model ."
    },
    {
        "table_id": "287",
        "table_info": {
            "table_caption": "Table 5: Model accuracy when noise is injected into WikiQA and TREC-QA datasets. ∗ indicates the target dataset for the second step of fine-tuning (adapt step).",
            "table_column_names": [
                "[BOLD] BERT-base",
                "WikiQA MAP",
                "WikiQA % Drop",
                "WikiQA MRR",
                "WikiQA % Drop",
                "TREC-QA MAP",
                "TREC-QA % Drop",
                "TREC-QA MRR",
                "TREC-QA % Drop"
            ],
            "table_content_values": [
                [
                    "No noise Fine-tuning",
                    "0.813",
                    "-",
                    "0.828",
                    "-",
                    "0.857",
                    "-",
                    "0.937",
                    "-"
                ],
                [
                    "10% noise Fine-tuning",
                    "0.775",
                    "4.67%",
                    "0.793",
                    "4.22%",
                    "0.826",
                    "3.62%",
                    "0.902",
                    "3.73%"
                ],
                [
                    "20% noise Fine-tuning",
                    "0.629",
                    "[BOLD] 22.63%",
                    "0.645",
                    "22.10%",
                    "0.738",
                    "[BOLD] 13.88%",
                    "0.843",
                    "10.03%"
                ],
                [
                    "No noise TandA (ASNQ → *)",
                    "0.893",
                    "-",
                    "0.903",
                    "-",
                    "0.912",
                    "-",
                    "0.951",
                    "-"
                ],
                [
                    "10% noise TandA (ASNQ → *)",
                    "0.876",
                    "1.90%",
                    "0.889",
                    "1.55%",
                    "0.896",
                    "1.75%",
                    "0.941",
                    "1.05%"
                ],
                [
                    "20% noise TandA (ASNQ → *)",
                    "0.870",
                    "[BOLD] 2.57%",
                    "0.886",
                    "1.88%",
                    "0.891",
                    "[BOLD] 2.30%",
                    "0.937",
                    "1.47%"
                ]
            ]
        },
        "gold_description": "table 5 : model accuracy when noise is injected into wikiqa and trec - qa datasets . ∗ indicates the target dataset for the second step of fine - tuning ( adapt step ) . table 5 shows the map and mrr of bertbase using ft and tanda , also indicating the drop percentage ( % ) in accuracy due to the injection of noise .",
        "generated_description": "s4ss3sss0px2 how much noise should we add noise ? we conduct experiments on bert - base with and without noise to see the effect of noise on the fine - tuning process . we can see that with 10 % or 20 % of the noise , the map and mrr drops on wikiqa and trec - qa are reduced by up to 13 . 88 % and 10 . 03 % respectively , which shows the robustness of bert against asnq . with 20 % noise , map drops on both datasets are reduced to 0 . 629 and 0 . 738 . this shows that the noise can effectively improve the model ’ s generalization ability to not only answer the given question but also reduce the drop in performance ."
    },
    {
        "table_id": "288",
        "table_info": {
            "table_caption": "Table 6: Impact of different labels of ASNQ on fine-tuning BERT for answer sentence selection. Neg and Pos refers to question-answer (QA) pairs of that particular label being chosen for fine-tuning.",
            "table_column_names": [
                "Model",
                "WikiQA MAP",
                "WikiQA MRR",
                "TREC-QA MAP",
                "TREC-QA MRR"
            ],
            "table_content_values": [
                [
                    "Neg: 1 Pos: 4",
                    "0.870",
                    "0.880",
                    "0.808",
                    "0.847"
                ],
                [
                    "Neg: 2 Pos: 4",
                    "0.751",
                    "0.763",
                    "0.662",
                    "0.751"
                ],
                [
                    "Neg: 3 Pos: 4",
                    "0.881",
                    "0.895",
                    "0.821",
                    "0.869"
                ],
                [
                    "Neg: 2,3 Pos: 4",
                    "0.883",
                    "0.898",
                    "0.823",
                    "0.871"
                ],
                [
                    "Neg: 1,2,3 Pos: 4",
                    "0.884",
                    "0.898",
                    "0.823",
                    "0.872"
                ]
            ]
        },
        "gold_description": "table 6 : impact of different labels of asnq on fine - tuning bert for answer sentence selection . neg and pos refers to question - answer ( qa ) pairs of that particular label being chosen for fine - tuning . we carried out experiments by fine - tuning bertbase on asnq with specific label categories assigned to the negative class . table 6 shows the results : label 3 is the most effective negative type of the three ,",
        "generated_description": "in this section , we explore the impact of different labels of asnq on fine - tuning bert for answer sentence selection . for each of the selected labels , we select the question - answer pairs of that particular label and fine - tune bert on these pairs with the selected label . the performance of bert is measured in terms of mean average precision ( map ) and mean reciprocal rank ( mrr ) , which are the two metrics used in this experiment . we can see that the choice of a particular label has a large impact on the final performance . for example , the best performing model on trec - qa has the maximum map value of 0 . 895 , and the best mrr value is obtained with neg : 1 pos : 4 . on the other hand , on wikiqa with the same set of labels neg : 2 pos : 4 , bert achieves the best performance with map value 0 . 808 , which is only 0 . 005 lower than the best model using the full set of pos ."
    },
    {
        "table_id": "289",
        "table_info": {
            "table_caption": "Table 7: Comparison of TandA with ASNQ and QNLI",
            "table_column_names": [
                "[BOLD] BERT-Base",
                "[BOLD] WikiQA MAP",
                "[BOLD] WikiQA MRR",
                "[BOLD] TREC-QA MAP",
                "[BOLD] TREC-QA MRR"
            ],
            "table_content_values": [
                [
                    "FT QNLI",
                    "0.760",
                    "0.766",
                    "0.820",
                    "0.890"
                ],
                [
                    "FT ASNQ",
                    "0.884",
                    "0.898",
                    "0.823",
                    "0.872"
                ],
                [
                    "TandA (QNLI →)",
                    "0.832",
                    "0.852",
                    "0.863",
                    "0.906"
                ],
                [
                    "TandA (ASNQ →)",
                    "0.893",
                    "0.903",
                    "0.912",
                    "0.951"
                ]
            ]
        },
        "gold_description": "table 7 shows that both ft and tanda using asnq provide significantly better performance than qnli on the wikiqa dataset .",
        "generated_description": "in this section , we compare tanda with existing q & a datasets : qnli , asnq and trec - qa . we find that bert - base outperforms the current state - of - the - art in terms of map and mrr on the wikiqa dataset . however , the performance on mrr is much lower than that on the trec dataset . we conjecture that the reason is that the dataset is too small to train large - scale pre - trained language models such as bert and xlnet . in the future , we plan to further investigate the pre - training of bert on a much larger dataset ."
    },
    {
        "table_id": "290",
        "table_info": {
            "table_caption": "Table 8: Comparison between FT and TandA on real-world datasets derived from Alexa Virtual Assistant traffic",
            "table_column_names": [
                "MODEL",
                "MODEL",
                "MODEL",
                "Sample 1 Prec@1",
                "Sample 1 MAP",
                "Sample 1 MRR",
                "Sample 2 Prec@1",
                "Sample 2 MAP",
                "Sample 2 MRR",
                "Sample 3 Prec@1",
                "Sample 3 MAP",
                "Sample 3 MRR"
            ],
            "table_content_values": [
                [
                    "BERT",
                    "Base",
                    "NAD",
                    "49.80",
                    "0.506",
                    "0.638",
                    "52.69",
                    "0.432",
                    "0.629",
                    "41.86",
                    "0.352",
                    "0.543"
                ],
                [
                    "BERT",
                    "Base",
                    "ASNQ",
                    "55.06",
                    "0.557",
                    "0.677",
                    "44.31",
                    "0.395",
                    "0.567",
                    "44.19",
                    "0.369",
                    "0.561"
                ],
                [
                    "BERT",
                    "Base",
                    "TANDA (ASNQ → NAD)",
                    "58.70",
                    "0.585",
                    "0.703",
                    "58.68",
                    "0.474",
                    "0.683",
                    "49.42",
                    "0.391",
                    "0.613"
                ],
                [
                    "BERT",
                    "Large",
                    "NAD",
                    "53.85",
                    "0.537",
                    "0.671",
                    "53.29",
                    "0.469",
                    "0.629",
                    "43.61",
                    "0.395",
                    "0.558"
                ],
                [
                    "BERT",
                    "Large",
                    "ASNQ",
                    "57.49",
                    "0.552",
                    "0.686",
                    "50.89",
                    "0.440",
                    "0.630",
                    "45.93",
                    "0.399",
                    "0.585"
                ],
                [
                    "BERT",
                    "Large",
                    "TANDA (ASNQ → NAD)",
                    "61.54",
                    "0.607",
                    "0.725",
                    "63.47",
                    "0.514",
                    "0.727",
                    "51.16",
                    "0.439",
                    "0.616"
                ],
                [
                    "RoBERTa",
                    "Base",
                    "NAD",
                    "59.11",
                    "0.563",
                    "0.699",
                    "56.29",
                    "0.511",
                    "0.670",
                    "48.26",
                    "0.430",
                    "0.612"
                ],
                [
                    "RoBERTa",
                    "Base",
                    "ASNQ",
                    "58.70",
                    "0.587",
                    "0.707",
                    "54.50",
                    "0.473",
                    "0.656",
                    "45.35",
                    "0.437",
                    "0.608"
                ],
                [
                    "RoBERTa",
                    "Base",
                    "TANDA (ASNQ → NAD)",
                    "65.59",
                    "0.623",
                    "0.757",
                    "62.87",
                    "0.537",
                    "0.714",
                    "56.98",
                    "0.473",
                    "0.679"
                ],
                [
                    "RoBERTa",
                    "Large",
                    "NAD",
                    "70.81",
                    "0.654",
                    "0.796",
                    "63.47",
                    "0.581",
                    "0.734",
                    "52.91",
                    "0.490",
                    "0.651"
                ],
                [
                    "RoBERTa",
                    "Large",
                    "ASNQ",
                    "64.37",
                    "0.627",
                    "0.750",
                    "59.88",
                    "0.526",
                    "0.705",
                    "54.65",
                    "0.478",
                    "0.674"
                ],
                [
                    "RoBERTa",
                    "Large",
                    "TANDA (ASNQ → NAD)",
                    "71.26",
                    "0.680",
                    "0.805",
                    "74.85",
                    "0.625",
                    "0.821",
                    "58.14",
                    "0.514",
                    "0.699"
                ]
            ]
        },
        "gold_description": "table 8 : comparison between ft and tanda on real - world datasets derived from alexa virtual assistant traffic in these experiments , we used , as usual , asnq for the transfer step , and nad as our target dataset for the adapt step . table 8 reports the comparative results using simple ft on nad ( denoted simply by nad ) and tested on samples 1 , 2 and 3 .",
        "generated_description": "in this section , we compare the performance of different variants of bert pre - trained on the asnq and nad tasks . we find that the large bert model outperforms the base model in terms of all the evaluation metrics . we also find that pre - training bert with the tanda task ( asnq → nad ) obtains the best performance on all the four sam / mrr metrics . in particular , we observe that bert - large obtains an improvement of + 0 . 8 % in map and + 1 % in mrr over the bert base model , which indicates that the pre - trainning of large model is beneficial ."
    },
    {
        "table_id": "291",
        "table_info": {
            "table_caption": "Table 5: Top ten and bottom ten features used in successful IC classifications using vocabulary features. Differentiation and integration terms are prefixed with dif and int, while has_dif and has_int are the binary features for whether any differentiation/integration terms are present at all. The bias term is the averaged sum of the value associated with each root node in the ensemble.",
            "table_column_names": [
                "IC = 1 Feature",
                "IC = 1 Value",
                "IC = 1 Contribution",
                "IC = 2 Feature",
                "IC = 2 Value",
                "IC = 2 Contribution",
                "IC = 3 Feature",
                "IC = 3 Value",
                "IC = 3 Contribution"
            ],
            "table_content_values": [
                [
                    "has_diff",
                    "0.0",
                    "+0.953",
                    "dif_too",
                    "1.0",
                    "+1.155",
                    "Bias term",
                    "1.0",
                    "+0.872"
                ],
                [
                    "Bias term",
                    "1.0",
                    "+0.419",
                    "Bias term",
                    "1.0",
                    "+0.594",
                    "has_int",
                    "0.0",
                    "+0.450"
                ],
                [
                    "dif_but",
                    "0.0",
                    "+0.114",
                    "dif_consider",
                    "1.0",
                    "+0.491",
                    "dif_may",
                    "1.0",
                    "+0.393"
                ],
                [
                    "dif_because",
                    "0.0",
                    "+0.058",
                    "dif_however",
                    "1.0",
                    "+0.186",
                    "dif_but",
                    "1.0",
                    "+0.306"
                ],
                [
                    "dif_how",
                    "0.0",
                    "+0.033",
                    "dif_how",
                    "1.0",
                    "+0.092",
                    "dif_hope",
                    "1.0",
                    "+0.215"
                ],
                [
                    "dif_yet",
                    "0.0",
                    "+0.033",
                    "dif_hope",
                    "0.0",
                    "+0.027",
                    "dif_while",
                    "0.0",
                    "+0.060"
                ],
                [
                    "int_unity",
                    "0.0",
                    "+0.029",
                    "dif_perhaps",
                    "0.0",
                    "+0.019",
                    "dif_rather",
                    "0.0",
                    "+0.058"
                ],
                [
                    "dif_depend",
                    "0.0",
                    "+0.024",
                    "dif_almost",
                    "0.0",
                    "+0.018",
                    "dif_too",
                    "0.0",
                    "+0.033"
                ],
                [
                    "dif_hope",
                    "0.0",
                    "+0.022",
                    "dif_sometimes",
                    "0.0",
                    "+0.012",
                    "dif_seem",
                    "0.0",
                    "+0.030"
                ],
                [
                    "dif_rather",
                    "0.0",
                    "+0.022",
                    "dif_although",
                    "0.0",
                    "+0.011",
                    "dif_differ",
                    "0.0",
                    "+0.026"
                ],
                [
                    "has_int",
                    "0.0",
                    "-0.009",
                    "dif_while",
                    "0.0",
                    "-0.032",
                    "int_remain",
                    "0.0",
                    "-0.024"
                ],
                [
                    "dif_close_to",
                    "0.0",
                    "-0.009",
                    "dif_rather",
                    "0.0",
                    "-0.033",
                    "dif_separate",
                    "0.0",
                    "-0.026"
                ],
                [
                    "dif_seem",
                    "0.0",
                    "-0.010",
                    "dif_different",
                    "0.0",
                    "-0.050",
                    "int_weigh",
                    "0.0",
                    "-0.028"
                ],
                [
                    "dif_consider",
                    "0.0",
                    "-0.011",
                    "dif_often",
                    "0.0",
                    "-0.050",
                    "dif_possible",
                    "0.0",
                    "-0.029"
                ],
                [
                    "int_account",
                    "0.0",
                    "-0.012",
                    "dif_each",
                    "0.0",
                    "-0.050",
                    "int_unity",
                    "0.0",
                    "-0.031"
                ],
                [
                    "dif_secret",
                    "0.0",
                    "-0.013",
                    "dif_either",
                    "0.0",
                    "-0.069",
                    "dif_however",
                    "0.0",
                    "-0.036"
                ],
                [
                    "dif_differ",
                    "0.0",
                    "-0.018",
                    "dif_about",
                    "0.0",
                    "-0.078",
                    "dif_often",
                    "0.0",
                    "-0.037"
                ],
                [
                    "dif_usually",
                    "0.0",
                    "-0.019",
                    "dif_both",
                    "0.0",
                    "-0.079",
                    "dif_about",
                    "0.0",
                    "-0.068"
                ],
                [
                    "int_remain",
                    "0.0",
                    "-0.024",
                    "dif_because",
                    "0.0",
                    "-0.284",
                    "dif_though",
                    "0.0",
                    "-0.073"
                ],
                [
                    "dif_may",
                    "0.0",
                    "-0.036",
                    "dif_but",
                    "1.0",
                    "-0.575",
                    "dif_because",
                    "0.0",
                    "-0.102"
                ]
            ]
        },
        "gold_description": "the vocabulary model correctly classified test items in three ic bands only ( table 5 ) .",
        "generated_description": "table 3 shows the impact of having ic = 1 , 2 , 3 and 4 features on overall performance . has_diff and has_too show positive impact in terms of correlation with human judgement , + 0 . 953 and + 1 . 155 , respectively . bias terms and bias terms are also positive for this group as well as the contribution of each of the individual features . the table also shows the effect of having two or three different number of contributing features for each individual feature . if an individual feature contributes 1 value to the overall performance , it means that it has a positive impact and should be considered as a contributing feature ."
    },
    {
        "table_id": "292",
        "table_info": {
            "table_caption": "Figure 1, Listing 1 & Table 1: (Right code snippet) Implementation of Kiperwasser and Goldberg (2016)’s neural parser in only a few lines using UniParse. (Right table and left figure) Seconds a decoder takes to decode an entire dataset, given a set of scores. Score matrix entries are generated uniformly on [0, 1]. The random generated data has an impact on CLE since worst-case performance depends on the sorting bottleneck; the figure demonstrates this by the increasingly broad standard deviation band. Experiments are run on an Ubuntu machine with an Intel Xeon E5-2660, 2.60GHz CPU.",
            "table_column_names": [
                "[BOLD] Algorithm",
                "[BOLD] en_ud",
                "[BOLD] en_ptb",
                "[BOLD] sents/s"
            ],
            "table_content_values": [
                [
                    "Eisner (generic)",
                    "96.35",
                    "479.1",
                    "∼ 80"
                ],
                [
                    "Eisner (ours)",
                    "1.496",
                    "6.31",
                    "∼ 6009"
                ],
                [
                    "CLE (generic)",
                    "19.12",
                    "93.8",
                    "∼ 404"
                ],
                [
                    "CLE (ours)",
                    "1.764",
                    "6.98",
                    "∼ 5436"
                ]
            ]
        },
        "gold_description": "seconds a decoder takes to decode an entire dataset , given a set of scores . score matrix entries are we compare cython implementations in table 1 and figure 1 over randomised score input . note that our implementations are significantly faster .",
        "generated_description": "in order to demonstrate the scalability of eisner , we measure the time it takes to decode an entire dataset given a random subset of the score matrix , given a set of scores . as expected , the random generated data has a big impact on cle since worst - case performance depends on the sorting bottleneck ; the figure demonstrates this by the increasingly broad standard deviation band . in terms of decoding time , eisner is more than three times faster than cle ( indicated in the right table by the number of sents / s ) given a fixed batch size of 80 sentences ."
    },
    {
        "table_id": "293",
        "table_info": {
            "table_caption": "Table 2: Results of form clustering, measured in % of 1−vmeasure (expressing the error, i.e. lower is better). Baseline (either full form or prefix of form of length 5), our system, and oracle upper bound. Last column is error reduction on the scale from baseline to upper bound, in %.",
            "table_column_names": [
                "Treebank ar_padt",
                "Baseline form",
                "Baseline 4.19",
                "Our 3.90",
                "Upp. 2.93",
                "Err.red. 23.1"
            ],
            "table_content_values": [
                [
                    "ca_ancora",
                    "form",
                    "4.65",
                    "4.35",
                    "3.32",
                    "22.3"
                ],
                [
                    "cs_cac",
                    "form5",
                    "3.56",
                    "2.25",
                    "1.14",
                    "54.0"
                ],
                [
                    "cs_fictree",
                    "form5",
                    "4.82",
                    "4.08",
                    "2.68",
                    "34.6"
                ],
                [
                    "cs_pdt",
                    "form5",
                    "4.93",
                    "3.41",
                    "1.65",
                    "46.6"
                ],
                [
                    "da_ddt",
                    "form",
                    "2.32",
                    "2.16",
                    "1.55",
                    "21.2"
                ],
                [
                    "en_ewt",
                    "form",
                    "2.29",
                    "2.22",
                    "1.78",
                    "13.8"
                ],
                [
                    "es_ancora",
                    "form",
                    "3.99",
                    "3.38",
                    "2.25",
                    "34.7"
                ],
                [
                    "et_edt",
                    "form5",
                    "4.78",
                    "4.31",
                    "2.54",
                    "20.9"
                ],
                [
                    "fa_seraji",
                    "form",
                    "8.99",
                    "8.76",
                    "7.44",
                    "14.8"
                ],
                [
                    "fr_gsd",
                    "form",
                    "4.12",
                    "3.81",
                    "2.70",
                    "22.0"
                ],
                [
                    "hi_hdtb",
                    "form",
                    "4.18",
                    "3.58",
                    "2.83",
                    "44.3"
                ],
                [
                    "hr_set",
                    "form5",
                    "4.04",
                    "2.87",
                    "1.71",
                    "50.2"
                ],
                [
                    "it_isdt",
                    "form",
                    "4.27",
                    "3.71",
                    "2.78",
                    "37.8"
                ],
                [
                    "it_postwita",
                    "form",
                    "3.60",
                    "4.07",
                    "2.37",
                    "-38.0"
                ],
                [
                    "ja_gsd",
                    "form",
                    "1.64",
                    "1.93",
                    "1.41",
                    "-123.1"
                ],
                [
                    "ko_kaist",
                    "form",
                    "0.14",
                    "2.41",
                    "0.11",
                    "-6392.8"
                ],
                [
                    "la_ittb",
                    "form5",
                    "6.53",
                    "6.97",
                    "3.85",
                    "-16.4"
                ],
                [
                    "la_proiel",
                    "form5",
                    "6.92",
                    "7.42",
                    "4.20",
                    "-18.4"
                ],
                [
                    "lv_lvtb",
                    "form5",
                    "3.90",
                    "3.39",
                    "2.10",
                    "28.0"
                ],
                [
                    "no_bokmaal",
                    "form",
                    "2.79",
                    "2.22",
                    "1.48",
                    "43.6"
                ],
                [
                    "no_nynorsk",
                    "form",
                    "2.73",
                    "2.52",
                    "1.48",
                    "16.7"
                ],
                [
                    "pl_lfg",
                    "form5",
                    "3.68",
                    "3.06",
                    "1.84",
                    "33.6"
                ],
                [
                    "pt_bosque",
                    "form",
                    "3.57",
                    "3.17",
                    "2.55",
                    "39.0"
                ],
                [
                    "ro_nonstd",
                    "form5",
                    "8.13",
                    "7.95",
                    "5.64",
                    "7.2"
                ],
                [
                    "sk_snk",
                    "form5",
                    "2.87",
                    "2.01",
                    "0.63",
                    "38.2"
                ],
                [
                    "uk_iu",
                    "form",
                    "2.66",
                    "1.94",
                    "0.88",
                    "40.7"
                ],
                [
                    "ur_udtb",
                    "form",
                    "3.95",
                    "3.79",
                    "2.65",
                    "12.3"
                ],
                [
                    "Average",
                    "[EMPTY]",
                    "4.08",
                    "3.77",
                    "2.45",
                    "-210.3"
                ],
                [
                    "Median",
                    "[EMPTY]",
                    "3.97",
                    "3.40",
                    "2.31",
                    "22.7"
                ]
            ]
        },
        "gold_description": "our experiments on 23 languages show our approach to be promising , surpassing the baseline on 23 of the 28 evaluation datasets . we evaluate our setup on 28 datasets for 23 languages , finding that it outperforms the baseline on 23 of the datasets , this results in a set of 28 treebanks for 23 languages listed in table 2 . the results are listed in table 2 . the oracle does not reach 100 % ; we also express the performance of our method as error reduction on the scale from baseline ( 0 % ) to upper bound ( 100 % ) . for 23 of the 28 datasets , our method achieves a positive error reduction ; the median error reduction is 23 % . because of the extreme result for korean , the average does not make much sense here . the results are worst for korean and japanese , the \" form \" baseline very close to the upper bound our results are very low here . we also observe deteriorations for a treebank of italian tweets and for treebanks of historical latin , on all other datasets , we observe an improvement over the baseline , with an error reduction typically between 10 % and 35 % . the performance is especially good for slavic languages ( cs , hr , pl , sk , uk ) , where the error reduction is often around 50 % . the evaluation showed the approach to be promising , surpassing the baseline on most of the evaluation datasets .",
        "generated_description": "in general , our proposed model outperforms the baseline in terms of bleu score with an average of 3 . 90 , and an average error rate of 2 . 93 , indicating an error reduction of 54 . 0 % and an increase of 23 . 1 % over the baseline ."
    },
    {
        "table_id": "294",
        "table_info": {
            "table_caption": "Table 3: Comparison of the word form similarities, in % of 1−vmeasure of the clustering. Average and median over the 28 datasets.",
            "table_column_names": [
                "Distance",
                "Average",
                "Median"
            ],
            "table_content_values": [
                [
                    "[ITALIC] JW",
                    "8.17",
                    "7.92"
                ],
                [
                    "[ITALIC] cos",
                    "4.39",
                    "3.87"
                ],
                [
                    "[ITALIC] JW⋅ [ITALIC] cos",
                    "3.77",
                    "3.40"
                ]
            ]
        },
        "gold_description": "in table 3 , we compare the combined distance measure with each of the two components used alone . the results show that combining the edit distance with the embedding similarity is stronger than using any of the measures alone . the embedding similarity alone performs much better than the edit distance alone .",
        "generated_description": "word form similarities : we measure the word form similarities between pairs of words according to the pearson ’ s correlation coefficient of their embedding vectors . the χ2 - test shows that the similarities are statistically significant ( p < 0 . 01 ) between the word vectors and the human - annotated scores . we observe that the average distance between the embedding vector of two words is 8 . 17 % and 7 . 92 % , and the median is 3 . 87 % . we can interpret this as a proof that the word embeddings of similar words tend to be more similar in the context of the same topic , i . e . , in the case of similar topics ."
    },
    {
        "table_id": "295",
        "table_info": {
            "table_caption": "TABLE II: Performance comparison of different models with pre-trained embedding",
            "table_column_names": [
                "#of Authors",
                "6",
                "6",
                "8",
                "10",
                "12",
                "14"
            ],
            "table_content_values": [
                [
                    "samples/author",
                    "350",
                    "1100",
                    "931",
                    "849",
                    "562",
                    "469"
                ],
                [
                    "Char-CNN",
                    "83",
                    "96",
                    "92",
                    "86",
                    "75",
                    "69"
                ],
                [
                    "W2V(CBOW)",
                    "65.3",
                    "97",
                    "82.8",
                    "83.3",
                    "76.4",
                    "71.8"
                ],
                [
                    "fastText(CBOW)",
                    "65",
                    "73",
                    "58",
                    "35.7",
                    "37.31",
                    "40.3"
                ],
                [
                    "W2V(Skip)",
                    "79",
                    "94",
                    "91.1",
                    "85.4",
                    "[BOLD] 82.2",
                    "78.6"
                ],
                [
                    "fastText(Skip)",
                    "[BOLD] 86",
                    "[BOLD] 98",
                    "[BOLD] 95.2",
                    "[BOLD] 86.35",
                    "80.9",
                    "[BOLD] 81.2"
                ]
            ]
        },
        "gold_description": "various subsets of authors were chosen and the dataset was truncated to each author having the same number of samples . the classification was carried out with 2 author attribution datasets , one with 6 authors and our dataset with maximum 14 authors . the larger dataset was trained with 6 , 8 , 10 , 12 and 14 authors to analyze the effects of increasing classes on the proposed model . we evaluate the performance of the proposed architecture in terms of accuracy , we also try to infer how the character - level model compares with the word level models . all models are compared for the increasing number of authors ( classes ) on the corpus mentioned to assess the quality of the models . both word and character levels are summarized in table ii . from the accuracy comparisons shown in table ii we see that skip - gram implemented by fasttext performs well in the given datasets . character level model performs reasonably well in competition with subword level as long as the dataset is big enough . with larger datasets , this model will be able to perform significantly better compare character embeddings with word embeddings showing that character embeddings perform almost as good as the best word embedding model .",
        "generated_description": "we first compare the performance of different models with pre - trained word embedding . we use cbow and skip - gram embeddings for character - based and word2vec embedding models . we observe that our model significantly outperforms the other models . the performance of char - cnn and w2v ( skip ) is comparable to that of fasttext ( skip ) . however , our model outperforms both of them by a large margin when the number of authors is relatively small ."
    },
    {
        "table_id": "296",
        "table_info": {
            "table_caption": "Table 3: Comparison of the Label accuracy on Development set.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Label Accuracy ( [ITALIC] ϕ = 0.76)",
                "[BOLD] Label Accuracy ( [ITALIC] ϕ = 0.67)"
            ],
            "table_content_values": [
                [
                    "HexaF - UCL",
                    "80.18",
                    "80.18"
                ],
                [
                    "Our Model (BERT)",
                    "[BOLD] 80.20",
                    "[BOLD] 86.7"
                ]
            ]
        },
        "gold_description": "although our unsupervised model doesn ' t support all the labels , to show the effectiveness of the approach , we compare the label accuracy of \" supports \" label against a supervised approach – hexaf . results from table 3 suggests that our approach is comparable to hexaf4 for φ = 0 . 76 .",
        "generated_description": "our model outperforms the previous state - of - the - art model ( hexaf - ucl ) by 0 . 4 % in terms of label accuracy ( ρ = 0 . 76 ) ."
    },
    {
        "table_id": "297",
        "table_info": {
            "table_caption": "Table 1: Performance of the question generation system on FEVER Dataset.",
            "table_column_names": [
                "[BOLD] Type of FEVER Set",
                "[BOLD] Total Claims",
                "[BOLD] Claims Converted to Questions",
                "[BOLD] Conversion Accuracy",
                "[BOLD] Total Questions",
                "[BOLD] Questions per claim (Median)"
            ],
            "table_content_values": [
                [
                    "Training Set",
                    "145449",
                    "131969",
                    "90.73",
                    "395717",
                    "3"
                ],
                [
                    "Development Set",
                    "19998",
                    "17749",
                    "88.75",
                    "54422",
                    "3"
                ],
                [
                    "Test Set",
                    "9999",
                    "8863",
                    "88.63",
                    "27359",
                    "3"
                ]
            ]
        },
        "gold_description": "for the subtask of question generation , the results in table 1 show that the system is able to generate questions given a claim with considerably good accuracy . the conversion accuracy is defined as the ratio of the number of claims in which the named entities are extracted to the number of claims . the results also support our assumption that the claims generally feature information about one or more entities . ta",
        "generated_description": "in order to evaluate the question generation system , we use fever dataset which is one of the most popular question answering datasets in recent years . in fever dataset , each claim is associated with a set of questions and converted to questions . the conversion accuracy is defined as the ratio of the number of questions converted to valid questions to the total number of valid questions in the dataset . the system is trained on the fever training set , and tested on the development and test sets . the evaluation metric used in this evaluation is the average of the converted questions and the average number of claims per claim . we report the converted accuracy of the system on the whole fever data as well as the average numbers of questions per claim in each training set and the test set ."
    },
    {
        "table_id": "298",
        "table_info": {
            "table_caption": "Table 2: Performance of the question generation system on FEVER Dataset.",
            "table_column_names": [
                "[BOLD] Type of Set",
                "[BOLD] Label Accuracy ( [ITALIC] ϕ = 0.76)",
                "[BOLD] Label Accuracy ( [ITALIC] ϕ = 0.67)"
            ],
            "table_content_values": [
                [
                    "Training Set",
                    "81.52",
                    "88.05"
                ],
                [
                    "Development Set",
                    "80.20",
                    "86.7"
                ],
                [
                    "Test Set",
                    "80.25",
                    "87.04"
                ]
            ]
        },
        "gold_description": "table 2 shows the performance of our fact checking system on the \" supports \" label , the output of our system . we compare the results against two different classification thresholds . here , φ = 0 . 76 suggests that at least 3 out of the 4 questions have to be answered correctly while φ = 0 . 67 suggests that at least 2 out of the 3 questions has to be answered correctly for the claim to be classified as \" supports \" . in contrast to the results reported in table 2 , here we consider φ = 0 . 76 to be a better classification threshold as it improvises over false positives considerably over the entire dataset . from the results , we conclude that it is possible to verify the facts with the right kind of factoid questions .",
        "generated_description": "from the table , we can see that the test set of fever achieves 87 . 04 % label accuracy , which is close to the system ’ s performance on the training set ( 87 . 04 % ) and the development set ( 86 . 7 % ) . from the results of the fever dataset , it can be seen that the system achieves high label accuracy on both the training and the test sets . from the result of the question generation system , it is clear that it can answer the question correctly on both types of sets ."
    },
    {
        "table_id": "299",
        "table_info": {
            "table_caption": "Table 1: Evaluation of trained models on all test sets. Gap and WinoBias (abbreviated WB) are additionally split into subsets, as introduced in Section 5. Double lines in the table separate results from three different scenarios: when no training data is available, when additional training data exists, and the transductive scenario. The table is further split into sections separated with single horizontal lines. Each section contains a model that has been trained on WikiCREM and models that have not been. The best result in each section is in bold. The best overall result is underlined. Scores on Gap are measured as F1-scores, while the performance on other datasets is given in accuracy. The source of each SOTA is listed in Section 5.",
            "table_column_names": [
                "[EMPTY]",
                "Transductive scenario Gap  [ITALIC] F1",
                "Transductive scenario Gap  [ITALIC] FF1",
                "Transductive scenario Gap  [ITALIC] FM1",
                "Transductive scenario Bias  [ITALIC] FF1 [ITALIC] FM1",
                "Dpr",
                "Wsc",
                "Wnli",
                "[EMPTY]"
            ],
            "table_content_values": [
                [
                    "SOTA",
                    "72.1%",
                    "71.4%",
                    "72.8%",
                    "0.98",
                    "76.4%",
                    "[BOLD] 72.5%–––––––",
                    "[BOLD] 74.7%–––––––",
                    "[EMPTY]"
                ],
                [
                    "Bert",
                    "50.0%",
                    "47.2%",
                    "52.7%",
                    "0.90",
                    "59.8%",
                    "61.9%",
                    "65.8%",
                    "no train data"
                ],
                [
                    "Bert_WikiRand",
                    "55.1%",
                    "51.8%",
                    "58.2%",
                    "0.89",
                    "59.2%",
                    "59.3%",
                    "65.8%",
                    "no train data"
                ],
                [
                    "Bert_WikiCREM",
                    "[BOLD] 59.0%",
                    "[BOLD] 57.5%",
                    "[BOLD] 60.5%",
                    "[BOLD] 0.95",
                    "[BOLD] 67.4%",
                    "[BOLD] 63.4%",
                    "[BOLD] 67.1%",
                    "no train data"
                ],
                [
                    "Bert_Gap",
                    "75.2%",
                    "75.1%",
                    "75.3%",
                    "[BOLD] 1.00–––––",
                    "66.8%",
                    "63.0%",
                    "68.5%",
                    "existing train data"
                ],
                [
                    "Bert_WikiCREM_Gap",
                    "[BOLD] 77.4%",
                    "[BOLD] 78.4%",
                    "[BOLD] 76.4%",
                    "1.03",
                    "[BOLD] 71.1%",
                    "[BOLD] 64.1%",
                    "[BOLD] 70.5%",
                    "existing train data"
                ],
                [
                    "Bert_Dpr",
                    "60.9%",
                    "61.3%",
                    "60.6%",
                    "1.01",
                    "[BOLD] 83.3%",
                    "67.0%",
                    "71.9%",
                    "existing train data"
                ],
                [
                    "Bert_Gap_Dpr",
                    "[BOLD] 70.0%",
                    "[BOLD] 70.4%",
                    "[BOLD] 69.5%",
                    "1.01",
                    "79.4%",
                    "65.6%",
                    "72.6%",
                    "existing train data"
                ],
                [
                    "Bert_WikiCREM_Dpr",
                    "64.2%",
                    "64.2%",
                    "64.1%",
                    "[BOLD] 1.00–––––",
                    "80.0%",
                    "[BOLD] 71.8%",
                    "[BOLD] 74.7%–––––––",
                    "existing train data"
                ],
                [
                    "Bert_all",
                    "76.0%",
                    "77.4%",
                    "74.7%",
                    "[BOLD] 1.04",
                    "80.1%",
                    "[BOLD] 70.0%",
                    "74.0%",
                    "existing train data"
                ],
                [
                    "Bert_WikiCREM_all",
                    "[BOLD] 78.0%–––––––",
                    "[BOLD] 79.4%–––––––",
                    "[BOLD] 76.7%–––––––",
                    "[BOLD] 1.04",
                    "[BOLD] 84.8%–––––––",
                    "[BOLD] 70.0%",
                    "[BOLD] 74.7%–––––––",
                    "existing train data"
                ]
            ]
        },
        "gold_description": "the results of the evaluation of the models on the test sets are shown in table 1 . we notice that additional training on wikicrem consistently improves the performance of the models in all scenarios and on most tests . due to the small size of some test sets , some of the results are subject to deviation . this especially applies to pdp ( 60 test samples ) and wnli ( 145 test samples ) . we observe that bert wikirand generally performs worse than bert , with gap and pdp being notable exceptions . this shows that bert is a strong baseline and that improved performance of bert wikicrem is not a consequence of training on shorter sentences or with different loss function . bert wikicrem consistently outperforms both baselines on all tests , showing that wikicrem can be used as a standalone dataset . we observe that training on the data from the target distribution improves the performance the most . models trained on gap - train usually show more than a 20 % increase in their f1 - score on gap - test . still , bert wikicrem gap shows a consistent improvement over bert gap on all subsets of the gap test set . this confirms that wikicrem works not just as a standalone dataset , but also as an additional pre - training in the transductive scenario . similarly , bert wikicrem dpr outpertasks , forms bert dpr on the majority of showing the applicability of wikicrem to the scenario where additional training data is available . however , good results of bert gap dpr show that additional training on a manually constructed dataset , such as gap , can yield similar results as additional training on wikicrem . the reason behind this difference is the impact of the data distribution . gap , dpr , and wikicrem contain data that",
        "generated_description": "in this section , we present the results of sota , bert and bert_wikirand trained with and without the proposed wikicrem . we observe that sota achieves the best performance in terms of all evaluation metrics in both transductive and non - transductive scenarios . in the non - ductive scenario , bert achieves a higher f1 score than bert but a lower fm1 score . bert achieves higher fm1 but lower f1 than bertrand , indicating that there is still a large gap between bert and the other models . bert with the wikirand training data ( fm1 score of 55 . 1 % ) achieves the worst performance , which indicates that the model does not have enough training data to generalize well in this case . on the other hand , bert ’ s performance is close to sota and even outperforms sota when there is no train data . this shows that the training data is sufficient to train bert and wikirand ."
    },
    {
        "table_id": "300",
        "table_info": {
            "table_caption": "TABLE III: pretrained vs non-pretrained comparison",
            "table_column_names": [
                "#of Authors",
                "6",
                "6",
                "8",
                "10",
                "12",
                "14"
            ],
            "table_content_values": [
                [
                    "#of samples/class",
                    "350",
                    "1100",
                    "931",
                    "849",
                    "562",
                    "469"
                ],
                [
                    "Pretrained Embedding",
                    "83",
                    "96",
                    "92",
                    "86",
                    "75",
                    "69"
                ],
                [
                    "Not pretrained",
                    "71",
                    "95",
                    "82",
                    "83",
                    "66",
                    "59.5"
                ]
            ]
        },
        "gold_description": "various subsets of authors were chosen and the dataset was truncated to each author having the same number of samples . with and without pre - training character level embedding and comparing the proposed architectures on the held - out dataset . to illustrate the need of pre - trained character embeddings , we see from iii that using a pre - trained embedding increases the accuracy across datasets and the different number of authors , regardless of the amount of data for each author . increase the performance a few degrees . we analyzed the importance of pretrained character embedding for author attribution and showed that pre - training can result in better performances .",
        "generated_description": "for the first experiment , we randomly sampled a set of authors from the training set and trained a classifier for each of them using the pretrained embeddings . in addition , we also trained a model using a non - pretrained embedding . the reason for the difference in performance between the two models is due to the fact that the embedding layer in the encoder is pretrained while the representation in the decoder is randomly sampled from a different training set . this makes the training more difficult . for the second experiment , authors were selected based on their popularity and number of reviews in the test set . in this experiment , the number of reviewers was set as low as six and as high as eight authors . the performance of the models was evaluated using accuracy for each author as well as for the entire dataset . the results show that the model trained using the pre - trained embedding outperforms the models pretrained on the entire training set by a large margin ."
    },
    {
        "table_id": "301",
        "table_info": {
            "table_caption": "Table 1: Accuracy for cross-lingual Wikipedia title linking, with the transfer HRL shown in parentheses. The best accuracy among input representations with graphemes, phonemes or articulatory features for Encode and PBEL is presented here. Complete results for each representation are in the supplementary material.",
            "table_column_names": [
                "Model",
                "bn",
                "jv",
                "lo",
                "mr",
                "pa",
                "te",
                "ti",
                "uk",
                "ug",
                "Avg."
            ],
            "table_content_values": [
                [
                    "[BOLD] Exact",
                    ".00",
                    ".63",
                    ".02",
                    ".00",
                    ".00",
                    ".00",
                    ".02",
                    ".02",
                    ".03",
                    ".08"
                ],
                [
                    "[BOLD] Trans",
                    ".00",
                    ".63",
                    ".02",
                    ".17",
                    ".00",
                    ".00",
                    ".46",
                    ".02",
                    ".03",
                    ".15"
                ],
                [
                    "[BOLD] Encode",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Manual",
                    ".36 (hi)",
                    ".70 (id)",
                    ".07 (th)",
                    ".46 (hi)",
                    ".31 (hi)",
                    ".20 (ta)",
                    ".44 (am)",
                    ".25 (ru)",
                    ".16 (tr)",
                    ".33"
                ],
                [
                    "Best-53",
                    ".38 (ms)",
                    ".70 (id)",
                    ".07 (th)",
                    ".46 (hi)",
                    ".36 (te)",
                    ".36 (pa)",
                    ".44 (am)",
                    ".41 (kk)",
                    ".16 (tr)",
                    ".37"
                ],
                [
                    "[BOLD] Pbel",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Manual",
                    ".48 (hi)",
                    ".86 (id)",
                    "[BOLD] .28 (th)",
                    "[BOLD] .62 (hi)",
                    "[BOLD] .49 (hi)",
                    ".33 (ta)",
                    "[BOLD] .69 (am)",
                    ".50 (ru)",
                    ".32 (tr)",
                    ".51"
                ],
                [
                    "Best-53",
                    ".48 (hi)",
                    ".86 (id)",
                    "[BOLD] .28 (th)",
                    "[BOLD] .62 (hi)",
                    "[BOLD] .49 (hi)",
                    "[BOLD] .47 (hi)",
                    "[BOLD] .69 (am)",
                    ".54 (kk)",
                    ".32 (tr)",
                    ".53"
                ],
                [
                    "Multi",
                    "[BOLD] .53",
                    "[BOLD] .87",
                    "[BOLD] .28",
                    "[BOLD] .62",
                    ".48",
                    ".46",
                    "[BOLD] .69",
                    "[BOLD] .56",
                    "[BOLD] .40",
                    "[BOLD] .54"
                ]
            ]
        },
        "gold_description": "the entity linking accuracy on the wikipedia test set are summarized in table 1 . on average , our proposed pbel method performs significantly better than the baselines , with significant accuracy gains in all nine test languages . the exact baseline , which is most often used for monolingual el ( sil et al . 2017 ) , performs reasonably only when the test language is in the same script as english ( i . e . , javanese ) . similarly trans , the current state - of - the - art retrieval method in cross - lingual el ( pan et al . 2017 ) , fails when zero data is available in the test language , unless the hrl is very closely - related to the lrl ( as with jv , mr and am ) . on the other hand , encode presents relatively strong zero - shot transfer results . pbel offers stronger performance than encode as seen in the best - 53 results , the hrl that performs best is closely - related to the respective test lrl we also observe that using multiple pivot hrls leads to better average accuracy , with considerable improvement for some languages . in most cases , the manual hrl is also the best performing in best - 53 . however , we see that the dravidian telugu ( te ) seems to obtain higher accuracy with indo - aryan hrls – punjabi ( pa ) or hindi ( hi ) . we also see that the ukrainian ( uk ) test set has better performance with another cyrillic script language , kazakh ( kk ) , rather than russian ( ru ) .",
        "generated_description": "table 1 shows the average accuracy of the different systems when using the manual and transformer baselines . as expected , the best results are obtained by using the transformer baseline . however , the results obtained using the encode baseline are slightly lower than those obtained with the manual baseline . in general , we see that the accuracy obtained using transformer is slightly better than using the exact match baseline ."
    },
    {
        "table_id": "302",
        "table_info": {
            "table_caption": "Table 2: Entity linking accuracy on non-Wikipedia data",
            "table_column_names": [
                "Lang.",
                "Tigrinya",
                "Oromo"
            ],
            "table_content_values": [
                [
                    "Exact",
                    "0.00",
                    "0.01"
                ],
                [
                    "Trans Supervised",
                    "0.21",
                    "0.05"
                ],
                [
                    "Trans Unsupervised",
                    "0.21",
                    "0.01"
                ],
                [
                    "Encode",
                    "0.16",
                    "0.10"
                ],
                [
                    "Pbel",
                    "[BOLD] 0.33",
                    "[BOLD] 0.11"
                ]
            ]
        },
        "gold_description": "entity linking accuracies on the lorelei dataset are shown in table 2 . pbel has considerably higher accuracy than the other methods . however , we see relatively lower improvement in accuracy with somali - oromo than amharic - tigrinya . the supervised trans model , which uses wikipedia parallel data in the lrl itself as a lexicon , does not perform better than the zero - shot pbel . the pbel model results for our test set with each input representation are presented in table 3 .",
        "generated_description": "for the tigrinya and oromo datasets , pbel achieves the best performance with accuracies of 0 . 33 and 0 . 11 , respectively . for the lang . dataset , the exact match score is 0 . 00 and the unsupervised baselines are 0 . 01 . the trans - supervised baseline is much lower than the trans - unsupervised and the encode baseline , indicating that the task is difficult even for humans . our method outperforms the baselines in terms of both precision and recall for both languages ."
    },
    {
        "table_id": "303",
        "table_info": {
            "table_caption": "Table 3: Entity linking accuracy with PBEL, using Graphemes, Phonemes or Articulatory features as input. The HRL used for training and pivoting is shown in parentheses in the first row. The pairs with the different scripts are marked with a “*”.",
            "table_column_names": [
                "Input",
                "*bn (hi)",
                "jv (id)",
                "*lo (th)",
                "mr (hi)",
                "*pa (hi)",
                "*te (ta)",
                "ti (am)",
                "*uk (ru)",
                "*ug (tr)"
            ],
            "table_content_values": [
                [
                    "Grapheme",
                    ".00",
                    "[BOLD] .86",
                    ".02",
                    "[BOLD] .62",
                    ".00",
                    ".00",
                    ".61",
                    "[BOLD] .50",
                    ".08"
                ],
                [
                    "Phoneme",
                    "[BOLD] .48",
                    ".84",
                    ".20",
                    ".58",
                    ".18",
                    ".10",
                    "[BOLD] .69",
                    ".23",
                    ".21"
                ],
                [
                    "Articulatory",
                    ".45",
                    ".82",
                    "[BOLD] .28",
                    ".56",
                    "[BOLD] .49",
                    "[BOLD] .33",
                    ".63",
                    ".42",
                    "[BOLD] .32"
                ]
            ]
        },
        "gold_description": "see we that using phonological representations ( phonemes and articulatory features ) offers the ability to map between languages that use different orthographies , explaining the convincing improvement over graphemes for hrl - lrl pairs that are written in different scripts ( table 3 ) . with graphemes , the experiments on these languages achieve ≈ 0 accuracy because the character vocabulary of the hrl encoder simply does not contain the low - resource test language characters . this is the case with lao - thai ( lo - th ) , telugu - tamil ( te - ta ) and bengali - hindi ( bn - hi ) . in contrast , we observe that the grapheme representation offers strong transfer performance when the lrl and hrl share orthography , notably javanese - indonesian ( jv - id ) , marathi - hindi ( mr - hi ) and ukrainian - russian ( uk - ru ) .",
        "generated_description": "we first compare entity linking accuracy with pbel , using graphemes , phonemes or articulatory features as input . the grapheme - based model achieves the best performance in 9 out of 12 pairs , with a spearman ’ s ρ between . 00 and . 50 . the addition of phonemes and articulatory features to the model improves performance in one language pair after another , with the largest improvement in the case of basque ( from . 86 to . 93 ) . the improvement in performance is less pronounced in other languages , with less than 2 % change in accuracy across all pairs ."
    },
    {
        "table_id": "304",
        "table_info": {
            "table_caption": "Table 1: Proportions of correctly answered questions.",
            "table_column_names": [
                "[EMPTY]",
                "Textbooks",
                "Wikipedia"
            ],
            "table_content_values": [
                [
                    "IR",
                    "25.24",
                    "25.14"
                ],
                [
                    "PMI",
                    "26.22",
                    "25.19"
                ],
                [
                    "ESIM w/o scenario",
                    "25.85",
                    "25.41"
                ],
                [
                    "ESIM w/ scenario",
                    "24.34",
                    "24.41"
                ],
                [
                    "DIIN w/o scenario",
                    "24.15",
                    "25.20"
                ],
                [
                    "DIIN w/ scenario",
                    "25.11",
                    "24.89"
                ],
                [
                    "BERT [ITALIC] NLI w/o scenario",
                    "24.29",
                    "24.17"
                ],
                [
                    "BERT [ITALIC] NLI w/ scenario",
                    "24.97",
                    "24.68"
                ],
                [
                    "BiMPM w/o scenario",
                    "24.13",
                    "24.51"
                ],
                [
                    "BiMPM w/ scenario",
                    "24.76",
                    "23.81"
                ],
                [
                    "BERT [ITALIC] RC w/o scenario",
                    "24.81",
                    "24.78"
                ],
                [
                    "BERT [ITALIC] RC w/ scenario",
                    "23.66",
                    "23.01"
                ]
            ]
        },
        "gold_description": "the results are summarized in table almost all the methods performed similar to random guess , showing that sqa on our dataset has its unique challenges .",
        "generated_description": "the results show that bert nli and bert rc achieve the best performance on both datasets . bertnli outperforms bertrc by 0 . 5 % and 0 . 8 % on the wikipedia dataset when the training data is the same as the test data . this shows that the scenario information is helpful for nli to predict the correct answer . in addition , the results indicate that scenario helps bertnli achieve better performance ."
    },
    {
        "table_id": "305",
        "table_info": {
            "table_caption": "Table 1: Experiment 1: Caption retrieval on the coco dataset. We compare the different reminders of the different methods first in English and then by adding new languages. We also evaluate variations of dsve method with different word embedding.",
            "table_column_names": [
                "Embedding",
                "lang.",
                "r@1",
                "r@5",
                "r@10"
            ],
            "table_content_values": [
                [
                    "vse ",
                    "en",
                    "64.6",
                    "∅",
                    "95.7"
                ],
                [
                    "dsve ",
                    "en",
                    "69.8",
                    "91.9",
                    "96.6"
                ],
                [
                    "dsve w/ w2v",
                    "en",
                    "63.48",
                    "89.48",
                    "95.64"
                ],
                [
                    "dsve w/ FastText",
                    "en",
                    "66.08",
                    "90.7",
                    "96.2"
                ],
                [
                    "Ours w/ bv",
                    "en",
                    "65.58",
                    "90.52",
                    "96.1"
                ],
                [
                    "[EMPTY]",
                    "en+fr",
                    "67.78",
                    "91.58",
                    "96.92"
                ],
                [
                    "Ours w/ muse",
                    "en",
                    "63.1",
                    "89.58",
                    "95.56"
                ],
                [
                    "[EMPTY]",
                    "en+fr",
                    "63.88",
                    "89.2",
                    "95.24"
                ],
                [
                    "[EMPTY]",
                    "en+fr+de",
                    "62.4",
                    "89.18",
                    "95.16"
                ],
                [
                    "[EMPTY]",
                    "all",
                    "63.28",
                    "88.3",
                    "94.6"
                ]
            ]
        },
        "gold_description": "the table 1 shows the caption retrieval recall on coco dataset . the first two lines show the state - of - the - art results . the second pair of lines present the results of our model , with w2v and fasttext embeddings used as the baseline . we can see that our model is close to the deep semanticvisual embedding ( dsve ) method while the w2v method is slightly worst , as the representation power of the word embedding is reduced . the bivec english - french method is used in english and on both languages simultaneously . if trained only on english , i . e . only on the coco dataset like the two previous methods , it shows performance similar to the one of the state - of - the - art . this means training using bivec does not weaken the english representation . when trained on english and french together , the recall is increased by 3 . 35 % , going from 65 . 58 % to 67 . 78 % . we can also see an improvement for recall @ 5 and recall @ 10 , with respectively 1 . 17 % and 0 . 85 % of increase . this implies that the similarity learning with french captions increases english recognition when using bivec . first of all , when training with muse for english only , we can see a sharp decrease in performance , with a recall going from 66 . 08 % to 63 . 10 % . by comparing the model trained with w2v , we obtain similar results . this could come from the fact that both muse and w2v embeddings do not have representation for out of vocabulary words like the fasttext ones . moreover , rare words have much more chance to be wrongly projected because of space transformation . when we train the model with additional languages , we can see a slight decrease in performance in english . the maximum decrease is 1 . 01",
        "generated_description": "in this section , we present the results of the first experiment on the caption retrieval task for the cocoder - decoder cross - lingual retrieval task . for this experiment , we use en as the embedding language and use the target language as the decoder and compare the retrieval performance with previous state - of - the - art methods . as can be seen , our reimplementation of vse obtains slightly better results than the original vse embeddings in terms of r @ 1 , r @ 5 and r @ 10 . our reimplementation using bv and w2v as the visual features achieves comparable retrieval performance to the original vse . the results show that the visual information captured by the two visual features are complementary to each other and that the combination of both visual and visual information contributes to the best results ."
    },
    {
        "table_id": "306",
        "table_info": {
            "table_caption": "Table 2: Experiment 2: Image retrieval on the coco dataset. The methods are the same as in table 1.",
            "table_column_names": [
                "Embedding",
                "lang.",
                "r@1",
                "r@5",
                "r@10"
            ],
            "table_content_values": [
                [
                    "vse ",
                    "en",
                    "52.00",
                    "∅",
                    "92.0"
                ],
                [
                    "dsve ",
                    "en",
                    "55.9",
                    "86.9",
                    "94.0"
                ],
                [
                    "dsve w/ w2v",
                    "en",
                    "51.87",
                    "84.31",
                    "92.48"
                ],
                [
                    "dsve w/ FastText",
                    "en",
                    "54.12",
                    "85.74",
                    "92.93"
                ],
                [
                    "Ours w/ bv",
                    "en",
                    "55.57",
                    "86.92",
                    "93.86"
                ],
                [
                    "[EMPTY]",
                    "en+fr",
                    "56.09",
                    "87.22",
                    "94.03"
                ],
                [
                    "Ours w/ muse",
                    "en",
                    "51.81",
                    "84.70",
                    "92.82"
                ],
                [
                    "[EMPTY]",
                    "en+fr",
                    "52.25",
                    "84.72",
                    "92.74"
                ],
                [
                    "[EMPTY]",
                    "en+fr+de",
                    "51.17",
                    "84.09",
                    "92.22"
                ],
                [
                    "[EMPTY]",
                    "all",
                    "50.44",
                    "83.39",
                    "91.80"
                ]
            ]
        },
        "gold_description": "given a sentence , in any language , we evaluate the rank of the corresponding image . the evaluation is again made by batches of 1000 . the results are presented in table 2 . the first two lines of the table present the state - of - the - art results , with w2v and fasttext embeddings . we can see similar results as in the previous experiment . with bivec , we have results close to the fasttext embeddings when training only in english . this time , the recall is better with an increase of 2 . 68 % for recall @ 1 . when trained with english and french , the recall @ 1 is increased by 3 . 65 % . this implies , again , that we can improve performance by learning on an additional language . our model is able to use the multi - language representing the effectiveness of muse embeddings . we train the model with english , and different combinations of french , german and czech . on english only , we have similar results to the w2v approach . when adding new languages , we can see a decrease in performance for english . we obtain a maximum decrease of 2 . 62 % for recall @ 1 when the models saw english , french , german and czech . for image retrieval from a caption in 4 languages , we obtain a 49 . 38 % recall @ 10 on the multi30k dataset .",
        "generated_description": "in this section , we compare the proposed muse and bv embeddings with the baseline models for image retrieval on the coc dataset . the results show that using either bv or w2v as the embedding pre - trained on the french captions of the ms coco dataset improves the retrieval performance by 0 . 6 % and 0 . 8 % in terms of r @ 1 and r @ 5 , respectively . we also observe that using the fasttext embedding as the pre - training embedding results in a slight improvement of 0 . 3 % in r @ 10 . the best result is achieved by using all the five senses together , i . e . , en + fr + de ."
    },
    {
        "table_id": "307",
        "table_info": {
            "table_caption": "Table 3: Image Recall@10 on the Multi30k dataset with different languages with muse.",
            "table_column_names": [
                "train. lang.",
                "en",
                "fr",
                "de",
                "cs",
                "all"
            ],
            "table_content_values": [
                [
                    "en",
                    "56.60",
                    "46.05",
                    "44.18",
                    "38.75",
                    "46.40"
                ],
                [
                    "en+fr",
                    "50.93",
                    "43.69",
                    "41.61",
                    "34.02",
                    "42.43"
                ],
                [
                    "en+fr+de",
                    "54.63",
                    "46.94",
                    "45.07",
                    "38.26",
                    "46.22"
                ],
                [
                    "all",
                    "55.32",
                    "49.30",
                    "46.84",
                    "46.06",
                    "49.38"
                ]
            ]
        },
        "gold_description": "the model is trained with english only , then with english and french ( en + fr ) , with english , french , and german ( en + fr + de ) and with english , french , german and czech ( all ) . we can see a decrease in performance when adding french that is not present with other languages . otherwise , every time we add a new language the recall for this language logically increase . the best performance is achieved with english + french + german + czech , with an increase of 6 . 42 % for multilingual retrieval . by using muse embeddings , we are able to embed more languages in the same model . we showed that adding other languages decrease performance for english , but increase the recall in a multilingual environment .",
        "generated_description": "muse . we use the multi30k dataset from muse . org to evaluate the impact of using different languages with muse . we train models with en , fr , de , en + fr , de and cs on the training set and evaluate them on the test set using 10 - fold cross validation . we observe that models trained with different languages benefit from each other , and the overall best performance is achieved when using all the languages together ."
    },
    {
        "table_id": "308",
        "table_info": {
            "table_caption": "Table 4: Image Recall@10 on Multi30k dataset with different languages with bv Embeddings.",
            "table_column_names": [
                "train. lang.",
                "en",
                "fr",
                "de",
                "en+fr",
                "en+de"
            ],
            "table_content_values": [
                [
                    "en",
                    "53.35",
                    "26.13",
                    "22.96",
                    "39.74",
                    "34.57"
                ],
                [
                    "en+fr",
                    "59.76",
                    "55.22",
                    "∅",
                    "57.50",
                    "∅"
                ],
                [
                    "en+de",
                    "61.44",
                    "∅",
                    "43.59",
                    "∅",
                    "52.51"
                ]
            ]
        },
        "gold_description": "with bivec embeddings , we learn two languages at the same time , and test retrieval on one or two of these languages . results are shown in table 4 . trained in english alone , the model gives worse performance than muse for languages not seen previously . for example , with english - german bivec and a model trained only in english , and test on german , we obtain only 22 . 96 % recall @ 10 , where muse embeddings obtain 44 . 18 % . but when train on english and french , we obtain 55 . 22 % recall , an increase of 26 . 39 % compared to muse . with german and english training , we have an increase of 15 . 16 % on english only recall , with a recall of 61 . 44 % . meaning that , once again , learning a new language with bivec enables better results in english , as the same kind of results is visible with french as well .",
        "generated_description": "multilingual image @ 10 : we train different languages with bv embeddings on multi30k dataset and compare their performance on different image captioning tasks . we use en , fr , de , en + fr and en + de as the training languages and use en as the inference languages . we observe that the performance degrades when using different training languages , which is consistent with our intuition . the bv - embeddings help to improve the image @ 10 performance for all the languages when trained with different languages . however , when using the en - fr and de - en languages , the performance drops significantly . we conjecture that this is due to the fact that the multi - lingual training data is small and the model needs to learn a common representation for all languages ."
    },
    {
        "table_id": "309",
        "table_info": {
            "table_caption": "Table 1: Non-expert human performance results for a randomly-selected validator per question.",
            "table_column_names": [
                "[BOLD] Resource",
                "[ITALIC] Dev  [ITALIC] EM",
                "[ITALIC] Dev  [ITALIC] F1",
                "[ITALIC] Test  [ITALIC] EM",
                "[ITALIC] Test  [ITALIC] F1"
            ],
            "table_content_values": [
                [
                    "DBiDAF",
                    "63.0",
                    "76.9",
                    "62.6",
                    "78.5"
                ],
                [
                    "DBERT",
                    "59.2",
                    "74.3",
                    "63.9",
                    "76.9"
                ],
                [
                    "DRoBERTa",
                    "58.1",
                    "72.0",
                    "58.7",
                    "73.7"
                ]
            ]
        },
        "gold_description": "we select a randomly chosen validator ' s answer to each question and compute exact match ( em ) and word overlap f1 scores with the original to calculate non - expert human performance ; table 1 shows the result . we observe a clear trend : the stronger the model in the loop used to construct the dataset , the harder the resulting questions become for humans .",
        "generated_description": "for dbidaf , dbert , droberta and their variants , we randomly sample 100 examples from the test set for each question , and ask one annotator ( a non - expert ) to answer the remaining 50 examples for a randomly - selected set of questions . the annotators are asked to choose the best answer according to their reasoning skills and previous knowledge about the dataset as well as the model ’ s architecture . we observe that human performance on this dataset is lower than on the development set , indicating that the dataset is more challenging ."
    },
    {
        "table_id": "310",
        "table_info": {
            "table_caption": "Table 1: Abstractive summarization results.",
            "table_column_names": [
                "[BOLD] Method",
                "[BOLD] F1 R-1",
                "[BOLD] F1 R-2",
                "[BOLD] F1 R-L",
                "[BOLD] Recall R-1",
                "[BOLD] Recall R-2",
                "[BOLD] Recall R-L",
                "[BOLD] Precision R-1",
                "[BOLD] Precision R-2",
                "[BOLD] Precision R-L"
            ],
            "table_content_values": [
                [
                    "PG",
                    "36.82",
                    "15.92",
                    "33.57",
                    "37.36",
                    "16.10",
                    "34.05",
                    "38.72",
                    "16.86",
                    "35.32"
                ],
                [
                    "+M1-latent",
                    "37.76",
                    "16.51",
                    "34.48",
                    "[BOLD] 40.15",
                    "[BOLD] 17.52",
                    "36.65",
                    "37.90",
                    "16.64",
                    "34.61"
                ],
                [
                    "+M1-shallow",
                    "37.45",
                    "16.23",
                    "34.22",
                    "[BOLD] 40.15",
                    "17.38",
                    "[BOLD] 36.68",
                    "37.34",
                    "16.24",
                    "34.13"
                ],
                [
                    "+M2-latent",
                    "[BOLD] 38.04",
                    "[BOLD] 16.73",
                    "[BOLD] 34.83",
                    "38.92",
                    "17.05",
                    "35.62",
                    "[BOLD] 39.54",
                    "[BOLD] 17.51",
                    "[BOLD] 36.23"
                ],
                [
                    "+M2-shallow",
                    "37.15",
                    "16.13",
                    "33.96",
                    "38.52",
                    "16.68",
                    "35.21",
                    "38.19",
                    "16.67",
                    "34.91"
                ],
                [
                    "+M3-latent",
                    "37.04",
                    "16.05",
                    "33.86",
                    "37.52",
                    "16.22",
                    "34.29",
                    "38.95",
                    "16.98",
                    "35.63"
                ],
                [
                    "+M3-shallow",
                    "37.09",
                    "16.15",
                    "33.95",
                    "39.05",
                    "16.97",
                    "35.73",
                    "37.62",
                    "16.46",
                    "34.45"
                ],
                [
                    "PG+Cov",
                    "39.32",
                    "17.22",
                    "36.02",
                    "40.33",
                    "17.61",
                    "36.93",
                    "[BOLD] 40.82",
                    "[BOLD] 17.99",
                    "[BOLD] 37.42"
                ],
                [
                    "+M1-latent",
                    "[BOLD] 40.06",
                    "[BOLD] 17.63",
                    "36.70",
                    "[BOLD] 44.44",
                    "[BOLD] 19.53",
                    "[BOLD] 40.69",
                    "38.60",
                    "17.05",
                    "35.39"
                ],
                [
                    "+M1-shallow",
                    "39.78",
                    "17.50",
                    "36.50",
                    "43.50",
                    "19.08",
                    "39.89",
                    "38.94",
                    "17.22",
                    "35.75"
                ],
                [
                    "+M2-latent",
                    "40.00",
                    "17.62",
                    "[BOLD] 36.72",
                    "43.53",
                    "19.17",
                    "39.94",
                    "39.28",
                    "17.37",
                    "36.09"
                ],
                [
                    "+M2-shallow",
                    "39.58",
                    "17.30",
                    "36.36",
                    "44.00",
                    "19.19",
                    "40.38",
                    "38.40",
                    "16.87",
                    "35.31"
                ],
                [
                    "+M3-latent",
                    "39.23",
                    "17.00",
                    "36.00",
                    "42.95",
                    "18.54",
                    "39.37",
                    "38.29",
                    "16.69",
                    "35.16"
                ],
                [
                    "+M3-shallow",
                    "39.57",
                    "17.31",
                    "36.28",
                    "43.85",
                    "19.14",
                    "40.17",
                    "38.37",
                    "168.6",
                    "35.20"
                ]
            ]
        },
        "gold_description": "we assess unigram ( r - 1 ) , bigram ( r - 2 ) , and longest - commonsubsequence ( r - l ) overlap , and present f1 , recall and precision scores in table 1 . for the first baseline ( pg ) , we see that incorporating discourse features consistently improves recall and f1 . we see similar observations for the second baseline ( pg + cov ) : recall is generally improved at the expense of precision . observing that our model generally has better recall ( table 1 )",
        "generated_description": "in this section , we evaluate the performance of pg + m1 , m1 − latent , m2 − shallow , and m2 + m2 - latent on the test set of mcnewsqa using rouge - l and f1 score . we can see that adding m1 to the baseline pg model results in a significant performance boost , which indicates that the document - level latent information is helpful to generate more informative summaries . moreover , the addition of m2 to the basic pg model leads to a slight performance drop . we conjecture that the reason may be that the latent information may help the model to focus more on important sentences in the first sentence and lose some important information in the subsequent sentences . on the other hand , employing m1 with shallow fusion also leads to performance drop , which shows that the fusion of document level and document level information is important for the final summary ."
    },
    {
        "table_id": "311",
        "table_info": {
            "table_caption": "Table 2: Average petition regression performance over 3 runs (noting that lower is better for both MAE and MAPE). One-sided t-tests show that both (1) and (2) are significantly better than the baseline (p<0.05 and p<0.005, resp.).",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] MAE",
                "[BOLD] MAPE"
            ],
            "table_content_values": [
                [
                    "CNN w/ GloVe",
                    "1.16",
                    "14.38"
                ],
                [
                    "+ M1-latent",
                    "1.15",
                    "14.66"
                ],
                [
                    "+ M1-shallow",
                    "[BOLD] 1.12(1) 14.19",
                    "[EMPTY]"
                ],
                [
                    "Bi-LSTM w/ GloVe",
                    "1.14",
                    "14.57"
                ],
                [
                    "+ M1-latent",
                    "1.13",
                    "14.39"
                ],
                [
                    "+ M1-shallow",
                    "1.13",
                    "14.25"
                ],
                [
                    "+ M2-latent",
                    "1.12",
                    "14.02"
                ],
                [
                    "+ M2-shallow",
                    "1.13",
                    "14.20"
                ],
                [
                    "Bi-LSTM w/ latent",
                    "[BOLD] 1.11(2) 13.91",
                    "[EMPTY]"
                ],
                [
                    "Bi-LSTM w/ shallow",
                    "1.15",
                    "14.67"
                ]
            ]
        },
        "gold_description": "we present the test results in table 2 . we are able to reproduce the performance of the baseline model ( \" cnn w / glove \" ) , and find that once again , adding the shallow discourse features improves results . interestingly , we found that replacing the cnn with an lstm results in improved mae , but worse mape . adding discourse features to this model generally has marginal improvement in all cases . when we replace the word sequence with edus ( \" bi - lstm w / latent \" and \" bi - lstm w / shallow \" ) , we see that the latent features outperform the shallow features .",
        "generated_description": "all models benefit from m1 in terms of both mae and mape . for the cnn w / glove model , m1 + m1 - latent shows significant improvements over the baseline ( p < 0 . 05 ) , confirming that m1 helps in reducing mae . the bi - lstm w / latent model shows the best performance overall , with m2 + m2 - shallow showing a small but consistent advantage over m1 . we also note that m2 does not seem to help in the cnn model , which we believe is similar to the observation of \\ newcitesennrich - haddow - birch : 2015 : emnlp2016 , i . e . the model does not benefit from using latent features ."
    },
    {
        "table_id": "312",
        "table_info": {
            "table_caption": "Table 2: Results on IMDb dataset over different teacher sizes. BERTSDV(K=1) is same as BERTSDA(K=1). For IMDb and AG’s News, we report test error rate (%). For SNLI, we report accuracy (%). T denotes the total number of iterations.",
            "table_column_names": [
                "Model",
                "[ITALIC] K",
                "IMDb",
                "AG’s",
                "SNLI"
            ],
            "table_content_values": [
                [
                    "Model",
                    "[ITALIC] K",
                    "IMDb",
                    "News",
                    "SNLI"
                ],
                [
                    "BERTSDV",
                    "1",
                    "5.39",
                    "[BOLD] 5.38",
                    "[BOLD] 91.2"
                ],
                [
                    "BERTSDV",
                    "2",
                    "5.44",
                    "5.39",
                    "91.1"
                ],
                [
                    "BERTSDV",
                    "3",
                    "5.40",
                    "5.50",
                    "[BOLD] 91.2"
                ],
                [
                    "BERTSDV",
                    "4",
                    "5.47",
                    "5.49",
                    "[BOLD] 91.2"
                ],
                [
                    "[EMPTY]",
                    "5",
                    "[BOLD] 5.35",
                    "5.55",
                    "91.1"
                ],
                [
                    "BERTSDA",
                    "[ITALIC] T−1",
                    "5.41",
                    "[BOLD] 5.29",
                    "91.0"
                ],
                [
                    "BERTSDA",
                    "2",
                    "5.46",
                    "5.49",
                    "[BOLD] 91.2"
                ],
                [
                    "BERTSDA",
                    "3",
                    "5.48",
                    "5.55",
                    "91.1"
                ],
                [
                    "BERTSDA",
                    "4",
                    "5.44",
                    "5.52",
                    "91.1"
                ],
                [
                    "[EMPTY]",
                    "5",
                    "[BOLD] 5.29",
                    "5.41",
                    "91.1"
                ]
            ]
        },
        "gold_description": "we choose different teacher size k and evaluate our models in three datasets . table 2 shows that teacher size is sensitive to datasets . therefore , we select the best teacher size for each dataset in the following experiment .",
        "generated_description": "ag ’ s and snli , and the movietriples dataset on which bertsdv performs the best on the imdb and news datasets respectively . we can see that the performance of our model decreases as the number of layers increases , which is consistent with the findings in [ liu2019multimodal ] . the reason may be that larger k allows the discriminative power of the pre - trained representations to be transferred to the downstream task , while smaller k introduces more parameters and thus more computation time ."
    },
    {
        "table_id": "313",
        "table_info": {
            "table_caption": "Table 3: Effects on fine-tuning the BERT-base model (BERTBASE). ‘*’ indicates using extra fine-tuning strategies and data preprocessing. ‘/’ means no available reported result. We implemented a “BERTBASE” without any extra fine-tuning strategy as our baseline. “BERTVOTE” and “BERTAVG” means ensemble BERT (See section 3.1). “BERTSE” means self-ensemble BERT (See section 3.2). “BERTSDV” and “BERTSDA” means self-distillation BERT (See section 3.3). ‘Avg. Δ’ means the average of relative change, respectively. We bold the better self-distillation results.",
            "table_column_names": [
                "Model",
                "IMDb",
                "AG’s News",
                "DBPedia",
                "Yelp P.",
                "Yelp F.",
                "Avg. Δ",
                "SNLI",
                "MNLI (m/mm)",
                "Avg. Δ"
            ],
            "table_content_values": [
                [
                    "Model",
                    "Test Error Rate (%)",
                    "Test Error Rate (%)",
                    "Test Error Rate (%)",
                    "Test Error Rate (%)",
                    "Test Error Rate (%)",
                    "Avg. Δ",
                    "Accuracy (%)",
                    "Accuracy (%)",
                    "Avg. Δ"
                ],
                [
                    "ULMFiT ",
                    "4.60",
                    "5.01",
                    "0.80",
                    "2.16",
                    "29.98",
                    "/",
                    "/",
                    "/",
                    "/"
                ],
                [
                    "BERTBASE *",
                    "5.40",
                    "5.25",
                    "0.71",
                    "2.28",
                    "30.06",
                    "/",
                    "/",
                    "/",
                    "/"
                ],
                [
                    "BERTBASE",
                    "5.80",
                    "5.71",
                    "0.71",
                    "2.25",
                    "30.37",
                    "-",
                    "90.7",
                    "84.6/83.3",
                    "-"
                ],
                [
                    "BERTVOTE ( [ITALIC] K=4)",
                    "5.60",
                    "5.41",
                    "0.67",
                    "2.03",
                    "29.44",
                    "5.44%",
                    "91.2",
                    "85.3/84.4",
                    "5.50%"
                ],
                [
                    "BERTAVG ( [ITALIC] K=4)",
                    "5.68",
                    "5.53",
                    "0.68",
                    "2.03",
                    "30.03",
                    "4.07%",
                    "90.8",
                    "85.1/84.2",
                    "3.24%"
                ],
                [
                    "BERTSE (ours)",
                    "5.82",
                    "5.59",
                    "0.65",
                    "2.19",
                    "30.48",
                    "2.50%",
                    "90.8",
                    "84.2/83.3",
                    "-0.51%"
                ],
                [
                    "BERTSDV (ours)",
                    "5.35",
                    "5.38",
                    "[BOLD] 0.68",
                    "2.05",
                    "[BOLD] 29.88",
                    "5.65%",
                    "[BOLD] 91.2",
                    "[BOLD] 85.3/84.3",
                    "[BOLD] 5.30%"
                ],
                [
                    "BERTSDA (ours)",
                    "[BOLD] 5.29",
                    "[BOLD] 5.29",
                    "[BOLD] 0.68",
                    "[BOLD] 2.04",
                    "[BOLD] 29.88",
                    "[BOLD] 6.26%",
                    "91.2",
                    "85.0/84.3",
                    "4.65%"
                ]
            ]
        },
        "gold_description": "table 3 shows the results of fine - tuning the bert - base model on five text classification datasets and two nli datasets . for ensemble bert , both the voted bert ( bertvote ) and averaged bert ( bertavg ) outperform the single bert ( bertbase ) . the average improvement of bertvote is 5 . 44 % ( for text clas sification ) and 5 . 50 % ( for nli ) , while bertavg follows closely with 4 . 07 % and 3 . 24 % . bertvote outperforms bertavg on all tasks , which adheres to our intuition since bertvote is more complicated . the self - ensemble bert ( bertse ) has a slight improvement in classification tasks of 2 . 50 % , but it does not work on nli tasks . this is also a reason why we need self - distillation to improve the base models . overall , self - distillation model has significant improvement on both classification and nli tasks . table 3 shows that bertsda and bertsdv outperform bertbase on all datasets . generally speaking , bertsda performs better than bertsdv on text classification tasks with the improvement of 6 . 26 % vs . 5 . 65 % , but the latter performs better on nli tasks ( bertsda vs . bertsdv is 4 . 65 % vs . 5 . 30 % ) . our proposed fine - tuning strategies also outperform the previous method in [ sun et al . , 2019 ] on text classification tasks , which makes extensive efforts to find sophisticated hyperparameters .",
        "generated_description": "from the table , we can see that : [ leftmargin = * ] bertvote outperforms bertbase and ulmfit by a large margin on all the test datasets , which verifies the effectiveness of the proposed model . compared with bert , our model achieves the best performance on ag ’ s news , dbpedia , yelp p . edia , and yelp f . reviewing the results , we find that our model outperforms the original bert by a significant margin , i . e . , outperforms it by a margin of 1 . 3 % ( p < 0 . 01 ) and 0 . 5 % ( 0 . 2 % ) on the test error rate . this verifies that the pre - trained bert has a strong ability of pre - training a large - scale language model on a wide range of tasks , such as nlp tasks , sentiment analysis , and multi - label text classification , and our model is able to adapt well to such tasks without any loss in performance . in addition , compared with the previous state - of - the - art bert models , our proposed model achieves an improvement of 0 . 6 % on the yelp f test set , which further verifies its effectiveness ."
    },
    {
        "table_id": "314",
        "table_info": {
            "table_caption": "Table 4: Effects on fine-tuning the BERT-large model (BERT-L). For IMDb and AG’s News, we report test error rate (%). For SNLI, we report accuracy (%). MT-DNN fine-tunes BERT with multi-task learning.",
            "table_column_names": [
                "Model",
                "IMDb",
                "AG’s",
                "Avg. Δ",
                "SNLI",
                "Δ"
            ],
            "table_content_values": [
                [
                    "Model",
                    "IMDb",
                    "News",
                    "Avg. Δ",
                    "SNLI",
                    "Δ"
                ],
                [
                    "MT-DNN ",
                    "/",
                    "/",
                    "/",
                    "91.6",
                    "/"
                ],
                [
                    "BERT-L",
                    "4.98",
                    "5.45",
                    "-",
                    "90.9",
                    "-"
                ],
                [
                    "(our implementation)",
                    "4.98",
                    "5.45",
                    "-",
                    "90.9",
                    "-"
                ],
                [
                    "BERT-LSDA( [ITALIC] K=1)",
                    "4.66",
                    "5.21",
                    "5.62%",
                    "[BOLD] 91.5",
                    "[BOLD] 6.59%"
                ],
                [
                    "BERT-LSDA( [ITALIC] K= [ITALIC] T−1)",
                    "[BOLD] 4.58",
                    "[BOLD] 5.15",
                    "[BOLD] 7.02%",
                    "91.4",
                    "5.49%"
                ]
            ]
        },
        "gold_description": "we also investigate whether self - distillation has similar findings for the bertlarge model ( bert - l ) , which contains 24 transformer layers . due to the limitation of our devices , we only conduct an experiment on two text classification datasets and one nli datasets and evaluate strategy bertsda , namely self - distillation with averaged bert as a teacher . we set two different teacher sizes for comparison . as shown in table 4 , self - distillation also gets a significant gain while fine - tuning the bert - large model . on two text classification tasks , bert - lsda ( k = − 1 ) gives better results and the average improvement is 7 . 02 % . for nli task , bert - lsda ( k = 1 ) gives better result and the improvement is 6 . 59 % .",
        "generated_description": "we report test error rate and accuracy on imdb and ag ’ s news for fine - tuning bert - large ( bert - l ) . for snli , we report accuracy of 5 . 49 % , 6 . 59 % , 7 . 02 % and 7 . 1 % , respectively . we can see that our method ( berert - lsda ( k = t − 1 ) consistently outperforms mt - dnn with a large margin on all three datasets . we attribute this to the fact that multi - task learning allows the model to benefit from knowledge obtained from other tasks while the model is pre - trained only on the supervised tasks ."
    },
    {
        "table_id": "315",
        "table_info": {
            "table_caption": "Table 2: Automatic evaluation with perplexity.",
            "table_column_names": [
                "Model",
                "Overall",
                "High",
                "Medium",
                "Low",
                "OOV"
            ],
            "table_content_values": [
                [
                    "Seq2Seq",
                    "47.02",
                    "42.41",
                    "47.25",
                    "48.61",
                    "49.96"
                ],
                [
                    "MemNet",
                    "46.85",
                    "41.93",
                    "47.32",
                    "48.86",
                    "49.52"
                ],
                [
                    "CopyNet",
                    "40.27",
                    "36.26",
                    "40.99",
                    "42.09",
                    "42.24"
                ],
                [
                    "CCM",
                    "39.18",
                    "35.36",
                    "39.64",
                    "40.67",
                    "40.87"
                ],
                [
                    "PostKS",
                    "43.56",
                    "40.65",
                    "44.06",
                    "46.36",
                    "49.32"
                ],
                [
                    "TransDG",
                    "[BOLD] 37.53",
                    "[BOLD] 32.18",
                    "[BOLD] 36.12",
                    "[BOLD] 38.46",
                    "[BOLD] 40.75"
                ]
            ]
        },
        "gold_description": "as shown in table 2 , transdg achieves the lowest perplexity on all the datasets , indicating that the generated responses are more grammatical .",
        "generated_description": "transdg achieves the best performance in all the three categories , and outperforms the other models in most of the perplexity values . it is worth noting that embedding - based models , such as ccm and postks , perform better than seq2seq , memnet , copynet , and memnet in the high , medium , low , and oov categories , but they do not perform well in the oov category . we think that this is because memnet and copynet are better at copying the words from the source sentence , which are more likely to appear in the target text than the words copied from the original text . in addition , the performance of postks and ccm is poor in the low and high categories , because the source sentences in these three categories are usually long and the target texts in these four categories contain a lot of words that are not seen in the training set , which also explain the poor performance of these two models in these two categories . on the other hand , transddg outperforms all the baseline models , especially in the low and high perplexity cases , which shows that transdgv is better at generating high - quality and low - quality words ."
    },
    {
        "table_id": "316",
        "table_info": {
            "table_caption": "Table 3: Automatic evaluation with entity score.",
            "table_column_names": [
                "Model",
                "Overall",
                "High",
                "Medium",
                "Low",
                "OOV"
            ],
            "table_content_values": [
                [
                    "Seq2Seq",
                    "0.717",
                    "0.713",
                    "0.740",
                    "0.721",
                    "0.669"
                ],
                [
                    "MemNet",
                    "0.761",
                    "0.764",
                    "0.788",
                    "0.760",
                    "0.706"
                ],
                [
                    "CopyNet",
                    "0.960",
                    "0.910",
                    "0.970",
                    "0.960",
                    "0.960"
                ],
                [
                    "CCM",
                    "1.180",
                    "1.156",
                    "1.191",
                    "1.196",
                    "1.162"
                ],
                [
                    "PostKS",
                    "1.041",
                    "1.007",
                    "1.028",
                    "0.993",
                    "0.978"
                ],
                [
                    "TransDG",
                    "[BOLD] 1.207",
                    "[BOLD] 1.195",
                    "[BOLD] 1.204",
                    "[BOLD] 1.232",
                    "[BOLD] 1.182"
                ]
            ]
        },
        "gold_description": "table 3 demonstrates that the models leveraging external knowledge achieve better performance than the standard seq2seq model in generating meaningful entity words and diverse responses . in particular , our model outperforms all the baselines significantly with highest entity score .",
        "generated_description": "the evaluation results show that transdg achieves the best performance in most of the categories and outperforms all the baseline models in all the settings . it is worth noting that the performance of other baseline models is worse than that of our proposed model , which indicates that there is still a lot of room for improvement in the quality of entity embeddings . in addition , we can see that the overall score of seq2seq is the worst among the baselines , which shows that the simple sequential encoder - decoder architecture is not good enough to capture the semantic information of multi - word entities . memnet and copynet are the best performing models in the “ low ” and “ oov ” categories , because they can capture the information of rare words and named entities . ccm and postks are the worst ones in “ high ” category , since they only capture information of named entities and cannot capture the semantics of the whole text ."
    },
    {
        "table_id": "317",
        "table_info": {
            "table_caption": "Table 4: Automatic evaluation with BLEU.",
            "table_column_names": [
                "Model",
                "BLEU-1",
                "BLEU-2",
                "BLEU-3",
                "BLEU-4"
            ],
            "table_content_values": [
                [
                    "Seq2Seq",
                    "0.0977",
                    "0.0098",
                    "0.0012",
                    "0.0002"
                ],
                [
                    "MemNet",
                    "0.1652",
                    "0.0174",
                    "0.0028",
                    "0.0004"
                ],
                [
                    "CopyNet",
                    "0.1715",
                    "[BOLD] 0.0181",
                    "0.0029",
                    "0.0005"
                ],
                [
                    "CCM",
                    "0.1625",
                    "0.0175",
                    "0.0030",
                    "0.0005"
                ],
                [
                    "PostKS",
                    "0.1683",
                    "0.0165",
                    "0.0029",
                    "0.0004"
                ],
                [
                    "TransDG",
                    "[BOLD] 0.1807",
                    "0.0178",
                    "[BOLD] 0.0031",
                    "[BOLD] 0.0006"
                ]
            ]
        },
        "gold_description": "the bleu values shown in table 4 demonstrates the comparison results from word - level overlaps . transdg tends to generate responses that are more similar to the gold responses than baselines in most cases .",
        "generated_description": "automatic evaluation we use bleu score because it is the most commonly used evaluation metric in automatic evaluation of summarization models . transdg significantly outperforms other models with large margins on all metrics , which indicates that our model generates more informative and grammatical summaries . besides , the performance of ccm and postks is slightly worse than that of copynet . we assume that this is because the pre - training data of these two models does not provide enough training data to improve their performance ."
    },
    {
        "table_id": "318",
        "table_info": {
            "table_caption": "Table 5: Human evaluation result.",
            "table_column_names": [
                "Model",
                "Fluency",
                "Relevance",
                "Correctness"
            ],
            "table_content_values": [
                [
                    "Seq2Seq",
                    "1.67",
                    "0.68",
                    "0.80"
                ],
                [
                    "MemNet",
                    "1.83",
                    "0.89",
                    "1.32"
                ],
                [
                    "CopyNet",
                    "2.36",
                    "1.13",
                    "1.08"
                ],
                [
                    "CCM",
                    "2.27",
                    "1.35",
                    "1.22"
                ],
                [
                    "PostKS",
                    "2.32",
                    "1.36",
                    "1.31"
                ],
                [
                    "TransDG",
                    "[BOLD] 2.41",
                    "[BOLD] 1.52",
                    "[BOLD] 1.34"
                ]
            ]
        },
        "gold_description": "the human evaluation results are reported in table 5 . as shown in table 5 , transdg tends to generate more appropriate and informative responses in terms of human annotation . specifically , the responses generated by transdg have higher knowledge relevance than other models , indicating that transdg is effective to incorporate appropriate commonsense knowledge .",
        "generated_description": "human evaluation we conduct human evaluation on the cnn / daily mail test set . we randomly select 200 samples from the test set and each sample consists of an input message and a response generated by different models . following zhou et al . the three aspects are : ( 1 ) fluency , ( 2 ) relevance , ( 3 ) correctness . from the table , we can see that transdg outperforms all the baselines significantly in terms of all three aspects , which indicates that our model is able to generate more fluent and grammatical responses . besides , our model also achieves the highest score in relevance and correctness , which means that our responses are more relevant to the source message and less grammatical ."
    },
    {
        "table_id": "319",
        "table_info": {
            "table_caption": "Table 7: Ablation results of TransDG on the test set. Here, Entity represents entity score.",
            "table_column_names": [
                "Model",
                "Perplexity",
                "Entity",
                "BLEU-1",
                "BLEU-2"
            ],
            "table_content_values": [
                [
                    "TransDG",
                    "37.53",
                    "1.207",
                    "0.1807",
                    "0.0178"
                ],
                [
                    "w/o QRT",
                    "42.17",
                    "1.076",
                    "0.1604",
                    "0.0171"
                ],
                [
                    "w/o KST",
                    "43.05",
                    "0.774",
                    "0.1643",
                    "0.0158"
                ],
                [
                    "w/o QRT+KST",
                    "44.15",
                    "0.772",
                    "0.1612",
                    "0.0170"
                ],
                [
                    "w/o RGA",
                    "38.62",
                    "1.106",
                    "0.1712",
                    "0.0170"
                ],
                [
                    "w/o SSD",
                    "38.18",
                    "1.114",
                    "0.1804",
                    "0.0178"
                ]
            ]
        },
        "gold_description": "the ablation test results are reported in table 7 . from the results , we can observe that the performance of transdg drops sharply when we discard the question representation module and the knowledge selection module transferred from kbqa . response guiding attention also has noticeable impact on the performance of transdg , especially on bleu scores . in addition , the second - step decoder can improve the ability of transdg to generate relevant entities per response .",
        "generated_description": "s4ss3sss0px3 component ablation to show the effectiveness of different components of transdg , we conduct an ablation study on the test set . we first remove the knowledge - to - text transformer ( qrt ) model , and then examine the performance of entity generation . we can see that the entity score drops from 37 . 53 to 37 . 17 , which indicates that knowledge is indeed important for the persona generation . besides , the performance also drops from 44 . 15 to 42 . 17 when removing qrt and kst , which means that both knowledge and knowledge are necessary to generate the persona responses . next , we remove the query rewriting module ( rga ) , and the performance drops to 38 . 62 and 38 . 18 , indicating that rga is also necessary to obtain the correct knowledge representation . finally , we replace the memory - augmented transformer with the plain transformer model ( transdg w / o qrt + kst ) , which also means that knowledge generation module is also important . the bleu score drops to 1 . 106 and 1 . 114 , which shows that the model is able to utilize the knowledge representation to improve the generation quality ."
    },
    {
        "table_id": "320",
        "table_info": {
            "table_caption": "Table 3: Test accuracy for all models on OVERNIGHT dataset, which has eight domains: Basketball, Blocks, Calendar, Housing, Publications, Recipes, Restaurants, and Social. We use the generator-reranker (G-R) architecture with different options. Beam-n: Beam search is applied with size n, pQ: The critic is pre-trained over the Quora dataset, TH1: rerank if there is at least one score above 0.5, TH2: rerank if best score−second best score>0.001. The candidate logical forms are processed with templated expansions method (Section 3.2.3) in this experiment.",
            "table_column_names": [
                "[BOLD] Method",
                "Bas.",
                "Blo.",
                "Cal.",
                "Hou.",
                "Pub.",
                "Rec.",
                "Res.",
                "Soc.",
                "[BOLD] Avg."
            ],
            "table_content_values": [
                [
                    "[BOLD] Previous Methods",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Han ( 2018 )",
                    "88.2",
                    "61.4",
                    "81.5",
                    "74.1",
                    "80.7",
                    "82.9",
                    "80.7",
                    "82.1",
                    "79.0"
                ],
                [
                    "Su and Yan ( 2017 )",
                    "88.2",
                    "62.2",
                    "82.1",
                    "78.8",
                    "80.1",
                    "86.1",
                    "83.7",
                    "83.1",
                    "80.6"
                ],
                [
                    "Herzig and Berant ( 2017 )",
                    "86.2",
                    "62.7",
                    "82.1",
                    "78.3",
                    "80.7",
                    "82.9",
                    "82.2",
                    "81.7",
                    "79.6"
                ],
                [
                    "[BOLD] Our Methods",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Shaw et al. ( 2019 )",
                    "[BOLD] 89.3",
                    "63.7",
                    "81.5",
                    "82.0",
                    "80.7",
                    "85.6",
                    "89.5",
                    "84.8",
                    "82.1"
                ],
                [
                    "G-R (Beam-10)",
                    "88.7",
                    "66.4",
                    "83.3",
                    "82.5",
                    "78.9",
                    "86.6",
                    "89.8",
                    "83.7",
                    "82.5"
                ],
                [
                    "G-R (Beam-10 & pQ)",
                    "89.0",
                    "65.2",
                    "83.3",
                    "83.6",
                    "78.3",
                    "87.5",
                    "89.5",
                    "85.5",
                    "82.7"
                ],
                [
                    "G-R (Beam-25)",
                    "89.0",
                    "[BOLD] 67.7",
                    "83.3",
                    "[BOLD] 84.1",
                    "[BOLD] 82.6",
                    "87.5",
                    "89.4",
                    "83.9",
                    "83.4"
                ],
                [
                    "G-R (Beam-25 & pQ)",
                    "[BOLD] 89.3",
                    "66.7",
                    "84.5",
                    "83.6",
                    "80.1",
                    "[BOLD] 88.0",
                    "[BOLD] 91.0",
                    "85.2",
                    "83.5"
                ],
                [
                    "G-R (Beam-25 & pQ & TH1)",
                    "89.0",
                    "65.7",
                    "[BOLD] 85.1",
                    "83.6",
                    "81.4",
                    "[BOLD] 88.0",
                    "[BOLD] 91.0",
                    "[BOLD] 86.0",
                    "[BOLD] 83.7"
                ],
                [
                    "G-R (Beam-25 & pQ & TH2)",
                    "88.7",
                    "66.4",
                    "82.7",
                    "83.1",
                    "82.0",
                    "87.0",
                    "89.8",
                    "85.8",
                    "83.2"
                ]
            ]
        },
        "gold_description": "we compare our model with the state - of - the - art models in table 3 . as table 3 shows , this model alone ( without any reranking ) improves the state - of - the - art performance from 79 . 6 % to 82 . 15 % accuracy increasing the beam size improves the performance as expected . using a pre - trained model improves the performance as well . reranking with a threshold rule may be helpful for the overall architecture . we observe that reranking by the critic at all times may not be the best approach . we note that choosing not to rerank when all scores are below 0 . 5 increases the performance further . on the other hand , reranking if the difference between the best score and second best score is above the threshold we set does not help in this case . the overall architecture improves the performance of the generator ( 82 . 1 % accuracy ) to 83 . 7 % accuracy .",
        "generated_description": "from the table , we can observe that : ( 1 ) our full model achieves the best average score across all the tasks , which verifies the effectiveness and generalizability of our proposed model . ( 2 ) compared with previous methods , our full model outperforms all the baselines by a large margin , which indicates that our model is able to capture the rich features of each task . ( 3 ) comparing with the best previous models , our final model significantly outperforms them on all the four major classes , which shows the advantage of our model in capturing richer features ."
    },
    {
        "table_id": "321",
        "table_info": {
            "table_caption": "Table 1: The performances of the IM2LATEX-100K Bi-LSTM model. We discover that the look-ahead improves the model from the greedy search method — noted that LA is more directly comparable to the greedy search because of their same beam size. We also show the scores of the beam search for the reference",
            "table_column_names": [
                "Search Strategy",
                "BLEU"
            ],
            "table_content_values": [
                [
                    "Greedy Search",
                    "86.24"
                ],
                [
                    "2-LA",
                    "86.65"
                ],
                [
                    "3-LA",
                    "86.71"
                ],
                [
                    "4-LA",
                    "86.77"
                ],
                [
                    "5-LA",
                    "[BOLD] 86.79"
                ],
                [
                    "Beam Search (B=10)",
                    "86.28"
                ]
            ]
        },
        "gold_description": "table 1 : the performances of the im2latex - 100k bi - lstm model . we discover that the look - ahead improves the model from the greedy search method — noted that la is more directly comparable to the greedy search because of their same beam size . we also show the scores of the beam search for the reference",
        "generated_description": "in this section , we compare the results of the three different look - ahead strategies with the greedy search and the beam search on the im2latex - 100k bi - lstm model . we observe that the first two strategies ( 2 - la and 3 - la ) are more directly comparable to the search because of their same beam size . however , the use of la leads to a slight drop in performance . on the other hand , the third and fourth strategies ( la = 1 , 2 , 3 , 4 , 5 ) improve the results compared to the results obtained using greedy search . finally , the results show that the best results are obtained when using la = 5 and beam search ( b = 10 ) . note that the result obtained using beam search is better than the results achieved with greedy search by using the best score ."
    },
    {
        "table_id": "322",
        "table_info": {
            "table_caption": "Table 3: Average number of words per question and answer, and average longest n-gram overlap between passage and question.",
            "table_column_names": [
                "[EMPTY]",
                "DSQuAD",
                "DBiDAF",
                "DBERT",
                "DRoBERTa"
            ],
            "table_content_values": [
                [
                    "Question length",
                    "10.3",
                    "9.8",
                    "9.8",
                    "10.0"
                ],
                [
                    "Answer length",
                    "2.6",
                    "2.9",
                    "3.0",
                    "3.2"
                ],
                [
                    "N-gram overlap",
                    "3.0",
                    "2.2",
                    "2.1",
                    "2.0"
                ]
            ]
        },
        "gold_description": "the average number of words in questions and answers , as well as the average longest n - gram overlap between passage and question are furthermore given in table 3 . we can again observe two clear trends : from weaker towards stronger models used in the annotation loop , the average length of answers increases , and the largest n - gram overlap drops from 3 to 2 tokens .",
        "generated_description": "the average number of words per question and answer for dsquad , dbidaf , dbert , and droberta is 10 . 3 , 9 . 8 , and 10 . 0 , respectively . the answer length of questions and answers are also relatively short , ranging from 2 . 6 to 3 . 2 words . the average n - gram overlap between passage and question is 3 . 0 for dbertt , 2 . 2 for dblt and 2 . 1 for droroberta . for answer length , answer length is also very short for all models . for example , in the average length of the answer for dbmt model , the answer is 2 . 9 , which is shorter than that of the dbidaf model ."
    },
    {
        "table_id": "323",
        "table_info": {
            "table_caption": "Table 2: The performances of the LSTM model trained on the WMT16 multimodal translation dataset with different LA steps. We show the look-ahead module is able to improve the model on the entire testing set. However, either the LA module or the beam search method harm the models when the length of the target sentences is longer than 25 words.",
            "table_column_names": [
                "Search Strategy",
                "BLEU",
                "BLEU (Target len≥25)"
            ],
            "table_content_values": [
                [
                    "Greedy Search",
                    "31.67",
                    "[BOLD] 23.86"
                ],
                [
                    "2-LA",
                    "32.07",
                    "21.50"
                ],
                [
                    "3-LA",
                    "32.20",
                    "22.78"
                ],
                [
                    "4-LA",
                    "[BOLD] 32.42",
                    "22.45"
                ],
                [
                    "5-LA",
                    "32.41",
                    "23.30"
                ],
                [
                    "Beam Search (B=10)",
                    "33.83",
                    "22.45"
                ]
            ]
        },
        "gold_description": "table 2 : the performances of the lstm model trained on the wmt16 multimodal translation dataset with different la steps . we show the look - ahead module is able to improve the model on the entire testing set . however , either the la module or the beam search method harm the models when the length of the target sentences is longer than 25 words .",
        "generated_description": "however , either the la module or the beam search method harm the performance when the length of the target sentences is longer than 25 words . we show that the look - ahead module is able to do the translation better than the other two search strategies . we also observe that the difference between 2 - la and 4 - la is not significant . however , we find that the greedy search strategy harms the performance of the lstm model when the target length is short or the source length is too long . for example , the bleu score drops from 31 . 67 to 32 . 20 when the source and target length are both short ( 1 - la ) and ( 2 - la ) , respectively . on the other hand , the results show that with the la - based search strategy , we can achieve the best performance ( 4 - la ) . in addition , we also find that using beam search ( b = 10 ) hurts the performance . we believe that the reason is that the search space of beam search is very small when compared to greedy search , and thus it consumes a lot of time and memory ."
    },
    {
        "table_id": "324",
        "table_info": {
            "table_caption": "Table 3: We show the results of applying LA module to the transformer model trained on the WMT14 dataset. We find that the LA module slightly improves the original model but harms the performance when the LA time step is 5. We suggest one of the reasons of these results are caused by the EOS problem.",
            "table_column_names": [
                "Search Strategy",
                "BLEU"
            ],
            "table_content_values": [
                [
                    "Greedy Search",
                    "27.50"
                ],
                [
                    "2-LA",
                    "[BOLD] 27.71"
                ],
                [
                    "3-LA",
                    "27.62"
                ],
                [
                    "4-LA",
                    "27.56"
                ],
                [
                    "5-LA",
                    "27.35"
                ],
                [
                    "Beam Search (B=10)",
                    "28.21"
                ]
            ]
        },
        "gold_description": "table 3 : we show the results of applying la module to the transformer model trained on the wmt14 dataset . we find that the la module slightly improves the original model but harms the performance when the la time step is 5 . we suggest one of the reasons of these results are caused by the eos problem .",
        "generated_description": "in this section , we show the results of applying the la module to the transformer model on the wmt14 dataset . we find that the addition of la module slightly improves the performance of the original search strategy but harms the training time step . we suggest one of the reasons of these results are caused by the eos problem . when the la time step is 5 , the model achieves the best bleu score of 27 . 5 , which is slightly better than the greedy search but still worse than the beam search strategy ."
    },
    {
        "table_id": "325",
        "table_info": {
            "table_caption": "Table 4: We show the results of integrating auxiliary EOS loss into the training state. γ is the weight of the auxiliary EOS loss. We find the EOS loss not only boosts the performance of the model when using the greedy search, the model is more robust to the larger Look-ahead steps with reasonable weights of auxiliary EOS loss.",
            "table_column_names": [
                "Search Strategy",
                "[ITALIC] γ 0.0",
                "[ITALIC] γ 0.25",
                "[ITALIC] γ 0.50",
                "[ITALIC] γ 0.75",
                "[ITALIC] γ 1.0",
                "[ITALIC] γ 1.25"
            ],
            "table_content_values": [
                [
                    "Greedy",
                    "27.50",
                    "27.81",
                    "27.74",
                    "27.75",
                    "[BOLD] 27.90",
                    "27.71"
                ],
                [
                    "2-LA",
                    "27.71",
                    "28.05",
                    "27.95",
                    "27.99",
                    "[BOLD] 28.20",
                    "27.85"
                ],
                [
                    "3-LA",
                    "27.89",
                    "27.82",
                    "27.87",
                    "27.82",
                    "[BOLD] 28.10",
                    "27.68"
                ],
                [
                    "4-LA",
                    "27.56",
                    "27.81",
                    "[BOLD] 27.87",
                    "27.74",
                    "27.84",
                    "27.68"
                ],
                [
                    "5-LA",
                    "27.35",
                    "27.71",
                    "27.74",
                    "27.63",
                    "[BOLD] 27.87",
                    "27.55"
                ]
            ]
        },
        "gold_description": "table 4 : we show the results of integrating auxiliary eos loss into the training state . γ is the weight of the auxiliary eos loss . we find the eos loss not only boosts the performance of the model when using the greedy search , the model is more robust to the larger look - ahead steps with reasonable weights of auxiliary eos loss .",
        "generated_description": "in order to integrate auxiliary eos loss into the training state , we need to balance the weight of the auxiliary loss with the greedy search . we find that the eos losses not only boosts the performance of the model when using greedy search , but also makes the model more robust to the hyperparameter search . for example , the model trained with γ = 0 . 25 achieves the best wer 27 . 71 % compared to 27 . 50 % obtained by the model with greedy search and 27 . 55 % relative wer reduction compared to the corresponding model trained without auxiliary loss . this shows that the proposed auxiliary loss not only improves the training performance but also the model is more robust ."
    },
    {
        "table_id": "326",
        "table_info": {
            "table_caption": "Table 1: Translation quality evaluation (BLEU scores).",
            "table_column_names": [
                "Model",
                "WMT’14 EN-DE",
                "WMT’14 EN-FR",
                "IWSLT’14 DE-EN"
            ],
            "table_content_values": [
                [
                    "vaswani2017transformer",
                    "28.4",
                    "41.0",
                    "34.4"
                ],
                [
                    "ahmed2018weighted",
                    "28.9",
                    "41.4",
                    "-"
                ],
                [
                    "chen2018combining",
                    "28.5",
                    "41.0",
                    "-"
                ],
                [
                    "shaw2018relative",
                    "29.2",
                    "41.5",
                    "-"
                ],
                [
                    "ott2018scaling",
                    "29.3",
                    "43.2",
                    "-"
                ],
                [
                    "wu2018dynconv",
                    "[BOLD] 29.7",
                    "43.2",
                    "35.2"
                ],
                [
                    "he2018layerwise",
                    "29.0",
                    "-",
                    "35.1"
                ],
                [
                    "Joint Self-attention",
                    "[BOLD] 29.7",
                    "43.2",
                    "35.3"
                ],
                [
                    "Local Joint Self-attention",
                    "[BOLD] 29.7",
                    "[BOLD] 43.3",
                    "[BOLD] 35.7"
                ]
            ]
        },
        "gold_description": "the entry joint self - attention corresponds to the results of our implementation of ( he et al . , 2018 ) , that significantly improves the original results by 0 . 7 bleu point on the wmt14 de - en benchmark , and 0 . 2 on iwslt . the same architecture with the proposed locality constraints ( local joint self - attention ) establishes a new state of the art in iwslt ' 14 de - en with 35 . 7 bleu , surpassing all previous published results by at least in 0 . 5 bleu , and our results with the unconstrained version by 0 . 4 . the table 1 presents a comparison of the translation quality measured via bleu score between the currently dominant transformer ( vaswani et al . , 2017 ) and dynamic convolutions ( wu et al . , 2019 ) models , as well as the work by he et al . ( 2018 ) , which also proposes a joint encoderdecoder structure , and also other refinements over the transformer architecture like ( ahmed et al . , 2017 ) , ( chen et al . , 2018 ) , ( shaw et al . , 2018 ) and ( ott et al . , 2018 ) . the joint self - attention model obtains the same sota bleu score of ( wu et al . , 2019 ) on wmt ' 14 en - de , and the same sota score of ( ott et al . , 2018 ) and ( wu et al . , 2019 ) on wmt ' 14 enfr . the local attention constraints do not provide a significant gain on these bigger models , but it improves the bleu score on wmt ' 14 en - fr to a new sota of 43 . 3 .",
        "generated_description": "on the wmt ’ 14 en - de and en - fr datasets , our proposed local joint self - attention model achieves 29 . 7 and 43 . 3 bleu score respectively , which outperforms the previous state - of - the - art models by 0 . 8 and 0 . 7 bleus on the two datasets respectively . on the iwli - 14 de - en dataset , our model also outperforms all the previous models and achieves the state of the art score of 35 . 7 , which also surpasses the performance of the previous best model ott2018scaling by 1 . 0 and 1 . 2 points on these two datasets ."
    },
    {
        "table_id": "327",
        "table_info": {
            "table_caption": "TABLE I: Results of text-line extraction on the DIVA-HisDB dataset (see Section III-A measured with the competition tool(see Section IV-A. Our proposed method outperforms state-of-the-art results by reducing the error by 80.7% and achieving nearly perfect results. Methods with * notation use semantic segmentation at pixel-level as pre-processing step.",
            "table_column_names": [
                "Method",
                "Line iu %",
                "Pixel iu %"
            ],
            "table_content_values": [
                [
                    "wavelength [seuret2017wavelength]",
                    "68.58",
                    "79.13"
                ],
                [
                    "Brigham Young University [simistira2017icdar2017]",
                    "81.50",
                    "83.07"
                ],
                [
                    "CITlab Argus LineDetect [gruuening2017robust]",
                    "96.99",
                    "93.01"
                ],
                [
                    "wavelength* (tight polygons) [simistira2017icdar2017]",
                    "97.86",
                    "97.05"
                ],
                [
                    "proposed method*",
                    "[BOLD] 99.42",
                    "96.11"
                ]
            ]
        },
        "gold_description": "in table there are the results on the diva - hisdb dataset ( see section iii - a ) . our method achieves nearly perfect results ( 99 . 42 % ) and outperforms state - of - the - art ( 97 . 86 % ) resulting in a error reduction of 80 . 7 % . note that the lower end of the heatmap scale compares favourably with state - ofthe - art ( see table i ) meaning that regardless of the choice of parameters , our method produces excellent results .",
        "generated_description": "the proposed method outperforms state - of - the - art results by 80 . 7 % in terms of pixel - wise error reduction and achieves nearly perfect accuracy . the pixel - level results of the proposed method are better than those of citlab argus linedetect [ gruuening2017robust ] and the tight polygons method [ simistira2017icdar2017 ] but worse than the method described in the paper on the diva - hisdb dataset ."
    },
    {
        "table_id": "328",
        "table_info": {
            "table_caption": "TABLE II: Results of the experiments shown in Table I with the difference that every method listed has received the ground truth of the semantic segmentation at pixel-level as input. Our proposed text-line extraction method is superior to state-of-the-art even if both methods run on the same perfect input. Moreover, in our experience, an algorithm which is not designed to take advantage of this pre-processing step will not benefit from it.",
            "table_column_names": [
                "Method from gt",
                "Line iu %",
                "Pixel iu %"
            ],
            "table_content_values": [
                [
                    "wavelength [seuret2017wavelength]",
                    "66.44",
                    "81.52"
                ],
                [
                    "wavelength (tight polygons) [simistira2017icdar2017]",
                    "99.25",
                    "98.95"
                ],
                [
                    "proposed method",
                    "[BOLD] 100.0",
                    "97.22"
                ]
            ]
        },
        "gold_description": "the answer is in table ii where we performed the same task , but this time we swapped our semantic segmentation network with the pixel - level ground - truth provided along with the data . this represents the upper - bound performances , as no tool will produce a better segmentation than the ground - truth . in this scenario our method performed at 100 % line iu , reinforcing our previous observation that our text - line extraction method has made the mistakes only in the presence of wrong results from the semantic segmentation step .",
        "generated_description": "the results show that our text - line extraction method outperforms all the other methods listed with the difference that every method listed has received the ground truth semantic segmentation at pixel - level as input . the proposed method achieves a pixel - wise accuracy of 100 . 0 % and a line - level accuracy of 97 . 22 % , which outperforms the previous state - of - the - art by a large margin and is a new state of the art in terms of both accuracy and efficiency . the results also show that the proposed method is capable of extracting text lines from the text data with a precision higher than 99 . 25 % ."
    },
    {
        "table_id": "329",
        "table_info": {
            "table_caption": "Table 1: Word Embedding Comparison Across Vision Language Tasks. (a) contains the results of learning an embedding from scratch random initialization with fine-tuning during training. The remaining sections compare (b) Word2Vec, (c) FastText, and (d) sentence level embeddings InferSent and BERT. All experiments show three model variants: Average Embedding, Self-Attention, and LSTM, with and without fine-tuning during training. Average Embedding and Self-Attention are not used in generation tasks for Image Captioning and VQA as they are known to show worse performance; sentence level embeddings are not applicable for these tasks. See text for discussion.",
            "table_column_names": [
                "[EMPTY]",
                "Task Dataset",
                "Image-Sentence Retrieval Flickr30K ",
                "Image-Sentence Retrieval MSCOCO ",
                "Phrase Grounding Flickr30K",
                "Phrase Grounding ReferIt ",
                "Text-to-Clip DiDeMo ",
                "Image Captioning MSCOCO ",
                "Image Captioning MSCOCO ",
                "VQA VQA "
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "Dataset",
                    "Flickr30K ",
                    "MSCOCO ",
                    "Entities ",
                    "ReferIt ",
                    "DiDeMo ",
                    "[EMPTY]",
                    "[EMPTY]",
                    "VQA "
                ],
                [
                    "[EMPTY]",
                    "Method",
                    "Embedding Network ",
                    "Embedding Network ",
                    "CITE ",
                    "CITE ",
                    "CITE ",
                    "ARNet ",
                    "ARNet ",
                    "EtEMN "
                ],
                [
                    "[EMPTY]",
                    "Metric",
                    "Mean Recall",
                    "Mean Recall",
                    "Accuracy",
                    "Accuracy",
                    "Average",
                    "BLEU-4",
                    "CIDEr",
                    "Accuracy"
                ],
                [
                    "[BOLD] (a)",
                    "[BOLD] Training from scratch",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "Average Embedding",
                    "44.3",
                    "73.7",
                    "70.46",
                    "51.70",
                    "33.02",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[EMPTY]",
                    "Self-Attention",
                    "44.6",
                    "77.6",
                    "70.68",
                    "52.39",
                    "33.48",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[EMPTY]",
                    "LSTM",
                    "60.0",
                    "77.5",
                    "70.47",
                    "51.57",
                    "32.83",
                    "26.7",
                    "89.7",
                    "60.95"
                ],
                [
                    "[BOLD] (b)",
                    "[BOLD] Word2Vec ",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "Average Embedding",
                    "62.5",
                    "75.0",
                    "70.03",
                    "52.51",
                    "32.95",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[EMPTY]",
                    "Average Embedding + ft",
                    "71.5",
                    "78.2",
                    "70.85",
                    "53.29",
                    "32.58",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[EMPTY]",
                    "Self-Attention",
                    "63.6",
                    "75.6",
                    "70.19",
                    "52.41",
                    "33.23",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[EMPTY]",
                    "Self-Attention + ft",
                    "71.9",
                    "79.9",
                    "70.94",
                    "53.54",
                    "33.26",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[EMPTY]",
                    "LSTM",
                    "68.5",
                    "72.5",
                    "69.83",
                    "52.86",
                    "33.73",
                    "[BOLD] 28.5",
                    "92.7",
                    "61.40"
                ],
                [
                    "[EMPTY]",
                    "LSTM + ft",
                    "69.0",
                    "78.2",
                    "70.55",
                    "53.58",
                    "[BOLD] 33.94",
                    "[BOLD] 28.5",
                    "[BOLD] 94.0",
                    "61.35"
                ],
                [
                    "[BOLD] (c)",
                    "[BOLD] FastText ",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "Average Embedding",
                    "69.2",
                    "78.5",
                    "69.75",
                    "51.27",
                    "32.45",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[EMPTY]",
                    "Average Embedding + ft",
                    "73.0",
                    "[BOLD] 80.7",
                    "70.62",
                    "53.24",
                    "32.01",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[EMPTY]",
                    "Self-Attention",
                    "69.5",
                    "78.6",
                    "69.87",
                    "52.49",
                    "33.31",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[EMPTY]",
                    "Self-Attention + ft",
                    "[BOLD] 73.1",
                    "80.6",
                    "[BOLD] 71.23",
                    "53.87",
                    "33.17",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[EMPTY]",
                    "LSTM",
                    "69.1",
                    "76.9",
                    "69.76",
                    "52.21",
                    "33.06",
                    "[BOLD] 28.5",
                    "92.7",
                    "[BOLD] 61.86"
                ],
                [
                    "[EMPTY]",
                    "LSTM + ft",
                    "68.5",
                    "80.1",
                    "71.09",
                    "[BOLD] 53.95",
                    "32.51",
                    "28.3",
                    "93.2",
                    "61.66"
                ],
                [
                    "[BOLD] (d)",
                    "[BOLD] Sentence-Level",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "InferSent ",
                    "71.2",
                    "76.4",
                    "57.83",
                    "52.29",
                    "31.87",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[EMPTY]",
                    "BERT ",
                    "71.8",
                    "75.4",
                    "69.38",
                    "50.37",
                    "32.46",
                    "–",
                    "–",
                    "–"
                ]
            ]
        },
        "gold_description": "the datasets and vision - language task models are described in the appendix , but are referenced in table 1 . unsurprisingly , when comparing the first lines of table 1 ( a , b ) , we find that using word2vec rather than an embedding trained from scratch tends to improve performance . this is more important when considering a larger vocabulary as seen comparing phrase grounding experiments on didemo and referit , whose embeddings trained from scratch using their smaller vocabulary compare favorably to word2vec . word2vec only falls behind within a point or two across all tasks , and even outperforms or performs equally as well as fasttext for certain tasks ( e . g . text - to - clip , image captioning ) . table 1 also contains a comparison of language model variants across the five vision - language tasks we evaluate on . we see that fine - tuning a word embedding on a visionlanguage task can have dramatic effects on the performance of the language model ( e . g . 5 - 10 % increase to mean recall on image - sentence retrieval ) . when comparing the architecture choices from figure 3 we see that for retrieval - based tasks ( i . e . where the output is not free - form text ) the average embedding and selfattention models perform better than a simple lstm - based approach , with self - attention being best on average . the only apparent exception to this is the text - to - clip task . infersent and bert reach comparable values to the best word2vec models for image - sentence retrieval on flickr30k , performing more poorly for the mscoco dataset . for the remaining retrieval tasks , metrics are below the best performing model and embedding combination within 1 - 3 points , again noting the unusual exception of infersent on phrase grounding of flickr30k entities , which significantly drops",
        "generated_description": "for the first task , the sentence retrieval task , we compare the performance of referit with other methods on flickr30k and mscoco with respect to the number of training images . the second task involves the phrase - grounding task , where the task is to find the sentence which contains the phrase that is semantically closest to the image . we compare the transfer learning methods with other state - of - the - art methods for this task . referit is a graph encoder - decoder model trained from scratch . the third task is the text - to - clip model , which uses the same image - caption retrieval task to generate the sentence , and the fourth task is vqa , which tests the ability of the model to visualise and reason about the words in the sentence . similar to the cite and eemn tasks , we train a model from scratch for each task , and evaluate its performance on the test set . we report the average of the 4 evaluation metrics ( bleu - 4 , cider , meteor , rouge - l , and sacred ) and the bleu score as well as the percentage of the correct answer among all the test images . we observe that referit outperforms other methods across all the three evaluation metrics on both datasets ."
    },
    {
        "table_id": "330",
        "table_info": {
            "table_caption": "Table 2: Modifications of Word2Vec. (a) contains Word2Vec retrofitted results using only the WordNet (wn) lexicon from [14]. Next, (b) is our baseline embedding which includes the new Visual Genome relational graph. Visual Word2Vec results are provided in (c), and (d), (e) are Fisher vectors on top of Word2Vec. See text for discussion.",
            "table_column_names": [
                "[EMPTY]",
                "Task Dataset",
                "Image-Sentence Retrieval Flickr30K",
                "Image-Sentence Retrieval MSCOCO",
                "Phrase Grounding Flickr30K",
                "Phrase Grounding ReferIt",
                "Text-to-Clip DiDeMo",
                "Image Captioning MSCOCO",
                "Image Captioning MSCOCO",
                "VQA"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "Dataset",
                    "Flickr30K",
                    "MSCOCO",
                    "Entities",
                    "ReferIt",
                    "DiDeMo",
                    "[EMPTY]",
                    "[EMPTY]",
                    "VQA"
                ],
                [
                    "[EMPTY]",
                    "Metric",
                    "Mean Recall",
                    "Mean Recall",
                    "Accuracy",
                    "Accuracy",
                    "Average",
                    "BLEU-4",
                    "CIDEr",
                    "Accuracy"
                ],
                [
                    "[BOLD] (a)",
                    "[BOLD] Word2Vec + wn ",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "Average Embedding + ft",
                    "72.0",
                    "79.2",
                    "70.51",
                    "53.93",
                    "33.24",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[EMPTY]",
                    "Self-Attention + ft",
                    "72.4",
                    "80.0",
                    "70.70",
                    "53.81",
                    "33.65",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[EMPTY]",
                    "LSTM + ft",
                    "69.3",
                    "78.9",
                    "70.80",
                    "53.67",
                    "34.16",
                    "28.6",
                    "93.3",
                    "61.06"
                ],
                [
                    "[BOLD] (b)",
                    "[BOLD] GrOVLE",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "Average Embedding + ft",
                    "72.3",
                    "80.2",
                    "70.77",
                    "[BOLD] 53.99",
                    "33.71",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[EMPTY]",
                    "Self-Attention + ft",
                    "72.1",
                    "80.5",
                    "70.95",
                    "53.75",
                    "33.14",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[EMPTY]",
                    "LSTM + ft",
                    "69.7",
                    "78.8",
                    "70.18",
                    "[BOLD] 53.99",
                    "34.47",
                    "28.3",
                    "92.5",
                    "61.22"
                ],
                [
                    "[BOLD] (c)",
                    "[BOLD] Visual Word2Vec ",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "Average Embedding + ft",
                    "66.8",
                    "78.7",
                    "70.61",
                    "53.14",
                    "31.73",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[EMPTY]",
                    "Self-Attention + ft",
                    "68.8",
                    "79.2",
                    "[BOLD] 71.07",
                    "53.26",
                    "31.15",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[EMPTY]",
                    "LSTM + ft",
                    "66.7",
                    "74.5",
                    "70.70",
                    "53.19",
                    "32.29",
                    "[BOLD] 28.8",
                    "[BOLD] 94.0",
                    "61.15"
                ],
                [
                    "[BOLD] (d)",
                    "[BOLD] HGLMM (300-D) ",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "Average Embedding + ft",
                    "71.0",
                    "79.8",
                    "70.64",
                    "53.71",
                    "32.62",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[EMPTY]",
                    "Self-Attention + ft",
                    "71.8",
                    "80.4",
                    "70.51",
                    "53.83",
                    "33.44",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[EMPTY]",
                    "LSTM + ft",
                    "69.5",
                    "77.9",
                    "70.37",
                    "53.10",
                    "33.85",
                    "28.7",
                    "[BOLD] 94.0",
                    "[BOLD] 61.44"
                ],
                [
                    "[BOLD] (e)",
                    "[BOLD] HGLMM (6K-D) ",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "[EMPTY]",
                    "Average Embedding + ft",
                    "73.5",
                    "[BOLD] 80.9",
                    "70.83",
                    "53.36",
                    "32.66",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[EMPTY]",
                    "Self-Attention + ft",
                    "[BOLD] 75.1",
                    "80.6",
                    "71.02",
                    "53.43",
                    "33.57",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "[EMPTY]",
                    "LSTM + ft",
                    "68.0",
                    "79.4",
                    "70.38",
                    "53.89",
                    "[BOLD] 34.62",
                    "28.0",
                    "92.8",
                    "60.58"
                ]
            ]
        },
        "gold_description": "we see a small , but consistent improvement across most of the vision - language tasks using grovle as seen in table 2 ( b ) . these changes result in an embedding with comparable performance to the hglmm 6k - d features , which are reported in table 2 ( e ) . however , our word embedding tends to perform better when embeddings are the same size ( i . e . 300 - d ) . for the generation - based tasks ( i . e . captioning and vqa ) , the benefits of using adapted embeddings are less clear . this may simply be an artifact of the challenges in evaluating these tasks ( i . e . , the captions are improving in a way the metrics don ' t capture ) . visual word2vec performs comparably amongst results for generation tasks ( i . e . image captioning and vqa ) , but these tasks have little variance in results , with less than a point of difference across the adapted embeddings . the small gain provided in generation tasks by visual word2vec does not out - weight the drops in performance across other tasks such as the significant mean recall drop of 6 . 3 compared to hglmm ' s 6k - d self - attention result in line two of table 2 ( c ) and table 2 ( e ) for image - sentence retrieval of flickr30k . for comparison , grovle ' s self - attention result in table 2 ( b ) is only 3 points lower . finally , we report results using hglmm of different dimension . hglmm 300 - d features are used for a more fair comparison to other embeddings . while the hglmm 6k - d representation primarily results in the highest performance , it performs more poorly on generation tasks and also results in high variance . for example , column one in table 2 ( e ) shows a range of 7 . 1",
        "generated_description": "for the first task , the sentence retrieval task , our word2vec + wn model significantly outperforms all baselines across all metrics on both datasets . it achieves a bleu - 4 score of 79 . 2 and a cider score of 33 . 24 on the flickr30k and mscoco datasets , which outperforms the previous state - of - the - art scores by a large margin . for the second task , we compare our model with the self - attention model proposed by \\ newcitelevy - etal : 2017 : p14 - 1 ( self - attention + ft ) , which uses a bidirectional lstm encoder - decoder architecture to encode the image and the question simultaneously . our model outperforms this model by a huge margin and achieves a much higher cider score of 51 . 6 . we believe that this is due to the fact that our model is pre - trained on image - caption pairs and not on the referit or text - to - clip datasets . the third task is the phrase - grounding task , which is more challenging than the image captioning task as it requires to find the sentence that is semantically coherent with the image . we compare our models with two baselines , average embedding + ft and our model consistently outperforms these baselines on all metrics and achieves the best results on all three datasets ."
    },
    {
        "table_id": "331",
        "table_info": {
            "table_caption": "Table 4: We include results with additional models to verify trends. See text for discussion and the appendix for more.",
            "table_column_names": [
                "Task Additional Models",
                "Image-Sentence Retrieval SCAN ",
                "Image-Sentence Retrieval SCAN ",
                "Phrase Grounding QA R-CNN ",
                "Phrase Grounding QA R-CNN ",
                "Text-to-Clip TGN ",
                "Image Captioning BUTD ",
                "Image Captioning BUTD ",
                "VQA BAN"
            ],
            "table_content_values": [
                [
                    "Metric",
                    "Mean Recall",
                    "Mean Recall",
                    "Accuracy",
                    "Accuracy",
                    "Average",
                    "BLEU-4",
                    "CIDEr",
                    "Accuracy"
                ],
                [
                    "Training from scratch",
                    "72.8",
                    "83.2",
                    "68.56",
                    "50.23",
                    "43.91",
                    "35.2",
                    "109.8",
                    "68.98"
                ],
                [
                    "FastText + ft",
                    "72.5",
                    "83.8",
                    "69.27",
                    "53.01",
                    "44.21",
                    "35.2",
                    "110.3",
                    "69.91"
                ],
                [
                    "GrOVLE (w/o multi-task pretraining) + ft",
                    "72.7",
                    "84.1",
                    "70.03",
                    "53.88",
                    "[BOLD] 45.26",
                    "35.1",
                    "110.4",
                    "69.36"
                ],
                [
                    "+ multi-task pretraining w/ target task + ft",
                    "[BOLD] 76.2",
                    "[BOLD] 84.7",
                    "[BOLD] 71.08",
                    "[BOLD] 54.10",
                    "43.61",
                    "[BOLD] 35.7",
                    "[BOLD] 111.6",
                    "[BOLD] 69.97"
                ]
            ]
        },
        "gold_description": "to address this , we fine - tune grovle across the five vl tasks . we provide results for a four and five multi - task trained embedding . the four task experiments are performed with the final task embedding fixed to demonstrate how well the embeddings would generalize to new tasks . we also provide results for pretraining on five tasks with and without finetuning during the last task . this multi - task variant is the best performing across all tasks , thus we release this embedding for public use . to verify that the multi - task grovle performance improvements generalize across task model architecture , we provide results using additional task models in table 4 . table 4 provides more models per task and demonstrates consistent results : embeddings can significantly affect performance and grovle variants are still the best embedding overall . as we move down the table we find even larger performance improvements made by using the five - task pretrained grovle with fine - tuning than in table 3 .",
        "generated_description": "in this section , we evaluate the performance of the butd and r - cnn models on the image captioning and phrase - grounding tasks . for the image - sentence retrieval task , we use the same evaluation metric as \\ newcitedblp : journals / corr / abs - 1911 - 10683 , i . e . , bleu - 4 , cider and accuracy . we compare our approach with several baselines : fasttext embeddings trained from scratch on the scan training set , a pre - trained embedding obtained by fasttext using the qa training set ( cf . training from scratch results in a significant drop in performance across all metrics . multi - task pretraining with the target task as the source task further improves the performance by a large margin compared to training from scratch . however , the overall performance of our approach is still lower than the state - of - the - art butd model . we believe that the reason is that butd uses a much larger number of task - specific embedding vectors ( up to 10 , 000 ) compared to our approach ( 10 , 000 ) . this results in more parameters that need to be learned during training ."
    },
    {
        "table_id": "332",
        "table_info": {
            "table_caption": "Table 3: Comparison of training our word embeddings on four tasks and testing on the fifth, as well as training on all five tasks.",
            "table_column_names": [
                "Task Metric",
                "Image-Sentence Retrieval Mean Recall",
                "Image-Sentence Retrieval Mean Recall",
                "Phrase Grounding Accuracy",
                "Phrase Grounding Accuracy",
                "Text-to-Clip Average",
                "Image Captioning BLEU-4",
                "Image Captioning CIDEr",
                "VQA Accuracy"
            ],
            "table_content_values": [
                [
                    "GrOVLE w/o multi-task pretraining",
                    "64.7",
                    "75.0",
                    "70.53",
                    "52.15",
                    "34.45",
                    "28.5",
                    "92.7",
                    "61.46"
                ],
                [
                    "+ multi-task pretraining w/o target task",
                    "65.8",
                    "76.4",
                    "70.82",
                    "52.21",
                    "34.57",
                    "[BOLD] 28.8",
                    "[BOLD] 93.3",
                    "61.47"
                ],
                [
                    "+ multi-task pretraining w/ target task",
                    "66.2",
                    "80.2",
                    "70.87",
                    "52.64",
                    "34.82",
                    "28.5",
                    "92.7",
                    "[BOLD] 61.53"
                ],
                [
                    "+ multi-task pretraining w/ target task + ft",
                    "[BOLD] 72.6",
                    "[BOLD] 81.3",
                    "[BOLD] 71.57",
                    "[BOLD] 54.51",
                    "[BOLD] 35.09",
                    "28.7",
                    "93.2",
                    "61.46"
                ]
            ]
        },
        "gold_description": "table 3 reports results of the multi - task training procedure described above . we use the best performing language model in our comparisons for each task , i . e . self - attention for image - sentence retrieval and phrase grounding , and the lstm language model for text - to - clip , image captioning , and vqa . the first lines of table 3 report the results of the original fixed grovle embedding , which should be considered the baseline . the second line of table 3 reports performance when the four - task pretrained grovle is fixed when used in the target task , i . e . the task currently being run . the third and fourth line of table 3 report the results of our embedding when they were trained on all five tasks , and kept fixed or fine - tuned for the target task , respectively . the results of line three and four demonstrate that our improved embedding tends to transfer better when applied with fine - tuning during the target task . we find similar trends in performance improvements across tasks : larger gains occur for image - sentence retrieval with + 7 . 9 mean recall for the flickr30k dataset and + 6 . 3 for mscoco . all other tasks have performance improvements under one point , showing that while the vision - language tasks appear to transfer well without harming performance , they are leveraged most in image - sentence retrieval , with an exception of phrase grounding accuracy on referit ( + 2 . 36 % ) .",
        "generated_description": "s4ss3sss0px3 multi - task pretraining we compare the effect of multi - task training for different tasks on the image captioning and text - to - clip tasks . in the first group of experiments , we pretrain the grovle model with the tasks that directly benefit from the image - sentence retrieval task , namely , phrase grounding and text captioning , and the vqa task . in this section , we study the effects of pretraining the model with other tasks by fine - tuning it with the target task as the source task . the results show that the performance of the model is improved significantly when the target tasks are more closely related to the training tasks . for example , the mean accuracy of the phrase grounding task increases from 61 . 47 % to 61 . 53 % when the training target task is the image captioning task , which is an absolute improvement of 2 . 5 % . in addition , the cider score increases from 93 . 3 % to 93 . 2 % and the bleu - 4 score improves from 76 . 4 % to 74 . 4 % ."
    },
    {
        "table_id": "333",
        "table_info": {
            "table_caption": "Table 4: Consistency of the adversarial effect (or lack thereof) for different models in the loop when retraining the model on the same data again, but with a new random seed.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Resource",
                "[ITALIC] Seed 1  [ITALIC] EM",
                "[ITALIC] Seed 1  [ITALIC] F1",
                "[ITALIC] Seed 2  [ITALIC] EM",
                "[ITALIC] Seed 2  [ITALIC] F1"
            ],
            "table_content_values": [
                [
                    "BiDAF",
                    "DBiDAF [ITALIC] -dev",
                    "0.0",
                    "5.3",
                    "10.3",
                    "19.4"
                ],
                [
                    "BERT",
                    "DBERT [ITALIC] -dev",
                    "0.0",
                    "4.9",
                    "20.5",
                    "30.3"
                ],
                [
                    "RoBERTa",
                    "DRoBERTa [ITALIC] -dev",
                    "0.0",
                    "6.1",
                    "16.5",
                    "26.4"
                ],
                [
                    "BiDAF",
                    "DBiDAF [ITALIC] -test",
                    "0.0",
                    "5.5",
                    "12.2",
                    "21.7"
                ],
                [
                    "BERT",
                    "DBERT [ITALIC] -test",
                    "0.0",
                    "5.3",
                    "18.6",
                    "29.6"
                ],
                [
                    "RoBERTa",
                    "DRoBERTa [ITALIC] -test",
                    "0.0",
                    "5.9",
                    "16.2",
                    "27.3"
                ]
            ]
        },
        "gold_description": "our annotation pipeline is designed to reject any samples where the model correctly predicts the answer . how reproducible is this when retraining the same model with the same data ? to measure this , we evaluate the performance of two models of identical setup for each respective architecture , which differ only in their random initialisation and data order during sgd sampling . we can thus isolate how strongly the resulting dataset depends on the particular random initialisation and order of data points used to train the model . the results of this experiment are shown in table 4 .",
        "generated_description": "adversarial effect . we evaluate the robustness of the adversarial effect by keeping the model in the loop when retraining the model and evaluating its performance on the dev and test set . for instance , the model is retrained using - dev and tested on the - test set . we can see that for both bidaf and droberta , the accuracy of - dev is lower than that of - test by a large margin . for bert , the opposite is true : the accuracy is higher in - dev but lower in - test . we hypothesize that this is due to the fact that bert has seen a large number of adversarial examples during training and is thus less likely to overfit to the dev set ."
    },
    {
        "table_id": "334",
        "table_info": {
            "table_caption": "Table 1: Overall performance in terms of personality (C Score), quality (Perplexity, BLEU), diversity (Distinct-1), structure similarity of different users (Diff Score (×10−10)), model change after adaptation (Δ score (×10−10)) .",
            "table_column_names": [
                "[BOLD] Method",
                "[BOLD] Perplexity",
                "[BOLD] C Score",
                "[BOLD] BLEU",
                "[BOLD] Distinct-1",
                "[BOLD] Diff Score",
                "Δ [BOLD]  Score"
            ],
            "table_content_values": [
                [
                    "Seq2seq",
                    "37.90",
                    "-0.16",
                    "1.29",
                    "0.0018",
                    "0.00",
                    "0.00"
                ],
                [
                    "Seq2seq-F",
                    "[BOLD] 34.42",
                    "-0.13",
                    "1.47",
                    "0.0033",
                    "63.44",
                    "32.73"
                ],
                [
                    "Speaker",
                    "40.17",
                    "-0.14",
                    "1.14",
                    "0.0039",
                    "0.00",
                    "0.00"
                ],
                [
                    "Speaker-F",
                    "36.33",
                    "-0.05",
                    "1.36",
                    "0.0048",
                    "53.36",
                    "27.92"
                ],
                [
                    "PAML",
                    "37.60",
                    "0.07",
                    "1.52",
                    "0.0067",
                    "36.59",
                    "37.36"
                ],
                [
                    "ATAML",
                    "40.27",
                    "[BOLD] 0.12",
                    "0.52",
                    "0.0074",
                    "105.87",
                    "50.35"
                ],
                [
                    "CMAML−",
                    "37.41",
                    "[BOLD] 0.13",
                    "[BOLD] 1.59",
                    "0.0076",
                    "379.72",
                    "193.23"
                ],
                [
                    "CMAML",
                    "35.88",
                    "[BOLD] 0.12",
                    "[BOLD] 1.57",
                    "[BOLD] 0.0089",
                    "[BOLD] 412.77",
                    "[BOLD] 210.07"
                ]
            ]
        },
        "gold_description": "we present our results in table 1 . as shown in table 1 , our model cmaml achieves good performance on quality , personality , and diversity .",
        "generated_description": "from the table , we can see that both ataml and cmaml significantly outperform both seq2seq and speaker - f in terms of all the automatic evaluation metrics , which indicates that these two methods are better at modeling the content of the multi - speaker responses than the baseline methods . the results also show that paml is slightly better than the other two aml variants , which suggests that the two measures are complementary to each other and can act as a better inductive bias for the model to learn better representations . atamaml achieves the highest diversity score , which means that the aml models can produce more diverse and grammatical responses , and the differences between different utterances are reduced by a large margin , which shows that the mutual information between utterances can help improve the quality of the generated responses . note that the differences in diff score and c score between different models are small , so we do not observe significant differences in these metrics ."
    },
    {
        "table_id": "335",
        "table_info": {
            "table_caption": "Table 5: Training models on various datasets, each with 10,000 samples, and measuring their generalisation to different evaluation datasets. Results in bold indicate the best result per model.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Trained On",
                "[BOLD] Evaluation (Test) Dataset DSQuAD",
                "[BOLD] Evaluation (Test) Dataset DSQuAD",
                "[BOLD] Evaluation (Test) Dataset DBiDAF",
                "[BOLD] Evaluation (Test) Dataset DBiDAF",
                "[BOLD] Evaluation (Test) Dataset DBERT",
                "[BOLD] Evaluation (Test) Dataset DBERT",
                "[BOLD] Evaluation (Test) Dataset DRoBERTa",
                "[BOLD] Evaluation (Test) Dataset DRoBERTa",
                "[BOLD] Evaluation (Test) Dataset DDROP",
                "[BOLD] Evaluation (Test) Dataset DDROP",
                "[BOLD] Evaluation (Test) Dataset DNQ",
                "[BOLD] Evaluation (Test) Dataset DNQ"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "[ITALIC] EM",
                    "[ITALIC] F1",
                    "[ITALIC] EM",
                    "[ITALIC] F1",
                    "[ITALIC] EM",
                    "[ITALIC] F1",
                    "[ITALIC] EM",
                    "[ITALIC] F1",
                    "[ITALIC] EM",
                    "[ITALIC] F1",
                    "[ITALIC] EM",
                    "[ITALIC] F1"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD(10K)",
                    "[BOLD] 40.6",
                    "[BOLD] 54.6",
                    "[BOLD] 7.0",
                    "[BOLD] 15.1",
                    "5.3",
                    "12.8",
                    "5.7",
                    "13.2",
                    "4.5",
                    "9.3",
                    "[BOLD] 26.7",
                    "[BOLD] 40.6"
                ],
                [
                    "[ITALIC] BiDAF",
                    "[ITALIC] DBiDAF",
                    "12.1",
                    "22.1",
                    "5.7",
                    "12.9",
                    "6.4",
                    "13.6",
                    "6.0",
                    "13.2",
                    "6.1",
                    "12.0",
                    "14.1",
                    "26.7"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DBERT",
                    "9.9",
                    "18.8",
                    "6.4",
                    "13.3",
                    "8.5",
                    "15.6",
                    "8.8",
                    "15.7",
                    "8.3",
                    "14.5",
                    "14.9",
                    "27.5"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DRoBERTa",
                    "10.9",
                    "20.8",
                    "6.6",
                    "13.8",
                    "[BOLD] 10.1",
                    "[BOLD] 18.0",
                    "[BOLD] 9.7",
                    "[BOLD] 16.7",
                    "[BOLD] 14.8",
                    "[BOLD] 23.3",
                    "13.3",
                    "26.0"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD(10K)",
                    "[BOLD] 70.5",
                    "[BOLD] 83.6",
                    "36.4",
                    "50.3",
                    "15.0",
                    "26.5",
                    "10.6",
                    "21.2",
                    "20.0",
                    "31.3",
                    "54.9",
                    "69.5"
                ],
                [
                    "[ITALIC] BERT",
                    "[ITALIC] DBiDAF",
                    "67.9",
                    "81.6",
                    "[BOLD] 46.5",
                    "[BOLD] 62.4",
                    "[BOLD] 37.5",
                    "[BOLD] 49.0",
                    "[BOLD] 32.3",
                    "[BOLD] 44.2",
                    "[BOLD] 41.1",
                    "[BOLD] 51.5",
                    "[BOLD] 55.8",
                    "[BOLD] 71.0"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DBERT",
                    "60.9",
                    "75.2",
                    "42.2",
                    "57.8",
                    "36.4",
                    "46.6",
                    "28.3",
                    "39.6",
                    "35.7",
                    "44.4",
                    "50.7",
                    "65.4"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DRoBERTa",
                    "57.6",
                    "71.8",
                    "36.8",
                    "50.9",
                    "34.1",
                    "44.9",
                    "31.0",
                    "41.7",
                    "37.6",
                    "45.9",
                    "48.2",
                    "63.8"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD(10K)",
                    "[BOLD] 70.0",
                    "[BOLD] 83.7",
                    "39.4",
                    "55.4",
                    "21.5",
                    "33.7",
                    "11.1",
                    "22.1",
                    "20.3",
                    "30.9",
                    "[BOLD] 48.0",
                    "64.8"
                ],
                [
                    "[ITALIC] RoBERTa",
                    "[ITALIC] DBiDAF",
                    "65.0",
                    "80.4",
                    "[BOLD] 46.6",
                    "[BOLD] 62.3",
                    "[BOLD] 38.9",
                    "[BOLD] 50.8",
                    "25.1",
                    "36.0",
                    "[BOLD] 40.0",
                    "[BOLD] 51.3",
                    "46.9",
                    "[BOLD] 65.3"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DBERT",
                    "58.7",
                    "74.1",
                    "42.5",
                    "58.0",
                    "34.8",
                    "45.6",
                    "24.7",
                    "34.6",
                    "37.8",
                    "48.5",
                    "42.7",
                    "60.4"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DRoBERTa",
                    "55.4",
                    "71.4",
                    "37.9",
                    "53.5",
                    "37.5",
                    "48.6",
                    "[BOLD] 28.2",
                    "[BOLD] 38.9",
                    "39.5",
                    "49.0",
                    "38.8",
                    "57.9"
                ]
            ]
        },
        "gold_description": "we next conduct a series of experiments in which we train on dbidaf , dbert , and droberta , and observe how well models can then learn to generalise on the respective test portions of these datasets . table 5 shows the results , and there is a multitude of observations . first , one clear trend we observe across all training data setups is a clear negative performance progression when evaluated against datasets constructed with a stronger model in the loop . this trend holds true for all but the bidaf model , in each of the training configurations , and for each of the evaluation datasets . for example , roberta trained on droberta achieves 71 . 4 , 53 . 5 , 48 . 6 and 38 . 9 f1 when evaluated on dsquad , dbidaf , dbert and droberta , respectively . second , we observe that the bidaf model is not able to generalise well to datasets constructed with a model in the loop , independent of its training setup . in particular it is unable to learn from dbidaf , thus failing to overcome some of its own blind spots through adversarial training . both when training only on dbidaf , as well as when adding dsquad to dbidaf during training ( cf . table 6 ) , bidaf performs poorly across all the adversarial datasets . results in table 5 , where training on dbidaf in several cases led to better generalisation than training on droberta . we further train each of our three models on either dbidaf , dbert , or droberta and test on dsquad , with results in the dsquad columns of table 5 . first , we observe clear generalisation improvements towards ddrop across all models compared to training on dsquad ( 10k ) when using any of the",
        "generated_description": "we conduct experiments on three datasets : dsquad , dbidaf , droberta and dnq . the size of the training dataset is set to 10k , 40 . 6k , 50k and 100k , and the test dataset is held out to evaluate the model ’ s ability to generalize to unseen large scale datasets . the model is trained for 10 epochs and evaluated on the test set after which the model is tested on the training set of the larger dataset . we can see that the model trained with dsqad outperforms its training dataset with 10k examples , which indicates that the generalization ability of the model has been greatly improved ."
    },
    {
        "table_id": "336",
        "table_info": {
            "table_caption": "Table 4: Corpus-level BLEU scores on the validation sets for the same model architecture trained on different data.",
            "table_column_names": [
                "↓train/eval→",
                "WebSplit 1.0",
                "WikiSplit"
            ],
            "table_content_values": [
                [
                    "Source",
                    "58.0",
                    "73.4"
                ],
                [
                    "SplitHalf",
                    "54.9",
                    "71.7"
                ],
                [
                    "WebSplit",
                    "35.3",
                    "4.2"
                ],
                [
                    "WikiSplit",
                    "59.4",
                    "76.0"
                ],
                [
                    "Both",
                    "[BOLD] 61.4",
                    "[BOLD] 76.1"
                ]
            ]
        },
        "gold_description": "we compare three training configurations : websplit only , wikisplit only , and both , which is simply their concatenation . the websplit model scores 35 . 3 bleu on the websplit validation set but fails to generalize beyond its narrow domain , as evidenced by reaching only 4 . 2 bleu on the wikisplit validation set . in contrast , the wikisplit model achieves 59 . 4 bleu on the websplit validation set , reintroducing the downsampled , in - domain training data ( both ) further improves performance on the websplit evaluation .",
        "generated_description": "we observe that the same architecture trained on one split of the two datasets consistently outperforms training on the other on both datasets , and that training on both gives the best results . therefore , we use this architecture for the rest of the experiments ."
    },
    {
        "table_id": "337",
        "table_info": {
            "table_caption": "Table 6: Manual evaluation results, as counts over the simple sentences predicted by each model for a random sample of 50 inputs from WebSplit 1.0 validation set.",
            "table_column_names": [
                "[ITALIC] Training",
                "Unsupported",
                "Missing",
                "Repeated",
                "Correct"
            ],
            "table_content_values": [
                [
                    "AG18",
                    "82",
                    "45",
                    "12",
                    "26/119 (22%)"
                ],
                [
                    "WebSplit",
                    "58",
                    "47",
                    "13",
                    "32/100 (32%)"
                ],
                [
                    "WikiSplit",
                    "8",
                    "5",
                    "0",
                    "91/100 (91%)"
                ],
                [
                    "Both",
                    "[BOLD] 4",
                    "[BOLD] 4",
                    "[BOLD] 0",
                    "[BOLD] 95/100 (95%)"
                ]
            ]
        },
        "gold_description": "as shown in table 6 , the both model produced the most accurate output ( 95 % correct simple sentences ) , with the lowest incidence of missed or unsupported statements . outputs from aharoni and goldberg ( 2018 ) ( ag18 ) , which were 22 % accurate .",
        "generated_description": "for each input sentence , we randomly sample a sample of 50 sentences ( out of the original 100 ) and manually count how many times it is unambiguous ( i . e . missing , incorrect , or repeated ) and ( ii ) how often it is exactly the same as the input sentence ( indicated in bold in the table as “ unsupported ” ) . the results show that the ag18 dataset is the most unambiguous , with 95 % of its sentences unambiguous . for the websplit and wikisplit datasets , the number of unambiguous sentences is 58 and 47 , respectively . the vast majority of these sentences ( 91 % ) are either missing ( 32 % ) or repeated ( 22 % ) ( 95 % ) or both ( 95 % ) . the remaining sentences ( 8 ) are either both ( 0 ) or neither ( 0 ) . this shows the difficulty of the task ."
    },
    {
        "table_id": "338",
        "table_info": {
            "table_caption": "Table 5: Results on the WebSplit v1.0 test set when varying the training data while holding model architecture fixed: corpus-level BLEU, sentence-level BLEU (to match past work), simple sentences per complex sentence, and tokens per simple sentence (micro-average). AG18 is the previous best model by aharoni:2018, which used the full WebSplit training set, whereas we downsampled it.",
            "table_column_names": [
                "[EMPTY]",
                "BLEU",
                "sBLEU",
                "#S/C",
                "#T/S"
            ],
            "table_content_values": [
                [
                    "Reference",
                    "[EMPTY]",
                    "–",
                    "2.5",
                    "10.9"
                ],
                [
                    "Source",
                    "58.7",
                    "56.1",
                    "1.0",
                    "20.5"
                ],
                [
                    "SplitHalf",
                    "55.7",
                    "53.0",
                    "2.0",
                    "10.8"
                ],
                [
                    "AG18",
                    "30.5",
                    "25.5",
                    "2.3",
                    "11.8"
                ],
                [
                    "WebSplit",
                    "34.2",
                    "30.5",
                    "2.0",
                    "8.8"
                ],
                [
                    "WikiSplit",
                    "60.4",
                    "58.0",
                    "2.0",
                    "11.2"
                ],
                [
                    "Both",
                    "[BOLD] 62.4",
                    "[BOLD] 60.1",
                    "2.0",
                    "11.0"
                ]
            ]
        },
        "gold_description": "we relate our approach to prior work on web - split v1 . 0 by reporting scores on its test set in table 5 . our best performance in bleu is again obtained by combining the proposed wikisplit dataset with the downsampled websplit , yielding a 32 point improvement over the prior best result .",
        "generated_description": "we see that the performance on the websplit v1 . 0 test set improves from 58 . 7 to 62 . 4 bleu and from 30 . 5 to 60 . 1 sbleu , compared to the previous state - of - the - art by aharoni : 2018 , which used the full websplit training set , whereas we downsampled the training set to hold the architecture fixed . as expected , the performance degrades as the training data size decreases , as expected . however , we still see strong performance across all metrics , with simple sentences being the easiest to parse , and tokens per simple sentence being the most efficient among the other metrics ."
    },
    {
        "table_id": "339",
        "table_info": {
            "table_caption": "Table 2: Quality results for local embeddings.",
            "table_column_names": [
                "[EMPTY]",
                "Basic: no walks MA",
                "Basic: no walks MR",
                "Basic: no walks MC",
                "Basic: no walks Avg",
                "EmbDI walks MA",
                "EmbDI walks MR",
                "EmbDI walks MC",
                "EmbDI walks Avg"
            ],
            "table_content_values": [
                [
                    "Movie",
                    "[BOLD] .80",
                    ".54",
                    ".41",
                    ".59",
                    ".55",
                    "[BOLD] .80",
                    "[BOLD] .90",
                    "[BOLD] .75"
                ],
                [
                    "RefS",
                    ".39",
                    ".40",
                    ".24",
                    ".34",
                    "[BOLD] .90",
                    "[BOLD] .59",
                    "[BOLD] .62",
                    "[BOLD] .70"
                ],
                [
                    "RefL",
                    "[BOLD] .63",
                    ".50",
                    ".29",
                    ".47",
                    "[BOLD] .63",
                    "[BOLD] .69",
                    "[BOLD] .78",
                    "[BOLD] .70"
                ]
            ]
        },
        "gold_description": "we report the quality results in table 2 , where each number represents the fraction of tests passed . while on average the local embeddings for embdi are largely superior to the baseline , our solution is beaten once for ma . by increasing the percentage of row permutations in basic , results for mr improve but decrease for ma , without significant benefit for mc .",
        "generated_description": "in this section , we compare the quality of the local embeddings produced by our model to those produced by a global one . for instance , for the basic embedding , the average number of walks in the movie and movie_en_movies dataset is . 80 and . 50 , respectively , as opposed to . 50 and . 70 for the average of local and global embedding . the reason for the difference is that movies and reviews in this dataset tend to have a shorter distance to the nearest neighbor in terms of the distance between the training and test sets , which makes local embedding extraction more effective . on the other hand , in the case of embdi , the distance is much larger due to the distance of the training set to the test set , which allows for a better local representation to be learned . as a result , the walks produced by embdi are much more accurate ."
    },
    {
        "table_id": "340",
        "table_info": {
            "table_caption": "Table 3: Quality results for SM.",
            "table_column_names": [
                "[EMPTY]",
                "EmbDI P",
                "EmbDI R",
                "EmbDI F",
                "Seep [ITALIC] P P",
                "Seep [ITALIC] P R",
                "Seep [ITALIC] P F",
                "Seep [ITALIC] L P",
                "Seep [ITALIC] L R",
                "Seep [ITALIC] L F"
            ],
            "table_content_values": [
                [
                    "Movie",
                    "[BOLD] .75",
                    "[BOLD] .86",
                    "[BOLD] .80",
                    "[BOLD] .75",
                    ".62",
                    ".68",
                    "[BOLD] .75",
                    ".75",
                    ".75"
                ],
                [
                    "RefS",
                    "[BOLD] 1",
                    "[BOLD] 1",
                    "[BOLD] 1",
                    ".75",
                    ".75",
                    ".75",
                    ".88",
                    ".75",
                    ".81"
                ],
                [
                    "RefL",
                    "[BOLD] .71",
                    ".83",
                    "[BOLD] .77",
                    ".50",
                    ".75",
                    ".60",
                    ".63",
                    "[BOLD] .88",
                    ".73"
                ]
            ]
        },
        "gold_description": "we test an unsupervised setting using ( 1 ) the algorithm proposed in section 4 . 4 with embdi local embeddings , and ( 2 ) an existing matching system with both pre - trained embeddings ( seepp ) and our local embeddings ( seepl ) . pre - trained embeddings for tokens and tuples have been obtained from glove . table 3 reports the results w . r . t . manually defined attribute matches . the simple unsupervised method with embdi local embeddings outperforms the baseline in terms of fmeasure in all scenarios . results of refs are the best because of the high overlap between its datasets . the baseline improves when it is executed with embdi local embeddings , showing their superior quality w . r . t . pre - trained ones . the basic local embeddings lead to 0 attribute matches . we also observe that results for seeppretrain depend on the quality of the original attribute labels . if we replace the original ( expressive and correct ) labels with synthetic ones , seep - pretrain obtains f - measure values between . 30 and . 38 . local embeddings from embdi do not depend on the presence of the attribute labels . similarly , decreasing the size of the walks to 5 for the sm task raises the f - measure for refl to . 92 ( from . 77 ) .",
        "generated_description": "for seepp , embdi and seepl , the results show that there is no statistically significant difference ( p - value ¡ 0 . 05 ) between the mean and the standard deviation of the scores . this is due to the fact that the majority of the referential pairs ( 87 out of 95 ) have a high standard deviation ( ≥ 0 . 05 ) . this shows that the quality of the generated references is not heavily dependent on the number of referents per category ."
    },
    {
        "table_id": "341",
        "table_info": {
            "table_caption": "Table 4: F-Measure results for ER.",
            "table_column_names": [
                "[EMPTY]",
                "Unsupervised. Basic",
                "Unsupervised. Glove",
                "Unsupervised. EmbDI",
                "Supervised DeepER [ITALIC] P",
                "Supervised DeepER [ITALIC] L"
            ],
            "table_content_values": [
                [
                    "Movie",
                    "0",
                    ".43",
                    "[BOLD] .78",
                    ".82",
                    "[BOLD] .88"
                ],
                [
                    "RefS",
                    "0",
                    ".84",
                    "[BOLD] .95",
                    ".84",
                    "[BOLD] .89"
                ],
                [
                    "RefL",
                    "0",
                    ".73",
                    "[BOLD] .80",
                    ".80",
                    "[BOLD] .87"
                ]
            ]
        },
        "gold_description": "as baseline , we use our unsupervised algorithm with embdi embeddings and pre - trained embeddings . we also test our local embeddings in the supervised setting with a state of the art er system ( deeperl ) , comparing its results to the ones obtained with pre - trained embeddings ( deeperp ) . results in table 4 show that embdi embeddings obtain better quality results in all scenarios in both settings . as observed in the sm experiments , using local embeddings instead of pre - trained ones increases significantly the quality of an existing system . in this case , supervised deeper shows an average 6 % absolute improvement in f - measure in the tested setting with 5 % of the ground truth passed as training data . the improvements decreases to 4 % with more training data ( 10 % ) . also for er , the local embeddings obtained with the basic method lead to 0 row matched . execut",
        "generated_description": "for er , the supervised deeper models outperform the unsupervised ones by a large margin ( 6 points absolute f - score on average ) , and the improvement is statistically significant ( p < 0 . 01 ) across all categories . the most significant improvement is observed for movie and movie reviews , where the supervised model outperforms the basic model by over 20 points ( p ≈ 8 . 5 ) and 10 points absolute ."
    },
    {
        "table_id": "342",
        "table_info": {
            "table_caption": "Table 6: Training models on SQuAD, as well as SQuAD combined with different adversarially created datasets. Results in bold indicate the best result per model.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Training Dataset",
                "[BOLD] Evaluation (Test) Dataset DSQuAD",
                "[BOLD] Evaluation (Test) Dataset DSQuAD",
                "[BOLD] Evaluation (Test) Dataset DBiDAF",
                "[BOLD] Evaluation (Test) Dataset DBiDAF",
                "[BOLD] Evaluation (Test) Dataset DBERT",
                "[BOLD] Evaluation (Test) Dataset DBERT",
                "[BOLD] Evaluation (Test) Dataset DRoBERTa",
                "[BOLD] Evaluation (Test) Dataset DRoBERTa"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "[ITALIC] EM",
                    "[ITALIC] F1",
                    "[ITALIC] EM",
                    "[ITALIC] F1",
                    "[ITALIC] EM",
                    "[ITALIC] F1",
                    "[ITALIC] EM",
                    "[ITALIC] F1"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD",
                    "[BOLD] 58.7",
                    "[BOLD] 71.9",
                    "0.0",
                    "5.5",
                    "8.9",
                    "17.6",
                    "8.3",
                    "17.0"
                ],
                [
                    "[ITALIC] BiDAF",
                    "[ITALIC] DSQuAD + DBiDAF",
                    "57.3",
                    "70.6",
                    "14.9",
                    "25.8",
                    "16.9",
                    "25.5",
                    "15.3",
                    "24.2"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD + DBERT",
                    "57.0",
                    "70.4",
                    "[BOLD] 16.3",
                    "[BOLD] 26.5",
                    "14.5",
                    "24.1",
                    "14.7",
                    "24.1"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD + DRoBERTa",
                    "55.9",
                    "69.6",
                    "16.2",
                    "25.6",
                    "[BOLD] 17.3",
                    "[BOLD] 26.2",
                    "[BOLD] 15.6",
                    "[BOLD] 25.0"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD",
                    "70.7",
                    "84.0",
                    "36.7",
                    "50.2",
                    "0.0",
                    "5.3",
                    "15.2",
                    "25.8"
                ],
                [
                    "[ITALIC] BERT",
                    "[ITALIC] DSQuAD + DBiDAF",
                    "[BOLD] 74.5",
                    "[BOLD] 85.9",
                    "47.2",
                    "[BOLD] 61.1",
                    "33.7",
                    "43.6",
                    "29.1",
                    "39.4"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD + DBERT",
                    "74.3",
                    "85.8",
                    "[BOLD] 48.1",
                    "[BOLD] 61.1",
                    "[BOLD] 37.8",
                    "[BOLD] 47.3",
                    "[BOLD] 31.1",
                    "[BOLD] 41.5"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD + DRoBERTa",
                    "73.2",
                    "85.2",
                    "47.3",
                    "60.5",
                    "36.8",
                    "46.2",
                    "30.1",
                    "39.7"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD",
                    "74.1",
                    "86.8",
                    "50.4",
                    "64.9",
                    "31.9",
                    "44.1",
                    "0.0",
                    "5.9"
                ],
                [
                    "[ITALIC] RoBERTa",
                    "[ITALIC] DSQuAD + DBiDAF",
                    "75.2",
                    "87.6",
                    "56.3",
                    "71.2",
                    "47.8",
                    "58.0",
                    "31.3",
                    "42.8"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD + DBERT",
                    "[BOLD] 76.2",
                    "[BOLD] 88.0",
                    "56.3",
                    "70.8",
                    "48.3",
                    "58.2",
                    "33.4",
                    "44.4"
                ],
                [
                    "[EMPTY]",
                    "[ITALIC] DSQuAD + DRoBERTa",
                    "75.1",
                    "87.5",
                    "[BOLD] 58.2",
                    "[BOLD] 73.2",
                    "[BOLD] 52.8",
                    "[BOLD] 62.7",
                    "[BOLD] 36.4",
                    "[BOLD] 47.2"
                ]
            ]
        },
        "gold_description": "for example , roberta trained on droberta reaches 38 . 9f1 on droberta , and this number further increases to 47 . 2f1 when including squad during training ( cf . table 6 ) . in table 6 we show experimental results for the same models and training datasets , but now including squad as additional training data . in this training setup we generally see improved generalisation to dbidaf , dbert , and droberta . interestingly , the relative differences between dbidaf , dbert , and droberta as training set used in conjunction with squad are now much diminished , and especially droberta as ( part of the ) training set now generalises substantially better . roberta achieves the strongest results on any of the dbidaf , dbert , and droberta evaluation sets , in particular when trained on dsquad + droberta . we identify a risk of datasets constructed with weaker models in the loop becoming outdated . for example , roberta achieves 58 . 2em / 73 . 2f1 on dbidaf , in contrast to 0 . 0em / 5 . 5f1 for bidaf – which is not far from non - expert human performance of 62 . 6em / 78 . 5f1 . we furthermore observe a gradual decrease in generalisation to squad when training on dbidaf towards training on droberta . this suggests that the stronger the model used in the annotation loop , the more dissimilar the data distribution becomes from the original squad distribution . we will later find further support for this explanation in a qualitative analysis ( section 5 ) . it may however also be due to a limitation of bert and roberta – similar to bidaf – in learning from a data",
        "generated_description": "in order to verify the effectiveness of our proposed framework , we conduct experiments on three different models : ( 1 ) bidaf , ( 2 ) dsquad , ( 3 ) dldaf and ( 4 ) droberta . for the first two models , we fine - tune them on the training dataset of their respective original datasets , and then evaluate their performance on the test dataset of the respective datasets . the results show that the fine - tuned models perform significantly better than the original models on both datasets , which verifies the effectiveness and generalizability of the proposed framework . in particular , we observe that the performance of dldafc reaches the best level when the model is pre - trained on both the datasets . this indicates that the pre - training of a large - scale language model such as dbidaf can help improve the generalization ability of a small - scale model . the second model ( dsquad + dla ) further boosts the performance by adding a large amount of supervised training data into the training data , which further verifies its effectiveness . finally , the third model ( bidaf + dla ) outperforms all the baselines by a large margin , showing the validity of our framework . from the results , we can see that dla can further improve the performances of both datasets ."
    },
    {
        "table_id": "343",
        "table_info": {
            "table_caption": "Table 4: Results of the automatic evaluation procedure on a random sample of 1000 sentences.",
            "table_column_names": [
                "[EMPTY]",
                "#T/S",
                "#S/C",
                "% SAME",
                "LDSC",
                "SAMSA",
                "SAMSAabl"
            ],
            "table_content_values": [
                [
                    "Complex",
                    "30.75",
                    "1.18",
                    "100",
                    "0.00",
                    "0.36",
                    "0.94"
                ],
                [
                    "MinWikiSplit",
                    "12.12",
                    "3.84",
                    "0.00",
                    "17.73",
                    "0.40",
                    "0.48"
                ]
            ]
        },
        "gold_description": "the results are provided in table 4 . the scores demonstrate that on average our proposed sentence splitting corpus contains four simplified target sentences per complex source sentence , with every target proposition consisting of 12 tokens . moreover , no input is simply copied to the output , but split into smaller components .",
        "generated_description": "for the automatic evaluation , we randomly sample a random sample of 1000 sentences from the evaluation set and manually evaluate the quality of the summarization results . we observe that the complex and minwikisplit summarization problems are the most difficult to solve , with the complex being the easiest to solve as the majority of the examples can be solved by the simplest baselines . the same and ldsc models are able to solve most of the same problems , however , the samsaabl and samsa models tend to solve more complex problems than the other two models . this is due to the fact that these two models are designed for single - sentence summarization , which may not be suitable for multi - document summarization . on the other hand , our samsa algorithms are more robust to the number of sentences in the summary , with only 3 . 84 % of examples having less than 10 sentences ( compared to 30 . 75 % and 17 . 73 % for same , and 1 . 18 % and 0 . 48 % for ldsc ) . this shows that our algorithms are capable of solving more complex and less complex problems compared to the other algorithms . in addition , we also notice that the difficulty of the tasks is more mitigated in the hierarchical same algorithm than in the simple samsa algorithm . this indicates that the hierarchical approach is more suitable for single document summarization than the simple one ."
    },
    {
        "table_id": "344",
        "table_info": {
            "table_caption": "Table 6: Averaged human evaluation ratings on a random sample of 300 sentences from MinWikiSplit. Grammaticality (G), meaning preservation (M) and structural simplicity (S) are measured using a 1 (very bad) to 5 (very good) scale.",
            "table_column_names": [
                "G",
                "M",
                "S"
            ],
            "table_content_values": [
                [
                    "[BOLD] 4.36",
                    "[BOLD] 4.10",
                    "[BOLD] 3.43"
                ]
            ]
        },
        "gold_description": "the results of the human evaluation are displayed in table 6 . these scores show that we succeed in producing output sequences that reach a high level of grammatical soundness and almost always perfectly preserve the original meaning of the input . the third dimension under consideration , structural simplicity , which captures the degree of minimality in the simplified sentences , scores high values , too .",
        "generated_description": "we conducted a qualitative evaluation to assess the quality of the generated summaries . we randomly sampled a sample of 300 sentences from minwikisplit and asked human annotators to rate these sentences from three aspects : grammar , meaning preservation , and structural simplicity . the rating scale ranges from 1 ( very bad ) to 5 ( very good ) . we observe that the summaries generated by our model are grammatical and meaning preservation the most , which is in line with the automatic evaluation results . the summaries also achieve higher structural simplicity than the ones generated by other models ."
    },
    {
        "table_id": "345",
        "table_info": {
            "table_caption": "Table 1: Comparative table of some of the available hate speech and abusive language corpora in terms of labels and sizes.",
            "table_column_names": [
                "Dataset",
                "# Tweets",
                "Labels",
                "Annotators/Tweet"
            ],
            "table_content_values": [
                [
                    "Chatzakou:2017:MBD:3091478.3091487",
                    "9,484",
                    "aggressive, bullying, spam, normal",
                    "5"
                ],
                [
                    "DBLP:conf/naacl/WaseemH16",
                    "16, 914",
                    "racist, sexist, normal",
                    "1"
                ],
                [
                    "DavidsonWMW17",
                    "24, 802",
                    "hateful, offensive (but not hateful), neither",
                    "3 or more"
                ],
                [
                    "Golbeck2017",
                    "35,000",
                    "the worst, threats, hate speech, direct",
                    "2 to 3"
                ],
                [
                    "Golbeck2017",
                    "35,000",
                    "harassment, potentially offensive, non-harassment",
                    "2 to 3"
                ],
                [
                    "FountaDCLBSVSK18",
                    "80, 000",
                    "offensive, abusive, hateful speech,",
                    "5 to 20"
                ],
                [
                    "FountaDCLBSVSK18",
                    "80, 000",
                    "aggressive, cyberbullying, spam, normal",
                    "5 to 20"
                ],
                [
                    "hatelingo",
                    "28,608",
                    "directed, generalized + target = archaic, class, disability,",
                    "3"
                ],
                [
                    "hatelingo",
                    "28,608",
                    "ethnicity, gender, nationality, religion, sexual orientation",
                    "3"
                ],
                [
                    "Ours",
                    "13,000",
                    "Labels for five different aspects",
                    "5"
                ]
            ]
        },
        "gold_description": "we use amazon mechanical turk to label around 13 , 000 potentially derogatory tweets table 1 compares different labelsets that exist in the literature . for instance , waseem and hovy ( 2016 ) use racist , sexist , and normal as labels ; davidson et al . ( 2017 ) label their data as hateful , offensive ( but not hateful ) , and neither , while elsherief et al . ( 2018 ) present an english dataset that records the target category based on which hate speech discriminates against people , such as ethnicity , gender , or sexual orientation and ask human annotators to classify the tweets as hate and non hate . founta et al . ( 2018 ) label their data as offensive , abusive , hateful , aggressive , cyberbullying , spam , and normal .",
        "generated_description": "table 1 shows the number of labels and annotation tags used by the creators of these datasets . golbeck2017 and davidsonwmw17 are among the best - known hate speech annotators , having labeled over a quarter of the tweets in the dataset as hateful ( or harassment ) and the other half as neither hateful nor offensive . dblp : conf / naacl / waseemh16 is a collection of tweets from 16 , 914 users that were annotated as racist , sexist , normal and able - to - harassment . the fountadclbsvsk18 and hatelingo datasets are collected from 20 , 000 tweets that were labeled abusive , hateful , abusive and hateful speech by 3 different annotators . our dataset is different from the other datasets in that we do not require annotators to label five different aspects , and we only require one annotator to label each tweet ."
    },
    {
        "table_id": "346",
        "table_info": {
            "table_caption": "Table 1: Development set sentence retrieval performance. * We calculated the scores using the official code, and for ** we used the F1 formula to calculate the score.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Precision(%)",
                "[BOLD] Recall@5(%)",
                "[BOLD] F1(%)"
            ],
            "table_content_values": [
                [
                    "UNC [unc]",
                    "36.39",
                    "86.79",
                    "51.38"
                ],
                [
                    "UCL [ucl]",
                    "22.74**",
                    "84.54",
                    "35.84"
                ],
                [
                    "UKP-Athene [athene]",
                    "23.67*",
                    "85.81*",
                    "37.11*"
                ],
                [
                    "DREAM-XLNet [xlnetgraph]",
                    "26.60",
                    "87.33",
                    "40.79"
                ],
                [
                    "DREAM-RoBERTa [xlnetgraph]",
                    "26.67",
                    "87.64",
                    "40.90"
                ],
                [
                    "Pointwise",
                    "25.14",
                    "88.25",
                    "39.13"
                ],
                [
                    "Pointwise + Threshold",
                    "[BOLD] 38.18",
                    "88.00",
                    "[BOLD] 53.25"
                ],
                [
                    "Pointwise + HNM",
                    "25.13",
                    "88.29",
                    "39.13"
                ],
                [
                    "Pairwise Ranknet",
                    "24.97",
                    "88.20",
                    "38.93"
                ],
                [
                    "Pairwise Ranknet + HNM",
                    "24.97",
                    "[BOLD] 88.32",
                    "38.93"
                ],
                [
                    "Pairwise Hinge",
                    "24.94",
                    "88.07",
                    "38.88"
                ],
                [
                    "Pairwise Hinge + HNM",
                    "25.01",
                    "88.28",
                    "38.98"
                ]
            ]
        },
        "gold_description": "table 1 compares the development set performance of different variants of the proposed sentence retrieval method with the state of the art results on the fever dataset . the results indicate that both pointwise and pairwise bert sentence retrieval improve the recall . the unc and dream precision scores are better than our methods without a decision threshold , however , a threshold can regulate the trade - off between the recall and precision , and achieve the best precision and f1 scores .",
        "generated_description": "we compare our proposed models with the state - of - the - art models in terms of precision ( % ) , recall ( % ) , and f1 - score ( % ) . our proposed models outperform the previous state of the art models by a large margin . for example , our pairwise ranknet achieves 24 . 97 % and 28 . 97 % , which outperforms the previous best model dream - xlnet [ xlnetgraph ] by 1 . 6 % and 1 . 4 % in terms , respectively . our pointwise and hnm - enhanced models also achieve competitive performance , which indicates that the hnm mechanism is complementary to the pointwise objective function ."
    },
    {
        "table_id": "347",
        "table_info": {
            "table_caption": "Table 3: Full evaluation scores of the only binary classification task where the single task single language model consistently outperforms multilingual multitask models.",
            "table_column_names": [
                "Attribute",
                "Model",
                "Macro-F1 EN",
                "Macro-F1 FR",
                "Macro-F1 AR",
                "Macro-F1 Avg",
                "Micro-F1 EN",
                "Micro-F1 FR",
                "Micro-F1 AR",
                "Micro-F1 Avg"
            ],
            "table_content_values": [
                [
                    "Directness",
                    "Majority",
                    "0.50",
                    "0.11",
                    "0.50",
                    "0.47",
                    "0.79",
                    "0.41",
                    "0.54",
                    "0.58"
                ],
                [
                    "Directness",
                    "LR",
                    "0.52",
                    "0.50",
                    "0.53",
                    "0.52",
                    "0.79",
                    "0.50",
                    "0.56",
                    "0.62"
                ],
                [
                    "Directness",
                    "STSL",
                    "[BOLD] 0.94",
                    "[BOLD] 0.80",
                    "[BOLD] 0.84",
                    "[BOLD] 0.86",
                    "[BOLD] 0.89",
                    "[BOLD] 0.69",
                    "[BOLD] 0.72",
                    "[BOLD] 0.76"
                ],
                [
                    "Directness",
                    "MTSL",
                    "[BOLD] 0.94",
                    "0.65",
                    "0.76",
                    "0.78",
                    "[BOLD] 0.89",
                    "0.58",
                    "0.65",
                    "0.70"
                ],
                [
                    "Directness",
                    "STML",
                    "[BOLD] 0.94",
                    "0.79",
                    "0.83",
                    "0.85",
                    "0.88",
                    "0.66",
                    "[BOLD] 0.72",
                    "0.75"
                ],
                [
                    "Directness",
                    "MTML",
                    "[BOLD] 0.94",
                    "0.78",
                    "0.74",
                    "0.82",
                    "0.88",
                    "0.66",
                    "0.65",
                    "0.73"
                ]
            ]
        },
        "gold_description": "single task single language ( stsl ) , single task multilingual ( stml ) , and multitask multilingual models ( mtml ) on our dataset . stsl performs the best among all models on the directness classification , and it is also consistent in both micro and macro - f1 scores . due to the fact that the directness has only two labels and multilabeling is not allowed in this task . tasks involving imbalanced data , multiclass and multilabel annotations harm the performance of the directness in multitask settings . since macro - f1 is the average of all f1 scores of individual labels , all deep learning models have high macro - f1 scores in english which indicates that they are particularly good at classifying the direct class . stsl is also comparable or better than traditional bow feature - based classifiers when performed on other tasks in terms of microf1 and for most of the macro - f1 scores . this shows the power of the deep learning approach . except for the directness , mtsl usually outperforms stsl or is comparable to it . mtml settings do not lead to a big improvement which may be due to the class imbalance , multilabel tasks , and the difference in the nature of the tasks .",
        "generated_description": "the macro - and micro - averaged f1 scores show that our models outperform the majority classifier and the lr classifier on both languages . the best performance is achieved by the mtsl model , which outperforms the lr and majority classifiers by a large margin on average . the stsl model shows the best performance on the direct relation extraction and the shortest path extraction tasks . on the other hand , we observe that the stsl models have the lowest scores on the shortest paths extraction ."
    },
    {
        "table_id": "348",
        "table_info": {
            "table_caption": "Table 4: Full evaluation of tasks where multilingual and multitask models outperform on average single task single language model on four different tasks.",
            "table_column_names": [
                "Attribute",
                "Model",
                "Macro-F1 EN",
                "Macro-F1 FR",
                "Macro-F1 AR",
                "Macro-F1 Avg",
                "Micro-F1 EN",
                "Micro-F1 FR",
                "Micro-F1 AR",
                "Micro-F1 Avg"
            ],
            "table_content_values": [
                [
                    "Tweet",
                    "Majority",
                    "0.24",
                    "0.19",
                    "0.20",
                    "0.21",
                    "0.41",
                    "0.27",
                    "0.27",
                    "0.32"
                ],
                [
                    "Tweet",
                    "LR",
                    "0.14",
                    "0.20",
                    "0.25",
                    "0.20",
                    "0.54",
                    "0.56",
                    "[BOLD] 0.48",
                    "0.53"
                ],
                [
                    "Tweet",
                    "STSL",
                    "0.24",
                    "0.12",
                    "0.31",
                    "0.23",
                    "0.49",
                    "0.51",
                    "0.47",
                    "0.49"
                ],
                [
                    "Tweet",
                    "MTSL",
                    "0.09",
                    "0.20",
                    "0.33",
                    "0.21",
                    "[BOLD] 0.55",
                    "[BOLD] 0.59",
                    "0.46",
                    "[BOLD] 0.54"
                ],
                [
                    "Tweet",
                    "STML",
                    "0.04",
                    "0.07",
                    "[BOLD] 0.35",
                    "0.16",
                    "0.54",
                    "0.47",
                    "0.37",
                    "0.46"
                ],
                [
                    "Tweet",
                    "MTML",
                    "[BOLD] 0.30",
                    "[BOLD] 0.28",
                    "[BOLD] 0.35",
                    "[BOLD] 0.31",
                    "0.45",
                    "0.48",
                    "0.44",
                    "0.46"
                ],
                [
                    "Target Attribute",
                    "Majority",
                    "0.15",
                    "0.13",
                    "0.28",
                    "0.19",
                    "0.25",
                    "0.32",
                    "0.40",
                    "0.32"
                ],
                [
                    "Target Attribute",
                    "LR",
                    "0.41",
                    "0.35",
                    "0.47",
                    "0.41",
                    "0.52",
                    "0.55",
                    "0.53",
                    "0.53"
                ],
                [
                    "Target Attribute",
                    "STSL",
                    "0.42",
                    "0.18",
                    "[BOLD] 0.63",
                    "0.41",
                    "[BOLD] 0.68",
                    "0.71",
                    "0.50",
                    "0.63"
                ],
                [
                    "Target Attribute",
                    "MTSL",
                    "0.41",
                    "[BOLD] 0.43",
                    "0.41",
                    "[BOLD] 0.42",
                    "[BOLD] 0.68",
                    "0.67",
                    "[BOLD] 0.56",
                    "[BOLD] 0.64"
                ],
                [
                    "Target Attribute",
                    "STML",
                    "0.39",
                    "0.09",
                    "0.24",
                    "0.24",
                    "0.67",
                    "0.62",
                    "0.53",
                    "0.61"
                ],
                [
                    "Target Attribute",
                    "MTML",
                    "[BOLD] 0.43",
                    "0.24",
                    "0.16",
                    "0.28",
                    "0.66",
                    "[BOLD] 0.72",
                    "0.51",
                    "0.63"
                ],
                [
                    "Target Group",
                    "Majority",
                    "0.07",
                    "0.06",
                    "0.08",
                    "0.07",
                    "0.18",
                    "0.14",
                    "0.35",
                    "0.22"
                ],
                [
                    "Target Group",
                    "LR",
                    "[BOLD] 0.18",
                    "0.33",
                    "[BOLD] 0.40",
                    "[BOLD] 0.30",
                    "0.34",
                    "0.40",
                    "0.62",
                    "0.46"
                ],
                [
                    "Target Group",
                    "STSL",
                    "0.04",
                    "0.21",
                    "0.04",
                    "0.10",
                    "0.48",
                    "[BOLD] 0.59",
                    "0.58",
                    "0.55"
                ],
                [
                    "Target Group",
                    "MTSL",
                    "0.04",
                    "0.27",
                    "0.15",
                    "0.15",
                    "[BOLD] 0.50",
                    "0.54",
                    "0.55",
                    "0.53"
                ],
                [
                    "Target Group",
                    "STML",
                    "0.11",
                    "[BOLD] 0.37",
                    "0.13",
                    "0.20",
                    "0.49",
                    "0.57",
                    "[BOLD] 0.64",
                    "[BOLD] 0.56"
                ],
                [
                    "Target Group",
                    "MTML",
                    "0.06",
                    "0.19",
                    "0.10",
                    "0.11",
                    "[BOLD] 0.50",
                    "0.54",
                    "0.56",
                    "0.53"
                ],
                [
                    "Annotator’s Sentiment",
                    "Majority",
                    "0.42",
                    "0.21",
                    "0.17",
                    "0.27",
                    "0.46",
                    "0.31",
                    "0.32",
                    "0.39"
                ],
                [
                    "Annotator’s Sentiment",
                    "LR",
                    "0.29",
                    "0.15",
                    "0.14",
                    "0.19",
                    "0.45",
                    "0.30",
                    "0.46",
                    "0.40"
                ],
                [
                    "Annotator’s Sentiment",
                    "STSL",
                    "[BOLD] 0.57",
                    "[BOLD] 0.30",
                    "0.12",
                    "[BOLD] 0.33",
                    "0.57",
                    "0.39",
                    "[BOLD] 0.48",
                    "0.48"
                ],
                [
                    "Annotator’s Sentiment",
                    "MTSL",
                    "[BOLD] 0.57",
                    "0.17",
                    "0.17",
                    "0.30",
                    "0.57",
                    "[BOLD] 0.50",
                    "0.45",
                    "0.51"
                ],
                [
                    "Annotator’s Sentiment",
                    "STML",
                    "0.47",
                    "0.22",
                    "0.13",
                    "0.27",
                    "[BOLD] 0.59",
                    "0.49",
                    "[BOLD] 0.48",
                    "[BOLD] 0.52"
                ],
                [
                    "Annotator’s Sentiment",
                    "MTML",
                    "0.55",
                    "0.20",
                    "[BOLD] 0.21",
                    "0.32",
                    "0.58",
                    "0.45",
                    "0.45",
                    "0.49"
                ]
            ]
        },
        "gold_description": "when we jointly train each task on the three languages , the performance decreases in most cases , other than the target group classification tasks . yet , multilingual training of the target group classification task improves in all languages . since the target group classification task involves 16 labels , the amount of data annotated for each label is lower than in other tasks . hence , when aggregating annotated data in different languages , the size of the training data also increases , due to the relative regularity of identification words of different groups in all three languages in comparison to other tasks .",
        "generated_description": "the macro - and micro - averaged f1 scores show that the neural models outperform the majority classifier and the lr classifier on average . stml shows the best performance on english and french datasets , and mtsl performs the best on german and romanian datasets ."
    },
    {
        "table_id": "349",
        "table_info": {
            "table_caption": "Table 1: Comparison of Exact Match and F1-score of multilingual BERT and the baseline on French and Japanese SQuAD. F1 and EM are the two official metrics of the SQuAD benchmark. EM measures the percentage of predictions that match exactly the ground-truth location of the answer. F1 measures the average overlap between the prediction and ground truth answer.",
            "table_column_names": [
                "[EMPTY]",
                "French F1",
                "French EM",
                "Japanese F1",
                "Japanese EM"
            ],
            "table_content_values": [
                [
                    "Baseline",
                    "61.88",
                    "40.67",
                    "52.19",
                    "37.00"
                ],
                [
                    "Multilingual BERT",
                    "[BOLD] 76.65",
                    "[BOLD] 61.77",
                    "[BOLD] 61.83",
                    "[BOLD] 59.94"
                ]
            ]
        },
        "gold_description": "a sample of the squad v1 . 1 test set ( only the first paragraph of each of the 48 wikipedia pages ) has been translated by humans in french and japanese . we here evaluate the performance of the fine - tuned multilingual bert on them and compare the results to a baseline . table 1 displays the exact match ( em ) and f1 - score of the baseline and multilingual bert on the selected datasets . we can observe that multilingual bert is able to significantly outperform the baseline on both the japanese and the french question answering task . was already noted in the public benchmarks and we add here that bert has a high ability for qa zero - shot transfer . it is even able to significantly outperform the baseline",
        "generated_description": "we can see that multilingual bert outperforms the baseline model in terms of both exact match and f1 - score on both french and japanese squad . for japanese , our model achieves the highest exact match ( 61 . 83 ) and the highest value of em ( 59 . 94 ) , which means that our model can find the exact location of the answer in a region that is far away from the ground - truth ."
    },
    {
        "table_id": "350",
        "table_info": {
            "table_caption": "Table 2: Exact Match and F1-score of multilingual BERT on each of the cross-lingual SQuAD datasets. The row language is the one of the paragraph and the column language is the one of the question. The figures in bold are the best exact match, for each language, among the datasets where they occur.",
            "table_column_names": [
                "[EMPTY]",
                "Question",
                "En F1",
                "En EM",
                "Fr F1",
                "Fr EM",
                "Jap F1",
                "Jap EM"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "En",
                    "90.57",
                    "[BOLD] 81.96",
                    "78.55",
                    "[BOLD] 67.28",
                    "66.22",
                    "52.91"
                ],
                [
                    "Context",
                    "Fr",
                    "81.10",
                    "65.14",
                    "76.65",
                    "61.77",
                    "60.28",
                    "42.20"
                ],
                [
                    "[EMPTY]",
                    "Jap",
                    "58.95",
                    "57.49",
                    "47.19",
                    "45.26",
                    "61.83",
                    "[BOLD] 59.93"
                ]
            ]
        },
        "gold_description": "to run cross - lingual tests , we build six additional datasets from the existing ones by mixing context in one language with question in another language . the mixed datasets will be made available online in a github repository . the performance of bert on all datasets is displayed in table 2 . the performance is the best for the en - en dataset . the performance on fr - fr and jap - jap is also very good as noted in the first experiment . we additionally note here that results on cross - lingual sets are close to monolingual results : either as good , or slightly worse or slightly better . for instance , the exact match on the en - fr dataset is higher than the exact match on the fr - fr dataset . we also observe that , in general , the exact match and f1 - score are close together when the context is in japanese whereas there is generally a larger gap for the other two languages . the performance on jap - en is lower than on jap - jap whereas the performance on en - fr is higher than on fr - fr , results for the jap - jap dataset are better than results for the jap - en dataset",
        "generated_description": "table 2 shows the best exact match and f1 - score of multilingual bert on each of the cross - lingual bert models for the paragraph and the question . the table shows that the model performs better than bilingual bert for the context questions , but worse for the english questions . the reason for this may be that the english context questions contain much more non - standard expressions and adverbs that are difficult to translate into the target language ."
    },
    {
        "table_id": "351",
        "table_info": {
            "table_caption": "Table 2: Development set verification scores.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] FEVER Score(%)",
                "[BOLD] Label Accuracy(%)"
            ],
            "table_content_values": [
                [
                    "UNC [unc]",
                    "66.14",
                    "69.60"
                ],
                [
                    "UCL [ucl]",
                    "65.41",
                    "69.66"
                ],
                [
                    "UKP-Athene [athene]",
                    "64.74",
                    "-"
                ],
                [
                    "BERT & UKP-Athene",
                    "69.79",
                    "71.70"
                ],
                [
                    "BERT Large & UKP-Athene",
                    "70.64",
                    "72.72"
                ],
                [
                    "BERT & BERT (Pointwise)",
                    "71.38",
                    "73.51"
                ],
                [
                    "BERT & BERT (Pointwise + HNM)",
                    "71.33",
                    "73.54"
                ],
                [
                    "BERT (Large) & BERT (Pointwise)",
                    "[BOLD] 72.42",
                    "74.58"
                ],
                [
                    "BERT (Large) & BERT (Pointwise + HNM)",
                    "[BOLD] 72.42",
                    "[BOLD] 74.59"
                ],
                [
                    "BERT & BERT (Pairwise Ranknet)",
                    "71.02",
                    "73.22"
                ],
                [
                    "BERT & BERT (Pairwise Ranknet + HNM)",
                    "70.99",
                    "73.02"
                ],
                [
                    "BERT & BERT (Pairwise Hinge)",
                    "71.60",
                    "72.74"
                ],
                [
                    "BERT & BERT (Pairwise Hinge + HNM)",
                    "70.70",
                    "72.76"
                ]
            ]
        },
        "gold_description": "in table 2 , we compare the development set results of the state of the art methods with the bert model trained on different retrieved evidence sets . the bert claim verification system even if it is trained on the ukp - athene sentence retrieval component ( hanselowski et al . , 2018 ) , the state of the art method with the highest recall , improves both label accuracy and fever score . training based on the bert sentence retrieval predictions significantly enhances the verification results because while it explicitly improves the fever score by providing more correct evidence sentences , it provides a better training set for the verification system . the large berts are only trained on the best retrieval systems , and as expected significantly improve the performance .",
        "generated_description": "from the table , we can observe that : 1 ) pairwise ranknet consistently outperforms the pointwise version of hnm in terms of both the fever score and the label accuracy . 2 ) bert & bert ( large ) outperforms all the other models , which indicates that the pre - trained bert embeddings are complementary to each other and can act as a better inductive bias for our task . 3 ) in the next experiment , we use bert large as the baseline model and bert + bert as the final model . 4 ) in this experiment , the bert based models achieve the best performance , which proves that bert is complementary to bert - large . 5 ) the best performance of bert + bert & ukp - athene is achieved by using bert large model as the initial model and then fine - tuning bert on the athene and ucl corpora jointly . this proves the effectiveness of the proposed approach ."
    },
    {
        "table_id": "352",
        "table_info": {
            "table_caption": "Table 3: Doc-level BLEU scores on the DGT valid and test sets of our submitted models in all tracks.",
            "table_column_names": [
                "Track",
                "Target",
                "Constrained",
                "Valid",
                "Test"
            ],
            "table_content_values": [
                [
                    "NLG",
                    "EN",
                    "no",
                    "23.5",
                    "20.5"
                ],
                [
                    "MT",
                    "EN",
                    "yes",
                    "60.2",
                    "58.2"
                ],
                [
                    "MT",
                    "EN",
                    "no",
                    "64.2",
                    "62.2"
                ],
                [
                    "MT+NLG",
                    "EN",
                    "yes",
                    "64.4",
                    "62.2"
                ],
                [
                    "NLG",
                    "DE",
                    "no",
                    "16.9",
                    "16.1"
                ],
                [
                    "MT",
                    "DE",
                    "yes",
                    "49.8",
                    "48.0"
                ],
                [
                    "MT+NLG",
                    "DE",
                    "yes",
                    "49.4",
                    "48.2"
                ]
            ]
        },
        "gold_description": "for each track , we selected the best models according to their bleu score on dgt - valid . the scores are shown in table 3 , we see that in the same data conditions ( unconstrained mode ) , the mt + nlg models are not better than the pure mt models .",
        "generated_description": "the “ constrained ” configuration corresponds to training and testing on the dev and test sets of each language pair separately , and the “ full dev ” condition corresponds to testing on both the test and the dev set . we observe that models trained in the constrained configuration perform significantly better than models trained with the unrestricted condition in both tracks . this is to be expected if the models are optimized for nlg , they are less likely to overfit to the constrained conditions in the dev or test set ."
    },
    {
        "table_id": "353",
        "table_info": {
            "table_caption": "Table 6: English NLG comparison against state-of-the-art on Rotowire-test. BLEU of submitted NLG (EN) model, averaged over 3 runs. Because Rotowire tokenization is slightly different, we apply a set of fixes to the model outputs (e.g., 1-of-3 → 1 - of - 3).",
            "table_column_names": [
                "Model",
                "Rotowire test"
            ],
            "table_content_values": [
                [
                    "Wiseman et al. ( 2017 )",
                    "14.5"
                ],
                [
                    "Puduppully et al. ( 2019 )",
                    "16.5"
                ],
                [
                    "Ours (4-player)",
                    "22.2"
                ]
            ]
        },
        "gold_description": "table 6 shows a 5 . 7 bleu improvement on rotowire - test by our english nlg model compared to the previous state of the art .",
        "generated_description": "we see that our 4 - player model significantly outperforms wiseman et al . ’ s model ( 14 . 5 bleu ) on the rotowire - test set , and is on par with the state - of - the - art ( 16 . 5 ) on opennli - test . this shows that our model is fairly robust to tokenization changes ."
    },
    {
        "table_id": "354",
        "table_info": {
            "table_caption": "Table 7: English NLG ablation study, starting from a 3 best player baseline (the submitted NLG model has 4 players). BLEU averages over 3 runs. Standard deviation ranges between 0.1 and 0.4.",
            "table_column_names": [
                "Model",
                "Valid",
                "Test"
            ],
            "table_content_values": [
                [
                    "Baseline (3 players, sorted)",
                    "[BOLD] 22.7",
                    "20.4"
                ],
                [
                    "No player",
                    "20.1",
                    "18.8"
                ],
                [
                    "All players, sorted",
                    "[BOLD] 22.7",
                    "20.9"
                ],
                [
                    "All players, shuffled",
                    "22.0",
                    "20.0"
                ],
                [
                    "(1) No next game",
                    "22.0",
                    "19.9"
                ],
                [
                    "(2) No week day",
                    "22.2",
                    "20.5"
                ],
                [
                    "(3) No player position",
                    "22.6",
                    "20.5"
                ],
                [
                    "(4) No team-level sums",
                    "22.5",
                    "20.5"
                ],
                [
                    "(5) Remove most tags",
                    "22.6",
                    "20.8"
                ],
                [
                    "(1) to (5)",
                    "21.3",
                    "19.7"
                ]
            ]
        },
        "gold_description": "from table 7 , we see that sorting players helps , but only slightly . using only team - level information , and no information about players gives worse but still decent bleu scores . week day , player position or team - level aggregated scores can be removed without hurting bleu . however , information about next games seems useful . interestingly , relying on position only and removing most tags ( e . g . , , ) seems to be fine . in this case , we also print all - zero stats , for the position of each statistic to be consistent across players and games .",
        "generated_description": "we perform an ablation study of the english nlg model , starting from a 3 best player baseline to a 10 best model ensemble . we find that removing any individual player from the model decreases performance from 22 . 7 to 20 . 4 bleu on the test set . removing the player position has no effect on the final score , but shuffling all the players has a negative impact ( 20 . 0 to 20 , 9 ) . next , we remove the week day and team - level sums , and remove most tags from the tags of the tags . this results in a slight drop in performance from 20 . 8 to 19 . 7 . we hypothesize that this is due to the fact that the sums are computed by summing up the sums of all the team - generated tags , which are then averaged over all the tags in the sequence ."
    },
    {
        "table_id": "355",
        "table_info": {
            "table_caption": "Table 3: F1 score on the development set for low-resource training setups (none, tiny 5k or small 10k labeled Danish sentences). Transfer via multilingual embeddings from Medium (3.2k sentences, 51k tokens) or Large English source data (14k sentences/203k tokens).",
            "table_column_names": [
                "[EMPTY]",
                "TnT",
                "neural in-lang. plain",
                "neural in-lang. +Poly",
                "neural transfer +Medium src",
                "neural transfer +Large src",
                "neural transfer FineTune"
            ],
            "table_content_values": [
                [
                    "zero-shot",
                    "—",
                    "—",
                    "—",
                    "58.29",
                    "61.18",
                    "—"
                ],
                [
                    "Tiny",
                    "37.48",
                    "36.17",
                    "56.05",
                    "67.14",
                    "67.49",
                    "62.07"
                ],
                [
                    "Small",
                    "44.30",
                    "51.90",
                    "67.18",
                    "[BOLD] 70.82",
                    "70.01",
                    "65.63"
                ]
            ]
        },
        "gold_description": "cross - lingual transfer is powerful ( rq1 ) . zero - shot learning reaches an f1 score of 58 % in the medium setup , which outperforms training the neural tagger on very limited gold data ( plain ) . neural ner is better than traditional hmm - based tagging ( tnt ) ( brants , 2000 ) and greatly improves by unsupervised word embedding initialization ( + poly ) . it is noteworthy that zero - shot transfer benefits only to a limiting degree from more source data ( f1 increases by almost 3 % when training on all english conll data ) . to compare cross - lingual transfer to limited gold data ( rq2 ) , we observe that training the neural system on the small amount of data together with polyglot embeddings is close to the tiny - shot transfer setup . few - shot learning greatly improves over zero - shot learning . the most beneficial way is to add the target data to the source , in comparison to fine - tuning . in both medium and large setups are further gains obtained by adding tiny or small amounts of danish gold data . interestingly , a ) finetuning is less effective ; b ) it is better to transfer from a medium - sized setup than from the entire conll source data .",
        "generated_description": "for low - resource training setups ( none , tiny 5k , tiny 10k or small 10k labeled danish sentences ) we use the same multilingual embedding matrix as in the zero - shot setting , i . e . , we use a 3 . 2k sentence , 51k token version of the english wikilarge corpus . we observe that the generalization is robust to the size of the source data , with the best performance coming from using either the large english corpus ( 14k sentences / 203k tokens ) or the medium corpus ( 3 . 8k sentences , 51k tokens ) which contains the same amount of training data as the tiny corpus . the best performance is obtained using the large corpus ( 70 . 01 % f1 score ) which is also much larger than the tiny one ( 37 . 48 % ) . this shows that the transfer method is robust even in very small training sets ."
    },
    {
        "table_id": "356",
        "table_info": {
            "table_caption": "Table 4: F1 score for Danish NER.",
            "table_column_names": [
                "Dev",
                "All",
                "PER",
                "LOC",
                "ORG",
                "MISC"
            ],
            "table_content_values": [
                [
                    "Majority",
                    "44.4",
                    "61.8",
                    "0.0",
                    "0.0",
                    "—"
                ],
                [
                    "DKIE",
                    "58.9",
                    "68.9",
                    "63.6",
                    "23.3",
                    "—"
                ],
                [
                    "Polyglot",
                    "64.5",
                    "73.7",
                    "[BOLD] 73.4",
                    "36.8",
                    "—"
                ],
                [
                    "Bilstm",
                    "[BOLD] 70.8",
                    "[BOLD] 83.3",
                    "71.8",
                    "[BOLD] 60.0",
                    "23.9"
                ],
                [
                    "Test",
                    "All",
                    "PER",
                    "LOC",
                    "ORG",
                    "MISC"
                ],
                [
                    "Polyglot",
                    "61.6",
                    "78.4",
                    "[BOLD] 69.7",
                    "24.7",
                    "—"
                ],
                [
                    "Bilstm",
                    "[BOLD] 66.0",
                    "[BOLD] 86.6",
                    "63.6",
                    "[BOLD] 42.5",
                    "24.8"
                ]
            ]
        },
        "gold_description": "existing systems ( rq3 ) are evaluated and results polyglot ( al - rfou et al . , 2013 ) overall performs better than dkie ( derczynski et al . , 2014 ) . 2 the best system is our cross - lingual transfer ner from medium source data paired with small amounts of gold data . per - entity evaluation shows that the neural bilstm tagger outperforms polyglot except for location , which is consistent across evaluation sets .",
        "generated_description": "on the test set , the polyglot model outperforms the dkie model and the majority baseline by a large margin . bilstm obtains the best overall f1 score of 70 . 8 % on per and 83 . 3 % on loc , and 86 . 6 % on org ."
    },
    {
        "table_id": "357",
        "table_info": {
            "table_caption": "Table 2: Results of present keyphrase prediction on five datasets. Suffix “-2RF1” denotes that a model is trained by our reinforcement learning approach.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Inspec  [ITALIC] F1@ [ITALIC] M",
                "[BOLD] Inspec  [ITALIC] F1@5",
                "[BOLD] Krapivin  [ITALIC] F1@ [ITALIC] M",
                "[BOLD] Krapivin  [ITALIC] F1@5",
                "[BOLD] NUS  [ITALIC] F1@ [ITALIC] M",
                "[BOLD] NUS  [ITALIC] F1@5",
                "[BOLD] SemEval  [ITALIC] F1@ [ITALIC] M",
                "[BOLD] SemEval  [ITALIC] F1@5",
                "[BOLD] KP20k  [ITALIC] F1@ [ITALIC] M",
                "[BOLD] KP20k  [ITALIC] F1@5"
            ],
            "table_content_values": [
                [
                    "catSeq",
                    "0.262",
                    "0.225",
                    "0.354",
                    "0.269",
                    "0.397",
                    "0.323",
                    "0.283",
                    "0.242",
                    "0.367",
                    "0.291"
                ],
                [
                    "catSeqD",
                    "0.263",
                    "0.219",
                    "0.349",
                    "0.264",
                    "0.394",
                    "0.321",
                    "0.274",
                    "0.233",
                    "0.363",
                    "0.285"
                ],
                [
                    "catSeqCorr",
                    "0.269",
                    "0.227",
                    "0.349",
                    "0.265",
                    "0.390",
                    "0.319",
                    "0.290",
                    "0.246",
                    "0.365",
                    "0.289"
                ],
                [
                    "catSeqTG",
                    "0.270",
                    "0.229",
                    "0.366",
                    "0.282",
                    "0.393",
                    "0.325",
                    "0.290",
                    "0.246",
                    "0.366",
                    "0.292"
                ],
                [
                    "catSeq-2 [ITALIC] RF1",
                    "0.300",
                    "0.250",
                    "0.362",
                    "0.287",
                    "0.426",
                    "0.364",
                    "0.327",
                    "0.285",
                    "0.383",
                    "0.310"
                ],
                [
                    "catSeqD-2 [ITALIC] RF1",
                    "0.292",
                    "0.242",
                    "0.360",
                    "0.282",
                    "0.419",
                    "0.353",
                    "0.316",
                    "0.272",
                    "0.379",
                    "0.305"
                ],
                [
                    "catSeqCorr-2 [ITALIC] RF1",
                    "0.291",
                    "0.240",
                    "[BOLD] 0.369",
                    "0.286",
                    "0.414",
                    "0.349",
                    "0.322",
                    "0.278",
                    "0.382",
                    "0.308"
                ],
                [
                    "catSeqTG-2 [ITALIC] RF1",
                    "[BOLD] 0.301",
                    "[BOLD] 0.253",
                    "[BOLD] 0.369",
                    "[BOLD] 0.300",
                    "[BOLD] 0.433",
                    "[BOLD] 0.375",
                    "[BOLD] 0.329",
                    "[BOLD] 0.287",
                    "[BOLD] 0.386",
                    "[BOLD] 0.321"
                ]
            ]
        },
        "gold_description": "the evaluation results of different models on predicting present keyphrases are shown in table 2 . we observe that our reinforcement learning algorithm catseqtg - 2 [ italic ] rf1 consistently improves the keyphrase extraction ability of all baseline generative models by a large margin .",
        "generated_description": "for each of the three models , we report f1 @ 5 scores on the test sets of inspec , krapivin , nus and nus . the results show that our model outperforms all the other models in terms of f1 - score on all the three datasets . in particular , our model achieves the best performance on the nus dataset with an absolute improvement of 4 . 8 % over the best performing baseline model , catseqd , and a significant improvement of 2 . 3 % over rnnseq and 2 . 7 % over cnnseq . the performance on inspec dataset is also boosted by the addition of the kb - level and document - level meta - data features , which improves the results by 1 . 4 % and 1 . 2 % respectively . for the semeval and kp20k datasets , the performance of our model is competitive with the best baseline models ."
    },
    {
        "table_id": "358",
        "table_info": {
            "table_caption": "Table 4: The abilities of predicting the correct number of keyphrases on the KP20k dataset. MAE denotes the mean absolute error (the lower the better), Avg. # denotes the average number of generated keyphrases per document.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Present MAE",
                "[BOLD] Present Avg. #",
                "[BOLD] Absent MAE",
                "[BOLD] Absent Avg. #"
            ],
            "table_content_values": [
                [
                    "oracle",
                    "0.000",
                    "2.837",
                    "0.000",
                    "2.432"
                ],
                [
                    "catSeq",
                    "2.271",
                    "3.781",
                    "1.943",
                    "0.659"
                ],
                [
                    "catSeqD",
                    "2.225",
                    "3.694",
                    "1.961",
                    "0.629"
                ],
                [
                    "catSeqCorr",
                    "2.292",
                    "3.790",
                    "1.914",
                    "0.703"
                ],
                [
                    "catSeqTG",
                    "2.276",
                    "3.780",
                    "1.956",
                    "0.638"
                ],
                [
                    "catSeq-2 [ITALIC] RF1",
                    "2.118",
                    "3.733",
                    "1.494",
                    "1.574"
                ],
                [
                    "catSeqD-2 [ITALIC] RF1",
                    "[BOLD] 2.087",
                    "[BOLD] 3.666",
                    "1.541",
                    "1.455"
                ],
                [
                    "catSeqCorr-2 [ITALIC] RF1",
                    "2.107",
                    "3.696",
                    "1.557",
                    "1.409"
                ],
                [
                    "catSeqTG-2 [ITALIC] RF1",
                    "2.204",
                    "3.865",
                    "[BOLD] 1.439",
                    "[BOLD] 1.749"
                ]
            ]
        },
        "gold_description": "we also report the average number of generated keyphrases per document , denoted as \" avg . # \" . the results are shown in table 4 , where oracle is a model that always generates the ground - truth keyphrases . the resultant maes demonstrate that our deep reinforced models notably outperform the baselines on predicting the number of absent keyphrases and slightly outperform the baselines on predicting the number of present keyphrases . moreover , our deep reinforced models generate significantly more absent keyphrases than the baselines besides , the baseline models and our reinforced models generate similar numbers of present keyphrases , while our reinforced models achieve notably higher f - measures , implying that our methods generate present keyphrases more accurately than the baselines .",
        "generated_description": "it can be observed that the three proposed models outperform the baselines for both present and absent user intent prediction . notably , the best performing model is catseqtg - 2rf1 rf1 , which outperforms the best non - rage - based model by 0 . 956 mae and 1 . 541 mae in the present condition , and outperforms all other models by a large margin in the absent condition , showing that the model is better at predicting the user intent in the past than in the future ."
    },
    {
        "table_id": "359",
        "table_info": {
            "table_caption": "Table 5: Ablation study on the KP20k dataset. Suffix “-2RF1” denotes our full RL approach. Suffix “-2F1” denotes that we replace our adaptive RF1 reward function in the full approach by an F1 reward function. Suffix “-RF1” denotes that we replace the two separate RF1 reward signals in our full approach with only one RF1 reward signal for all the generated keyphrases.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Present  [ITALIC] F1@ [ITALIC] M",
                "[BOLD] Present  [ITALIC] F1@5",
                "[BOLD] Absent  [ITALIC] F1@ [ITALIC] M",
                "[BOLD] Absent  [ITALIC] F1@5"
            ],
            "table_content_values": [
                [
                    "catSeq",
                    "0.367",
                    "0.291",
                    "0.032",
                    "0.015"
                ],
                [
                    "catSeq- [ITALIC] RF1",
                    "0.380",
                    "0.336",
                    "0.006",
                    "0.003"
                ],
                [
                    "catSeq-2 [ITALIC] F1",
                    "0.378",
                    "0.278",
                    "0.042",
                    "0.020"
                ],
                [
                    "catSeq-2 [ITALIC] RF1",
                    "0.383",
                    "0.310",
                    "0.047",
                    "0.024"
                ]
            ]
        },
        "gold_description": "we conduct an ablation study to further analyze our reinforcement learning algorithm . the results are reported in table 5 . as seen in table 5 , although the performance of catseq - rf1 is competitive to catseq - 2rf1 on predicting present keyphrases , it yields an extremely poor performance on absent keyphrase prediction . by comparing the last two rows in table 5 , we observe that our rf1 reward function slightly outperforms the f1 reward function .",
        "generated_description": "in this section , we conduct an ablation study on the kp20k dataset to evaluate the contribution of each component in our full rl approach . in particular , we replace the two separate rf1 reward signals in our approach with only one rf1 ( i . e . , rf1 - 2rf1 ) for all the generated keyphrases in the training set . the results show that both the adaptive and f1 reward functions play an important role in the rl approach , and replacing any of them with a single rf1 signal degrades the performance . more importantly , the performance of the catseq - 2f1 model is even worse than that of the vanilla seq2seq model . this indicates that the two kinds of rf1 signals are more important for the rl task at present keyphrase extraction ."
    },
    {
        "table_id": "360",
        "table_info": {
            "table_caption": "Table 6: Keyphrase prediction results on the KP20k dataset with our new evaluation method.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Present  [ITALIC] F1@ [ITALIC] M",
                "[BOLD] Present  [ITALIC] F1@ [ITALIC] M",
                "[BOLD] Absent  [ITALIC] F1@ [ITALIC] M",
                "[BOLD] Absent  [ITALIC] F1@ [ITALIC] M"
            ],
            "table_content_values": [
                [
                    "[BOLD] Model",
                    "old",
                    "new",
                    "old",
                    "new"
                ],
                [
                    "catSeq",
                    "0.367",
                    "0.376",
                    "0.032",
                    "0.034"
                ],
                [
                    "catSeqD",
                    "0.363",
                    "0.372",
                    "0.031",
                    "0.033"
                ],
                [
                    "catSeqCorr",
                    "0.365",
                    "0.375",
                    "0.032",
                    "0.034"
                ],
                [
                    "catSeqTG",
                    "0.366",
                    "0.374",
                    "0.032",
                    "0.033"
                ],
                [
                    "catSeq-2 [ITALIC] RF1",
                    "0.383",
                    "0.396",
                    "0.047",
                    "0.054"
                ],
                [
                    "catSeqD-2 [ITALIC] RF1",
                    "0.379",
                    "0.390",
                    "0.046",
                    "0.052"
                ],
                [
                    "catSeqCorr-2 [ITALIC] RF1",
                    "0.382",
                    "0.393",
                    "0.045",
                    "0.051"
                ],
                [
                    "catSeqTG-2 [ITALIC] RF1",
                    "0.386",
                    "0.398",
                    "0.050",
                    "0.056"
                ]
            ]
        },
        "gold_description": "table 6 shows that for all generative models , the evaluation scores computed by our method are higher than those computed by prior method .",
        "generated_description": "for both the old and the new settings , the best performing models are catseq - 2rf1 and catseacorr rf1 , with the best performance for the present split being obtained by the latter outperforming the latter in both present and absent cases . notably , in both cases , the improvements are statistically significant ( p < 0 . 05 ) with a 95 % bootstrap re - sampling ( conclusion test , fig ."
    },
    {
        "table_id": "361",
        "table_info": {
            "table_caption": "Table 1: Automatic Metrics Evaluation for Different Models.",
            "table_column_names": [
                "Model",
                "BLEU",
                "distinct-1",
                "distinct-2",
                "Avg.length",
                "Stopword",
                "adv succ"
            ],
            "table_content_values": [
                [
                    "Human",
                    "-",
                    "16.8%",
                    "58.1%",
                    "14.2",
                    "69.8%",
                    "[EMPTY]"
                ],
                [
                    "AR",
                    "1.64",
                    "3.7%",
                    "9.5%",
                    "6.4",
                    "82.3%",
                    "2.7%"
                ],
                [
                    "AR+MMI",
                    "2.10",
                    "10.6%",
                    "20.5%",
                    "7.2",
                    "76.4%",
                    "6.3%"
                ],
                [
                    "AR+MMI+diverse",
                    "2.16",
                    "16.0%",
                    "27.3%",
                    "7.5",
                    "72.1%",
                    "6.4%"
                ],
                [
                    "AR+MMI+RL",
                    "2.34",
                    "13.7%",
                    "25.2%",
                    "7.3",
                    "73.0%",
                    "8.0%"
                ],
                [
                    "NonAR",
                    "1.54",
                    "8.9%",
                    "14.6%",
                    "7.1",
                    "77.9%",
                    "2.4%"
                ],
                [
                    "NonAR+MMI",
                    "2.68",
                    "15.9%",
                    "27.0%",
                    "7.4",
                    "71.9%",
                    "9.2%"
                ]
            ]
        },
        "gold_description": "results are shown in table 1 . when comparing ar with ar + mmi , ar + mmi significantly outperforms ar across all metrics , lookahead strategy to estimate for the variants of ar + mmi , ar + mmi + diverse generates a more diverse n - best list for reranking , and thus outperforms ar + mmi ; ar + mmi + rl uses future and thus outperforms backward probability , ar + mmi as well . it ' s hard to tell which model performs better , ar or non - ar : ar performs better than non - ar for bleu and adversarial success , but worse for the other metrics . this means comparing with ar model , non - ar model tends to generate more diverse responses , but might be less coherent . when comparing non - ar with ar + mmi + diverse , non - ar has relatively lower distinct score , but significantly higher scores bleu and adversarial success .",
        "generated_description": "the percentage of stopwords in generated responses is much higher than that of human responses ( e . g . , 13 . 7 % vs . 6 . 3 % ) . this shows that the generated responses are more likely to be end - to - end ( i . e . , non - overlapping ) with respect to the ground truth responses . the diversity of responses is also significantly higher than human responses , indicating that mmi can encourage the generation of diverse responses . although the percentage of distinct - 1 and distinct - 2 of responses generated by mmi + rl is much lower than those of the original models , we hypothesize that this is because mmi encourages the model to generate more diverse responses but also more stopwords ."
    },
    {
        "table_id": "362",
        "table_info": {
            "table_caption": "Table 3: Human judgments for Coherence and Content Richeness of the different models.",
            "table_column_names": [
                "Model",
                "disagr (%)",
                "un(%)",
                "agr(%)"
            ],
            "table_content_values": [
                [
                    "Coherence",
                    "Coherence",
                    "Coherence",
                    "Coherence"
                ],
                [
                    "Human",
                    "17.4",
                    "20.8",
                    "61.8"
                ],
                [
                    "AR",
                    "28.6",
                    "29.5",
                    "41.9"
                ],
                [
                    "AR+MMI",
                    "25.3",
                    "27.9",
                    "46.8"
                ],
                [
                    "AR+MMI+diverse",
                    "24.8",
                    "27.8",
                    "47.4"
                ],
                [
                    "AR+MMI+RL",
                    "24.1",
                    "26.5",
                    "49.4"
                ],
                [
                    "nonAR",
                    "29.9",
                    "28.7",
                    "41.4"
                ],
                [
                    "nonAR+MMI",
                    "23.1",
                    "24.0",
                    "52.9"
                ],
                [
                    "Content Richness",
                    "Content Richness",
                    "Content Richness",
                    "Content Richness"
                ],
                [
                    "Human",
                    "14.0",
                    "16.6",
                    "69.4"
                ],
                [
                    "AR",
                    "38.2",
                    "30.4",
                    "31.4"
                ],
                [
                    "AR+MMI",
                    "30.6",
                    "26.2",
                    "43.2"
                ],
                [
                    "AR+MMI+diverse",
                    "23.9",
                    "21.3",
                    "54.8"
                ],
                [
                    "AR+MMI+RL",
                    "26.4",
                    "24.9",
                    "48.7"
                ],
                [
                    "NonAR",
                    "31.4",
                    "25.0",
                    "44.6"
                ],
                [
                    "NonAR+MMI",
                    "24.2",
                    "20.5",
                    "55.3"
                ]
            ]
        },
        "gold_description": "for dialogue coherence trend is 3 . that nonar + mmi than ar + mmi , followed by ar and non - ar . ar is slightly better than non - ar . for content richness , the is significantly better proposed nonar + mmi than ar + mmi , and the gap is greater than dialogue coherence . the output from the ar + mmi model is thus by far less diverse than nonar + mmi , which obtains the mmi score for each generated token .",
        "generated_description": "first of all , we observe that mmi improves the coherence of the utterances produced by the augmented and non - ar models . the relative improvement of mmi over the human utterances is 17 . 4 % , 20 . 8 % , and 61 . 8 % for nonar , ar , and mmi + diverse , respectively . this shows that diverse and diverse diverse utterances benefit more from mmi than the original utterances in terms of coherence . moreover , the performance of nonar + mmi is much lower than that of ar , which shows that the adversarial training helps more in improving the content richness of the generated utterances than training the baseline models directly ."
    },
    {
        "table_id": "363",
        "table_info": {
            "table_caption": "Table 4: The performances of NonAR+MMI methods on WMT14 En↔De and WMT16 Ro→En. Results from Gu et al. (2018); Lee et al. (2018); Ma et al. (2019) are copied from original papers for reference purposes.",
            "table_column_names": [
                "[EMPTY]",
                "[BOLD] WMT14 En→De",
                "[BOLD] WMT14 De→En",
                "[BOLD] WMT16 Ro→En"
            ],
            "table_content_values": [
                [
                    "NAT (Gu et al.,  2018 )",
                    "17.69",
                    "20.62",
                    "29.79"
                ],
                [
                    "iNAT (Lee et al.,  2018 )",
                    "21.54",
                    "25.43",
                    "29.32"
                ],
                [
                    "FlowSeq-large (raw data) (Ma et al.,  2019 )",
                    "20.85",
                    "25.40",
                    "29.86"
                ],
                [
                    "NAT (our implementation)",
                    "22.32",
                    "24.83",
                    "29.93"
                ],
                [
                    "NAT +MMI",
                    "23.80",
                    "26.05",
                    "30.50"
                ],
                [
                    "[EMPTY]",
                    "(+1.48)",
                    "(+1.22)",
                    "(+0.57)"
                ]
            ]
        },
        "gold_description": "results are shown in table 4 . as can be seen , the incorporation of mmi model significantly improves mt performances .",
        "generated_description": "gu et al . from the table , we observe that our implementation of nat ( our implementation ) achieves 22 . 32 and 24 . 83 bleu score on wmt14 en → de and 26 . 05 and 30 . 50 on de → en and wmt16 ro → en , respectively , which outperforms the previous state - of - the - art models by 1 . 48 and 1 . 22 points , respectively . combining nat with mmi obtains a further improvement of + 0 . 57 and + 1 . 22 on en ↔ de and de ↔ en over the nat model alone , which indicates the effectiveness of mmi in improving the translation quality of rnn - based nat models ."
    },
    {
        "table_id": "364",
        "table_info": {
            "table_caption": "Table 3: Different weighting variations evaluated on the compounds dataset (32,246 nominal compounds). All variations use t=100 transformations, word representations with n=200 dimensions and the dropout rate that was observed to work best on the dev dataset (see Appendix B for details). Results on the 6442 compounds in the test set of the German compounds dataset.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] W param",
                "[BOLD] Cos-d",
                "[BOLD] Q1",
                "[BOLD] Q2",
                "[BOLD] Q3",
                "≤ [BOLD] 5"
            ],
            "table_content_values": [
                [
                    "TransWeight-feat",
                    "[ITALIC] n+ [ITALIC] n",
                    "0.344",
                    "2",
                    "5",
                    "28",
                    "50.82%"
                ],
                [
                    "TransWeight-trans",
                    "[ITALIC] t+ [ITALIC] n",
                    "0.338",
                    "2",
                    "5",
                    "24",
                    "52.90%"
                ],
                [
                    "TransWeight-mat",
                    "[ITALIC] tn+ [ITALIC] n",
                    "0.338",
                    "2",
                    "5",
                    "25",
                    "53.24%"
                ],
                [
                    "[BOLD] TransWeight",
                    "[ITALIC] tn2+ [ITALIC] n",
                    "[BOLD] 0.310",
                    "[BOLD] 1",
                    "[BOLD] 3",
                    "[BOLD] 11",
                    "[BOLD] 65.21%"
                ]
            ]
        },
        "gold_description": "table 3 compares the performance of the four weighting variants introduced in section 3 . 2 . transweight - feat , which sums the transformed representations and then weights each component of the summed representation , has the weakest performance , with only 50 . 82 % of the test compounds receiving a rank that is lower than 5 . a better performance – 52 . 90 % – is obtained by applying the same weighting for each column of the transformations matrix h . the results of transweight - trans are interesting in two respects : first , it outperforms the feature variation , transweight - feat , despite training a smaller number of parameters ( 300 vs . 400 in our setup ) . second , it performs on par with the transweight - mat variation , although the latter has a larger number of parameters ( 20 , 200 in our setup ) . this suggests that an effective combination method needs to take into account full transformations , i . e . entire rows of h and combine them in a systematic way . transweight builds on this insight by making each element of the final composed representation p dependent on each component of the transformed representation h . the result is a noteworthy increase in the quality of the predictions , with ∼ 12 % more of the test representations having a rank ≤ 5 . although this weighting does use significantly more parameters than the previous weightings ( 4 , 000 , 200 parameters ) , the number of parameters is relative to the number of transformations t and does not grow with the size of the vocabulary . as the results in the next subsection show , a relatively small number of transformations is sufficient even for larger training vocabularies .",
        "generated_description": "for the german compounds dataset , we use a nominal compound dataset with a total of 32 , 246 nominal compounds and a training set of 6442 compounds . we observe that transweight - feat , which uses a matrix transformation with t = 100 transformations and word embeddings with n = 200 , works best on the test set . the transweight variants , which use tn2 + n and tn + n , both outperform the variants using matrix transformations , and also the transweight variant using a dropout rate of 0 . 1 . for the romanian compounds , we observe that using a matrix weighting with tn = 3 works best , which is in line with the results on the english compounds dataset ."
    },
    {
        "table_id": "365",
        "table_info": {
            "table_caption": "Table 4: Results for English, German and Dutch on the composition of nominal compounds, adjective-noun phrases and adverb-adjective phrases.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Nominal Compounds  [BOLD] Cos-d",
                "[BOLD] Nominal Compounds  [BOLD] Q1",
                "[BOLD] Nominal Compounds  [BOLD] Q2",
                "[BOLD] Nominal Compounds  [BOLD] Q3",
                "[BOLD] Nominal Compounds ≤ [BOLD] 5",
                "[BOLD] Adjective-Noun Phrases  [BOLD] Cos-d",
                "[BOLD] Adjective-Noun Phrases  [BOLD] Q1",
                "[BOLD] Adjective-Noun Phrases  [BOLD] Q2",
                "[BOLD] Adjective-Noun Phrases  [BOLD] Q3",
                "[BOLD] Adjective-Noun Phrases ≤ [BOLD] 5",
                "[BOLD] Adverb-Adjective Phrases  [BOLD] Cos-d",
                "[BOLD] Adverb-Adjective Phrases  [BOLD] Q1",
                "[BOLD] Adverb-Adjective Phrases  [BOLD] Q2",
                "[BOLD] Adverb-Adjective Phrases  [BOLD] Q3",
                "[BOLD] Adverb-Adjective Phrases ≤ [BOLD] 5"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English",
                    "[BOLD] English"
                ],
                [
                    "Addition",
                    "0.408",
                    "2",
                    "7",
                    "38",
                    "46.14%",
                    "0.431",
                    "2",
                    "7",
                    "32",
                    "44.25%",
                    "0.447",
                    "2",
                    "5",
                    "15",
                    "53.01%"
                ],
                [
                    "SAddition",
                    "0.408",
                    "2",
                    "7",
                    "38",
                    "46.14%",
                    "0.421",
                    "2",
                    "5",
                    "26",
                    "50.95%",
                    "0.420",
                    "1",
                    "3",
                    "8",
                    "67.76%"
                ],
                [
                    "VAddition",
                    "0.403",
                    "2",
                    "6",
                    "33",
                    "47.95%",
                    "0.415",
                    "2",
                    "5",
                    "22",
                    "53.30%",
                    "0.410",
                    "1",
                    "2",
                    "6",
                    "71.94%"
                ],
                [
                    "Matrix",
                    "0.354",
                    "1",
                    "2",
                    "9",
                    "67.37%",
                    "0.365",
                    "1",
                    "2",
                    "6",
                    "74.38%",
                    "0.343",
                    "1",
                    "1",
                    "2",
                    "91.17%"
                ],
                [
                    "WMask+",
                    "0.344",
                    "1",
                    "2",
                    "7",
                    "71.53%",
                    "0.342",
                    "1",
                    "1",
                    "3",
                    "82.67%",
                    "0.335",
                    "1",
                    "1",
                    "2",
                    "93.27%"
                ],
                [
                    "BiLinear",
                    "0.335",
                    "1",
                    "2",
                    "6",
                    "73.63%",
                    "0.332",
                    "1",
                    "1",
                    "3",
                    "85.32%",
                    "0.331",
                    "1",
                    "1",
                    "1",
                    "93.59%"
                ],
                [
                    "FullLex+",
                    "0.338",
                    "1",
                    "2",
                    "7",
                    "72.82%",
                    "0.309",
                    "1",
                    "1",
                    "2",
                    "90.74%",
                    "0.327",
                    "1",
                    "1",
                    "1",
                    "94.28%"
                ],
                [
                    "[BOLD] TransWeight",
                    "[BOLD] 0.323",
                    "[BOLD] 1",
                    "[BOLD] 1",
                    "[BOLD] 4.5",
                    "[BOLD] 77.31%",
                    "[BOLD] 0.307",
                    "[BOLD] 1",
                    "[BOLD] 1",
                    "[BOLD] 2",
                    "[BOLD] 91.39%",
                    "[BOLD] 0.311",
                    "[BOLD] 1",
                    "[BOLD] 1",
                    "[BOLD] 1",
                    "[BOLD] 95.78%"
                ],
                [
                    "[EMPTY]",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German",
                    "[BOLD] German"
                ],
                [
                    "Addition",
                    "0.439",
                    "9",
                    "48",
                    "363",
                    "17.49%",
                    "0.428",
                    "4",
                    "13",
                    "71",
                    "32.95%",
                    "0.500",
                    "4",
                    "19",
                    "215.5",
                    "29.87%"
                ],
                [
                    "SAddition",
                    "0.438",
                    "9",
                    "46",
                    "347",
                    "18.02%",
                    "0.414",
                    "2",
                    "8",
                    "53",
                    "42.80%",
                    "0.473",
                    "2",
                    "7",
                    "99.5",
                    "45.44%"
                ],
                [
                    "VAddition",
                    "0.430",
                    "8",
                    "39",
                    "273",
                    "19.02%",
                    "0.408",
                    "2",
                    "7",
                    "43",
                    "45.14%",
                    "0.461",
                    "2",
                    "5",
                    "52",
                    "51.12%"
                ],
                [
                    "Matrix",
                    "0.363",
                    "3",
                    "8",
                    "45",
                    "41.88%",
                    "0.355",
                    "1",
                    "2",
                    "8",
                    "68.67%",
                    "0.398",
                    "1",
                    "1",
                    "5",
                    "76.41%"
                ],
                [
                    "WMask+",
                    "0.340",
                    "2",
                    "5",
                    "25",
                    "52.05%",
                    "0.332",
                    "1",
                    "2",
                    "5",
                    "77.68%",
                    "0.387",
                    "1",
                    "1",
                    "3",
                    "80.94%"
                ],
                [
                    "BiLinear",
                    "0.339",
                    "2",
                    "5",
                    "26",
                    "53.46%",
                    "0.322",
                    "1",
                    "1",
                    "3",
                    "81.84%",
                    "0.383",
                    "1",
                    "1",
                    "3",
                    "83.02%"
                ],
                [
                    "FullLex+",
                    "0.329",
                    "2",
                    "4",
                    "20",
                    "56.83%",
                    "0.306",
                    "1",
                    "1",
                    "2",
                    "86.29%",
                    "0.383",
                    "1",
                    "1",
                    "3",
                    "83.13%"
                ],
                [
                    "[BOLD] TransWeight",
                    "[BOLD] 0.310",
                    "[BOLD] 1",
                    "[BOLD] 3",
                    "[BOLD] 11",
                    "[BOLD] 65.21%",
                    "[BOLD] 0.297",
                    "[BOLD] 1",
                    "[BOLD] 1",
                    "[BOLD] 2",
                    "[BOLD] 89.28%",
                    "[BOLD] 0.367",
                    "[BOLD] 1",
                    "[BOLD] 1",
                    "[BOLD] 2",
                    "[BOLD] 87.17%"
                ],
                [
                    "[EMPTY]",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch",
                    "[BOLD] Dutch"
                ],
                [
                    "Addition",
                    "0.477",
                    "5",
                    "27",
                    "223.5",
                    "27.74%",
                    "0.476",
                    "3",
                    "13",
                    "87",
                    "35.63%",
                    "0.532",
                    "3",
                    "9",
                    "75",
                    "38.04%"
                ],
                [
                    "SAddition",
                    "0.477",
                    "5",
                    "27",
                    "221",
                    "27.71%",
                    "0.462",
                    "2",
                    "7",
                    "65",
                    "44.95%",
                    "0.503",
                    "2",
                    "4",
                    "34",
                    "55.57%"
                ],
                [
                    "VAddition",
                    "0.470",
                    "4",
                    "22",
                    "177",
                    "29.09%",
                    "0.454",
                    "2",
                    "6",
                    "47",
                    "48.13%",
                    "0.486",
                    "1",
                    "3",
                    "14",
                    "63.18%"
                ],
                [
                    "Matrix",
                    "0.411",
                    "2",
                    "5",
                    "26",
                    "52.19%",
                    "0.394",
                    "1",
                    "2",
                    "6",
                    "74.92%",
                    "0.445",
                    "1",
                    "1",
                    "4",
                    "78.39%"
                ],
                [
                    "WMask+",
                    "0.378",
                    "1",
                    "3",
                    "15",
                    "60.14%",
                    "0.378",
                    "1",
                    "1",
                    "4",
                    "80.78%",
                    "0.429",
                    "1",
                    "1",
                    "2",
                    "83.02%"
                ],
                [
                    "BiLinear",
                    "0.375",
                    "1",
                    "3",
                    "19",
                    "59.23%",
                    "0.375",
                    "1",
                    "1",
                    "3",
                    "81.50%",
                    "0.426",
                    "1",
                    "1",
                    "2",
                    "83.57%"
                ],
                [
                    "FullLex+",
                    "0.388",
                    "1",
                    "3",
                    "14",
                    "60.84%",
                    "0.362",
                    "1",
                    "1",
                    "2",
                    "85.24%",
                    "0.433",
                    "1",
                    "1",
                    "3",
                    "82.36%"
                ],
                [
                    "[BOLD] TransWeight",
                    "[BOLD] 0.376",
                    "[BOLD] 1",
                    "[BOLD] 2",
                    "[BOLD] 11",
                    "[BOLD] 66.61%",
                    "[BOLD] 0.349",
                    "[BOLD] 1",
                    "[BOLD] 1",
                    "[BOLD] 2",
                    "[BOLD] 88.55%",
                    "[BOLD] 0.423",
                    "[BOLD] 1",
                    "[BOLD] 1",
                    "[BOLD] 2",
                    "[BOLD] 84.01%"
                ]
            ]
        },
        "gold_description": "the results using the corrected rank eval transweight , the composition model proposed in this paper , delivers consistent results , being the best performing model across all languages and phrase types . the difference in performance to the runner - up model , fulllex + , translates into more of the test phrases being close to the original repachieving a rank ≤ 5 . this resentations , i . e . difference ranges from 8 % of the test phrases in the german compounds dataset to less than 1 % for english adjective - noun phrases . however , it is important to note the substantial difference in the number of parameters used by the two models : all transweight models use 100 transformations and have , therefore , a constant number of 12 , 020 , 200 parameters . in contrast the number of parameters used by fulllex + increases with the size of the training vocabulary , reaching 739 , 320 , 200 parameters in the case of the english adjective - noun dataset . the most difficult task for all the composition models in any of the three languages is compound composition . we believe this difficulty can be mainly attributed to the complexity introduced by the position . for example in adjective - noun composition , the adjective always takes the first position , and the noun the second . however , in compounds the same noun can occur in both positions throughout different training examples . consider for example the compounds boat house and house boat . in boat house – a house to store boats – the meaning of house is shifted towards shelter for an inanimate object , whereas house boat selects from house aspects related to human beings and their daily lives happening on the boat . these positionrelated differences can make it more challenging to create composed representations . that makes adverbadjective easier is the high dataset frequency of some of the adverbs / adjectives . for example , in the english and adjective - noun datasets another aspect the adjective - noun dataset a small subset of 52",
        "generated_description": "observations regarding the qualitative analysis of the extracted lexical units , we observe that the cosine similarity between english and german nominal compounds is higher for the first compound type ( q1 – q3 ) than for the second one ( q2 – q4 ) . this is due to the fact that german and english have very different lexical relations , i . e . , german is more similar to english than english is to german and german to english . in addition , the quantity of nominal compounds in our extracted lexicons is higher than that of the other lexical types , so the lexical quantity extracted from the training corpus is larger than that extracted lexically from the english training corpus . this phenomenon is observed in the case of the adjective - noun and adverb - adjective phrases types , where the quantity in the training set is much larger than in the english lexicon . the reason is that these types of phrases tend to include more words that are used in the target language , e . g . , “ favour ” and “ wanting ” , which are used to refer to the purchase of stuff ."
    },
    {
        "table_id": "366",
        "table_info": {
            "table_caption": "Table 1: Results on DDI 2013",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] P",
                "[BOLD] R",
                "[BOLD] F1"
            ],
            "table_content_values": [
                [
                    "[ITALIC] CNN",
                    "[ITALIC] CNN",
                    "[ITALIC] CNN",
                    "[ITALIC] CNN"
                ],
                [
                    "+  [ITALIC] ENT-ONLY",
                    "52.7",
                    "43.1",
                    "47.4"
                ],
                [
                    "+  [ITALIC] ENT-SENT",
                    "75.8",
                    "60.7",
                    "67.3"
                ],
                [
                    "+  [ITALIC] ENT-DYM",
                    "66.5",
                    "70.6",
                    "68.5"
                ],
                [
                    "+  [ITALIC] ENT-DEP0",
                    "59.8",
                    "61.5",
                    "60.6"
                ],
                [
                    "+  [ITALIC] ENT-DEP1",
                    "67.6",
                    "65.1",
                    "66.3"
                ],
                [
                    "[ITALIC] BiLSTM",
                    "[ITALIC] BiLSTM",
                    "[ITALIC] BiLSTM",
                    "[ITALIC] BiLSTM"
                ],
                [
                    "+  [ITALIC] ENT-ONLY",
                    "74.0",
                    "69.4",
                    "71.6"
                ],
                [
                    "+  [ITALIC] ENT-SENT",
                    "74.8",
                    "71.7",
                    "73.1"
                ],
                [
                    "+  [ITALIC] ENT-DYM",
                    "71.5",
                    "73.4",
                    "72.4"
                ],
                [
                    "+  [ITALIC] ENT-DEP0",
                    "72.8",
                    "69.4",
                    "71.1"
                ],
                [
                    "+  [ITALIC] ENT-DEP1",
                    "71.6",
                    "76.4",
                    "[BOLD] 73.9"
                ],
                [
                    "[ITALIC] BiLSTM-CNN",
                    "[ITALIC] BiLSTM-CNN",
                    "[ITALIC] BiLSTM-CNN",
                    "[ITALIC] BiLSTM-CNN"
                ],
                [
                    "+  [ITALIC] ENT-ONLY",
                    "69.6",
                    "72.3",
                    "70.9"
                ],
                [
                    "+  [ITALIC] ENT-SENT",
                    "69.4",
                    "74.9",
                    "72.0"
                ],
                [
                    "+  [ITALIC] ENT-DYM",
                    "71.0",
                    "69.7",
                    "71.8"
                ],
                [
                    "+  [ITALIC] ENT-DEP0",
                    "72.2",
                    "69.5",
                    "70.8"
                ],
                [
                    "+  [ITALIC] ENT-DEP1",
                    "71.0",
                    "74.3",
                    "72.6"
                ],
                [
                    "[ITALIC] BiLSTM-GCNN",
                    "[ITALIC] BiLSTM-GCNN",
                    "[ITALIC] BiLSTM-GCNN",
                    "[ITALIC] BiLSTM-GCNN"
                ],
                [
                    "+  [ITALIC] ENT-ONLY",
                    "69.3",
                    "71.4",
                    "70.4"
                ],
                [
                    "+  [ITALIC] ENT-SENT",
                    "72.2",
                    "71.9",
                    "72.0"
                ],
                [
                    "+  [ITALIC] ENT-DYM",
                    "69.7",
                    "73.9",
                    "71.7"
                ],
                [
                    "+  [ITALIC] ENT-DEP0",
                    "70.1",
                    "71.1",
                    "70.6"
                ],
                [
                    "+  [ITALIC] ENT-DEP1",
                    "72.7",
                    "72.9",
                    "72.8"
                ]
            ]
        },
        "gold_description": "1 . comparing ent - sent , ent - dym and ent - only , we see that the pooling methods over the whole sentence ( i . e . , ent - sent and ent - dym ) are significantly better than ent - only that only focuses on the two entity mentions of interest in the ddi - 2013 dataset . this is true across different deep learning models in this work . comparing ent - sent and ent - dym , their performance are comparable in ddi - 2013 ( except for cnn where ent - dym is better ) . comparing the syntax - based pooling methods and the non - syntax pooling methods , the pooling based on dependency paths ( i . e . , ent - dep0 ) is worse than the non - syntax pooling methods ( i . e . , ent - sent and ent - dym ) and perform comparably with ent - only in the ddi - 2013 dataset over all the models ( except for the cnn model where ent - only is much worse ) .",
        "generated_description": "for the cnn models , we can see that the addition of ent - sent and ent - dym significantly improves the performance in both bilstm and cnn models . however , the performance of cnn models is slightly worse than that of bilaser models . we conjecture that this is due to the fact that cnn models can capture more fine - grained entity - level features , such as the presence or absence of specific entity tags in a sentence , and thus tend to overfit on the training data . on the other hand , bert models suffer from the data sparsity issue ."
    },
    {
        "table_id": "367",
        "table_info": {
            "table_caption": "Table 2: Results of BERTbase in test set of five datasets with different epochs. The SOTA results are from Madabushi et al. (2018) (TrecQA), Sha et al. (2018) (WikiQA, SemEvalcQA-16), Tay et al. (2018b) (YahooQA), Nakov et al. (2017) (SemEvalcQA-17).",
            "table_column_names": [
                "[EMPTY]",
                "TrecQA MRR",
                "TrecQA MAP",
                "WikiQA MRR",
                "WikiQA MAP",
                "YahooQA MRR",
                "YahooQA MAP",
                "SemEvalcQA-16 MRR",
                "SemEvalcQA-16 MAP",
                "SemEvalcQA-17 MRR",
                "SemEvalcQA-17 MAP"
            ],
            "table_content_values": [
                [
                    "epoch=3",
                    "0.927",
                    "0.877",
                    "0.770",
                    "0.753",
                    "0.942",
                    "0.942",
                    "0.872",
                    "0.810",
                    "0.951",
                    "0.909"
                ],
                [
                    "epoch=5",
                    "0.944",
                    "0.883",
                    "0.784",
                    "0.769",
                    "0.942",
                    "0.942",
                    "0.890",
                    "0.816",
                    "0.953",
                    "0.908"
                ],
                [
                    "SOTA",
                    "0.865",
                    "0.904",
                    "0.758",
                    "0.746",
                    "-",
                    "0.801",
                    "0.872",
                    "0.801",
                    "0.926",
                    "0.887"
                ]
            ]
        },
        "gold_description": "we show the main result in table 2 and 3 . despite training on a fraction of the data available , the proposed bert - based models surpass the previous state - of - the - art models by a large margin on all datasets",
        "generated_description": "we observe that bertbase with epoch = 3 performs the best on trecqa and wikiqa , while it performs the worst on the other four datasets . we believe that the reason is that the training data of semevalcqa is relatively small and the model needs to be trained for long epochs to get good performance . we also observe that with the increase of the number of epochs , the map and mrr scores of the model get improved . however , the mrr and map scores do not follow a consistent trend . for example , at the epoch of 3 , the model is able to achieve mrr of 0 . 877 and 0 . 942 , but the map score is 0 . 872 , which is worse than the one obtained by the model with epochs of 3 . in addition , we observe that the map scores at the end of training also get improved , i . e . , when the epochs are close to the end ( e . g . , from 3 to 5 ) , but the result is still worse than that obtained with the whole epochs . we conjecture that the problem of overfitting is more severe when training the model for long ."
    },
    {
        "table_id": "368",
        "table_info": {
            "table_caption": "Table 2: Results on BioNLP BB3",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] P",
                "[BOLD] R",
                "[BOLD] F1"
            ],
            "table_content_values": [
                [
                    "[ITALIC] CNN",
                    "[ITALIC] CNN",
                    "[ITALIC] CNN",
                    "[ITALIC] CNN"
                ],
                [
                    "+  [ITALIC] ENT-ONLY",
                    "54.2",
                    "65.7",
                    "59.1"
                ],
                [
                    "+  [ITALIC] ENT-SENT",
                    "55.0",
                    "62.5",
                    "59.1"
                ],
                [
                    "+  [ITALIC] ENT-DYM",
                    "54.6",
                    "53.3",
                    "53.5"
                ],
                [
                    "+  [ITALIC] ENT-DEP0",
                    "55.9",
                    "65.8",
                    "60.6"
                ],
                [
                    "+  [ITALIC] ENT-DEP1",
                    "55.7",
                    "67.7",
                    "61.1"
                ],
                [
                    "[ITALIC] BiLSTM",
                    "[ITALIC] BiLSTM",
                    "[ITALIC] BiLSTM",
                    "[ITALIC] BiLSTM"
                ],
                [
                    "+  [ITALIC] ENT-ONLY",
                    "58.9",
                    "59.6",
                    "59.2"
                ],
                [
                    "+  [ITALIC] ENT-SENT",
                    "60.7",
                    "59.2",
                    "59.9"
                ],
                [
                    "+  [ITALIC] ENT-DYM",
                    "50.2",
                    "66.0",
                    "56.9"
                ],
                [
                    "+  [ITALIC] ENT-DEP0",
                    "51.6",
                    "78.0",
                    "61.9"
                ],
                [
                    "+  [ITALIC] ENT-DEP1",
                    "54.7",
                    "72.6",
                    "62.4"
                ],
                [
                    "[ITALIC] BiLSTM-CNN",
                    "[ITALIC] BiLSTM-CNN",
                    "[ITALIC] BiLSTM-CNN",
                    "[ITALIC] BiLSTM-CNN"
                ],
                [
                    "+  [ITALIC] ENT-ONLY",
                    "56.4",
                    "66.2",
                    "60.8"
                ],
                [
                    "+  [ITALIC] ENT-SENT",
                    "53.6",
                    "69.2",
                    "60.5"
                ],
                [
                    "+  [ITALIC] ENT-DYM",
                    "47.1",
                    "78.0",
                    "58.7"
                ],
                [
                    "+  [ITALIC] ENT-DEP0",
                    "55.9",
                    "71.4",
                    "[BOLD] 62.5"
                ],
                [
                    "+  [ITALIC] ENT-DEP1",
                    "54.1",
                    "74.7",
                    "62.4"
                ],
                [
                    "[ITALIC] BiLSTM-GCNN",
                    "[ITALIC] BiLSTM-GCNN",
                    "[ITALIC] BiLSTM-GCNN",
                    "[ITALIC] BiLSTM-GCNN"
                ],
                [
                    "+  [ITALIC] ENT-ONLY",
                    "62.7",
                    "56.1",
                    "58.9"
                ],
                [
                    "+  [ITALIC] ENT-SENT",
                    "58.4",
                    "58.7",
                    "58.5"
                ],
                [
                    "+  [ITALIC] ENT-DYM",
                    "56.8",
                    "58.4",
                    "56.6"
                ],
                [
                    "+  [ITALIC] ENT-DEP0",
                    "55.6",
                    "67.4",
                    "60.8"
                ],
                [
                    "+  [ITALIC] ENT-DEP1",
                    "54.4",
                    "71.1",
                    "61.5"
                ]
            ]
        },
        "gold_description": "however , this comparison is reversed for the bb3 dataset where ent - only is in general better or comparable to ent - sent and ent - dym over different deep learning models . in the bb3 dataset , ent - sent singificantly outperforms ent - dym over all the models . when we switch to the bb3 dataset , it turns out thatent - dep0 is significantly better than all the non - syntax pooling methods ( i . e . , ent - only , ent - sent and ent - dym ) for all the comparing models .",
        "generated_description": "for the cnn models , adding the ent - only and ent - sent vectors improves the f1 score from 59 . 1 % to 59 . 2 % for the bilstm models . adding the same set of additional vectors from the original ent - dep1 and dep0 embeddings does not improve the performance further . the reason may be that the embedding space of the first sentence of a news article is much smaller than that of the whole article , and thus more training data is needed to learn the compositionality of the sentences ."
    },
    {
        "table_id": "369",
        "table_info": {
            "table_caption": "Table 2: Statistics for different OPIEC corpora. All frequencies are in millions. We count triples with annotations (not annotations directly). Percentages refer to the respective subcorpus.",
            "table_column_names": [
                "Total triples (millions)",
                "OPIEC 341.0",
                "OPIEC",
                "OPIEC-Clean 104.0",
                "OPIEC-Clean",
                "OPIEC-Linked 5.8",
                "OPIEC-Linked"
            ],
            "table_content_values": [
                [
                    "Triples with semantic annotations",
                    "166.3",
                    "(49%)",
                    "51.46",
                    "(49%)",
                    "3.37",
                    "(58%)"
                ],
                [
                    "negative polarity",
                    "5.3",
                    "(2%)",
                    "1.33",
                    "(1%)",
                    "0.01",
                    "(0%)"
                ],
                [
                    "possibility modality",
                    "13.9",
                    "(4%)",
                    "3.27",
                    "(3%)",
                    "0.04",
                    "(1%)"
                ],
                [
                    "quantities",
                    "59.4",
                    "(17%)",
                    "15.91",
                    "(15%)",
                    "0.45",
                    "(8%)"
                ],
                [
                    "attribution",
                    "6.4",
                    "(2%)",
                    "1.44",
                    "(1%)",
                    "0.01",
                    "(0%)"
                ],
                [
                    "time",
                    "65.3",
                    "(19%)",
                    "19.66",
                    "(19%)",
                    "0.58",
                    "(1%)"
                ],
                [
                    "space",
                    "61.5",
                    "(18%)",
                    "22.11",
                    "(21%)",
                    "2.64",
                    "(45%)"
                ],
                [
                    "space OR time",
                    "111.3",
                    "(33%)",
                    "37.22",
                    "(36%)",
                    "3.01",
                    "(52%)"
                ],
                [
                    "space AND time",
                    "15.4",
                    "(5%)",
                    "4.54",
                    "(4%)",
                    "0.20",
                    "(4%)"
                ],
                [
                    "Triple length in tokens ( [ITALIC] μ± [ITALIC] σ)",
                    "7.66±4.25",
                    "7.66±4.25",
                    "6.06±2.82",
                    "6.06±2.82",
                    "6.45±2.65",
                    "6.45±2.65"
                ],
                [
                    "subject ( [ITALIC] μ± [ITALIC] σ)",
                    "2.12±2.12",
                    "2.12±2.12",
                    "1.48±0.79",
                    "1.48±0.79",
                    "1.92±0.94",
                    "1.92±0.94"
                ],
                [
                    "relation ( [ITALIC] μ± [ITALIC] σ)",
                    "3.01±2.47",
                    "3.01±2.47",
                    "3.10±2.56",
                    "3.10±2.56",
                    "2.77±2.14",
                    "2.77±2.14"
                ],
                [
                    "object ( [ITALIC] μ± [ITALIC] σ)",
                    "2.52±2.69",
                    "2.52±2.69",
                    "1.48±0.79",
                    "1.48±0.79",
                    "1.76±0.94",
                    "1.76±0.94"
                ],
                [
                    "Confidence score ( [ITALIC] μ± [ITALIC] σ)",
                    "0.53±0.23",
                    "0.53±0.23",
                    "0.59±0.23",
                    "0.59±0.23",
                    "0.61±0.26",
                    "0.61±0.26"
                ]
            ]
        },
        "gold_description": "basic statistics such as corpus sizes , frequency of various semantic annotations , and information about the length of the extracted triples of opiec and its subcorpora are shown in tab . 2 . roughly 30 % of the triples ( 104m ) in opiec are clean according to the above constraints . table 2 shows that clean triples are generally shorter on average and tend to have a higher confidence score than the full set of triples in opiec . about 49 % of all triples in opiec contain some sort of semantic annotation ( cf . tab . 2 ) ; in opiec - linked , the fraction increases to 58 % . most of the semantic annotations referred to quantities , space or time ; these annotations provide important context for the extractions . there is a significantly smaller amount of negative polarity and possibility modality annotations . one reason for the lack of such annotations may be in the nature of the wikipedia articles , which aim to contain encyclopedic , factual statements and are thus more rarely negated or hedged .",
        "generated_description": "semantic annotations are used to label approximately half of the triples ( 166 . 3 % ) in the original opiec dataset , and the proportion is even higher ( 51 . 46 % ) if we compare the opiec - clean dataset with its version ( 0 . 37 % ) . the proportion of triples with semantic annotations is higher ( 58 % ) for negative polarity and 51 . 46 % for possibility modality , and between 15 . 91 % and 22 . 11 % for space and time ( space and time are the most frequent labels with 18 % and 19 % , respectively ) ."
    },
    {
        "table_id": "370",
        "table_info": {
            "table_caption": "Table 4: The most frequent open relations aligned to the DBpedia relations location, associatedMusicalArtist, and spouse in OPEIC-Linked",
            "table_column_names": [
                "location  [ITALIC] “be in”",
                "location (43,842)",
                "associatedMusicalArtist  [ITALIC] “be”",
                "associatedMusicalArtist (6,273)",
                "spouse  [ITALIC] “be wife of”",
                "spouse (1,965)"
            ],
            "table_content_values": [
                [
                    "[ITALIC] “have”",
                    "(3,175)",
                    "[ITALIC] “have”",
                    "(3,600)",
                    "[ITALIC] “be”",
                    "(1,308)"
                ],
                [
                    "[ITALIC] “be”",
                    "(1,901)",
                    "[ITALIC] “be member of”",
                    "(740)",
                    "[ITALIC] “marry”",
                    "(702)"
                ],
                [
                    "[ITALIC] “be at”",
                    "(1,109)",
                    "[ITALIC] “be guitarist of”",
                    "(703)",
                    "[ITALIC] “be widow of”",
                    "(479)"
                ],
                [
                    "[ITALIC] “be of”",
                    "(706)",
                    "[ITALIC] “be drummer of”",
                    "(458)",
                    "[ITALIC] “have”",
                    "(298)"
                ],
                [
                    "[ITALIC] “be historic home",
                    "(491)",
                    "[ITALIC] “be feature”",
                    "(416)",
                    "[ITALIC] “be husband of”",
                    "(284)"
                ],
                [
                    "[ITALIC] located at”",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ]
            ]
        },
        "gold_description": "tab . 4 shows the most frequent open relations aligned to the dbpedia relations location , associatedmusicalartist , and spouse . the frequencies correspond to the number of oie triples that ( 1 ) have the specified open relation ( e . g . , \" be wife of \" ) and ( 2 ) have a kb hit with the specified kb relation ( e . g . , spouse ) . there is clearly no 1 : 1 correspondence between open relations and kb relations . on the one hand , open relations can be highly ambiguous ( e . g . , \" be \" has hits to location and associatedmusicalartits ) . on the other hand , open relations can also be more specific than kb relations ( e . g . , \" be guitarist of \" is more specific than associatedmusicalartist ) or semantically different ( e . g . , \" be widow of \" and spouse ) than the kb relations they align to .",
        "generated_description": "for example , the term “ be in ” has a large number of occurrences ( 43 , 842 ) in the associatedmusicalartist data set , and the corresponding words associated with this term have a similar number of instances in the spouse data set ( 1 , 965 ) . this is due to the fact that many of the terms have strong senses , such as strong possessive and possessive concatenations . for instance , the word spouse has a strong sense of association with the term spouse . another example is the term band of , where the first sense is band and the second and third senses are band and band , respectively . the first sense of band is band ( “ be at ” ) and the third and fourth sense are band , where band members are identified as musical artist and band members . the confusion matrix for band of is shown in fig . we can see that some of the words have strong semantic associations with the concept of being in , e . g . , “ jointly ” , “ part - of ” and “ frequent ” . in some cases , the context of the phrase is ambiguous . for example the word band of has the sense “ music ” is not in the data set at all , but the second sense of the term is band ."
    },
    {
        "table_id": "371",
        "table_info": {
            "table_caption": "Table 2: Results with frame filtering",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] FrameNet 1.5  [BOLD] All",
                "[BOLD] FrameNet 1.5  [BOLD] Ambiguous",
                "[BOLD] FrameNet 1.7  [BOLD] All",
                "[BOLD] FrameNet 1.7  [BOLD] Ambiguous"
            ],
            "table_content_values": [
                [
                    "[ITALIC] Das’s Test Set ",
                    "[ITALIC] Das’s Test Set ",
                    "[ITALIC] Das’s Test Set ",
                    "[ITALIC] Das’s Test Set ",
                    "[ITALIC] Das’s Test Set "
                ],
                [
                    "SEMAFOR ",
                    "83.60",
                    "69.19",
                    "-",
                    "-"
                ],
                [
                    "Hermann et al. ",
                    "88.73",
                    "73.67",
                    "-",
                    "-"
                ],
                [
                    "Yang and Mitchell ",
                    "88.20",
                    "75.70",
                    "-",
                    "-"
                ],
                [
                    "Hartmann et al. ",
                    "87.63",
                    "73.80",
                    "-",
                    "-"
                ],
                [
                    "Botschen et al. ",
                    "88.82",
                    "75.28",
                    "-",
                    "-"
                ],
                [
                    "Peng et al. ",
                    "90.00",
                    "78.00",
                    "89.10",
                    "77.50"
                ],
                [
                    "[BOLD] PAFIBERT - Filtered by LUs",
                    "[BOLD] 92.22",
                    "[BOLD] 82.90",
                    "[BOLD] 91.44",
                    "[BOLD] 82.55"
                ],
                [
                    "[BOLD] PAFIBERT - Filtered by Targets",
                    "[BOLD] 91.39",
                    "[BOLD] 82.80",
                    "[BOLD] 90.15",
                    "[BOLD] 81.92"
                ],
                [
                    "[ITALIC] YAGS Test Set ",
                    "[ITALIC] YAGS Test Set ",
                    "[ITALIC] YAGS Test Set ",
                    "[ITALIC] YAGS Test Set ",
                    "[ITALIC] YAGS Test Set "
                ],
                [
                    "SEMAFOR (Reported by )",
                    "60.01",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "Hartmann et al. ",
                    "62.51",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "[BOLD] PAFIBERT - Filtered by LUs",
                    "[BOLD] 75.06",
                    "[BOLD] 69.07",
                    "[BOLD] -",
                    "[BOLD] -"
                ],
                [
                    "[BOLD] PAFIBERT - Filtered by Targets",
                    "[BOLD] 75.01",
                    "[BOLD] 69.07",
                    "[BOLD] -",
                    "[BOLD] -"
                ]
            ]
        },
        "gold_description": "table 2 and 3 compare the accuracies of our models to the results reported by several representative studies in frame - semantic parsing . as can be seen from the tables , pafibert outperformed other models on both the in - domain das ' s test set and the out - of - domain yags test set , yielding new state - of - the - art results for frame identification . as expected , the results obtained with lu - based frame filtering were slightly better than those produced using target - based frame filtering , suggesting that a realistic setup is crucial in measuring model performance more precisely . on das ' s test set , for example , pafibert achieved absolute improvements of 4 - 5 % and 9 % with frame filtering ( table 2 ) and without frame filtering ( table 3 ) respectively . as reported by , frame identification models usually suffer a drastic drop in performance when tested on out - of - domain data . the same trend was observed in our experiments for the yags test set . nevertheless , pafibert still outperformed existing methods by a large margin on this out - of - domain test set . besides , in the more challenging setup that involved no frame filtering , the results obtained by pafibert were on par with the prior state - of - the - art accuracies achieved with frame filtering .",
        "generated_description": "from the table , we observe that pafibert - filtered by lus outperforms the previous state - of - the - art by a large margin on the framenet 1 . 5 test set and achieves the best overall performance across all the three types of ambiguous triples . on the other hand , our model ’ s performance on the ambiguous triples test set is competitive with the best previously published results ."
    },
    {
        "table_id": "372",
        "table_info": {
            "table_caption": "Table 3: Results of BERTbase and BERTlarge in test set of five datasets. The number of training epochs is 3.",
            "table_column_names": [
                "[EMPTY]",
                "TrecQA base",
                "TrecQA large",
                "WikiQA base",
                "WikiQA large",
                "YahooQA base",
                "YahooQA large",
                "SemEvalcQA-16 base",
                "SemEvalcQA-16 large",
                "SemEvalcQA-17 base",
                "SemEvalcQA-17 large"
            ],
            "table_content_values": [
                [
                    "MRR",
                    "0.927",
                    "[BOLD] 0.961",
                    "0.770",
                    "[BOLD] 0.875",
                    "[BOLD] 0.942",
                    "0.938",
                    "0.872",
                    "[BOLD] 0.911",
                    "0.951",
                    "[BOLD] 0.958"
                ],
                [
                    "MAP",
                    "0.877",
                    "[BOLD] 0.904",
                    "0.753",
                    "[BOLD] 0.860",
                    "[BOLD] 0.942",
                    "0.938",
                    "0.810",
                    "[BOLD] 0.844",
                    "[BOLD] 0.909",
                    "0.907"
                ]
            ]
        },
        "gold_description": "we show the main result in table 2 and 3 . despite training on a fraction of the data available , the proposed bert - based models surpass the previous state - of - the - art models by a large margin on all datasets",
        "generated_description": "in this section , we evaluate the performance of bertbase and bertlarge on the trecqa and wikiqa datasets . we can see that in terms of mrr , map and f1 score , bert achieves the best performance on all datasets . in addition , we also observe that the map score is improved by a large margin in the semevalcrca - 17 large dataset , which shows that the large dataset is more challenging than the smaller datasets for bert ."
    },
    {
        "table_id": "373",
        "table_info": {
            "table_caption": "Table 6: Hyperparameter tuning methods with original pre-processing and fixed CRCNN model. Test set results at the top with cross validated results (average with standard deviation) below. All cross validated results are statistically significant compared to Default with p<0.05 except those marked with a •. Note that hyperparameter tuning can involve much higher performance variation depending on the distribution of the data. Therefore, even though there is no statistical significance in the manual search case for the held out fold in the ddi dataset, there was statistical significance for the dev fold which drove those set of hyperparameters. For both ddi and i2b2 datasets, manual search is better than random search with p<0.05.",
            "table_column_names": [
                "[width=13em]Hyperparam TuningDataset",
                "semeval",
                "ddi Class",
                "ddi Detect",
                "i2b2 Class",
                "i2b2 Detect"
            ],
            "table_content_values": [
                [
                    "Default",
                    "81.55",
                    "62.55",
                    "80.29",
                    "55.15",
                    "81.98"
                ],
                [
                    "Default",
                    "80.85 (1.31)",
                    "81.62 (1.35)",
                    "87.76 (1.03)",
                    "67.28 (1.83)",
                    "86.57 (0.58)"
                ],
                [
                    "Manual Search",
                    "-",
                    "[BOLD] 65.53",
                    "[BOLD] 81.74",
                    "[BOLD] 59.75",
                    "[BOLD] 83.17"
                ],
                [
                    "Manual Search",
                    "[EMPTY]",
                    "82.23 (0.32)•",
                    "88.40 (0.48)•",
                    "70.10 (0.85)",
                    "86.45 (0.58)•"
                ],
                [
                    "Random Search",
                    "[BOLD] 82.2",
                    "62.29",
                    "79.04",
                    "55.0",
                    "80.77"
                ],
                [
                    "Random Search",
                    "81.10 (1.26)•",
                    "75.43 (1.48)",
                    "83.54 (0.60)",
                    "60.66 (1.43)",
                    "82.73 (0.49)"
                ]
            ]
        },
        "gold_description": "table 6 , where we compare hyperparameter tuning methodologies . perturbations on the hyperparameter search are listed in table 6 and compare performance with different hyperparameter values found using different tuning strategies . we tested how manual tuning , requiring less expert knowledge than bayesian optimization , would compare to the random search strategy in table 6 . for both i2b2 and ddi corpora , manual search outperformed random search .",
        "generated_description": "hyperparameter tuning we conduct hyperparameter tuning experiments on two datasets , i2b2 and semeval , to evaluate the performance of the proposed method . for each dataset , we tune hyperparameters using 10 - fold cross validation on the development set . we report the results on the test set of each dataset . we find that both the default and the manual search methods achieve comparable performance on the ddi dataset . however , when using the random search method , the performance is significantly lower than that of the other methods . the reason is that the dictionary - based method ( random search ) tends to assign high probabilities to false positives and false negatives , which will increase the difficulty of classifying the true positives . on the other hand , with the help of the multi - task learning method ( manual search ) , the accuracy of the classifier is significantly improved , which indicates that the task of detecting di - and i - b2 triples is more difficult than detecting true positives and vice versa ."
    },
    {
        "table_id": "374",
        "table_info": {
            "table_caption": "Table 4: Pre-processing techniques with CRCNN model. Row labels Original = simple tokenization and lower casing of words, Punct = punctuation removal, Digit = digit removal and Stop = stop word removal. Test set results at the top with cross validated results (average with standard deviation) below. All cross validated results are statistically significant compared to Original pre-processing (p<0.05) using a paired t-test except those marked with a •",
            "table_column_names": [
                "[width=10em]PreprocessDataset",
                "semeval",
                "ddi Class",
                "ddi Detect",
                "i2b2 Class",
                "i2b2 Detect"
            ],
            "table_content_values": [
                [
                    "Original",
                    "[BOLD] 81.55",
                    "65.53",
                    "81.74",
                    "59.75",
                    "83.17"
                ],
                [
                    "Original",
                    "80.85 (1.31)",
                    "82.23 (0.32)",
                    "88.40 (0.48)",
                    "70.10 (0.85)",
                    "86.45 (0.58)"
                ],
                [
                    "Entity Blinding",
                    "72.73",
                    "[BOLD] 67.02",
                    "[BOLD] 82.37",
                    "[BOLD] 68.76",
                    "[BOLD] 84.37"
                ],
                [
                    "Entity Blinding",
                    "71.31 (1.14)",
                    "83.56 (2.05)•",
                    "89.45 (1.05)•",
                    "76.59 (1.07)",
                    "88.41 (0.37)"
                ],
                [
                    "Punct and Digit",
                    "81.23",
                    "63.41",
                    "80.49",
                    "58.85",
                    "81.96"
                ],
                [
                    "Punct and Digit",
                    "80.95 (1.21)•",
                    "80.44 (1.77)",
                    "87.52 (0.98)",
                    "69.37 (1.43)•",
                    "85.82 (0.43)"
                ],
                [
                    "Punct, Digit and Stop",
                    "72.92",
                    "55.87",
                    "76.57",
                    "56.19",
                    "80.47"
                ],
                [
                    "Punct, Digit and Stop",
                    "71.61 (1.25)",
                    "78.52 (1.99)",
                    "85.65 (1.21)",
                    "68.14 (2.05)•",
                    "84.84 (0.77)"
                ],
                [
                    "NER Blinding",
                    "81.63",
                    "57.22",
                    "79.03",
                    "50.41",
                    "81.61"
                ],
                [
                    "NER Blinding",
                    "80.85 (1.07)•",
                    "78.06 (1.45)",
                    "86.79 (0.65)",
                    "66.26 (2.44)",
                    "86.72 (0.57)•"
                ]
            ]
        },
        "gold_description": "importance of pre - processing in performance improvements . experiments in table 4 reveal that they can cause larger variations in performance than modeling . punctuation and digits hold more importance for the ddi dataset , which is a biomedical dataset , compared to the other two datasets . we found that stop words seem to be important for relation extraction for all three datasets that we looked at , to a smaller degree for i2b2 compared to the other two datasets . entity blinding causes almost 9 % improvement in classification performance and 1 % improvement in detection performance . while entity blinding hurts performance for semeval , possibly due to the coarse - grained nature of the replacement , ner blinding does not hurt performance . entity blinding seems to help test set performance for ddi in table 4 , but shows no statistical significance .",
        "generated_description": "due to space limitation , we only show results on the digit and punct and digit datasets in the table . we can observe that the performance of the classifiers on the original and detection datasets are much lower than those on the semeval and i2b2 datasets . the reason is that the classifier has a hard time to distinguish the true and false positives in the original dataset , and thus it assigns a high probability to false positives . on the other hand , the detection and classifier datasets have a much easier time in distinguishing true from false positives , because the detection accuracy is much higher than the classifier accuracy on these two datasets ."
    },
    {
        "table_id": "375",
        "table_info": {
            "table_caption": "Table 5: Modeling techniques with original pre-processing. Test set results at the top with cross validated results (average with standard deviation) below. All cross validated results are statistically significant compared to CRCNN model (p<0.05) using a paired t-test except those marked with a •. In terms of statistical significance, comparing contextualized embeddings with each other reveals that BERT-tokens is equivalent to ELMo for i2b2, but for semeval BERT-tokens is better than ELMo and for ddi BERT-tokens is better than ELMo only for detection.",
            "table_column_names": [
                "[width=10em]ModelingDataset",
                "semeval",
                "ddi Class",
                "ddi Detect",
                "i2b2 Class",
                "i2b2 Detect"
            ],
            "table_content_values": [
                [
                    "CRCNN",
                    "81.55",
                    "65.53",
                    "81.74",
                    "59.75",
                    "83.17"
                ],
                [
                    "CRCNN",
                    "80.85 (1.31)",
                    "82.23 (0.32)",
                    "88.40 (0.48)",
                    "70.10 (0.85)",
                    "86.45 (0.58)"
                ],
                [
                    "Piecewise pool",
                    "81.59",
                    "63.01",
                    "80.62",
                    "60.85",
                    "83.69"
                ],
                [
                    "Piecewise pool",
                    "80.55 (0.99)•",
                    "81.99 (0.38)•",
                    "88.47 (0.48)•",
                    "73.79 (0.97)",
                    "89.29 (0.61)"
                ],
                [
                    "BERT-tokens",
                    "85.67",
                    "[BOLD] 71.97",
                    "[BOLD] 86.53",
                    "63.11",
                    "[BOLD] 84.91"
                ],
                [
                    "BERT-tokens",
                    "85.63 (0.83)",
                    "85.35 (0.53)",
                    "90.70 (0.46)",
                    "72.06 (1.36)",
                    "87.57 (0.75)"
                ],
                [
                    "BERT-CLS",
                    "82.42",
                    "61.3",
                    "79.63",
                    "56.79",
                    "81.91"
                ],
                [
                    "BERT-CLS",
                    "80.83 (1.18)•",
                    "82.71 (0.68)•",
                    "88.35 (0.77)•",
                    "67.37 (1.08)",
                    "85.43 (0.36)"
                ],
                [
                    "ELMo",
                    "[BOLD] 85.89",
                    "66.63",
                    "83.05",
                    "[BOLD] 63.18",
                    "84.54"
                ],
                [
                    "ELMo",
                    "84.79 (1.08)",
                    "84.53 (0.96)",
                    "90.11 (0.56)",
                    "72.53 (0.80)",
                    "87.81 (0.34)"
                ]
            ]
        },
        "gold_description": "no statistical significance is seen even when the test set result worsens in performance for bertcls and piecewise pool in table 5 where it hurts test set performance on ddi but is not statistically significant when cross validation is performed . in table 5 , we tested the generalizability of the commonly used piecewise pooling technique while piecewise pooling helps i2b2 by 1 % , it hurts test set performance on ddi and doesn ' t affect performance on semeval . we found elmo and ber tokens to boost performance significantly for all datasets , but that bert - cls hurt performance for the medical datasets . while bert - cls boosted test set performance for semeval , this was not found to be a statistically significant difference for cross validation .",
        "generated_description": "we can see that the proposed bert - tokens significantly outperforms both the baseline and the chunkwise pooling models . for example , the accuracy of the classifier is improved from 80 . 55 % to 71 . 97 % and from 60 . 85 % to 63 . 11 % for the ddi class and the i2b2 detect datasets , respectively . the results demonstrate the effectiveness of our proposed model ."
    },
    {
        "table_id": "376",
        "table_info": {
            "table_caption": "Table 1: Machine translation tokenized BLEU test results on IWSLT 2017 de→en, KFTT ja→en, WMT 2016 ro→en and WMT 2014 en→de, respectively.",
            "table_column_names": [
                "activation",
                "de→en",
                "ja→en",
                "ro→en",
                "en→de"
            ],
            "table_content_values": [
                [
                    "softmax",
                    "29.79",
                    "21.57",
                    "32.70",
                    "26.02"
                ],
                [
                    "1.5-entmax",
                    "29.83",
                    "[BOLD] 22.13",
                    "[BOLD] 33.10",
                    "25.89"
                ],
                [
                    "[ITALIC] α-entmax",
                    "[BOLD] 29.90",
                    "21.74",
                    "32.89",
                    "[BOLD] 26.93"
                ]
            ]
        },
        "gold_description": "we report test set tokenized bleu ( papineni et al . , 2002 ) results in table 1 . we can see that replacing softmax by entmax does not hurt performance in any of the datasets ; indeed , sparse attention transformers tend to have slightly higher bleu , but their sparsity leads to a better potential for analysis . in the next section , we make use of this potential by exploring the learned internal mechanics of the self - attention heads .",
        "generated_description": "automatic translation we use tokenized bleu as the metric to measure the performance of machine translation models . we report the results on iwslt 2017 de → en , kftt ja → en and wmt 2016 ro → en as well as wmt 2014 en → de . we observe that α - entmax consistently outperforms the other two max - pooling activation functions , yielding the best results in all cases . in addition , we also observe that the difference between 1 . 5 and 1 . 8 maximum attention is marginal ."
    },
    {
        "table_id": "377",
        "table_info": {
            "table_caption": "Table 6: Results for C-LSTM models trained with CC and arXiv embeddings on both subtasks.",
            "table_column_names": [
                "[EMPTY]",
                "subtask 1.1 macro F1",
                "subtask 1.1 micro F1",
                "subtask 1.2 macro F1",
                "subtask 1.2 micro F1"
            ],
            "table_content_values": [
                [
                    "CC",
                    "54.42",
                    "67.61",
                    "[BOLD] 74.42",
                    "[BOLD] 78.87"
                ],
                [
                    "arXiv",
                    "[BOLD] 67.49",
                    "[BOLD] 70.96",
                    "67.02",
                    "74.37"
                ]
            ]
        },
        "gold_description": "table 6 shows that the c - lstm model performs well on the scientific embeddings , but consistently worse than the svm model using handcrafted features and achieves a macro - f1 score of 67 . 49 and 67 . 02 for subtask 1 . 1 and subtask 1 . 2 respectively .",
        "generated_description": "for each dataset , we train a c - lstm model with the cc and arxiv embeddings and tune hyper - parameters on the development set . we observe that the model trained on the cc dataset achieves the best results for both subtasks . the micro - f1 score is also slightly higher than the macro f1 score , suggesting that there is some overlap between the datasets ."
    },
    {
        "table_id": "378",
        "table_info": {
            "table_caption": "Table 1: Comparison of segmentation performance with the state-of-the-art methods on four evaluation datasets in terms of IoU.",
            "table_column_names": [
                "[EMPTY]",
                "val",
                "UNC testA",
                "testB",
                "val",
                "UNC+ testA",
                "testB",
                "G-Ref val",
                "ReferIt test"
            ],
            "table_content_values": [
                [
                    "LSTM-CNN ",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "28.14",
                    "48.03"
                ],
                [
                    "RMI ",
                    "45.18",
                    "45.69",
                    "45.57",
                    "29.86",
                    "30.48",
                    "29.50",
                    "34.52",
                    "58.73"
                ],
                [
                    "DMN ",
                    "49.78",
                    "54.83",
                    "45.13",
                    "38.88",
                    "44.22",
                    "32.29",
                    "36.76",
                    "52.81"
                ],
                [
                    "KWA ",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "-",
                    "36.92",
                    "59.09"
                ],
                [
                    "RRN ",
                    "55.33",
                    "57.26",
                    "53.93",
                    "39.75",
                    "42.15",
                    "36.11",
                    "36.45",
                    "63.63"
                ],
                [
                    "Ours",
                    "[BOLD] 58.32",
                    "[BOLD] 60.61",
                    "[BOLD] 55.09",
                    "[BOLD] 43.76",
                    "[BOLD] 47.60",
                    "[BOLD] 37.89",
                    "[BOLD] 39.98",
                    "[BOLD] 63.80"
                ]
            ]
        },
        "gold_description": "table 1 presents comparisons of our method with existing state - of - the - art approaches . our proposed method consistently outperforms all other methods on all four datasets . the improvement is particularly significant on the more challenging datasets , such as unc + which has no location words and g - ref which contains longer and richer query expressions .",
        "generated_description": "our full model significantly outperforms previous state - of - the - art methods on all the test sets , which demonstrates the effectiveness and generalizability of our proposed model . our model also achieves the best performance on the referit test set , which further verifies the effectiveness of our model on referit ."
    },
    {
        "table_id": "379",
        "table_info": {
            "table_caption": "Table 2: Ablation study of different attention methods for multimodal features on the UNC val set.",
            "table_column_names": [
                "Method",
                "IoU"
            ],
            "table_content_values": [
                [
                    "No attention",
                    "45.63"
                ],
                [
                    "Word attention",
                    "47.01"
                ],
                [
                    "Pixel attention",
                    "47.84"
                ],
                [
                    "Word-pixel pair attention",
                    "47.57"
                ],
                [
                    "Cross-modal self-attention",
                    "[BOLD] 50.12"
                ]
            ]
        },
        "gold_description": "as shown in table 2 , the proposed cross - modal self - attention outperforms all other attention methods significantly .",
        "generated_description": "we can see that cross - modal self - attention performs the best across all the methods . we attribute this to the fact that multimodal features are highly dependent on the attention mechanism . without attention , the model is forced to rely on a fixed - dimensional representation of each modality , which may introduce noise into the representation of the whole feature space . in addition , the pixel attention and the word - pixel pair attention do not perform well . we conjecture that these two types of attention mechanisms cause over - fitting in the feature space , which is harmful to the model ."
    },
    {
        "table_id": "380",
        "table_info": {
            "table_caption": "Table 3: Ablation study on the UNC val set. The top four methods compare results of different methods for multimodal feature representations. The bottom five results show comparisons of multi-level feature fusion methods. CMSA and GF denote the proposed cross-modal self-attention and gated multi-level fusion modules. All methods use the same base model (DeepLab-101) and DenseCRF for postprocessing. ∗The numbers for [15] are slightly higher than original numbers reported in their paper which did not use DenseCRF postprocessing.",
            "table_column_names": [
                "Method",
                "prec@0.5",
                "prec@0.6",
                "prec@0.7",
                "prec@0.8",
                "prec@0.9",
                "IoU"
            ],
            "table_content_values": [
                [
                    "RMI-LSTM ",
                    "42.99",
                    "33.24",
                    "22.75",
                    "12.11",
                    "2.23",
                    "45.18"
                ],
                [
                    "RRN-CNN ∗",
                    "47.59",
                    "38.76",
                    "26.53",
                    "14.79",
                    "3.17",
                    "46.95"
                ],
                [
                    "CMSA-S",
                    "51.19",
                    "41.31",
                    "29.57",
                    "14.99",
                    "2.61",
                    "48.53"
                ],
                [
                    "CMSA-W",
                    "[BOLD] 51.95",
                    "[BOLD] 43.11",
                    "[BOLD] 32.74",
                    "[BOLD] 19.28",
                    "[BOLD] 4.11",
                    "[BOLD] 50.12"
                ],
                [
                    "CMSA+PPM",
                    "58.25",
                    "49.82",
                    "39.09",
                    "24.76",
                    "5.73",
                    "53.54"
                ],
                [
                    "CMSA+Deconv",
                    "58.29",
                    "49.94",
                    "39.16",
                    "25.42",
                    "6.75",
                    "54.18"
                ],
                [
                    "CMSA+ConvLSTM",
                    "64.73",
                    "56.03",
                    "45.23",
                    "29.15",
                    "7.86",
                    "56.56"
                ],
                [
                    "CMSA+Gated",
                    "65.17",
                    "57.25",
                    "47.37",
                    "33.31",
                    "9.66",
                    "57.08"
                ],
                [
                    "CMSA+GF(Ours)",
                    "[BOLD] 66.44",
                    "[BOLD] 59.70",
                    "[BOLD] 50.77",
                    "[BOLD] 35.52",
                    "[BOLD] 10.96",
                    "[BOLD] 58.32"
                ]
            ]
        },
        "gold_description": "as shown in table 3 ( top 4 rows ) , the proposed crossmodal self - attentive feature based approaches achieve significantly better performance than other baselines . the word based method cmsa - w outperforms sentence based method cmsa - s for multimodal feature representation . as presented in the bottom 5 rows in table 3 , the proposed gated multi - level fusion outperforms these other multi - scale feature fusion methods .",
        "generated_description": "cmsa - w performs the best among the cmsa variants with a large margin over the state - of - the - art method rrn - cnn . the performance of cmsa is better than that of rmi - lstm , which is a convolutional neural network based cmsa . this is due to the fact that the convolution layer is able to capture the temporal relationship between words in a sentence and the global context of the whole sentence , allowing the model to better learn the temporal association between words and sentences ."
    },
    {
        "table_id": "381",
        "table_info": {
            "table_caption": "Table 2: Experimental results of the evaluation metrics.",
            "table_column_names": [
                "[EMPTY]",
                "[ITALIC]  [BOLD] Model 1",
                "[ITALIC]  [BOLD] Model 2",
                "[ITALIC]  [BOLD] Model 3"
            ],
            "table_content_values": [
                [
                    "First metric",
                    "First metric",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "0.08 [ITALIC]  [BOLD] Disp.",
                    "[BOLD] 0.94",
                    "0.72",
                    "0"
                ],
                [
                    "0.08 [ITALIC]  [BOLD] Gen. perf.",
                    "0.68",
                    "0.63",
                    "1"
                ],
                [
                    "Second metric",
                    "Second metric",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "0.08 [ITALIC]  [BOLD] Disp.",
                    "0.93",
                    "[BOLD] 0.96",
                    "0"
                ],
                [
                    "0.08 [ITALIC]  [BOLD] Gen. perf.",
                    "0.68",
                    "0.68",
                    "1"
                ]
            ]
        },
        "gold_description": "we report metric results when grouping crowd workers into 5 bins using the full range of adr of the dataset . the adr - based bias measures ( tab . 2 ) follow the expected trend ( model 1 appears more biased than models 2 and 3 ) contrary to the protected attributes - based measures . model 1 trained with mv labels exhibits similar performance across demographic groups model 2 ' s performance is different across groups",
        "generated_description": "the first metric is the first one , which is the one based on the ratio of the number of system events to the total number of events in the whole dialogue . we observe that the first metric achieves a high value of 0 . 08 disp . perf . and a low value of 1 . this indicates that the system is able to generate a concise and coherent summary . the second metric uses the second metric to measure the dispersion of the system events and the quality of the generated summary . as we can see that dispersion is higher than the first and second metrics , indicating that our system generates more coherent and consistent summaries ."
    },
    {
        "table_id": "382",
        "table_info": {
            "table_caption": "Table 3: DA Classification Accuracy",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] SwDA",
                "[BOLD] MRDA"
            ],
            "table_content_values": [
                [
                    "TF-IDF GloVe",
                    "66.5",
                    "78.7"
                ],
                [
                    "Kalchbrenner and Blunsom ( 2013 )",
                    "73.9",
                    "-"
                ],
                [
                    "Lee and Dernoncourt ( 2016 )",
                    "73.9",
                    "84.6"
                ],
                [
                    "Khanpour et al. ( 2016 )",
                    "75.8",
                    "86.8"
                ],
                [
                    "Ji et al. ( 2016 )",
                    "77.0",
                    "-"
                ],
                [
                    "Shen and Lee ( 2016 )",
                    "72.6",
                    "-"
                ],
                [
                    "Li and Wu ( 2016 )",
                    "79.4",
                    "-"
                ],
                [
                    "Ortega and Vu ( 2017 )",
                    "73.8",
                    "84.3"
                ],
                [
                    "Tran et al. ( 2017 )",
                    "74.5",
                    "-"
                ],
                [
                    "Kumar et al. ( 2018 )",
                    "79.2",
                    "90.9"
                ],
                [
                    "Chen et al. ( 2018 )",
                    "81.3",
                    "[BOLD] 91.7"
                ],
                [
                    "[BOLD] Our Method",
                    "[BOLD] 82.9",
                    "91.1"
                ],
                [
                    "[BOLD] Human Agreement",
                    "84.0",
                    "-"
                ]
            ]
        },
        "gold_description": "we compare the classification accuracy of our model against several other recent methods ( table 3 ) . our model outperforms state - of - the - art methods by 1 . 6 % on swda , the primary dataset for this task , and comes within 0 . 6 % on mrda . it also beats a tf - idf glove baseline ( described in section 5 . 2 ) by 16 . 4 % and 12 . 2 % , respectively . the improvements that the model is able to make over the other methods are significant , however , the gains on mrda still fall short of the state - of - the - art by 0 . 6 % .",
        "generated_description": "we can see that our proposed method achieves the best accuracy on both datasets , 82 . 9 % and 91 . 1 % for the swda and mrda datasets , respectively . the results of the mrda dataset indicate that the majority of the da instances in this dataset can be solved by the proposed model , and the human agreement between annotators is 84 . 0 % and 90 . 9 % , respectively , which is a new state - of - the - art da classification accuracy ."
    },
    {
        "table_id": "383",
        "table_info": {
            "table_caption": "Table 1: Present keyphrase prediction performance (F1-score) on four benchmark datasets. Dataset names are listed in the header followed by the average number of target phrases per document. The baseline One2One model is on the first row, followed by different One2Seq variants. Bold indicates best score in each column.",
            "table_column_names": [
                "[BOLD] Method",
                "[BOLD] Inspec (Avg=7.8)  [BOLD] F1@5",
                "[BOLD] Inspec (Avg=7.8)  [BOLD] F1@10",
                "[BOLD] Krapivin (Avg=3.4)  [BOLD] F1@5",
                "[BOLD] Krapivin (Avg=3.4)  [BOLD] F1@10",
                "[BOLD] NUS (Avg=6.1)  [BOLD] F1@5",
                "[BOLD] NUS (Avg=6.1)  [BOLD] F1@10",
                "[BOLD] SemEval (Avg=6.7)  [BOLD] F1@5",
                "[BOLD] SemEval (Avg=6.7)  [BOLD] F1@10",
                "[BOLD] Average  [BOLD] F1@5",
                "[BOLD] Average  [BOLD] F1@10"
            ],
            "table_content_values": [
                [
                    "[BOLD] One2One",
                    "0.244",
                    "0.289",
                    "0.305",
                    "[BOLD] 0.266",
                    "[BOLD] 0.376",
                    "[BOLD] 0.352",
                    "0.318",
                    "[BOLD] 0.318",
                    "0.311",
                    "[BOLD] 0.306"
                ],
                [
                    "[BOLD] Random",
                    "0.283",
                    "0.206",
                    "0.288",
                    "0.183",
                    "0.344",
                    "0.238",
                    "0.304",
                    "0.218",
                    "0.305",
                    "0.211"
                ],
                [
                    "[BOLD] Length",
                    "0.298",
                    "0.224",
                    "0.321",
                    "0.206",
                    "0.364",
                    "0.259",
                    "0.311",
                    "0.222",
                    "0.324",
                    "0.228"
                ],
                [
                    "[BOLD] No-Sort",
                    "0.323",
                    "0.253",
                    "0.317",
                    "0.209",
                    "[BOLD] 0.376",
                    "0.264",
                    "0.318",
                    "0.244",
                    "0.333",
                    "0.243"
                ],
                [
                    "[BOLD] Alpha",
                    "0.319",
                    "0.283",
                    "[BOLD] 0.329",
                    "0.238",
                    "[BOLD] 0.376",
                    "0.289",
                    "[BOLD] 0.343",
                    "0.266",
                    "[BOLD] 0.342",
                    "0.269"
                ],
                [
                    "[BOLD] Appear-Pre",
                    "0.320",
                    "0.307",
                    "0.322",
                    "0.245",
                    "0.369",
                    "0.302",
                    "0.327",
                    "0.291",
                    "0.334",
                    "0.286"
                ],
                [
                    "[BOLD] Appear-Ap",
                    "[BOLD] 0.344",
                    "[BOLD] 0.333",
                    "0.320",
                    "0.236",
                    "0.367",
                    "0.295",
                    "0.324",
                    "0.286",
                    "0.339",
                    "0.287"
                ]
            ]
        },
        "gold_description": "we report experimental results on four common benchmark datasets ( totalling 1241 testing data points ) in table 1 . as shown in table 1 , the average performance increases from random to appear - ap : a trend that becomes particularly obvious for f1 @ 10 .",
        "generated_description": "from the table , we can see that one2one consistently outperforms all the other methods in terms of f1 - score . the average f1 @ 5 score of inspec , nus and semeval is 0 . 244 , 0 . 266 and 0 . 276 , respectively . the best f1 scores of krapivin and semaval are 3 . 4 and 3 . 7 respectively , which are higher than the best scores reported by other methods . in addition , it is worth mentioning that the inspec and nus datasets have a large number of sentences per document ( up to 7 . 8 ) ; thus , the results obtained by our method are better than the ones obtained by the baselines . the results demonstrate the effectiveness of our proposed model ."
    },
    {
        "table_id": "384",
        "table_info": {
            "table_caption": "Table 4: F1 scores on present keyphrase generation of One2Seq models with different model complexities (BeamWidth=50).",
            "table_column_names": [
                "[BOLD] Model  [BOLD] #(Param)",
                "[BOLD] BaseRNN 13M",
                "[BOLD] BaseRNN 13M",
                "[BOLD] BigRNN 37M",
                "[BOLD] BigRNN 37M",
                "[BOLD] Transformer 80M",
                "[BOLD] Transformer 80M"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "[BOLD] F@5",
                    "[BOLD] F@10",
                    "[BOLD] F@5",
                    "[BOLD] F@10",
                    "[BOLD] F@5",
                    "[BOLD] F@10"
                ],
                [
                    "[BOLD] Random",
                    "0.358",
                    "0.304",
                    "0.356",
                    "0.305",
                    "0.359",
                    "0.289"
                ],
                [
                    "[BOLD] Length",
                    "0.351",
                    "0.319",
                    "0.349",
                    "0.321",
                    "[BOLD] 0.361",
                    "0.318"
                ],
                [
                    "[BOLD] No-Sort",
                    "[BOLD] 0.364",
                    "0.325",
                    "[BOLD] 0.361",
                    "0.329",
                    "0.358",
                    "0.329"
                ],
                [
                    "[BOLD] Alpha",
                    "0.354",
                    "0.341",
                    "0.358",
                    "0.341",
                    "0.353",
                    "0.336"
                ],
                [
                    "[BOLD] Appear-Pre",
                    "0.337",
                    "0.345",
                    "0.339",
                    "0.341",
                    "0.352",
                    "0.343"
                ],
                [
                    "[BOLD] Appear-Ap",
                    "0.339",
                    "[BOLD] 0.347",
                    "0.344",
                    "[BOLD] 0.346",
                    "0.357",
                    "[BOLD] 0.345"
                ]
            ]
        },
        "gold_description": "as shown in table 4 , bigrnn and transformer are not able to outperform basernn .",
        "generated_description": "we can see that the bigrnn outperforms the basernn and transformer models in terms of most of the metrics under both settings . among the different types of relation types , the no - sort relation type performs the best , which is consistent with the findings in [ hovy - etal : 2015 : acl - ijcnlp ] . however , the performance of the alpha relation type and appear - pre are slightly worse than the random relation type . we conjecture that this is due to the fact that this relation type has the least number of training examples , and thus is less stable than the other relations ."
    },
    {
        "table_id": "385",
        "table_info": {
            "table_caption": "Table 2: Per-token macro-F1 scores. For ADR, the F1 scores are for chunks via approximate matching (Nikfarjam et al., 2015; Tsai et al., 2006). ‘rand-LSTM’ is an LSTM with randomly initialized word vectors. ‘ELMo-LSTM’ is an LSTM initialized with pretrained ELMo embeddings. ‘HB’ signals sparse, high-dimensional feature representations based on hand-built feature functions. The mean values and standard deviations are calculated using F1 scores of three runs of repeated experiments, as discussed in section 3. Statistical significance notation for the last two rows (two top-performing models) is ∗: p<0.05; ∗∗: p<0.01; ∗∗∗: p<0.001.",
            "table_column_names": [
                "[EMPTY]",
                "Diagnosis",
                "Prescription",
                "Penn Adverse Drug",
                "Chemical–Disease",
                "Drug–Disease"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "Detection",
                    "Reasons",
                    "Reactions (ADR)",
                    "Relations (CDR)",
                    "Relations"
                ],
                [
                    "rand-LSTM-CRF",
                    "77.3 ± 0.05",
                    "69.6 ± 0.25",
                    "53.8 ± 0.88",
                    "85.1 ± 0.10",
                    "48.2 ± 1.12"
                ],
                [
                    "HB-CRF",
                    "82.0 ± 0.05",
                    "78.5 ± 0.01",
                    "58.8 ± 0.12",
                    "86.2 ± 0.02",
                    "42.3 ± 0.30"
                ],
                [
                    "ELMo-LSTM-CRF",
                    "83.9 ± 0.35",
                    "81.0 ± 0.20",
                    "65.7 ± 0.35",
                    "88.2 ± 0.34",
                    "50.6 ± 0.64"
                ],
                [
                    "ELMo-LSTM-CRF-HB",
                    "[BOLD] 85.3 ± 0.24∗∗∗",
                    "[BOLD] 82.0 ± 0.03∗∗∗",
                    "[BOLD] 68.5 ± 1.67∗",
                    "[BOLD] 89.9 ± 0.12∗∗∗",
                    "[BOLD] 51.9 ± 0.52∗∗"
                ]
            ]
        },
        "gold_description": "the power these features bring to the model , beyond what is captured in the elmo - lstm representations , is evident in table 2 , column 1 . the results in table 2 , column 2 , clearly favor the combined model elmo - lstm - crf - hb that uses both these features and the elmo - lstm . our test - set results , given in table 2 , column 3 , show the power of our combined model elmo - lstm - crf - hb . we report results for the standard test set . the power of the combined model elmo - lstm - crf - hb is again evident in the results in table 2 , column 4 . our results for this experiment are given in table 2 , column 5 , and point to the superiority of our combined model elmo - lstm - crf - hb . our discussion seeks to show that the combined model elmo - lstm - crf - hb , which shows superior performance in all tasks ( table 2 ) , is making meaningful use of both kinds of features ( hand - built and elmo ) and both of the major model components ( lstm and crf ) . we note also that , where the performance of the two base models is very similar ( table 2 ) , the potential scores in the combined model are also more similar .",
        "generated_description": "although elmo - lstm - crf - hb does not achieve the best performance on all the tasks , it achieves the highest performance on diagnosis , prescription and penn adverse drug , chemical – disease and drug – risease , and has the second highest performance for drug – disease . this indicates that the neural network is better at detecting and analyzing the diagnosis and treatment codes than the lstm and random forest classifiers . hb - based methods also achieve better performance than random forest classifiers on most of the tasks ."
    },
    {
        "table_id": "386",
        "table_info": {
            "table_caption": "Table 3: Relative F1 score improvements of different labels. For each label, we give the number of supporting examples (Support), the F1 score of our combined model, and the relative improvements over the HB-CRF model. The F1 scores of minor labels suffer from insufficient training data, and thus have lower values. However, the combined model shows the largest relative improvements in these categories. ADR results are shown in table A4.",
            "table_column_names": [
                "Diagnosis Detection Label",
                "Diagnosis Detection Support",
                "Diagnosis Detection F1 score",
                "Diagnosis Detection Improvement",
                "Prescription Reasons Label",
                "Prescription Reasons Support",
                "Prescription Reasons F1 score",
                "Prescription Reasons Improvement"
            ],
            "table_content_values": [
                [
                    "Other",
                    "74888",
                    "95.3",
                    "1.4%",
                    "Other",
                    "83618",
                    "95.8",
                    "0.9%"
                ],
                [
                    "Positive",
                    "24489",
                    "86.1",
                    "4.4%",
                    "Reason",
                    "9114",
                    "64.7",
                    "8.6%"
                ],
                [
                    "Ruled-out",
                    "2797",
                    "86.4",
                    "3.6%",
                    "Prescribed",
                    "5967",
                    "84.7",
                    "4.4%"
                ],
                [
                    "Concern",
                    "2780",
                    "72.1",
                    "5.6%",
                    "Discontinued",
                    "2754",
                    "82.7",
                    "5.6%"
                ],
                [
                    "Chemical–Disease Relations (CDR)",
                    "Chemical–Disease Relations (CDR)",
                    "Chemical–Disease Relations (CDR)",
                    "Chemical–Disease Relations (CDR)",
                    "Drug–Disease Relations",
                    "Drug–Disease Relations",
                    "Drug–Disease Relations",
                    "Drug–Disease Relations"
                ],
                [
                    "Label",
                    "Support",
                    "F1 score",
                    "Improvement",
                    "Label",
                    "Support",
                    "F1 score",
                    "Improvement"
                ],
                [
                    "Other",
                    "104530",
                    "98.3",
                    "0.5%",
                    "Other",
                    "10634",
                    "90.8",
                    "2.3%"
                ],
                [
                    "Disease",
                    "6887",
                    "84.2",
                    "6.3%",
                    "Treats",
                    "3671",
                    "76.0",
                    "5.7%"
                ],
                [
                    "Chemical",
                    "6270",
                    "87.0",
                    "6.7%",
                    "Unrelated",
                    "1145",
                    "53.8",
                    "71.3%"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "Prevents",
                    "320",
                    "41.1",
                    "103.5%"
                ],
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "Contraindicated-for",
                    "69",
                    "0",
                    "–"
                ]
            ]
        },
        "gold_description": "table 3 suggests that the combined model does make progress here , in that the largest gains , across all relevant datasets , tend to be for the smallest categories .",
        "generated_description": "diagnosis detection . in terms of diagnosis detection , the most frequent labels are positive , negative , and other which accounts for 95 . 8 % of all the labels . the second most frequent label is ruled - out which has 3 . 6 % of the total labels and the second most frequently labeled label is concern which has 5 . 6 % . the f1 score of this class is 84 . 7 % which is lower than that of positive and negative labels and explains the lower performance of this category . for drug – disease relations , we observe that the most frequently classified labels are discontininued , partially because discontininuations are often not mentioned explicitly in the text and the model does not have enough training data to learn to recognize them . on the other hand , in terms of support , we find that most of the prescription reasons are explained explicitly and the f1 scores for these reasons are higher than that for positive or negative labels , which explains the higher recall of these reasons . for both drug – drugs and chemical – data relations , there is a higher recall for discontinuities and partially explained reasons ."
    },
    {
        "table_id": "387",
        "table_info": {
            "table_caption": "Table 2: Corpus statistics and label distributions of Friends and EmotionPush datasets.",
            "table_column_names": [
                "[BOLD] Dataset Friends",
                "[BOLD] Dataset Training",
                "#Dialogues / #Utterances 4,000 / 58,012",
                "#Avg. utterances per dialogue 14.50",
                "#Avg. length of dialogues 160.92",
                "Neutral 45.0%",
                "Joy 11.8%",
                "Sadness 3.4%",
                "Anger 5.2%",
                "Out-Of-Domain 34.6%"
            ],
            "table_content_values": [
                [
                    "Friends",
                    "Test",
                    "240 / 3296",
                    "13.73",
                    "156.38",
                    "31.4%",
                    "15.3%",
                    "3.7%",
                    "4.3%",
                    "45.3%"
                ],
                [
                    "EmotionPush",
                    "Training",
                    "4,000 / 58,968",
                    "14.74",
                    "114.96",
                    "66.8%",
                    "14.2%",
                    "3.5%",
                    "0.9%",
                    "14.6%"
                ],
                [
                    "EmotionPush",
                    "Test",
                    "240 / 3536",
                    "14.73",
                    "92.43",
                    "60.7%",
                    "17.0%",
                    "3.1%",
                    "0.8%",
                    "18.4%"
                ]
            ]
        },
        "gold_description": "we describe data and label distribution of friends and emotionpush datasets . in terms of label distribution for both datasets , neutral are the most common class , followed by joy , sadness , and anger . both datasets have imbalanced class distribution , and especially the ratio of sadness and anger is very small . for instance , they account for only 3 . 4 % and 5 . 2 % , respectively in the friends dataset . in the case of emotionpush , anger label accounts for less than 1 % of the training set .",
        "generated_description": "for the friends dataset , we randomly sample 4 , 000 conversations from the friends training set and 58 , 012 utterances from the testing set for training and testing . the test set contains 3296 utterances and 156 . 38 utterances per dialogue for friends . for the emotionpush dataset , the training and test sets contain 240 and 3536 utterances , respectively . the label distribution of friends is very similar to that of the original dataset , which shows that the language quality and content of the datasets are very similar . however , there is a significant difference in the out - of - domain and in - domain datasets , where the sadness and anger rates are much higher than in the training set , and the neutral and joy rates are higher than that of anger and sadness . we also observe that the sadness rate is much higher on the friends test set than on the training set , which indicates that the dataset contains a lot of negative experiences . on the other hand , the joy , sadness , anger , and fear rates on the test set are very low , which means that the models can easily learn to associate sadness and fear with each other ."
    },
    {
        "table_id": "388",
        "table_info": {
            "table_caption": "Table 1: Comparison between Essentia and FSA baseline on paraphrase extraction",
            "table_column_names": [
                "[EMPTY]",
                "[BOLD] Dataset",
                "[BOLD] # of extracted pairs",
                "[BOLD] # of valid pairs",
                "[BOLD] Precision"
            ],
            "table_content_values": [
                [
                    "Essentia",
                    "Snips",
                    "173",
                    "84",
                    "48.55%"
                ],
                [
                    "Essentia",
                    "HotelQA",
                    "2221",
                    "642",
                    "28.91%"
                ],
                [
                    "FSA",
                    "Snips",
                    "18",
                    "15",
                    "83.33%"
                ],
                [
                    "FSA",
                    "HotelQA",
                    "342",
                    "185",
                    "54.09 %"
                ]
            ]
        },
        "gold_description": "table 1 compares the performance of essentia with the fsa baseline for paraphrase mining . specifically , we show the number of phrase pairs extracted by essentia and fsa from both datasets ( \" # of extracted pairs \" column ) , number of valid paraphrases within these pairs ( \" # of valid pairs \" column ) , and precision ( \" precision \" column ) . although fsa has higher precision due to conservative sentence alignment , essentia extracts significantly more paraphrases , improving the recall by 460 % ( snips ) and 247 % ( hotelqa ) over the baseline .",
        "generated_description": "in this section , we compare the performance of essentia and fsa models for paraphrase extraction on the snips and hotelqa datasets in terms of the number of extracted pairs , number of valid pairs and precision . for the first set of experiments , we focus on the fine - grained examples from the two datasets , which are the ones found in the training set of the original snips dataset . we evaluate the quality of the extracted pairs by calculating the cosine similarity between the gold standard answer and all the candidate answers in the test set . we achieve a high precision of 48 . 55 % and a low precision of 28 . 91 % when we extract the essentia dataset and the hotelqa dataset , respectively . in case of the second dataset , we obtain a higher precision of 83 . 33 % and 54 . 09 % , respectively . we obtain a high recall of 84 . 55 % , which is the same as the score obtained by the baseline system ."
    },
    {
        "table_id": "389",
        "table_info": {
            "table_caption": "Table 5: Classification test scores for classifying R vs U in the BR, US, and combined BR + US dataset. The baseline score is 50%.",
            "table_column_names": [
                "[BOLD] BR",
                "[BOLD] US",
                "[BOLD] BR + US"
            ],
            "table_content_values": [
                [
                    "85%",
                    "72%",
                    "70%"
                ]
            ]
        },
        "gold_description": "the classification of br and us datasets used the 60 and 49 most relevant features , respectively . the test accuracy for the br and us datasets were 85 % and 72 % , respectively , with a baseline score of 50 % . the combined dataset used a reduced set of features consisting of an intersection of the most relevant features observed in both br and us , achieving a test score of 70 % using only 18 features . classes . we have shown that these features may be used to classify news articles in a language other than english .",
        "generated_description": "as can be seen , the br dataset is much easier to classifying than the us dataset , and the combined br + us dataset is able to achieve an accuracy of 85 % which is already close to the state - of - the - art ."
    },
    {
        "table_id": "390",
        "table_info": {
            "table_caption": "Table 1: Storage size(MB) comparison of full archive sync between geth and ethanos from 7M+1 to 8M block.",
            "table_column_names": [
                "2 Data type",
                "geth",
                "ethanos",
                "Diff"
            ],
            "table_content_values": [
                [
                    "Headers",
                    "316.68",
                    "348.16",
                    "+31.48"
                ],
                [
                    "Bodies",
                    "11,110.00",
                    "12,750.00",
                    "+1,640.00"
                ],
                [
                    "Receipts",
                    "3,578.97",
                    "3,605.66",
                    "+26.69"
                ],
                [
                    "Difficulties",
                    "15.57",
                    "15.33",
                    "-0.24"
                ],
                [
                    "Block number -> hash",
                    "39.66",
                    "39.35",
                    "-0.31"
                ],
                [
                    "Block hash -> number",
                    "39.10",
                    "39.10",
                    "0"
                ],
                [
                    "Transaction Index",
                    "3,650.00",
                    "3,650.00",
                    "0"
                ],
                [
                    "Bloombit index",
                    "20.49",
                    "20.49",
                    "0"
                ],
                [
                    "Trie nodes",
                    "183,100.00",
                    "165,140.00",
                    "-17,960.00"
                ],
                [
                    "Trie preimages",
                    "857.00",
                    "888.11",
                    "+31.11"
                ],
                [
                    "total",
                    "202,727.47",
                    "186,496.20",
                    "-16,231.27"
                ],
                [
                    "2",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ]
            ]
        },
        "gold_description": "table 1 gives full archive sync size of each data type for geth and ethanos at the 8m block . ethanos reduces the total blockchain size about 16gb compared to geth . especially , it reduces the size of trie nodes about 18gb , because ethanos only maintains the state trie only with active accounts during a month which are only 10 % of the total accounts . however , hash field of bloom filter in block header slightly increases headers , and restore transactions increment bodies and receipts about 1 . 6gb . as a result , ethanos reduced full archive node by about 8 % compared to ethereum .",
        "generated_description": "s5ss0sss0px2 analysis of the extractions we perform an ablation study to understand the effect of the different types of extractions in the geth and ethanos corpora . we compare the number of different extractions per type in the two corpora , as well as the total number of extracted sentences in total across different types . we observe that the average number of sentences per type is close to 1 , 640 , and the average length of the extracted sentences is over 18 , 000 . the number of hashing operations per type ranges from 0 to over 32 , 000 , with most of the gains coming from single - token extractions ( e . g . , in the case of geth , headings are up to 26 . 69 % in average length , and up to 17 . 960 % for the cases of ethanos ) . the gains stem mostly from single token extractions , but can also be observed for longer sequences of length . for instance , we see a gain of up to 31 . 11 % in length for the headings ( cf . \\ newcitelei - etal - 2019 - headings ) , but only up to 16 . 27 % in case of the hard problems ( difficulties ) . the trend is similar for other types as well ."
    },
    {
        "table_id": "391",
        "table_info": {
            "table_caption": "Table 2: Storage size(MB) comparison of fast sync and compact sync between geth and ethanos from 7M+1 to 7M+864K (5th checkpoint) block.",
            "table_column_names": [
                "2 Data type",
                "fast sync geth",
                "fast sync ethanos",
                "compact sync geth",
                "compact sync ethanos"
            ],
            "table_content_values": [
                [
                    "Headers",
                    "276.66",
                    "303.83",
                    "276.66",
                    "303.83"
                ],
                [
                    "Bodies",
                    "9,260.00",
                    "10,130.00",
                    "8.12",
                    "13.02"
                ],
                [
                    "Receipts",
                    "2,973.66",
                    "2,979.81",
                    "10.43",
                    "10.44"
                ],
                [
                    "Difficulties",
                    "11.49",
                    "11.48",
                    "11.49",
                    "11.48"
                ],
                [
                    "Block number -> hash",
                    "31.86",
                    "31.88",
                    "31.86",
                    "31.88"
                ],
                [
                    "Block hash -> number",
                    "33.79",
                    "33.79",
                    "33.79",
                    "33.79"
                ],
                [
                    "Transaction Index",
                    "3,020.00",
                    "3,020.00",
                    "0.19",
                    "0.19"
                ],
                [
                    "Bloombit index",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "Trie nodes",
                    "2,220.00",
                    "804.79",
                    "2,220.00",
                    "802.33"
                ],
                [
                    "Trie preimages",
                    "0",
                    "0.28",
                    "0",
                    "0"
                ],
                [
                    "total",
                    "17,827.46",
                    "17,315.86",
                    "2,592.54",
                    "1,206.97"
                ],
                [
                    "2",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ]
            ]
        },
        "gold_description": "table 2 shows storage size of geth and ethanos for each bootstrapping mode . ethanos reduces trie nodes size of geth from 2 , 220mb to 804 . 79mb ; however , it increases the transaction size ( bodies , receipts ) with block headers ( headers ) . therefore , ethanos did not reduce the size of fast sync much , but it significantly reduced compact sync from 2 , 592 . 54mb of geth to 1 , 206 . 97mb , which is less than 50 % . our result shows that the trie node size of each checkpoint is about 800mb ,",
        "generated_description": "in order to better understand the difficulty of the problem , we perform a more in - depth analysis of the generated data . we compare the performance of the model on different types of headings , bodies , transactions , block hash , and attributes . we observe that the performance is best on the headers and bodies types , and worst on the arterial arterial artery and bloombit index types . this is not surprising as these two types are often the most difficult to resolve in a real - world production system . in addition , we observe that in both cases the model is able to solve most of the intractable problems , with the exception of the hard problems ( e . g . , “ difficulties ” , “ receipts ” and “ bloombits ” ) . however , in the compact sync setting , these problems are much easier to solve as the model can exploit the large set of features in the memory network to solve the problem ."
    },
    {
        "table_id": "392",
        "table_info": {
            "table_caption": "Table 3: Experiment results. Note that sequence length for word level system is shorter, thus we search by ratio 0.3 which shares similar chrF1 with search on subword level system with ratio 0.4. An ideal adversarial sample generation must achieve degradation with respect to higher semantic similarity with origin inputs (HE).",
            "table_column_names": [
                "[EMPTY]",
                "Avg MT02 – 08 BLEU",
                "Avg MT02 – 08 HE",
                "Avg MT02 – 08 chrF1",
                "Avg MT02 – 08 RD"
            ],
            "table_content_values": [
                [
                    "Transformer-word",
                    "39.75",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Search (0.2)",
                    "32.42",
                    "3.22",
                    "0.82",
                    "0.184"
                ],
                [
                    "Search (0.3)",
                    "28.83",
                    "2.45",
                    "0.77",
                    "0.275"
                ],
                [
                    "Ours",
                    "[BOLD] 33.28",
                    "[BOLD] 3.69",
                    "[BOLD] 0.80",
                    "[BOLD] 0.163"
                ],
                [
                    "Transformer-BPE",
                    "43.38",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Search (0.2)",
                    "34.27",
                    "3.87",
                    "0.89",
                    "0.210"
                ],
                [
                    "Search (0.4)",
                    "27.27",
                    "2.91",
                    "0.80",
                    "0.371"
                ],
                [
                    "Ours",
                    "[BOLD] 31.35",
                    "[BOLD] 3.66",
                    "[BOLD] 0.80",
                    "[BOLD] 0.277"
                ],
                [
                    "RNN-search-BPE",
                    "39.38",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Search (0.2)",
                    "31.83",
                    "3.83",
                    "0.89",
                    "0.192"
                ],
                [
                    "Search (0.4)",
                    "26.13",
                    "2.82",
                    "0.79",
                    "0.336"
                ],
                [
                    "Ours",
                    "[BOLD] 31.18",
                    "[BOLD] 3.60",
                    "[BOLD] 0.83",
                    "[BOLD] 0.208"
                ]
            ]
        },
        "gold_description": "our model can stably generate adversarial samples without significant change in semantics with the same training setting for different models ,",
        "generated_description": "the results show that our model outperforms all the baselines in terms of bleu , he , chrf1 , and rd . particularly , the he score of our model is close to that of the state - of - the - art model transformer - bpe ( 43 . 38 vs 39 . 75 ) , which shows that the bpe units are very helpful for mt02 – 08 data . the results also show that the performance of the rnn - search model is slightly lower than that of search ( 0 . 4 ) , which we believe is caused by the fact that the training data of mt02 – 08 is much smaller than those of mt01 – 06 , and thus it is hard to tune hyper - parameters to achieve optimal results ."
    },
    {
        "table_id": "393",
        "table_info": {
            "table_caption": "Table 1: Sentence-level (left) and fragment-level experiments (right). All-propaganda is a baseline that always output the propaganda class.",
            "table_column_names": [
                "Model",
                "Task SLC P",
                "Task SLC R",
                "Task SLC F1",
                "Task FLC P",
                "Task FLC R",
                "Task FLC F1"
            ],
            "table_content_values": [
                [
                    "All-Propaganda",
                    "23.92",
                    "100.0",
                    "38.61",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "BERT",
                    "[BOLD] 63.20",
                    "53.16",
                    "57.74",
                    "21.48",
                    "[BOLD] 21.39",
                    "21.39"
                ],
                [
                    "Joint",
                    "62.84",
                    "55.46",
                    "58.91",
                    "20.11",
                    "19.74",
                    "19.92"
                ],
                [
                    "Granu",
                    "62.80",
                    "55.24",
                    "58.76",
                    "23.85",
                    "20.14",
                    "21.80"
                ],
                [
                    "Multi-Granularity",
                    "Multi-Granularity",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "ReLU",
                    "60.41",
                    "[BOLD] 61.58",
                    "[BOLD] 60.98",
                    "23.98",
                    "20.33",
                    "21.82"
                ],
                [
                    "Sigmoid",
                    "62.27",
                    "59.56",
                    "60.71",
                    "[BOLD] 24.42",
                    "21.05",
                    "[BOLD] 22.58"
                ]
            ]
        },
        "gold_description": "the left side of table 1 shows the performance for the three baselines and for our multi - granularity network on the flc task . table 1 ( right ) shows that using additional information from the sentence - level for the token - level classification ( bert - granularity ) yields small improvements . the multi - granularity models outperform all baselines thanks to their higher precision . this shows the effect of the model excluding sentences that it determined to be non - propagandistic from being considered for token - level classification . the right side of table 1 shows the results for the slc task . we apply our multi - granularity network model to the sentence - level classification task to see its effect on low granularity when we train the model with a high granularity task . interestingly , it yields huge performance improvements on the sentence - level classification result . compared to the bert baseline , it increases the recall by 8 . 42 % , resulting in a 3 . 24 % increase of the f1 score .",
        "generated_description": "for the propaganda task , the best results are achieved by the bert model , which achieves a precision of 63 . 20 % and an f1 score of 57 . 74 % . for the flc task , relu achieves the best precision of 61 . 58 % and f1 - score of 20 . 14 % . multi - granularity models outperform the baseline models by a large margin , obtaining f1 scores of 21 . 80 % and 21 . 82 % for slc and flc , respectively ."
    },
    {
        "table_id": "394",
        "table_info": {
            "table_caption": "Table 2: Results on CQA dev-random-split with CoS-E used during training.",
            "table_column_names": [
                "[BOLD] Method",
                "[BOLD] Accuracy (%)"
            ],
            "table_content_values": [
                [
                    "BERT (baseline)",
                    "63.8"
                ],
                [
                    "CoS-E-open-ended",
                    "65.5"
                ],
                [
                    "CAGE-reasoning",
                    "[BOLD] 72.6"
                ]
            ]
        },
        "gold_description": "table 2 shows results that compare a bert baseline that uses only the cqa inputs and the same architecture but trained using inputs that contain explanations from cos - e during training . the bert baseline model reaches 64 % accuracy and adding open - ended human explanations ( cos - e - open - ended ) alongside the questions during training results in a 2 % boost in accuracy . in table 2 , using cage - reasoning at both train and validation resulted in an accuracy of 72 % , but table 4 shows that if cage - reasoning truly captured all information provided in cos - e - openended , performance would be 90 % .",
        "generated_description": "cage - reasoning outperforms cos - e - open - ended by a large margin and achieves a new state - of - the - art performance of 72 . 6 % on dev - random - split ."
    },
    {
        "table_id": "395",
        "table_info": {
            "table_caption": "Table 3: Test accuracy on CQA v1.0. The addition of CoS-E-open-ended during training dramatically improves performance. Replacing CoS-E during training with CAGE reasoning during both training and inference leads to an absolute gain of 10% over the previous state-of-the-art.",
            "table_column_names": [
                "[BOLD] Method",
                "[BOLD] Accuracy (%)"
            ],
            "table_content_values": [
                [
                    "RC (Talmor et al.,  2019 )",
                    "47.7"
                ],
                [
                    "GPT (Talmor et al.,  2019 )",
                    "54.8"
                ],
                [
                    "CoS-E-open-ended",
                    "60.2"
                ],
                [
                    "CAGE-reasoning",
                    "[BOLD] 64.7"
                ],
                [
                    "Human (Talmor et al.,  2019 )",
                    "95.3"
                ]
            ]
        },
        "gold_description": "table 3 shows the results obtained on the cqa test split . we report our two best models that represent using human explanations ( cos - e - openended ) for training only and using language model explanations ( cage - reasoning ) during both train and test .",
        "generated_description": "s4ss3sss0px2 cqa v1 . 0 we observe that both cos - e and cage are essential to the performance of the existing state - of - the - art , with the addition of cage reasoning during training leading to an absolute improvement of 10 % over the previous state of the art . cage - reasoning alone achieves an absolute gain of 64 . 7 % in test accuracy ."
    },
    {
        "table_id": "396",
        "table_info": {
            "table_caption": "Table 4: Oracle results on CQA dev-random-split using different variants of CoS-E for both training and validation. * indicates CoS-E-open-ended used during both training and validation to contrast with CoS-E-open-ended used only during training in Table 2.",
            "table_column_names": [
                "[BOLD] Method",
                "[BOLD] Accuracy (%)"
            ],
            "table_content_values": [
                [
                    "CoS-E-selected w/o ques",
                    "53.0"
                ],
                [
                    "CoS-E-limited-open-ended",
                    "67.6"
                ],
                [
                    "CoS-E-selected",
                    "70.0"
                ],
                [
                    "CoS-E-open-ended w/o ques",
                    "84.5"
                ],
                [
                    "CoS-E-open-ended*",
                    "[BOLD] 89.8"
                ]
            ]
        },
        "gold_description": "table 4 also contains results that use only the explanation and exclude the original question from cqa denoted by ' w / o question ' . these variants also use explanation during both train and validation . we observe that even using these limited kind of explanations improves over the bert baseline in table 4 , which suggests that the explanations are providing useful information beyond just mentioning the correct or incorrect answers . in table 2 , using cage - reasoning at both train and validation resulted in an accuracy of 72 % , but table 4 shows that if cage - reasoning truly captured all information provided in cos - e - openended , performance would be 90 % .",
        "generated_description": "we observe that cos - e - selected w / o ques is the worst performing variant , which demonstrates the importance of explicitly resolving the quingual relatedness between questions and answers during training . we also observe that the limited - open - ended variant is the most effective way to resolve open - ended questions , but is still outperformed by the fully - trained agent , with 89 . 8 % accuracy when used during both training and validation . this is to be expected as the agent has to learn to predict the answer span given the limited number of training examples per question ."
    },
    {
        "table_id": "397",
        "table_info": {
            "table_caption": "Table 6: Results for explanation transfer from CQA to out-of-domain SWAG and Sotry Cloze tasks.",
            "table_column_names": [
                "[BOLD] Method",
                "[BOLD] SWAG",
                "[BOLD] Story Cloze"
            ],
            "table_content_values": [
                [
                    "BERT",
                    "84.2",
                    "89.8"
                ],
                [
                    "+ expl transfer",
                    "83.6",
                    "89.5"
                ]
            ]
        },
        "gold_description": "table 6 shows the results obtained by the bert baseline without explanations and using our transferred explanations from cqa to swag and story cloze . we observed that adding explanations led to a very small decrease ( < 0 . 6 % ) in the performance compared to the baseline for both tasks .",
        "generated_description": "explanation transfer next , we investigate the effect of explanation transfer for bert on the swag and story cloze tasks . we use the same experimental setup as explained in the main paper , i . e . , we use bert with explanation embeddings as input to the cqa layer , and the output of the explanation transfer model as output to the story cloze task . we observe that expl transfer provides consistent gains over bert ( + 0 . 8 % and + 0 . 7 % ) on both tasks , which validates our hypothesis that bert benefits from explanation transfer ."
    },
    {
        "table_id": "398",
        "table_info": {
            "table_caption": "Table 1: F1 and ensembled F1 score for citation needed detection training on the FA split and testing on the LQN split of Redi et al. (2019). The FA split contains statements with citations from featured articles and the LQN split consists of statements which were flagged as not having a citation but needing one. Listed are the mean, standard deviation, and ensembled results across 15 seeds (eP, eR, and eF1). Bold indicates best performance, underline indicates second best. *The reported value is from rerunning their released model on the test dataset. The value in brackets is the value reported in the original paper.",
            "table_column_names": [
                "Method",
                "P",
                "R",
                "F1",
                "eP",
                "eR",
                "eF1"
            ],
            "table_content_values": [
                [
                    "Redi et al.  2019",
                    "0.753",
                    "0.709",
                    "0.730 [0.76]*",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "BERT",
                    "0.788 ± 0.013",
                    "0.837 ± 0.045",
                    "0.810 ± 0.015",
                    "0.790",
                    "0.853",
                    "0.820"
                ],
                [
                    "BERT + PU",
                    "0.788 ± 0.009",
                    "0.843 ± 0.030",
                    "0.814 ± 0.010",
                    "0.790",
                    "0.856",
                    "0.822"
                ],
                [
                    "BERT +  [ITALIC] PUC",
                    "0.784 ± 0.009",
                    "[BOLD] 0.856 ± 0.032",
                    "[BOLD] 0.818 ± 0.010",
                    "0.786",
                    "[BOLD] 0.871",
                    "[BOLD] 0.826"
                ]
            ]
        },
        "gold_description": "our results for citation needed detection are given in table 1 . the vanilla bert model already significantly outperforms the state of the art model from redi et al . ( 2019 ) ( a gru network with global attention ) by 6 f1 points . we saw further gains in performance with pu learning , as well as when using puc . additionally , the models using pu learning had lower variance , indicating more consistent performance across runs . the best performing model we saw was the one trained using puc with an f1 score of 0 . 826 .",
        "generated_description": "the fa split contains statements with citations from featured articles , while the lqn split includes statements which were flagged as not having a citation but needing one . redi et al . ’ s method for citation needed detection does not perform well on the fa split . however , it significantly outperforms the method proposed by \\ newciteredi - etal - 2019 - emnlp ( p < 0 . 01 ) when it is trained using bert and fine - tuned using the pu and puc corpora . the ensembled results show that the proposed method outperforms both the bert - based and the pu - based methods by a large margin and achieves an ef1 score of 0 . 826 ."
    },
    {
        "table_id": "399",
        "table_info": {
            "table_caption": "Table 1: Properties of the training and test set: the number of synsets, the maximum depth of the hierarchy counted from (and including) the top synset of the domain, the number of basic level concepts, and the inter-rater agreement (κ).",
            "table_column_names": [
                "Top-synset",
                "Size",
                "Max.",
                "#basic",
                "[ITALIC] κ"
            ],
            "table_content_values": [
                [
                    "of domain",
                    "#syns.",
                    "depth",
                    "level",
                    "[EMPTY]"
                ],
                [
                    "Hand tool",
                    "157",
                    "6",
                    "30",
                    "0.73"
                ],
                [
                    "Edible fruit",
                    "197",
                    "5",
                    "77",
                    "0.78"
                ],
                [
                    "Musical Instr.",
                    "164",
                    "7",
                    "54",
                    "0.64"
                ],
                [
                    "All",
                    "518",
                    "7",
                    "161",
                    "0.73"
                ]
            ]
        },
        "gold_description": "table 1 lists the properties of the training and test set including the inter - rater agreement ( κ ) . the agreement is substantial , with some variation between the domains : κ is higher for tools and fruit than for the musical domain .",
        "generated_description": "hand tool and musical instruments are the two most difficult domains to classify , and the inter - rater agreement ( κ ) values range from 0 . 64 to 0 . 73 . the maximum depth of the hierarchy is listed in the column of the top - synset of the domain , counted from ( and including ) the top synset of that domain . the table also shows the number of basic level concepts of each domain in the training and test sets . each concept is annotated by at least three annotators , and in some cases more than five or six annotators . each annotator annotates one or more of the synsets in the three domains separately , and agrees with or opposes the label assigned by the majority of the annotators on the classification decisions . for example , in the hand tool domain , annotators agree on hand tool , musical instruments , and acoustics , and vice versa ."
    },
    {
        "table_id": "400",
        "table_info": {
            "table_caption": "Table 2: Features ranked in order of importance.",
            "table_column_names": [
                "Feature",
                "All",
                "Tool",
                "Fruit",
                "Music"
            ],
            "table_content_values": [
                [
                    "depth_in_hierarchy",
                    "1",
                    "1",
                    "1",
                    "3"
                ],
                [
                    "G.Ngrams_2008_max",
                    "2",
                    "2",
                    "5",
                    "2"
                ],
                [
                    "gloss_length",
                    "3",
                    "4",
                    "7",
                    "1"
                ],
                [
                    "polysemy_max",
                    "4",
                    "3",
                    "4",
                    "6"
                ],
                [
                    "word_length_min",
                    "5",
                    "5",
                    "3",
                    "4"
                ],
                [
                    "nr_of_partOfs",
                    "6",
                    "8",
                    "2",
                    "8"
                ],
                [
                    "nr_of_hyponyms",
                    "7",
                    "6",
                    "6",
                    "5"
                ],
                [
                    "nr_of_synonyms",
                    "8",
                    "7",
                    "8",
                    "7"
                ],
                [
                    "nr_of_direct_hypernyms",
                    "9",
                    "9",
                    "9",
                    "9"
                ]
            ]
        },
        "gold_description": "table 2 lists the importance of each variable in the global model and in the three single domain models , where the variable with the highest weight is ranked 1 . the lists are relatively stable , with some marked differences , such as the importance of the gloss length and the number of partof relations for music and fruit , respectively .",
        "generated_description": "in order to gain insight into the importance of each feature , we ranked the features ranked in order of importance . we can observe that gloss_length and word_length_min are two of the most important features for the tool domain , while polysemy_max and g . ngrams_2008_max are less important for the music domain . according to the table , the second most important feature is depth_in_hierarchy , which is a hierarchical feature that measures the extent to which a concept descends into the hierarchy of the wordnet before reaching the top of the tree . this feature can be seen as a variant of depth in the hierarchy that measures how deep the concept is in the tree ; for instance , if a word has a depth of three or more , it is considered to be at least three levels deeper than the first level . gloss_length measures the glosses ’ length relative to the ground truth glosses , which can be viewed as a proxy for judging the gloss ’ s depth . in addition , the next feature is nr_of_partofs , which ranks the number of partofs in the target domain . in the case of the fruit and music genres , there are a number of synonyms and hypernyms in the feature set , which we can observe as a way of measuring the degree of overlap between the target and the source domain words . in this sense , the ranking is similar to that of direct hypernym removal , where words which are not directly used in the original wordnet are removed , and words which appear in both the source and target domains are removed . the ranking of feature importance can also be seen in the table ."
    },
    {
        "table_id": "401",
        "table_info": {
            "table_caption": "Table 3: Balanced accuracy and κ of predictions made in a new domain, with or without normalization.",
            "table_column_names": [
                "Normalized features: New",
                "Normalized features: Trained",
                "None Bal.",
                "None  [ITALIC] κ",
                "Structural Bal.",
                "Structural  [ITALIC] κ",
                "Lexical Bal.",
                "Lexical  [ITALIC] κ",
                "Frequency Bal.",
                "Frequency  [ITALIC] κ"
            ],
            "table_content_values": [
                [
                    "domain",
                    "on",
                    "Acc.",
                    "[EMPTY]",
                    "Acc.",
                    "[EMPTY]",
                    "Acc.",
                    "[EMPTY]",
                    "Acc.",
                    "[EMPTY]"
                ],
                [
                    "Tools",
                    "Fruit+Music",
                    "0.69",
                    "0.40",
                    "0.84",
                    "0.74",
                    "0.83",
                    "0.68",
                    "0.65",
                    "0.30"
                ],
                [
                    "Fruit",
                    "Tools+Music",
                    "0.66",
                    "0.30",
                    "0.82",
                    "0.62",
                    "0.73",
                    "0.43",
                    "0.66",
                    "0.31"
                ],
                [
                    "Music",
                    "Tools+Fruit",
                    "0.62",
                    "0.27",
                    "0.68",
                    "0.37",
                    "0.73",
                    "0.41",
                    "0.55",
                    "0.12"
                ]
            ]
        },
        "gold_description": "we find that normalization of both structural and lexical features leads to a significant performance gain . normalization of the frequency feature seems to hurt performance ( table 3 )",
        "generated_description": "in order to test the importance of normalization for domain adaptation , we evaluate the accuracy and κ of the new domain adaptation experiments made in a new domain , with or without normalization . we see that , in general , using normalized features yields balanced accuracy across all domains . the exception is the “ none ” domain , where the performance of the models using none or structural normalization is higher . this indicates that the models benefit from having access to information about the target domain during training . the fact that these models are able to adapt to new domains confirms that they are not overfit to the new ones ."
    },
    {
        "table_id": "402",
        "table_info": {
            "table_caption": "Table 1: Distribution of the event mentions per POS per token in all datasets of the EVENTI corpus.",
            "table_column_names": [
                "POS",
                "Training",
                "Dev.",
                "Test"
            ],
            "table_content_values": [
                [
                    "Noun",
                    "6,710",
                    "111",
                    "1,499"
                ],
                [
                    "Verb",
                    "11,269",
                    "193",
                    "2,426"
                ],
                [
                    "Adjective",
                    "610",
                    "9",
                    "118"
                ],
                [
                    "Preposition",
                    "146",
                    "1",
                    "25"
                ],
                [
                    "Overall Event Tokens",
                    "18,735",
                    "314",
                    "4,068"
                ]
            ]
        },
        "gold_description": "tables 1 and 2 report , respectively , the distribution of the events per token part - of speech ( pos ) and per event class . verbs are the largest annotated category , followed by nouns , adjectives , and prepositional phrases .",
        "generated_description": "for pos tagging , we use the official training , development , and test sets provided by the multinli ( muse ) command center . in total , we obtain 18 , 735 event tokens and 4 , 068 event mentions per pos tag . the distribution of pos tags per token in all datasets of the eventi corpus is similar to that of the conll - 2005 ner corpus , i . e . , a large number of event mentions are nouns , verbs , adjectives , adverbs , and prepositions , with the exception of a single preposition token ."
    },
    {
        "table_id": "403",
        "table_info": {
            "table_caption": "Table 2: Distribution of the event mentions per class in all datasets of the EVENTI corpus.",
            "table_column_names": [
                "Class",
                "Training",
                "Dev.",
                "Test"
            ],
            "table_content_values": [
                [
                    "OCCURRENCE",
                    "9,041",
                    "162",
                    "1,949"
                ],
                [
                    "ASPECTUAL",
                    "446",
                    "14",
                    "107"
                ],
                [
                    "I_STATE",
                    "1,599",
                    "29",
                    "355"
                ],
                [
                    "I_ACTION",
                    "1,476",
                    "25",
                    "357"
                ],
                [
                    "PERCEPTION",
                    "162",
                    "2",
                    "37"
                ],
                [
                    "REPORTING",
                    "714",
                    "8",
                    "149"
                ],
                [
                    "STATE",
                    "4,090",
                    "61",
                    "843"
                ],
                [
                    "Overall Events",
                    "17,528",
                    "301",
                    "3,798"
                ]
            ]
        },
        "gold_description": "tables 1 and 2 report , respectively , the distribution of the events per token part - of speech ( pos ) and per event class . as for the classes , occurrence and state represent the large majority of all events , followed by the intensional ones ( i state and action ) , expressing some factual relationship between the target events and their arguments , and finally the others ( reporting , aspectual , and perception ) .",
        "generated_description": "the distribution of the event mentions per class is shown in table 2 . the first class is occurrence , which is composed of 9 , 041 event mentions . the second class is aspectual , and the third class is i_state , which consists of 29 events . finally , the last class is reporting , which accounts for 843 events . in total , 3 , 798 event mentions are associated with three main events , i . e . , state , perception and reporting ."
    },
    {
        "table_id": "404",
        "table_info": {
            "table_caption": "Table 3: Results for Bubtask B Main Task - Event detection and classification.",
            "table_column_names": [
                "Embedding Parameter",
                "Strict Evaluation R",
                "Strict Evaluation P",
                "Strict Evaluation F1",
                "Strict Evaluation F1-class",
                "Relaxed Evaluation R",
                "Relaxed Evaluation P",
                "Relaxed Evaluation F1",
                "Relaxed Evaluation F1-class"
            ],
            "table_content_values": [
                [
                    "Berardi2015_w2v",
                    "0.868",
                    "0.868",
                    "0.868",
                    "0.705",
                    "0.892",
                    "0.892",
                    "0.892",
                    "0.725"
                ],
                [
                    "Berardi2015_Glove",
                    "0.848",
                    "0.872",
                    "0.860",
                    "0.697",
                    "0.870",
                    "0.895",
                    "0.882",
                    "0.714"
                ],
                [
                    "Fastext-It",
                    "[BOLD] 0.897",
                    "0.863",
                    "[BOLD] 0.880",
                    "[BOLD] 0.736",
                    "[BOLD] 0.921",
                    "0.887",
                    "[BOLD] 0.903",
                    "[BOLD] 0.756"
                ],
                [
                    "ILC-ItWack",
                    "0.831",
                    "[BOLD] 0.884",
                    "0.856",
                    "0.702",
                    "0.860",
                    "[BOLD] 0.914",
                    "0.886",
                    "0.725"
                ],
                [
                    "DH-FBK_100",
                    "0.855",
                    "0.859",
                    "0.857",
                    "0.685",
                    "0.881",
                    "0.885",
                    "0.883",
                    "0.705"
                ],
                [
                    "FBK-HLT@EVENTI 2014",
                    "0.850",
                    "[ITALIC] 0.884",
                    "0.867",
                    "0.671",
                    "0.868",
                    "0.902",
                    "0.884",
                    "0.685"
                ]
            ]
        },
        "gold_description": "results for the experiments are illustrated in table 3 . the network obtains the best f1 score , both for detection ( f1 of 0 . 880 for strict evaluation and 0 . 903 for relaxed evaluation with fastext - it embeddings ) and for classification ( f1 - class of 0 . 756 for strict evaluation , and 0 . 751 for relaxed evaluation with fastext - it embeddings ) . although fbkhlt suffers in the classification subtask , it qualifies as a highly competitive system for the detection subtask . by observing the strict f1 scores , fbk - hlt beats three configurations ( dh - fbk100 , ilc - itwack , berardi2015 glove ) 11 , almost equals one ( berardi2015 w2v ) 12 , and it is outperformed only by one ( fastext - it ) 13 . in the relaxed evaluation setting , dh - fbk - 100 is the only configuration that does not beat fbk - hlt ( although the difference is only 0 . 001 point ) . nevertheless , it is remarkable to observe that fbk - hlt has a very high precision ( 0 . 902 , relaxed evaluation mode ) , that is overcome by only one embedding configuration , ilc - itwack . the results also indicates that word embeddings have a major contribution on recall , these results are obtained using a single step approach ,",
        "generated_description": "we compare the performance of the proposed model fastext - it with the original embedding methods developed by berardi et al . the original embeddings were developed by \\ newciteberardi2015w2v and \\ newciteneelakantan2015glove . the relaxed version of this model uses glove pennington2014glove as the penultimate layer of the network and uses a number of hand - crafted features including pos tags , collocations , and relational tags as the final layer . we also compare our model with the proposed ilc - itwack , which is a variant of the model developed by ilc researches and tested on the fbk dataset . for fair comparison , we use the same hyperparameters as in the original paper , i . e . , the number of lstm layers , number of nodes in the network , the distance between the embedding vector and the true label of the tweet , and the tweet ’ s character embedding . we observe that our model performs the best on both strict and relaxed evaluation . our model significantly outperforms all the other models and achieves the best relaxed accuracy ."
    },
    {
        "table_id": "405",
        "table_info": {
            "table_caption": "Table 1: Performance comparison on the SwitchBoard dataset (P: n-gram precision, R: n-gram recall, A: Average, E: Extrema, G: Greedy, L: average length)",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] BLEU R",
                "[BOLD] BLEU P",
                "[BOLD] BLEU F1",
                "[BOLD] BOW Embedding A",
                "[BOLD] BOW Embedding E",
                "[BOLD] BOW Embedding G",
                "[BOLD] intra-dist dist-1",
                "[BOLD] intra-dist dist-2",
                "[BOLD] inter-dist dist-1",
                "[BOLD] inter-dist dist-2",
                "[BOLD] L"
            ],
            "table_content_values": [
                [
                    "HRED",
                    "0.262",
                    "0.262",
                    "0.262",
                    "0.820",
                    "0.537",
                    "0.832",
                    "0.813",
                    "0.452",
                    "0.081",
                    "0.045",
                    "12.1"
                ],
                [
                    "SeqGAN",
                    "0.282",
                    "[BOLD] 0.282",
                    "0.282",
                    "0.817",
                    "0.515",
                    "0.748",
                    "0.705",
                    "0.521",
                    "0.070",
                    "0.052",
                    "[BOLD] 17.2"
                ],
                [
                    "CVAE",
                    "0.295",
                    "0.258",
                    "0.275",
                    "0.836",
                    "0.572",
                    "0.846",
                    "0.803",
                    "0.415",
                    "0.112",
                    "0.102",
                    "12.4"
                ],
                [
                    "CVAE-BOW",
                    "0.298",
                    "0.272",
                    "0.284",
                    "0.828",
                    "0.555",
                    "0.840",
                    "0.819",
                    "0.493",
                    "0.107",
                    "0.099",
                    "12.5"
                ],
                [
                    "CVAE-CO",
                    "0.299",
                    "0.269",
                    "0.283",
                    "0.839",
                    "0.557",
                    "0.855",
                    "0.863",
                    "0.581",
                    "0.111",
                    "0.110",
                    "10.3"
                ],
                [
                    "VHRED",
                    "0.253",
                    "0.231",
                    "0.242",
                    "0.810",
                    "0.531",
                    "0.844",
                    "[BOLD] 0.881",
                    "0.522",
                    "0.110",
                    "0.092",
                    "8.74"
                ],
                [
                    "VHCR",
                    "0.276",
                    "0.234",
                    "0.254",
                    "0.826",
                    "0.546",
                    "0.851",
                    "0.877",
                    "0.536",
                    "0.130",
                    "0.131",
                    "9.29"
                ],
                [
                    "DialogWAE",
                    "0.394",
                    "0.254",
                    "0.309",
                    "0.897",
                    "0.627",
                    "0.887",
                    "0.713",
                    "0.651",
                    "0.245",
                    "0.413",
                    "15.5"
                ],
                [
                    "DialogWAE-GMP",
                    "[BOLD] 0.420",
                    "0.258",
                    "[BOLD]  0.319",
                    "[BOLD] 0.925",
                    "[BOLD]  0.661",
                    "[BOLD]  0.894",
                    "0.713",
                    "[BOLD]  0.671",
                    "[BOLD] 0.333",
                    "[BOLD]  0.555",
                    "15.2"
                ]
            ]
        },
        "gold_description": "tables 1 and 2 show the performance of dialogwae and baselines on the two datasets . dialogwae outperforms the baselines in the majority of the experiments . in terms of bleu scores , dialogwae ( with a gaussian mixture prior network ) generates more relevant responses , with the average recall of 42 . 0 % and 37 . 2 % on both of the datasets . these are significantly higher than those of the cvae baselines ( 29 . 9 % and 26 . 5 % ) . we observe a similar trend to the bow embedding metrics .",
        "generated_description": "from the table , we can see that our model outperforms all the baseline models on intra - and inter - distributional bleu scores , which indicates the effectiveness of our model in response selection and response selection . our model also achieves the best inter - and intra - distinct score on l , which means our model can generate more diverse responses by choosing the appropriate response for different queries ."
    },
    {
        "table_id": "406",
        "table_info": {
            "table_caption": "Table 5: Human judgments for models trained on the Dailydialog dataset",
            "table_column_names": [
                "Model",
                "Coherence",
                "Diversity",
                "Informative"
            ],
            "table_content_values": [
                [
                    "CVAE-CO",
                    "14.4%",
                    "19.2%",
                    "24.8%"
                ],
                [
                    "VHCR",
                    "26.8%",
                    "22.4%",
                    "20.4%"
                ],
                [
                    "DialogWAE",
                    "27.6%",
                    "[BOLD] 29.2%",
                    "25.6%"
                ],
                [
                    "DialogWAE-GMP",
                    "[BOLD] 31.6%",
                    "[BOLD] 29.2%",
                    "[BOLD] 29.6%"
                ]
            ]
        },
        "gold_description": "to validate the previous results , we further conduct a human evaluation with amazon mechanical turk . we randomly selected 50 dialogues from the test set of dailydialog . for each dialogue context , we generated 10 responses from each of the four models . responses for each context were inspected by 5 participants who were asked to choose the model which performs the best in regarding to coherence , diversity and informative while being blind to the underlying algorithms . the average percentages that each model was selected as the best to a specific criterion are shown in table 5 .",
        "generated_description": "the results show that dialogwae outperforms all other models in terms of diversity , coherence , and informativeness by a large margin ( p - value < 0 . 01 ) , which shows that the generated responses are more fluent and context - coherent than those from the baseline models . it is worth noting that the diversity of generated responses from vhcr and cvae - co is lower than that from dialoggmp , which we attribute to the limited number of human - generated response examples in the dailydialog dataset . the results also show that the proposed dialogdialogwae - gmp outperforms the other two variants significantly . we attribute this to the fact that the dialogues in dailydialog are more natural and conversational than those in the wikiqa dataset ."
    },
    {
        "table_id": "407",
        "table_info": {
            "table_caption": "Table 2: Comparison between our proposed methods and baselines. RL Look-ahead model achieves the highest score for all three aspects: Empathy, Relevance, and Fluency. MultiSeq model achieves highest BLEU score, but not significant.",
            "table_column_names": [
                "[EMPTY]",
                "BLEU",
                "Empathy",
                "Relevance",
                "Fluency"
            ],
            "table_content_values": [
                [
                    "Gold",
                    "-",
                    "3.651",
                    "3.752",
                    "3.718"
                ],
                [
                    "Seq2Seq",
                    "2.29",
                    "3.013",
                    "3.173",
                    "2.967"
                ],
                [
                    "MultiSeq",
                    "[BOLD] 2.45",
                    "2.979",
                    "3.25",
                    "2.952"
                ],
                [
                    "RL Current",
                    "2.31",
                    "3.18",
                    "3.187",
                    "3.14"
                ],
                [
                    "RL Look-ahead",
                    "2.32",
                    "[BOLD] 3.327",
                    "[BOLD] 3.593",
                    "[BOLD] 3.327"
                ]
            ]
        },
        "gold_description": "from the bleu scores in table 2 , we can see that although multiseq has the highest bleu score , all models have similar bleu scores and the difference is not that significant . we can clearly notice from human evaluations shown in table 2 that our model , rl look - ahead , outperforms all of the others in all three evaluated categories .",
        "generated_description": "our proposed method outperforms all baselines in terms of all three aspects : empathy , relevance , and fluency . multiseq achieves the highest bleu score , but not significant . seq2seq and rl current are two neural sequence - to - sequence models with reinforcement learning , and rl look - ahead achieves the best score for the other two aspects . this indicates that our proposed method can respond more appropriately to the user ’ s emotional needs ."
    },
    {
        "table_id": "408",
        "table_info": {
            "table_caption": "Table 1: We report the best performance observed in 5 runs on the development sets of both SParC and CoSQL, since their test sets are not public. We also conduct Wilcoxon signed-rank tests between our method and the baselines, and the bold results show the improvements of our model are significant with p < 0.005.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] SParC Ques.Match",
                "[BOLD] SParC Int.Match",
                "[BOLD] CoSQL Ques.Match",
                "[BOLD] CoSQL Int.Match"
            ],
            "table_content_values": [
                [
                    "SyntaxSQL-con",
                    "18.5",
                    "4.3",
                    "15.1",
                    "2.7"
                ],
                [
                    "CD-Seq2Seq",
                    "21.9",
                    "8.1",
                    "13.8",
                    "2.1"
                ],
                [
                    "EditSQL",
                    "33.0",
                    "16.4",
                    "22.2",
                    "5.8"
                ],
                [
                    "Ours",
                    "[BOLD] 41.8",
                    "[BOLD] 20.6",
                    "[BOLD] 33.5",
                    "[BOLD] 9.6"
                ],
                [
                    "EditSQL + BERT",
                    "47.2",
                    "29.5",
                    "40.0",
                    "11.0"
                ],
                [
                    "Ours + BERT",
                    "[BOLD] 52.6",
                    "[BOLD] 29.9",
                    "[BOLD] 41.0",
                    "[BOLD] 14.0"
                ]
            ]
        },
        "gold_description": "taking concat as a representative , we compare the performance of our model with other models , as shown in table 1 . as illustrated , our model outperforms baselines by a large margin with or without bert , achieving new sota performances on both datasets . compared with the previous sota without bert on sparc , our model improves ques . match and int . match by 10 . 6 and 5 . 4 points , respectively .",
        "generated_description": "we report the best observed performance observed in 5 runs on the development set of both sparc and cosql , since their test sets are not public . we also conduct wilcoxon signed - rank tests between our method and the baselines , and the significant differences are shown with p - value < 0 . 005 . as we can see , our model significantly outperforms the three supervised baselines across all the metrics . the editsql + bert model achieves the best performance , since it is pre - trained on a large amount of sql training data . however , the performance of our model is still significantly better than the other two supervised models and close to that of the state - of - the - art ."
    },
    {
        "table_id": "409",
        "table_info": {
            "table_caption": "Table 3: Results on the unsupervised hypernym detection and direction prediction tasks. The first three rows of results are from Roller, Kiela, and Nickel (2018). The HyperbolicCones results were reported by Le et al. (2019). The improvements in LEDS and BLESS benchmark are statistically significant with two-tailed p values being 0.019 and ≤ 0.001 respectively.",
            "table_column_names": [
                "[EMPTY]",
                "Detection (Average Precision) BLESS",
                "Detection (Average Precision) EVAL",
                "Detection (Average Precision) LEDS",
                "Detection (Average Precision) SHWARTZ",
                "Detection (Average Precision) WBLESS",
                "Direction (Average Accuracy) BLESS",
                "Direction (Average Accuracy) WBLESS",
                "Direction (Average Accuracy) BIBLESS"
            ],
            "table_content_values": [
                [
                    "Count based p(x,y)",
                    ".49",
                    ".38",
                    ".71",
                    ".29",
                    ".74",
                    ".46",
                    ".69",
                    ".62"
                ],
                [
                    "ppmi(x,y)",
                    ".45",
                    ".36",
                    ".70",
                    ".28",
                    ".72",
                    ".46",
                    ".68",
                    ".61"
                ],
                [
                    "SVD ppmi(x,y)",
                    ".76",
                    ".48",
                    ".84",
                    ".44",
                    ".96",
                    ".96",
                    ".87",
                    ".85"
                ],
                [
                    "HyperbolicCones",
                    "[BOLD] .81",
                    "[BOLD] .50",
                    ".89",
                    "[BOLD] .50",
                    "[BOLD] .98",
                    ".94",
                    ".90",
                    ". [BOLD] 87"
                ],
                [
                    "Proposed SPON",
                    "[BOLD] .81",
                    "[BOLD] .50",
                    "[BOLD] .91",
                    "[BOLD] .50",
                    "[BOLD] .98",
                    "[BOLD] .97",
                    "[BOLD] .91",
                    "[BOLD] .87"
                ]
            ]
        },
        "gold_description": "table 3 shows the results on the unsupervised tasks of hypernym detection and direction predictions , reporting average precision and average accuracy , respectively . the first row titled count based ( in table 3 ) depicts the performance of a hearst - like pattern system baseline , that uses a frequency based threshold to classify candidate hyponym - hypernym pairs as positive ( i . e . exhibiting hypernymy ) or negative ( i . e . not exhibiting hypernymy ) . the ppmi approach in table 3 builds upon the count based approach by using pointwise mutual information values for classification . svd ppmi approach , the main contribution from roller , kiela , and nickel ( 2018 ) builds low - rank embeddings of the ppmi matrix , which allows to make predictions for unseen pairs as well . hyperboliccones is the sota ( le et al . 2019 ) in both these tasks . the final row reports the application of spon ( on the input provided by svd ppmi ) which is an original contribution of our work .",
        "generated_description": "detection . as we can see , our proposed spon outperforms the count based p ( x , y ) method and the svd - based ppmi method on bless and wbless , and the hyperboliccones method on leds and shwartz . it should be noted that , in terms of the average accuracy , our spon method outperforms all the other methods on all the three tasks . in addition , our method has the highest average precision on the bless direction and the second highest on the bibless direction , which shows that our method is better at focusing on the right direction ."
    },
    {
        "table_id": "410",
        "table_info": {
            "table_caption": "Table 4: Ablation tests reporting Average Precision values on the unsupervised hypernym detection task, signifying the choice of layers utilized in our proposed SPON model. The first row represents SPON i.e. a RELU layer followed by a Residual connection. The second row removes the Residual connection, whereas the third row substitutes the non-negative activation layer RELU with Tanh that can take negative values.",
            "table_column_names": [
                "[EMPTY]",
                "BLESS",
                "EVAL",
                "LEDS",
                "WBLESS"
            ],
            "table_content_values": [
                [
                    "[ITALIC] RELU+ [ITALIC] Residual",
                    "[BOLD] .81",
                    "[BOLD] .50",
                    "[BOLD] .91",
                    "[BOLD] .98"
                ],
                [
                    "[ITALIC] RELU Only",
                    ".73",
                    ".49",
                    ".82",
                    ".96"
                ],
                [
                    "[ITALIC] Tanh+ [ITALIC] Residual",
                    ".79",
                    ".49",
                    ".90",
                    "[BOLD] .98"
                ]
            ]
        },
        "gold_description": "the analysis in section 4 which shows that our choice of function f satisfies asymmetry and transitive properties , holds true because f satisfies f ( ( cid : 126 ) x ) ≥ ( cid : 126 ) x component - wise . table 4 shows the results for each of these ablation experiments , when evaluated on the unsupervised hypernym detection task across four datasets chosen randomly . removing the residual layer and using relu activation function only , violates the aforementioned component - wise inequality f ( ( cid : 126 ) x ) ≥ ( cid : 126 ) x , and has the worst results out of the three . on the other hand , using residual connections with tanh activations may not violate the aforementioned inequality , since , it depends upon the sign of the activation outputs . this argument is supported by the results in table 4 , wherein using tanh activations instead of relu almost provides identical results , except for the bless dataset . nevertheless , the results in table 4 show that encouraging asymmetry and transitive properties for this task , in fact improves the results as opposed to not doing the same .",
        "generated_description": "we perform ablation tests reporting average precision ( ap ) on the unsupervised hypernym detection task using the different layers in our proposed spon model . the first row reports the performance of a relu layer followed by a residual connection , while the second row reports spon with a non - negative activation layer relu replaced by tanh that can take negative values . we observe that the combination of relu and residual connections leads to the best performance , indicating the importance of the two complementary layers in spon . we also observe that removing the residual connection results in a decrease in performance ."
    },
    {
        "table_id": "411",
        "table_info": {
            "table_caption": "Table 5: Results on the unsupervised hypernym detection task for BLESS dataset. With 13,089 test instances, the improvement in Average Precision values obtained by SPON as compared against Smoothed Box model is statistically significant with two-tailed p value equals 0.00116.",
            "table_column_names": [
                "Method",
                "Average Precision"
            ],
            "table_content_values": [
                [
                    "OE ",
                    "0.761"
                ],
                [
                    "Smoothed Box ",
                    "0.795"
                ],
                [
                    "SPON (Our Approach)",
                    "[BOLD] 0.811"
                ]
            ]
        },
        "gold_description": "furthermore , table 5 illustrates the results on the unsupervised hypernym detection task for bless dataset , wherein we compare our proposed spon model to other supervised sota approaches for hypernym prediction task , namely order embeddings ( oe ) approach as introduced by ( vendrov et al . 2016 ) , and smoothed box model as introduced by ( li et al . 2019 ) .",
        "generated_description": "spon outperforms smoothed box in unsupervised hypernym detection task . with 13 , 089 test instances , the difference in average precision values obtained by spon is statistically significant compared with the baseline ( 0 . 761 ) with two - tailed p - value equals 0 . 00116 ."
    },
    {
        "table_id": "412",
        "table_info": {
            "table_caption": "Table 2: Limited length ROUGE recall results on the NYT50 test set.",
            "table_column_names": [
                "Model",
                "R-1",
                "R-2"
            ],
            "table_content_values": [
                [
                    "First sentences",
                    "28.6",
                    "17.3"
                ],
                [
                    "First  [ITALIC] k words",
                    "35.7",
                    "21.6"
                ],
                [
                    "Full ",
                    "42.2",
                    "24.9"
                ],
                [
                    "ML+RL+intra-attn ",
                    "42.94",
                    "26.02"
                ],
                [
                    "Two-Stage + RL (Ours)",
                    "[BOLD] 45.33",
                    "[BOLD] 26.53"
                ]
            ]
        },
        "gold_description": "also we compare performance of our model with two recent models , we see 2 . 39 rouge - 1 improvements compared to the ml + rl with intra - attn approach ( previous sota ) carries over to this dataset , which is a large margin . on rouge - 2 , our model also get an im - provement of 0 . 51 . the experiment proves that our approach can outperform competitive methods on different data distributions .",
        "generated_description": "our two - stage + rl model outperforms the previous best model using only the first k words ( row “ full ” ) and the ml + rl + intra - attn model by a large margin ( 5 . 3 rouge - 1 and 2 points ) . our model with two - stage learning and rl achieves a further improvement of 1 . 2 and 0 . 7 rouges on the limited length version of the nyt50 test set respectively ."
    },
    {
        "table_id": "413",
        "table_info": {
            "table_caption": "Table 7: Results on SemEval 2018 Domain-specific hypernym discovery task. CRIM is the best system on the domain specific datasets.",
            "table_column_names": [
                "[EMPTY]",
                "[BOLD] Music MAP",
                "[BOLD] Music MRR",
                "[BOLD] Music P@5"
            ],
            "table_content_values": [
                [
                    "CRIM",
                    "40.97",
                    "60.93",
                    "41.31"
                ],
                [
                    "SPON",
                    "[BOLD] 54.70",
                    "[BOLD] 71.20",
                    "[BOLD] 56.30"
                ],
                [
                    "[EMPTY]",
                    "[BOLD] Medical",
                    "[BOLD] Medical",
                    "[BOLD] Medical"
                ],
                [
                    "[EMPTY]",
                    "MAP",
                    "MRR",
                    "P@5"
                ],
                [
                    "CRIM",
                    "[BOLD] 34.05",
                    "[BOLD] 54.64",
                    "[BOLD] 36.77"
                ],
                [
                    "SPON",
                    "33.50",
                    "50.60",
                    "35.10"
                ]
            ]
        },
        "gold_description": "similarly , table 7 shows the results on the two domain - specific tasks of music and medical domain corpora . spon outperforms the sota systems in all tasks except for the medical domain in which it achieves comparable results .",
        "generated_description": "domain - specific hypernym discovery task . as we can see , crim is the best system on the general domain specific datasets , and spon is the worst one on the domain - specific datasets . crim outperforms spon by a large margin in terms of map , mrr and p @ 5 on all three datasets ."
    },
    {
        "table_id": "414",
        "table_info": {
            "table_caption": "Table 1: Average embedding similarity scores between the output and the target output in terms of Real target output list.",
            "table_column_names": [
                "[BOLD] Length",
                "1-3",
                "4-6",
                "7-10"
            ],
            "table_content_values": [
                [
                    "Real Input",
                    "0.439",
                    "0.518",
                    "0.566"
                ],
                [
                    "Pre-trained Greedy",
                    "0.446",
                    "0.529",
                    "0.559"
                ],
                [
                    "RL Greedy",
                    "0.486",
                    "0.560",
                    "0.588"
                ],
                [
                    "RL BeamSearch(50)",
                    "0.599",
                    "0.678",
                    "0.709"
                ],
                [
                    "RL BeamSearch(200)",
                    "0.621",
                    "0.694",
                    "0.726"
                ]
            ]
        },
        "gold_description": "we calculate these similarity scores for each real targeted output for the real inputs and the inputs found by the proposed model and report the average value in table 1 . according to the table , we observe that even inputting the real inputs , the similarity scores between the outputs and the target outputs are not high . besides , with the crafted inputs from the proposed framework , these similarity scores are significantly improved . for example , for rl beamsearch ( 200 ) , the similarity is improved by 41 . 5 % , 34 . 0 % and 28 . 3 % for the target outputs with length 1 - 3 , 4 - 6 and 7 - 10 , respectively .",
        "generated_description": "the embedding similarity score between the output and the target output is defined as the ratio of the number of items in the embedding space that are similar to the real target output list . higher similarity scores indicate that the output is more semantically closer to the target . for example , the similarity score of the output with the real input is 0 . 518 and 0 . 566 in length 1 − 3 , 4 - 6 and 7 - 10 , respectively . the similarity score for the output from the pre - trained greedy and rl models is much higher than that of the real inputs , indicating that the target responses are very similar to those of the input . on the other hand , the rl models achieve much higher similarity scores than the pre - trained greedy and rl greedy models . this indicates that the rl model is able to exploit the similarity information in the input to improve the target response generation ."
    },
    {
        "table_id": "415",
        "table_info": {
            "table_caption": "Table 1: Test classification accuracy (and the number of parameters used). The bottom part shows our ablation results: SoPa: our full model. SoPams1: running with max-sum semiring (rather than max-product), with the identity function as our encoder E (see Equation 3). sl: self-loops, ϵ: ϵ transitions. The final row is equivalent to a one-layer CNN.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] ROC",
                "[BOLD] SST",
                "[BOLD] Amazon"
            ],
            "table_content_values": [
                [
                    "[BOLD] Hard",
                    "62.2 (4K)",
                    "75.5 (6K)",
                    "88.5 (67K)"
                ],
                [
                    "[BOLD] DAN",
                    "64.3 (91K)",
                    "83.1 (91K)",
                    "85.4 (91K)"
                ],
                [
                    "[BOLD] BiLSTM",
                    "65.2 (844K)",
                    "84.8 (1.5M)",
                    "[BOLD] 90.8 (844K)"
                ],
                [
                    "[BOLD] CNN",
                    "64.3 (155K)",
                    "82.2 (62K)",
                    "90.2 (305K)"
                ],
                [
                    "[BOLD] SoPa",
                    "[BOLD] 66.5 (255K)",
                    "[BOLD] 85.6 (255K)",
                    "90.5 (256K)"
                ],
                [
                    "[BOLD] SoPa [ITALIC] ms1",
                    "64.4",
                    "84.8",
                    "90.0"
                ],
                [
                    "[BOLD] SoPa [ITALIC] ms1∖{ [ITALIC] sl}",
                    "63.2",
                    "84.6",
                    "89.8"
                ],
                [
                    "[BOLD] SoPa [ITALIC] ms1∖{ [ITALIC] ϵ}",
                    "64.3",
                    "83.6",
                    "89.7"
                ],
                [
                    "[BOLD] SoPa [ITALIC] ms1∖{ [ITALIC] sl, [ITALIC] ϵ}",
                    "64.0",
                    "85.0",
                    "89.5"
                ]
            ]
        },
        "gold_description": "table 1 shows our main experimental results . in two of the cases ( sst and roc ) , sopa outperforms all models . on amazon , sopa performs within 0 . 3 points of cnn and bilstm , and outperforms the other two baselines . the table also shows the number of parameters used by each model for each task . sopa performs better or roughly the same as a bilstm , which has 3 – 6 times as many parameters . table 1 also shows an ablation of the differences between sopa and cnn : max - product semiring with sigmoid vs . max - sum semiring with identity , self - loops , and ( cid : 15 ) - transitions . the last line is equivalent to a cnn with multiple window sizes . interestingly , the most notable difference between sopa and cnn is the semiring and encoder function , while ( cid : 15 ) transitions and self - loops have little effect on performance . 1",
        "generated_description": "in general , our sopa method outperforms all the baselines and achieves the best performance on all the four datasets . on the roc and sst datasets , sopa has an average sopams1 of 66 . 5 ms1 and 85 . 6 ms1 , outperforming the previous state - of - the - art cnn and bilstm by a large margin . on amazon , the sopa ms1 ∖ { s sl } baseline sopam serves as a strong baseline with an accuracy of 89 . 8 % , compared to 90 . 0 % and 90 . 5 % by the original sopa . the sopa results on the amazon dataset are competitive with sopa , and outperform the original cnn by a significant margin of 1 . 4 % . on the yelp sentiment analysis dataset , there is a huge performance gap between sopa and original cnn , showing the importance of explicitly modeling the sentiment information ."
    },
    {
        "table_id": "416",
        "table_info": {
            "table_caption": "Table 4: Cohen’s Kappa score (κ) and observed agreement (Ao) for gold standard dialogue",
            "table_column_names": [
                "[BOLD] Annotators",
                "[ITALIC] Ao",
                "[ITALIC] κ"
            ],
            "table_content_values": [
                [
                    "Questions",
                    "0.73",
                    "0.63"
                ],
                [
                    "Features",
                    "0.90",
                    "0.67"
                ],
                [
                    "Answers",
                    "0.59",
                    "0.49"
                ]
            ]
        },
        "gold_description": "to evaluate the annotations , inter - annotator agreement was calculated based on a subset of the gold standard corpus . 4 table 4 illustrates the values of observed agreement ( ao ) and cohen ' s κ ( cohen , 1960 ) obtained for question , feature and answer annotation . the agreement values obtained for question types were over 0 . 6 ( for all annotators combined ) .",
        "generated_description": "for each annotator , we measured the cohen ’ s kappa score ( κ ) and the observed agreement ( ao ) between the annotators . the higher the value of the kappa value , the more agreed the annotator was on the given definition of the “ gold standard ” . as expected , features are the most helpful features , with a value of 0 . 90 for the features , and 0 . 67 for answers . questions are the second best source of agreement , followed by answers , which are the next best source ."
    },
    {
        "table_id": "417",
        "table_info": {
            "table_caption": "Table 3: Experimental results. The best result for each dataset is indicated in bold, and marked with “†” if it is significantly higher than the second best result (based on a one-tailed Wilcoxon signed-rank test; p<0.05). The results of Benchmark on the arXiv dataset are from the original paper, where the standard deviation values were not reported. All neural models except for Inceptionfixed have larger standard deviation values on arXiv than Wikipedia, which can be explained by the small size of the arXiv test set.",
            "table_column_names": [
                "Wikipedia",
                "Wikipedia",
                "Majority 16.7%",
                "Benchmark 46.7±0.34%",
                "Doc2Vec 23.2±1.41%",
                "Inceptionfixed 43.7±0.51",
                "biLSTM 54.1±0.47%",
                "Inception 57.0±0.63%",
                "Joint  [BOLD] 59.4±0.47%†"
            ],
            "table_content_values": [
                [
                    "arXiv",
                    "cs.ai",
                    "92.2%",
                    "92.6%",
                    "73.3±9.81%",
                    "92.3±0.29",
                    "91.5±1.03%",
                    "92.8±0.79%",
                    "[BOLD] 93.4±1.07%†"
                ],
                [
                    "arXiv",
                    "cs.cl",
                    "68.9%",
                    "75.7%",
                    "66.2±8.38%",
                    "75.0±1.95",
                    "76.2±1.30%",
                    "76.2±2.92%",
                    "[BOLD] 77.1±3.10%"
                ],
                [
                    "arXiv",
                    "cs.lg",
                    "67.9%",
                    "70.7%",
                    "64.7±9.08%",
                    "73.9±1.23",
                    "[BOLD] 81.1±0.83%",
                    "79.3±2.94%",
                    "79.9±2.54%"
                ]
            ]
        },
        "gold_description": "experimental results table 3 shows the performance of the different models over our two datasets , in the form of the average accuracy on the test set ( along with the standard deviation ) over 10 runs , with different random initializations . on wikipedia , we observe that the performance of bilstm , inception , and joint is much better than that of all four baselines . inception achieves 2 . 9 % higher accuracy than bilstm . the performance of joint achieves an accuracy of 59 . 4 % , which is 5 . 3 % higher than using textual features alone ( bilstm ) and 2 . 4 % higher than using visual features alone ( inception ) . based on a one - tailed wilcoxon signed - rank test , the performance of jis sta of - the - art results in combination . for arxiv , baseline methods majority , benchmark , and inceptionfixed outperform bilstm over cs . ai , in large part because of the class imbalance in this dataset ( 90 % of papers are rejected ) . surprisingly , inceptionfixed is better than majority and benchmark over the arxiv cs . lg subset , which verifies the usefulness of visual features , even when only the last layer is fine - tuned . table 3 also shows that inception and bilstm achieve similar performance on arxiv , showing that textual and visual representations are equally discriminative : inception and bilstm are indistinguishable over cs . cl ; bilstm achieves 1 . 8 % higher accuracy over cs . lg , while inception achieves 1 . 3 % higher accuracy over cs . ai . once again , the joint model achieves the highest accuracy on cs . ai and cs . cl by combining textual and visual representations (",
        "generated_description": "in detail , on wikipedia , we compare our model with majority voting , benchmark , inception fixed , doc2vec , bilstm , cs . cl , cs . ai , and cs . lg . the best result for cs . cl is on the arxiv dataset , and the second best result is achieved by our joint model with cs . ai . on the other hand , on the wikipedia dataset , our model outperforms the previous state - of - the - art by a large margin , achieving accuracies ranging from 92 . 6 % to 93 . 4 % . the accuracies range from 77 . 1 % to 81 . 1 % . on arxibooks , the accuracies vary from 73 . 9 % to 95 . 2 % . on the wiki dataset , the accuracy range from 93 . 3 % to 99 . 2 % , and the difference is statistically significant ( p < 0 . 001 ) . the accuracy of joint model is higher than that of other models on both datasets , which shows that our model is able to generalize better ."
    },
    {
        "table_id": "418",
        "table_info": {
            "table_caption": "Table 4: Confusion matrix of the Joint model on Wikipedia. Rows are the actual quality classes and columns are the predicted quality classes. The diagonal (gray cells) indicates correct predictions.",
            "table_column_names": [
                "Quality",
                "FA",
                "GA",
                "B",
                "C",
                "Start",
                "Stub"
            ],
            "table_content_values": [
                [
                    "FA",
                    "397",
                    "83",
                    "20",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "GA",
                    "112",
                    "299",
                    "65",
                    "22",
                    "2",
                    "0"
                ],
                [
                    "B",
                    "23",
                    "53",
                    "253",
                    "75",
                    "44",
                    "7"
                ],
                [
                    "C",
                    "5",
                    "33",
                    "193",
                    "124",
                    "100",
                    "12"
                ],
                [
                    "Start",
                    "1",
                    "6",
                    "36",
                    "85",
                    "239",
                    "84"
                ],
                [
                    "Stub",
                    "0",
                    "0",
                    "6",
                    "7",
                    "63",
                    "345"
                ]
            ]
        },
        "gold_description": "table 4 shows the confusion matrix of joint on wikipedia . we can see that more than 50 % of documents for each quality class are correctly classified , except for the class where more documents are misclassified into b .",
        "generated_description": "in order to evaluate the performance of the joint model , we perform an error analysis of the reported results . the misclassification of articles according to the four quality classes fa , ga , b , c , and start lead to an increase in the number of misclassified articles ( from 397 to 397 ) and worsen the performance for the start of the article class ( from 0 to 1 ) . in addition , we also find that for articles classified as start of an article , the model misclassifies the entire article ( from 1 to 6 ) as a span ( i . e . , it should have been taken into account ) . in the future , we can improve the recall of the start class to better predict the actual quality classes ."
    },
    {
        "table_id": "419",
        "table_info": {
            "table_caption": "Table 1: Large-scale text classification data sets",
            "table_column_names": [
                "Data set",
                "Train",
                "Test",
                "classes",
                "Task"
            ],
            "table_content_values": [
                [
                    "AG news",
                    "120k",
                    "7.6k",
                    "4",
                    "English news categorization"
                ],
                [
                    "AG news",
                    "5k",
                    "7.6k",
                    "4",
                    "English news categorization"
                ],
                [
                    "Sogou news",
                    "450k",
                    "60k",
                    "5",
                    "Chinese news categorization"
                ],
                [
                    "Sogou news",
                    "10k",
                    "60k",
                    "5",
                    "Chinese news categorization"
                ]
            ]
        },
        "gold_description": "we present our results on two freely available large scale datasets introduced by zhang et al . . we also shrink the scale of training samples by randomly picking up the same amount of data in each category ( see table 1 ) . ag news the ag news corpus consists of news articles from the ag ' s corpus of news articles on the web pertaining to the 4 largest classes , which are work , sports , business , sci / tech . the data set contains 30 , 000 training samples for each class , 1 , 900 samples for each class for testing . in this data set , there are three columns which are label , title , description , we treat title as abstract input and description as contents input . sogou news a chinese news data set . this data set is a combination of the sogouca and sogoucs news corpora pertaining to 5 categories , which are sports , finance , entertainment , automobile and technology . it contains 450 , 000 training samples and 60 , 000 samples for testing in total . sogou new also has three columns in data set files , label , title , description , similarly , we treat title as abstract input and description as contents input .",
        "generated_description": "we use ag news and sogou news data sets to train and test our models for both english and chinese news categorization tasks . the ag news data set is used for fine - tuning the classification model for ag news , while the soguo news data is used to fine - tune the model for chinese news . for each task , we randomly sample 5k and 10k samples from the training data set and divide them into training , development , and test sets , respectively . we set the number of classifiers for each task as 4 and evaluate them on the held - out test set . we conduct experiments on two large - scale text classification data sets , which are also used to evaluate the performance of our proposed models ."
    },
    {
        "table_id": "420",
        "table_info": {
            "table_caption": "Table 2: Testing error of our model and related studies",
            "table_column_names": [
                "Data set",
                "AG",
                "Sogou",
                "AG(5k)",
                "Sogou(10k)"
            ],
            "table_content_values": [
                [
                    "[ITALIC] - our model",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "SeMemNN-ct",
                    "9.29",
                    "4.73",
                    "16.72",
                    "10.82"
                ],
                [
                    "SeMemNN-abs",
                    "9.04",
                    "4.62",
                    "15.32",
                    "9.80"
                ],
                [
                    "B-SeMemNN-ct",
                    "9.01",
                    "4.52",
                    "15.37",
                    "9.37"
                ],
                [
                    "B-SeMemNN-abs",
                    "8.68",
                    "4.19",
                    "14.35",
                    "8.76"
                ],
                [
                    "SAB-SeMemNN-ct",
                    "8.88",
                    "4.33",
                    "14.07",
                    "7.95"
                ],
                [
                    "SAB-SeMemNN-abs",
                    "8.37",
                    "3.67",
                    "13.79",
                    "7.89"
                ],
                [
                    "[ITALIC] - related studies",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Bow ",
                    "11.19",
                    "7.15",
                    "-",
                    "-"
                ],
                [
                    "Bow TFIDF ",
                    "10.36",
                    "6.55",
                    "-",
                    "-"
                ],
                [
                    "ngrams TFIDF ",
                    "7.64",
                    "2.81",
                    "-",
                    "-"
                ],
                [
                    "Bag-of-means ",
                    "16.91",
                    "10.79",
                    "-",
                    "-"
                ],
                [
                    "LSTM ",
                    "13.94",
                    "4.82",
                    "-",
                    "-"
                ],
                [
                    "char-CNN ",
                    "9.51",
                    "4.39",
                    "-",
                    "-"
                ],
                [
                    "VDCNN ",
                    "[BOLD] 8.67",
                    "[BOLD] 3.18",
                    "-",
                    "-"
                ],
                [
                    "VDCNN",
                    "10.64",
                    "6.53",
                    "19.25",
                    "unable"
                ],
                [
                    "XLNet ",
                    "4.49",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "ULMFiT ",
                    "5.01",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "CNN ",
                    "6.57",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "DPCNN ",
                    "6.87",
                    "1.84",
                    "-",
                    "-"
                ]
            ]
        },
        "gold_description": "the experiment results are presented in table 2 . the best performances of our configurations are highlighted in red , which are 8 . 37 , 3 . 67 , 13 . 79 and 7 . 89 for the error rates of our proposed model on ag , sogou , ag ( 5k ) , sogou ( 10k ) respectively . the bold numbers are the officially reported accuracy of vdcnn , to which our proposed model is close . the numbers in blue are the results coming from our comparison experiment by using vdcnn where we set the sequence length to 256 under word level . from these results we can see that our model outperforms vdcnn on ag news for the official error rate , and is very close to vdcnn ' s performance on sogou news . if we set vcdnn with the same input sequence length ( 256 ) in word - level , the performance of our proposed model is obviously better . most of the contributions come from external matrix from the results of table 2 , in the case of training with a large scale dataset , no matter we use simple lstm or more complex bi - directional lstm with self - attention , the testing error rates of different configurations are basically similar to each other . it can be said that such near state - of - the - art performance mainly attributes to the contribution from external memory . using abstract to build the external memory is better than contents from table 2 we can see that the results of using the description to construct the semantics matrix are better than using the abstract . sememnn can still work on a few - shot learning table 2 shows that although we have greatly shrank the scale of the training set , our proposed method can still outperform vdcnn . after shrinking the scale of the data , the performance of vdcnn has been greatly decreased , especially for sogou news , vdcnn",
        "generated_description": "sab - sememnn - ct outperforms its ct counterpart by a large margin on ag and sogou ( 10k ) data sets , which demonstrates the effectiveness of our proposed model on the task of extractive sentence extraction . on ag data set , our model outperforms all the baselines and achieves comparable performance with the state - of - the - art results of the tfidf - based models , which shows the effectiveness and generalizability of the proposed model ."
    },
    {
        "table_id": "421",
        "table_info": {
            "table_caption": "TABLE II: Summary of BLEU scores from Table I.",
            "table_column_names": [
                "[BOLD] Description",
                "[BOLD] Average"
            ],
            "table_content_values": [
                [
                    "All models",
                    "91.51"
                ],
                [
                    "All prefix models",
                    "[BOLD] 94.43"
                ],
                [
                    "All postfix models",
                    "92.37"
                ],
                [
                    "All infix models",
                    "87.72"
                ],
                [
                    "All pre-trained models",
                    "91.06"
                ],
                [
                    "All non-pre-trained models",
                    "91.96"
                ],
                [
                    "All medium (type 1) models",
                    "90.49"
                ],
                [
                    "All small (type 2) models",
                    "92.09"
                ],
                [
                    "All minimal (type 3) models",
                    "91.94"
                ]
            ]
        },
        "gold_description": "table iii provides detailed results of experiment 2 . the numbers are absolute accuracies , i . e . , they correspond to cases where the arithmetic expression generated is 100 % correct , leading to the correct numeric answer . results by , , , are sparse but indicate the scale of success compared to recent past approaches . prefix , postfix , and infix representations in table iii show that network capabilities are changed by how teachable the target data is . while our networks fell short of ai2 testing accuracy , we present state - of - the - art results for the remaining three datasets . the type 2 postfix transformer received the highest testing average of 87 . 2 % . our attempt at language pre - training fell short of our expectations in all but one tested dataset .",
        "generated_description": "we can see that the best results are obtained by using all prefixes , infixes , and pre - trained word embeddings . the least performing type 1 model is the medium ( type 1 ) model , which achieves 90 . 49 bleu score . among the different types of models , the minimal ( type 3 ) model achieves the best score ."
    },
    {
        "table_id": "422",
        "table_info": {
            "table_caption": "TABLE III: Test results for Experiment 2 (* denotes averages on present values only).",
            "table_column_names": [
                "[BOLD] (Type) Model",
                "[BOLD] AI2",
                "[BOLD] CC",
                "[BOLD] IL",
                "[BOLD] MAWPS",
                "[BOLD] Average"
            ],
            "table_content_values": [
                [
                    " Hosseini, et.al.",
                    "77.7",
                    "–",
                    "–",
                    "–",
                    "∗77.7"
                ],
                [
                    " Kushman, et.al.",
                    "64.0",
                    "73.7",
                    "2.3",
                    "–",
                    "∗46.7"
                ],
                [
                    " Roy, et.al.",
                    "–",
                    "–",
                    "52.7",
                    "–",
                    "∗52.7"
                ],
                [
                    " Robaidek, et.al.",
                    "–",
                    "–",
                    "–",
                    "62.8",
                    "∗62.8"
                ],
                [
                    " Wang, et.al.",
                    "[BOLD] 78.5",
                    "75.5",
                    "73.3",
                    "–",
                    "∗75.4"
                ],
                [
                    "[ITALIC] Pre-trained",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "(1) Prefix-Transformer",
                    "70.2",
                    "91.1",
                    "95.2",
                    "82.4",
                    "84.7"
                ],
                [
                    "(1) Postfix-Transformer",
                    "68.4",
                    "90.0",
                    "92.9",
                    "82.7",
                    "83.5"
                ],
                [
                    "(1) Infix-Transformer",
                    "75.4",
                    "74.4",
                    "64.3",
                    "56.4",
                    "67.6"
                ],
                [
                    "(2) Prefix-Transformer",
                    "66.7",
                    "91.1",
                    "[BOLD] 96.4",
                    "82.1",
                    "84.1"
                ],
                [
                    "(2) Postfix-Transformer",
                    "73.7",
                    "93.3",
                    "94.1",
                    "82.4",
                    "85.9"
                ],
                [
                    "(2) Infix-Transformer",
                    "75.4",
                    "75.6",
                    "66.7",
                    "59.0",
                    "69.2"
                ],
                [
                    "(3) Prefix-Transformer",
                    "70.2",
                    "91.1",
                    "95.2",
                    "82.4",
                    "84.7"
                ],
                [
                    "(3) Postfix-Transformer",
                    "73.7",
                    "92.2",
                    "94.1",
                    "82.1",
                    "85.5"
                ],
                [
                    "(3) Infix-Transformer",
                    "75.4",
                    "75.6",
                    "64.3",
                    "58.7",
                    "68.5"
                ],
                [
                    "[ITALIC] Non-pre-trained",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "(1) Prefix-Transformer",
                    "71.9",
                    "[BOLD] 94.4",
                    "95.2",
                    "83.4",
                    "86.3"
                ],
                [
                    "(1) Postfix-Transformer",
                    "73.7",
                    "81.1",
                    "92.9",
                    "75.7",
                    "80.8"
                ],
                [
                    "(1) Infix-Transformer",
                    "77.2",
                    "73.3",
                    "61.9",
                    "56.8",
                    "67.3"
                ],
                [
                    "(2) Prefix-Transformer",
                    "71.9",
                    "[BOLD] 94.4",
                    "94.1",
                    "[BOLD] 84.7",
                    "86.3"
                ],
                [
                    "(2) Postfix-Transformer",
                    "77.2",
                    "[BOLD] 94.4",
                    "94.1",
                    "83.1",
                    "[BOLD] 87.2"
                ],
                [
                    "(2) Infix-Transformer",
                    "77.2",
                    "76.7",
                    "66.7",
                    "61.5",
                    "70.5"
                ],
                [
                    "(3) Prefix-Transformer",
                    "71.9",
                    "93.3",
                    "95.2",
                    "84.1",
                    "86.2"
                ],
                [
                    "(3) Postfix-Transformer",
                    "77.2",
                    "94.4",
                    "94.1",
                    "82.4",
                    "87.0"
                ],
                [
                    "(3) Infix-Transformer",
                    "77.2",
                    "76.7",
                    "66.7",
                    "62.4",
                    "70.7"
                ]
            ]
        },
        "gold_description": "the values in the last column of table iii are summarized in table iv . how the models compare with respect to accuracy closely resembles the comparison of bleu scores , presented earlier . thus , bleu scores seem to correlate well with accuracy values in our case .",
        "generated_description": "prefix - transformer outperforms all the other models by a large margin and achieves the best performance across all the datasets . we also observe that the performance of the postfix - transformers is slightly worse than that of the prefix transformers . we believe that this is due to the fact that some of the pre - trained word embeddings are not fine - tuned on the training set and thus suffer from overfitting when used in downstream tasks ."
    },
    {
        "table_id": "423",
        "table_info": {
            "table_caption": "Table 1: Joint goal accuracy on the evaluation dataset of WOZ 2.0 corpus.",
            "table_column_names": [
                "Model",
                "Joint Accuracy"
            ],
            "table_content_values": [
                [
                    "NBT-DNN Mrkšić et al. ( 2017 )",
                    "0.844"
                ],
                [
                    "BT-CNN Ramadan et al. ( 2018 )",
                    "0.855"
                ],
                [
                    "GLAD Zhong et al. ( 2018 )",
                    "0.881"
                ],
                [
                    "GCE Nouri and Hosseini-Asl ( 2018 )",
                    "0.885"
                ],
                [
                    "StateNetPSI Ren et al. ( 2018 )",
                    "0.889"
                ],
                [
                    "BERT+RNN (baseline 1)",
                    "0.892 (±0.011)"
                ],
                [
                    "BERT+RNN+Ontology (baseline 2)",
                    "0.893 (±0.013)"
                ],
                [
                    "Slot-dependent SUMBT (baseline 3)",
                    "0.891 (±0.010)"
                ],
                [
                    "Slot-independent SUMBT (proposed)",
                    "[BOLD] 0.910 (±0.010)"
                ]
            ]
        },
        "gold_description": "the experimental results on woz 2 . 0 corpus are presented in table 1 . the three baseline models , bert + rnn , bert + rnn + ontology , and the slot - dependent sumbt , showed no significant performance differences . on the other hand , the slot - independent sumbt which learned the shared information from all across domains and slots significantly outperformed those baselines , resulting in 91 . 0 % joint accuracy . the proposed model achieved state - of - theart performance in both woz 2 . 0 and multiwoz datasets .",
        "generated_description": "table 1 : joint goal accuracy on the evaluation dataset of woz 2 . 0 . from the table , we can see that our proposed joint goal accuracy ( slot - independent sumbt ) outperforms all the baseline models and achieves the new state - of - the - art performance , exceeding the accuracy of 0 . 910 with a significant margin ( p < 0 . 001 ) ."
    },
    {
        "table_id": "424",
        "table_info": {
            "table_caption": "Table 2: Joint goal accuracy on the evaluation dataset of MultiWOZ corpus.",
            "table_column_names": [
                "Model",
                "Joint Accuracy"
            ],
            "table_content_values": [
                [
                    "Benchmark baseline",
                    "0.2583"
                ],
                [
                    "GLAD Zhong et al. ( 2018 )",
                    "0.3557"
                ],
                [
                    "GCE Nouri and Hosseini-Asl ( 2018 )",
                    "0.3558"
                ],
                [
                    "SUMBT",
                    "[BOLD] 0.4240 (±0.0187)"
                ]
            ]
        },
        "gold_description": "table 2 shows the experimental results of the slot - independent sumbt model on multiwoz corpus . the sumbt greatly surpassed the performances of previous approaches by yielding 42 . 4 % joint accuracy .",
        "generated_description": "joint goal accuracy is the evaluation metric to measure the joint goal accuracy on the evaluation dataset of multiwoz . the benchmark baseline is the same as in previous works . we can see that sumbt achieves the new state - of - the - art goal accuracy of 0 . 4240 , which is a significant improvement over the previous best reported goal accuracy ( 0 . 2583 ) of glad zhong et al ."
    },
    {
        "table_id": "425",
        "table_info": {
            "table_caption": "Table 3: Transfer learning results for GLUE tasks. The source corpus is MNLI. Baseline accuracy is when Transfer BERT, Filler, and Role are all False, equivalent to no transfer. Fine-tuned accuracy is the best accuracy among all possible transfer options.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Target Corpus",
                "[BOLD] Transfer BERT",
                "[BOLD] Transfer Filler",
                "[BOLD] Transfer Role",
                "[BOLD] Baseline Acc. (%)",
                "[BOLD] Fine-tuned Acc. (%)",
                "[BOLD] Gain (%)"
            ],
            "table_content_values": [
                [
                    "BERT",
                    "QNLI",
                    "True",
                    "–",
                    "–",
                    "91.60",
                    "91.27",
                    "– 0.33"
                ],
                [
                    "BERT",
                    "QQP",
                    "True",
                    "–",
                    "–",
                    "91.45",
                    "91.12",
                    "– 0.33"
                ],
                [
                    "BERT",
                    "RTE",
                    "True",
                    "–",
                    "–",
                    "71.12",
                    "73.65",
                    "+ 2.53"
                ],
                [
                    "BERT",
                    "SNLI",
                    "True",
                    "–",
                    "–",
                    "90.45",
                    "90.69",
                    "+ 0.24"
                ],
                [
                    "BERT",
                    "SST",
                    "True",
                    "–",
                    "–",
                    "93.23",
                    "92.78",
                    "– 0.45"
                ],
                [
                    "HUBERT (Transformer)",
                    "QNLI",
                    "True",
                    "True",
                    "False",
                    "90.56",
                    "91.16",
                    "[BOLD] + 0.60"
                ],
                [
                    "HUBERT (Transformer)",
                    "QQP",
                    "False",
                    "False",
                    "True",
                    "90.81",
                    "91.42",
                    "[BOLD] + 0.61"
                ],
                [
                    "HUBERT (Transformer)",
                    "RTE",
                    "True",
                    "True",
                    "True",
                    "61.73",
                    "74.01",
                    "[BOLD] + 12.28"
                ],
                [
                    "HUBERT (Transformer)",
                    "SNLI",
                    "True",
                    "False",
                    "True",
                    "90.66",
                    "91.36",
                    "[BOLD] + 0.70"
                ],
                [
                    "HUBERT (Transformer)",
                    "SST",
                    "True",
                    "False",
                    "True",
                    "91.28",
                    "92.43",
                    "[BOLD] + 1.15"
                ]
            ]
        },
        "gold_description": "table 3 summarizes the results for these transfer learning experiments when the source task is mnli . gain shows the difference between fine - tuned model ' s accuracy and baseline ' s accuracy . for hubert ( transformer ) , we observe substantial gain across all 5 target corpora after transfer . however , for bert we have a drop for qnli , qqp , and sst .",
        "generated_description": "fine - tuning bert and hubert ( transformer ) on different target corpora : we first fine - tune bert on the target qqp and snli training corpora , and then transfer the learned representations to other nli tasks . for each task , we evaluate the model ’ s performance on the test set of the target corpus that the model was trained on , and report the performance of the model on the dev set of that task . we also report the percentage of positive transfer cases where the model outperforms the baseline by a large margin ( + 2 . 53 % and + 0 . 24 % in accuracy for the transformer model and the xlnet model , respectively ) ."
    },
    {
        "table_id": "426",
        "table_info": {
            "table_caption": "Table 4: Transfer learning results for GLUE tasks. The source corpus is QQP. Baseline accuracy is for when Transfer BERT, Filler, and Role are all False, which is equivalent to no transfer. Fine-tuned accuracy is the best accuracy among all possible transfer options.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Target Corpus",
                "[BOLD] Transfer BERT",
                "[BOLD] Transfer Filler",
                "[BOLD] Transfer Role",
                "[BOLD] Baseline Acc. (%)",
                "[BOLD] Fine-tuned Acc. (%)",
                "[BOLD] Gain (%)"
            ],
            "table_content_values": [
                [
                    "BERT",
                    "QNLI",
                    "True",
                    "–",
                    "–",
                    "91.60",
                    "90.96",
                    "– 0.64"
                ],
                [
                    "BERT",
                    "MNLI",
                    "True",
                    "–",
                    "–",
                    "84.15",
                    "84.41",
                    "+ 0.26"
                ],
                [
                    "BERT",
                    "RTE",
                    "True",
                    "–",
                    "–",
                    "71.12",
                    "62.45",
                    "– 8.67"
                ],
                [
                    "BERT",
                    "SNLI",
                    "True",
                    "–",
                    "–",
                    "90.45",
                    "90.88",
                    "+ 0.43"
                ],
                [
                    "BERT",
                    "SST",
                    "True",
                    "–",
                    "–",
                    "93.23",
                    "92.09",
                    "– 1.14"
                ],
                [
                    "HUBERT (Transformer)",
                    "QNLI",
                    "False",
                    "True",
                    "True",
                    "88.32",
                    "90.55",
                    "[BOLD] + 2.23"
                ],
                [
                    "HUBERT (Transformer)",
                    "MNLI",
                    "False",
                    "True",
                    "True",
                    "84.30",
                    "85.24",
                    "[BOLD] + 0.94"
                ],
                [
                    "HUBERT (Transformer)",
                    "RTE",
                    "False",
                    "True",
                    "False",
                    "61.73",
                    "65.70",
                    "[BOLD] + 3.97"
                ],
                [
                    "HUBERT (Transformer)",
                    "SNLI",
                    "False",
                    "False",
                    "True",
                    "90.63",
                    "91.20",
                    "[BOLD] + 0.57"
                ],
                [
                    "HUBERT (Transformer)",
                    "SST",
                    "True",
                    "True",
                    "True",
                    "86.12",
                    "91.06",
                    "[BOLD] + 4.94"
                ]
            ]
        },
        "gold_description": "again transferring roles gives positive results except for rte . filler vectors learned from qqp are more transferable compared to mnli and gives a boost to all tasks except for snli . surprisingly , transferring bert parameters is hurting the results now even when tpr is present .",
        "generated_description": "fine - tuning bert and hubert ( transformer ) on the target nli task leads to a gain of 2 . 23 % and 2 . 14 % in accuracy on the snli and sst test sets , respectively . for the rte test set , the gain is only 1 . 14 % , but the filler strategy leads to an improvement of 8 . 67 % ( qnli ) . we also find that the mnli and snli test sets benefit more from the fine - tuned bert ( 2 . 43 % and 0 . 43 % , respectively ) ."
    },
    {
        "table_id": "427",
        "table_info": {
            "table_caption": "Table 5: Test set results for HUBERT (Transformer) and BERT. BERT accuracy indicates test results on target corpus (without transfer) for bert-base-uncased which are directly taken from the GLUE leaderboard. Fine-tuned accuracy are the test results for best performing HUBERT (Transformer) model on target dev set after transfer (see Tables 3 and 4).",
            "table_column_names": [
                "[BOLD] Source Corpus",
                "[BOLD] Target Corpus",
                "[BOLD] HUBERT Transfer BERT",
                "[BOLD] HUBERT Transfer Filler",
                "[BOLD] HUBERT Transfer Role",
                "[BOLD] BERT Acc. (%)",
                "[BOLD] HUBERT Acc. (%)"
            ],
            "table_content_values": [
                [
                    "MNLI",
                    "QNLI",
                    "True",
                    "True",
                    "False",
                    "[BOLD] 90.50",
                    "[BOLD] 90.50"
                ],
                [
                    "MNLI",
                    "QQP",
                    "False",
                    "False",
                    "True",
                    "89.20",
                    "[BOLD] 89.30"
                ],
                [
                    "MNLI",
                    "RTE",
                    "True",
                    "True",
                    "True",
                    "66.40",
                    "[BOLD] 69.30"
                ],
                [
                    "MNLI",
                    "SNLI",
                    "True",
                    "False",
                    "True",
                    "89.20",
                    "[BOLD] 90.35"
                ],
                [
                    "MNLI",
                    "SST",
                    "True",
                    "False",
                    "True",
                    "[BOLD] 93.50",
                    "92.60"
                ],
                [
                    "QQP",
                    "QNLI",
                    "False",
                    "True",
                    "True",
                    "90.50",
                    "[BOLD] 90.70"
                ],
                [
                    "QQP",
                    "MNLI",
                    "False",
                    "True",
                    "True",
                    "84.60",
                    "[BOLD] 84.70"
                ],
                [
                    "QQP",
                    "RTE",
                    "False",
                    "True",
                    "False",
                    "[BOLD] 66.40",
                    "63.20"
                ],
                [
                    "QQP",
                    "SNLI",
                    "False",
                    "False",
                    "True",
                    "89.20",
                    "[BOLD] 90.36"
                ],
                [
                    "QQP",
                    "SST",
                    "True",
                    "True",
                    "True",
                    "[BOLD] 93.50",
                    "91.00"
                ]
            ]
        },
        "gold_description": "we also verified that our tpr layer is not hurting the performance by comparing the test set results for hubert ( transformer ) and bert . the results are obtained by submitting models to the glue evaluation server . the results are presented in table 5 .",
        "generated_description": "the filler and bert variants outperform the original bert by a large margin when the source and target datasets are both held out for fine - tuning . however , the filler variant performs worse when the target dataset is snli , sst , rte , and qqp datasets are held out . we attribute this to the fact that these tasks are different from ner and entailment classification tasks and thus require different knowledge base understanding abilities . the role of filler is to fill in blanks in the source corpus of the target task , while the role of bert is to fine - tune the model on the target corpus ."
    },
    {
        "table_id": "428",
        "table_info": {
            "table_caption": "Table 1: Model performance results on the tasks of interest. Best values for each metric are bolded.",
            "table_column_names": [
                "Model",
                "Model",
                "Mortality AUPRC",
                "Mortality AUROC",
                "Primary CCS Top-1 Recall",
                "Primary CCS Top-5 Recall",
                "All ICD-9 AUPRC",
                "All ICD-9 AUROC, weighted"
            ],
            "table_content_values": [
                [
                    "No notes",
                    "-",
                    "0.449 (0.006)",
                    "0.869 (0.001)",
                    "0.526 (0.006)",
                    "0.796 (0.003)",
                    "0.305 (0.001)",
                    "0.873 (<0.001)"
                ],
                [
                    "Bag-of-words",
                    "Unigrams (notes only)",
                    "0.383 (0.004)",
                    "0.832 (0.003)",
                    "0.591 (0.004)",
                    "0.849 (0.002)",
                    "0.328 (0.002)",
                    "0.880 (0.001)"
                ],
                [
                    "[EMPTY]",
                    "Unigrams (all features)",
                    "[BOLD] 0.479 (0.008)",
                    "0.880 (0.001)",
                    "0.592 (0.003)",
                    "0.842 (0.003)",
                    "0.331 (0.001)",
                    "0.883 (0.001)"
                ],
                [
                    "[EMPTY]",
                    "Unigrams and bigrams (all features)",
                    "0.460 (0.005)",
                    "0.872 (0.002)",
                    "0.587 (0.008)",
                    "0.829 (0.005)",
                    "0.325 (0.002)",
                    "0.881 (<0.001)"
                ],
                [
                    "Hierarchical (without pretraining)",
                    "Notes only",
                    "0.351 (0.003)",
                    "0.825 (0.003)",
                    "0.606 (0.003)",
                    "0.850 (0.001)",
                    "0.345 (0.005)",
                    "0.887 (0.002)"
                ],
                [
                    "[EMPTY]",
                    "All features",
                    "0.471 (0.006)",
                    "0.876 (0.003)",
                    "0.591 (0.008)",
                    "0.833 (0.006)",
                    "0.301 (0.004)",
                    "0.868 (0.001)"
                ],
                [
                    "SHiP",
                    "Notes only",
                    "0.353 (0.005)",
                    "0.825 (0.004)",
                    "0.667 (0.006)",
                    "[BOLD] 0.897*† (0.003)",
                    "[BOLD] 0.352† (0.001)",
                    "[BOLD] 0.891† (0.001)"
                ],
                [
                    "[EMPTY]",
                    "All features",
                    "[BOLD] 0.479 (0.007)",
                    "[BOLD] 0.882 (0.001)",
                    "[BOLD] 0.671*† (0.004)",
                    "0.890 (0.001)",
                    "0.345 (0.005)",
                    "0.889 (0.002)"
                ]
            ]
        },
        "gold_description": "table 1 compares the performance of all model variants . the ship models significantly improved over the bow baselines on the two diagnosis tasks ( p < 0 . 001 under welch ' s t - test ) : for ccs prediction , the best ship models improved top - 1 recall by 7 . 9 percentage points and top - 5 recall by 4 . 8 percentage points , respectively , over the best bow models ; for icd - 9 prediction , area under the precision - recall curve ( auprc ) increased by 2 . 1 percentage points and weighted area under the roc curve ( auroc ) increased by 0 . 8 percentage points . for mortality prediction , we saw negligible benefit from the ship architecture . the ship models also improved over the corresponding hierarchical models without pretraining . for mortality , pretraining the all - features model increased auprc by 0 . 8 percentage points ( p = 0 . 06 ) and auroc by 0 . 6 percentage points ( p = 0 . 004 ) ; for primary ccs , pretraining the all - feature model increased top - 1 recall by 8 . 0 percentage points ( p < 0 . 001 ) , while pretraining the notes - only model increased top - 5 recall by 4 . 7 percentage points ( p < 0 . 001 ) ; for all icd - 9 , pretraining the notes - only model increased auprc by 0 . 7 percentage points ( p = 0 . 03 ) and weighted auroc by 0 . 4 percentage points ( p = 0 . 01 ) .",
        "generated_description": "in the first set of experiments , we investigate the effect of individual unigram and bigram features on the overall performance of the model . we observe that the best performance is achieved using unigrams and bigrams ( using all features ) , which is consistent with the findings of \\ newcitedblp : journals / corr / abs - 1911 - 10683 . in the primary ccs dataset , the best results are obtained using bag - of - words , which is unsurprising as this dataset was annotated by the same authors who also wrote the notes . however , there is no significant difference between using oracle and non - oracle bigrams in terms of the other two metrics . using all features together , the model achieves a top - 1 and top - 5 auroc of 0 . 479 and 0 . 845 on the primary and mortality datasets , respectively , which are similar to the results achieved by computing the weighted average of all individual features ."
    },
    {
        "table_id": "429",
        "table_info": {
            "table_caption": "Table 1: The compression results including model efficiency and accuracy from the GLUE test server, and the MNLI result is evaluated for matched-accuracy (MNLI-m). BERT12 indicates the results of the fine-tuned BERT-base from Devlin et al. (2019) and BERT12-T indicates the results of the fine-tuned BERT-base in our implementation. The results of BERT-PKD are from Sun et al. (2019), the results of DistilBERT4 and TinyBERT4 are from Jiao et al. (2019), and the results of BiLSTMSOFT is from Tang et al. (2019). The number of model parameters includes the embedding size, and the inference time is tested with a batch size of 128 over 50,000 samples. The bold numbers and underlined numbers indicate the best and the second-best performance respectively.",
            "table_column_names": [
                "Method",
                "# Params",
                "Inference",
                "SST-2",
                "MRPC",
                "QQP",
                "MNLI",
                "QNLI",
                "RTE",
                "Average"
            ],
            "table_content_values": [
                [
                    "Method",
                    "# Params",
                    "Speedup",
                    "SST-2",
                    "MRPC",
                    "QQP",
                    "MNLI",
                    "QNLI",
                    "RTE",
                    "Average"
                ],
                [
                    "BERT12",
                    "109M",
                    "1x",
                    "93.5",
                    "88.9",
                    "71.2",
                    "84.6",
                    "90.5",
                    "66.4",
                    "82.5"
                ],
                [
                    "BERT12-T",
                    "109M",
                    "1x",
                    "93.3",
                    "88.7",
                    "71.1",
                    "84.8",
                    "90.4",
                    "66.1",
                    "82.4"
                ],
                [
                    "BERT6-PKD",
                    "67.0M",
                    "1.9x",
                    "92.0",
                    "85.0",
                    "70.7",
                    "81.5",
                    "[BOLD] 89.0",
                    "[BOLD] 65.5",
                    "[BOLD] 80.6"
                ],
                [
                    "BERT3-PKD",
                    "45.7M",
                    "3.7x",
                    "87.5",
                    "80.7",
                    "68.1",
                    "76.7",
                    "84.7",
                    "58.2",
                    "76.0"
                ],
                [
                    "DistilBERT4",
                    "52.2M",
                    "3.0x",
                    "91.4",
                    "82.4",
                    "68.5",
                    "78.9",
                    "85.2",
                    "54.1",
                    "76.8"
                ],
                [
                    "TinyBert4",
                    "14.5M",
                    "9.4x",
                    "[BOLD] 92.6",
                    "[BOLD] 86.4",
                    "[BOLD] 71.3",
                    "[BOLD] 82.5",
                    "87.7",
                    "62.9",
                    "[BOLD] 80.6"
                ],
                [
                    "BiLSTM [ITALIC] SOFT",
                    "10.1M",
                    "7.6x",
                    "90.7",
                    "-",
                    "68.2",
                    "73.0",
                    "-",
                    "-",
                    "-"
                ],
                [
                    "AdaBERT",
                    "[BOLD] 6.4M ∼ 9.5M",
                    "[BOLD] 12.7x ∼ 29.3x",
                    "91.8",
                    "85.1",
                    "70.7",
                    "81.6",
                    "86.8",
                    "64.4",
                    "80.1"
                ]
            ]
        },
        "gold_description": "the compression results on the six adopted datasets , including parameter size , inference speedup and classification accuracy , are summarized in table 1 . overall speaking , on all the evaluated datasets , the proposed adabert method achieves significant efficiency improvement while maintaining comparable performance . compared to the bert12 - t , the compressed models are 11 . 5x to 17 . 0x smaller in parameter size and 12 . 7x to 29 . 3x faster in inference speed with an average performance degradation of 2 . 79 % . comparing structureheterogeneous method , bilstmsof , adabert searches cnn - based models and achieves much another with better improvements , especially on the mnli dataset . comparing with different transformers - based compression baselines , the proposed adabert method is 1 . 35x to 3 . 12x faster than the fastest baseline , tinybert4 , and achieves comparable performance with the two baselines that have the best averaged accuracy , bert6 - pkd and tinybert4 .",
        "generated_description": "for bert12 - t , we use 1 × the number of parameters and inference speed as the main advantage of the proposed pkd method . the results show that with the increase in inference speed , the performance can be further improved by a large margin , e . g . , from 1 . 9x to 3 . 7x speedup on sst - 2 and from 2 . 2x to 4 . 1x on mrpc , and the average accuracy can be improved to 80 . 6 % on qnli and from 80 . 4 % to 89 . 0 % on rte , with an average speedup of 2 . 4x . for bber12 - 12 , we also use the transformer network with 109m parameters , which is 1x larger than the original bert model . we can see that even with 1x more parameters , the proposed method can still outperform the original model with a large speedup and achieve comparable performance with the original 12m parameters ."
    },
    {
        "table_id": "430",
        "table_info": {
            "table_caption": "Table 3: Accuracy comparison on the dev sets with the searched compression structures applying to different tasks. For Random, 5-times averaging results with standard deviations are reported.",
            "table_column_names": [
                "StructureTask",
                "SST-2",
                "MRPC",
                "QQP",
                "MNLI",
                "QNLI",
                "RTE"
            ],
            "table_content_values": [
                [
                    "AdaBERT-SST-2",
                    "[BOLD] 91.9",
                    "78.1",
                    "58.6",
                    "64.0",
                    "74.1",
                    "53.8"
                ],
                [
                    "AdaBERT-MRPC",
                    "81.5",
                    "[BOLD] 84.7",
                    "68.9",
                    "75.9",
                    "82.2",
                    "60.3"
                ],
                [
                    "AdaBERT-QQP",
                    "81.9",
                    "84.1",
                    "[BOLD] 70.5",
                    "76.3",
                    "82.5",
                    "60.5"
                ],
                [
                    "AdaBERT-MNLI",
                    "82.1",
                    "81.5",
                    "66.8",
                    "[BOLD] 81.3",
                    "86.1",
                    "63.2"
                ],
                [
                    "AdaBERT-QNLI",
                    "81.6",
                    "82.3",
                    "67.7",
                    "79.2",
                    "[BOLD] 87.2",
                    "62.9"
                ],
                [
                    "AdaBERT-RTE",
                    "82.9",
                    "81.1",
                    "66.5",
                    "79.8",
                    "86.0",
                    "[BOLD] 64.1"
                ],
                [
                    "Random",
                    "80.4 ± 4.3",
                    "79.2 ± 2.8",
                    "61.8 ± 4.9",
                    "69.7 ± 6.7",
                    "78.2 ± 5.5",
                    "55.3 ± 4.1"
                ]
            ]
        },
        "gold_description": "the results of cross - task validation is summarized in table 3 . from table 3 , we can observe that : the searched structures achieve the best performance on their original target tasks compared with other tasks , in other words , the performance numbers along the diagonal line of this table are the best . further , the performance degradation is quite significant across different kinds of tasks ( for example , applying the searched structures of sentiment classification tasks to entailment recognition task , or vice verse ) , while the performance degradations within the same kind of tasks ( for example , mrpc and qqp for semantic equivalence classification ) are relatively small , since they have the same input format ( i . e . , a pair of sentences ) and similar targets . from the last row of table 3 , we can see that the randomly sampled structures perform worse than the searched structures and their performances are not stable .",
        "generated_description": "adabert is a bert - based pre - trained language model . we use the following structure tasks : sst - 2 , mrpc , qqp , mnli , qnli , rte , and rte . for each structure task , we fine - tune the model on the training data of each task and report the results on the test data . we also report the accuracy of random initialization , which randomly partitions the dataset into ten groups and treats each group as a different instance . for structure tasks that have no training examples , we train a separate model for each group . for example , in the case of mrpc and the rte task , the random split results in an accuracy of 80 . 4 % and 80 . 2 % , respectively . for the other group of structure tasks , the accuracy results are close to that of 100 % ."
    },
    {
        "table_id": "431",
        "table_info": {
            "table_caption": "Table 5: The effect of efficiency loss term.",
            "table_column_names": [
                "[EMPTY]",
                "SST-2",
                "MRPC",
                "QNLI",
                "RTE"
            ],
            "table_content_values": [
                [
                    "[ITALIC] β = 0",
                    "91.8",
                    "84.5",
                    "87.1",
                    "63.9"
                ],
                [
                    "[ITALIC] β = 0",
                    "(7.5M)",
                    "(7.8M)",
                    "(8.3M)",
                    "(9.1M)"
                ],
                [
                    "[ITALIC] β = 4",
                    "[BOLD] 91.9",
                    "[BOLD] 84.7",
                    "[BOLD] 87.2",
                    "[BOLD] 64.1"
                ],
                [
                    "[ITALIC] β = 4",
                    "(6.4M)",
                    "(7.5M)",
                    "(7.9M)",
                    "(8.6M)"
                ],
                [
                    "[ITALIC] β = 8",
                    "91.3",
                    "84.2",
                    "86.4",
                    "63.3"
                ],
                [
                    "[ITALIC] β = 8",
                    "(5.3M)",
                    "(6.4M)",
                    "(7.1M)",
                    "(7.8M)"
                ]
            ]
        },
        "gold_description": "the model performance and corresponding model size are reported in table 5 . on the one hand , removing the efficiency - aware loss ( β = 0 ) leads to the increase in model parameter size , on the other hand , a more aggressive efficiency preference ( β = 8 ) results in the small model size but degraded performance , since a large β encourages the compressed model to adopt more lightweight operations such as zero and skip which hurt the performance . a moderate efficiency constraint ( β = 4 ) provides a regularization , guiding the adabert method to achieve a trade - off between the small parameter size and the good performance .",
        "generated_description": "effect of efficiency loss term : we investigate the effect of the efficiency loss term β in our proposed model by varying its value from 0 to 8 . we can see that the performance of the model achieves its best when β is set to 4 , which consumes less than 1 % of the total training data . however , the size of the training data is reduced by a factor of 4 when β = 0 . we find that the model size reduces from 9 . 1m to 6 . 4m ( mrpc , qnli , rte ) and from 7 . 8m to 5 . 3m ( sst - 2 and rte ) , which consumes only 0 . 1 % and 0 . 2 % of training data , respectively . the results indicate that β is an important hyper - parameter to trade - off between the performance and the computational cost ."
    },
    {
        "table_id": "432",
        "table_info": {
            "table_caption": "Table 4: The effect of knowledge loss terms.",
            "table_column_names": [
                "[EMPTY]",
                "SST-2",
                "MRPC",
                "QNLI",
                "RTE"
            ],
            "table_content_values": [
                [
                    "Base-KD",
                    "86.6",
                    "77.2",
                    "82.0",
                    "56.7"
                ],
                [
                    "+ Probe",
                    "88.4",
                    "78.7",
                    "83.3",
                    "58.1"
                ],
                [
                    "+ DA",
                    "91.4",
                    "83.9",
                    "86.5",
                    "63.2"
                ],
                [
                    "+ L [ITALIC] CE (All)",
                    "91.9",
                    "84.7",
                    "87.2",
                    "64.1"
                ]
            ]
        },
        "gold_description": "the base - kd is a naive knowledge distillation version in which only the logits of the last layer are distilled without considering hidden layer knowledge and supervised label knowledge . by incorporating the probe models , the performance ( line 2 in table 4 ) is consistently improved , indicating the benefits from hierarchically decomposed taskoriented knowledge . we then leverage data augmentation ( da ) to enrich task - oriented knowledge and this technique also improves performance for all tasks , especially for tasks that have a limited scale of data ( i . e . , mrpc and rte ) . da is also adopted in existing kd - based compression studies ( tang et al . , 2019 ; jiao et al . , 2019 ) . when taking the supervised label knowledge ( lce ) into consideration , the performance is further boosted , showing that this term is also important for adabert by providing focused search hints .",
        "generated_description": "sst - 2 , mrpc , qnli and rte . we observe that knowledge loss terms significantly improve the performance of base - kd , with + probe , + da and + lce ( all ) being the most effective ones . the reason is that these two terms help the model to better exploit the correlations between the context and the target knowledge . meanwhile , the effect of da and lce is less significant than that of lce . we conjecture that this is because that the da term only focuses on the correlation between the target and the context but not the other terms ."
    },
    {
        "table_id": "433",
        "table_info": {
            "table_caption": "Table 1: Sentiment prediction results on CMU-MOSI. SOTA1, SOTA2 and SOTA3 refer to the previous best, second best and third best state of the art models respectively. Best results are highlighted in bold and ΔSOTA represents the change in performance of M-BERT model over SOTA1. Our model significantly outperforms the current SOTA across all evaluation metrics.",
            "table_column_names": [
                "Task Metric",
                "BA↑",
                "F1↑",
                "MAE↓",
                "Corr↑"
            ],
            "table_content_values": [
                [
                    "SOTA3",
                    "77.1",
                    "77.0",
                    "0.968",
                    "0.625"
                ],
                [
                    "SOTA2",
                    "77.4",
                    "77.3",
                    "0.965",
                    "0.632"
                ],
                [
                    "SOTA1",
                    "78.4",
                    "78.0",
                    "0.922",
                    "0.681"
                ],
                [
                    "BERT",
                    "83.36",
                    "85.53",
                    "0.736",
                    "0.777"
                ],
                [
                    "M-BERT",
                    "[BOLD] 84.38",
                    "[BOLD] 86.34",
                    "[BOLD] 0.732",
                    "[BOLD] 0.790"
                ],
                [
                    "Δ [ITALIC] SOTA",
                    "↑  [BOLD] 5.98",
                    "↑  [BOLD] 8.34",
                    "↓  [BOLD] 0.19",
                    "↑ [BOLD] 0.11"
                ]
            ]
        },
        "gold_description": "our proposed approach sets a new state of the art of 84 . 38 % binary accuracy on cmu - mosi dataset of multimodal sentiment analysis ; a significant leap from previous state of we compare the performance of m - bert with the following models on the multimodal sentiment analysis task : rmfn ( sota1 ) 1 fuses multimodal information in multiple stages by focusing on a subset of signals in each stage ( liang et al . , 2018 ) . mfn ( sota2 ) synchronizes states of three separate lstms with a multi - view gated memory ( zadeh et al . , 2018a ) . marn ( sota3 ) models view - specific interactions using hybrid lstm memories and cross - modal interactions using a multi - attention block ( mab ) ( zadeh et al . , 2018c ) . we perform two different evaluation tasks on cmu - mosi datset : i ) binary classification , and ii ) regression . we formulate it as a regression problem and report mean - absolute error ( mae ) and the correlation of model predictions with true labels . besides , we convert the regression outputs into categorical values to obtain binary classification accuracy ( ba ) and f1 score . the performances of m - bert and bert are described in table 1 . m - bert model outperforms all the baseline models ( described in sec . 4 . 4 ) on every evaluation metrics with large margin . it sets new state - of - the - art performance for this task and achieves 84 . 38 % accuracy , a 5 . 98 % increase with respect to the sota1 and 1 . 02 % increase with respect to bert ( text - only ) . even bert ( text - only ) model achieves 83 . 36 % accuracy , an increase of 4 . 96 % from the sota1",
        "generated_description": "sota1 , sota2 and sota3 refer to the previous best , second best and third best state of the art models respectively . we can see that m - bert significantly outperforms the current sota across all evaluation metrics with an average improvement of 5 . 98 % , 8 . 34 % and 3 . 19 % in terms of bertscore , mae , correlation and f1 score respectively ."
    },
    {
        "table_id": "434",
        "table_info": {
            "table_caption": "Table 1: Inference times in ms",
            "table_column_names": [
                "[BOLD] Length of sentence",
                "[BOLD] Attention",
                "[BOLD] Gaussian Mask only",
                "[BOLD] Gaussian Mask + RL model"
            ],
            "table_content_values": [
                [
                    "4-7",
                    "1.46",
                    "1.13",
                    "1.33"
                ],
                [
                    "8-11",
                    "2.32",
                    "1.96",
                    "2.12"
                ],
                [
                    "12-15",
                    "2.86",
                    "2.43",
                    "2.61"
                ]
            ]
        },
        "gold_description": "we find the inference times are lower in the former case with negligible difference in bleu scores . the inference times are of the various models are mentioned in table 1 .",
        "generated_description": "the inference time is measured on a cluster server with intel i7 - 7700k cpu , using 8 intel xeon e5 - 2690 v2 . 2 processors . both the attention mechanism and the gaussian mask layer can be seen to be very efficient for short sentences ( 4 - 7 words or less ) . however , for long sentences , the two attention mechanisms lead to a significant increase in inference time . for instance , for sentences with 12 - 15 words , the rl layer is able to achieve an inference time of 2 . 86 ms per epoch , while the attention layer is only able to get 2 . 12ms per epoch . for longer sentences , both of the attention mechanisms are more than 2 . 5x faster ."
    },
    {
        "table_id": "435",
        "table_info": {
            "table_caption": "Table 3: Number of images after which the ME Score falls below threshold.",
            "table_column_names": [
                "[BOLD] Score",
                "Omniglot",
                "Omniglot Classifier",
                "Imagenet",
                "Imagenet Classifier"
            ],
            "table_content_values": [
                [
                    "0.2",
                    "24,304",
                    "2,144",
                    "1,280",
                    "2,048"
                ],
                [
                    "0.1",
                    "99,248",
                    "22,912",
                    "8,448",
                    "3,072"
                ],
                [
                    "0.05",
                    "160,608",
                    "43,328",
                    "111,872",
                    "8,960"
                ]
            ]
        },
        "gold_description": "the results are summarized in figure 6 and table 3 . the probability that a new image belongs to an unseen class p ( n | t ) is higher than the me score of the classifier through most of the learning phase . comparing the statistics of the datasets to the inductive biases in the classifiers , the me score for the classifiers is substantially lower than the baseline me measure in the dataset , p ( n | t ) ( table 3 ) . for instance , the imagenet classifier drops its me score below 0 . 05 after about 8 , 960 images , while the approximate me measure for the dataset shows that new classes are encountered at above this rate until at least 111 , 000 images .",
        "generated_description": "table 3 shows the number of images after which the me score threshold of 0 . 1 falls below the threshold for both the omniglot and imaginet models . the results show that the classification performance of the two models is very close . the imagenet classifier outperforms its counterpart for most of the images ."
    },
    {
        "table_id": "436",
        "table_info": {
            "table_caption": "TABLE IV: Success rate (in %) and number of replacements for different methods. NOR represents the mean/median of the normalized Number Of Replacements across all the sentences. The highest Success rate is marked in bold.",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] Method",
                "[BOLD] en-de  [BOLD] Success Rate",
                "[BOLD] en-de  [BOLD] NOR",
                "[BOLD] en-fr  [BOLD] Success Rate",
                "[BOLD] en-fr  [BOLD] NOR"
            ],
            "table_content_values": [
                [
                    "BLSTM",
                    "random + HotFlip",
                    "25.4%",
                    "0.23, 0.21",
                    "28.2%",
                    "0.21, 0.18"
                ],
                [
                    "BLSTM",
                    "Min-Grad + HotFlip",
                    "31.8%",
                    "0.22, 0.19",
                    "40.2%",
                    "0.19, 0.17"
                ],
                [
                    "BLSTM",
                    "random + Soft-Att",
                    "61.2%",
                    "0.58, 0.62",
                    "64.6%",
                    "0.62, 0.67"
                ],
                [
                    "BLSTM",
                    "Min-Grad + Soft-Att",
                    "[BOLD] 67.8%",
                    "0.58, 0.61",
                    "[BOLD] 70.8%",
                    "0.61, 0.66"
                ],
                [
                    "Transformer",
                    "random + HotFlip",
                    "35.0%",
                    "0.26, 0.24",
                    "40.6%",
                    "0.24, 0.21"
                ],
                [
                    "Transformer",
                    "Min-Grad + HotFlip",
                    "45.0%",
                    "0.26, 0.24",
                    "44.0%",
                    "0.23, 0.21"
                ],
                [
                    "Transformer",
                    "random + Soft-Att",
                    "50.2%",
                    "0.40, 0.39",
                    "59.0%",
                    "0.37, 0.35"
                ],
                [
                    "Transformer",
                    "Min-Grad + Soft-Att",
                    "[BOLD] 61.6%",
                    "0.41, 0.42",
                    "[BOLD] 64.8%",
                    "0.36, 0.34"
                ]
            ]
        },
        "gold_description": "table iv shows the success rate and the mean , median of the number of replacements ( normalized by the length of original sentence ) for different methods . as we can see from table iv , for both hotflip and soft - att , min - grad method gives significant improvement in success rate in comparison with random baseline across all the nmt models . the number of replacement for min - grad is comparable with random . table iv shows that transformer is more robust to our proposed method than blstm . this is because our proposed method has less number of replacements and lower success rate in case of transformer than blstm for both the language pairs . interestingly , hotflip has higher success rate and similar number of replacement in case for transformer than blstm . overall , as is evident from table iv , our proposed method ( min - grad + soft - att ) achieves the highest success rate across the nmt models . from table iv , across all the nmt models , we can see that soft - att significantly outperforms hotflip both in terms of success rate and number of replacements .",
        "generated_description": "as we can see , min - grad with soft - att outperforms random and transformer in both en - de and en - fr tasks , with an average improvement of 2 . 3 % and 2 . 8 % in terms of the accuracies of min - gradient and 3 . 5 % and 1 . 2 % for the random and transformer models , respectively . for the transformer model , although the accuracy is still lower than that of random , we can observe an improvement of 4 . 5 % . for the blstm model , the accuracy improves by 4 . 4 % and 5 . 1 % for min - graduation and soft - attention respectively ."
    },
    {
        "table_id": "437",
        "table_info": {
            "table_caption": "TABLE V: BLEU scores for the original/adversarial sentence (src) and their respective translation by the four NMT models. l1 denotes the model under attack, l2 denotes the other Transformer model. lblstm1,lblstm2 are the BLSTM counterparts of l1 and l2.",
            "table_column_names": [
                "[BOLD] Transformer",
                "[BOLD] Method",
                "src",
                "[ITALIC] l1",
                "[ITALIC] l2",
                "[ITALIC] lblstm1",
                "[ITALIC] lblstm2"
            ],
            "table_content_values": [
                [
                    "[BOLD] en-de",
                    "random + HotFlip",
                    "51.04",
                    "80.49",
                    "47.53",
                    "36.42",
                    "43.66"
                ],
                [
                    "[BOLD] en-de",
                    "Min-Grad + HotFlip",
                    "53.23",
                    "83.13",
                    "49.15",
                    "36.51",
                    "44.76"
                ],
                [
                    "[BOLD] en-de",
                    "random + Soft-Att",
                    "32.01",
                    "84.79",
                    "[BOLD] 29.72",
                    "[BOLD] 20.62",
                    "27.85"
                ],
                [
                    "[BOLD] en-de",
                    "Min-Grad + Soft-Att",
                    "[BOLD] 31.17",
                    "[BOLD] 88.55",
                    "31.09",
                    "20.63",
                    "[BOLD] 27.43"
                ],
                [
                    "[BOLD] en-fr",
                    "random + HotFlip",
                    "55.51",
                    "85.18",
                    "40.35",
                    "52.00",
                    "36.18"
                ],
                [
                    "[BOLD] en-fr",
                    "Min-Grad + HotFlip",
                    "57.92",
                    "88.40",
                    "41.98",
                    "54.39",
                    "37.68"
                ],
                [
                    "[BOLD] en-fr",
                    "random + Soft-Att",
                    "[BOLD] 33.61",
                    "89.77",
                    "[BOLD] 21.59",
                    "[BOLD] 32.37",
                    "[BOLD] 19.09"
                ],
                [
                    "[BOLD] en-fr",
                    "Min-Grad + Soft-Att",
                    "35.40",
                    "[BOLD] 91.99",
                    "23.28",
                    "34.32",
                    "20.29"
                ]
            ]
        },
        "gold_description": "1 2 , lblstm table v shows the bleu scores for the original / adversarial sentence ( src ) and their respective translation by the four nmt models . in table v , l1 denotes the transformer model under attack ( e . g . en - de ) , l2 denotes the other transformer model ( e . g . en - fr ) , and lblstm are the blstm counterparts of l1 and l2 .",
        "generated_description": "we can see that min - grad + soft - att significantly outperforms random and min - filter + hotflip with a large margin on all the three transformer configurations . for instance , in the en - de and en - fr tasks , the accuracy increases from 51 . 04 % to 53 . 23 % and from 47 . 53 % to 52 . 00 % , respectively , when using random masking ."
    },
    {
        "table_id": "438",
        "table_info": {
            "table_caption": "TABLE VI: BLEU scores for the original/adversarial sentence (src) and their respective translation by the four NMT models. l1 denotes the model under attack, l2 denotes the other BLSTM model. ltrans1,ltrans2 are the Transformer counterparts of l1 and l2.",
            "table_column_names": [
                "[BOLD] BLSTM",
                "[BOLD] Method",
                "src",
                "[ITALIC] l1",
                "[ITALIC] l2",
                "[ITALIC] ltrans1",
                "[ITALIC] ltrans2"
            ],
            "table_content_values": [
                [
                    "[BOLD] en-de",
                    "random + HotFlip",
                    "57.09",
                    "71.35",
                    "48.84",
                    "43.90",
                    "49.42"
                ],
                [
                    "[BOLD] en-de",
                    "Min-Grad + HotFlip",
                    "59.28",
                    "75.55",
                    "50.38",
                    "45.96",
                    "52.26"
                ],
                [
                    "[BOLD] en-de",
                    "random + Soft-Att",
                    "[BOLD] 13.77",
                    "87.14",
                    "[BOLD] 19.20",
                    "[BOLD] 18.36",
                    "[BOLD] 21.62"
                ],
                [
                    "[BOLD] en-de",
                    "Min-Grad + Soft-Att",
                    "14.49",
                    "[BOLD] 89.86",
                    "19.74",
                    "18.51",
                    "21.98"
                ],
                [
                    "[BOLD] en-fr",
                    "random + HotFlip",
                    "60.87",
                    "79.62",
                    "39.28",
                    "58.60",
                    "41.73"
                ],
                [
                    "[BOLD] en-fr",
                    "Min-Grad + HotFlip",
                    "63.97",
                    "84.87",
                    "41.16",
                    "61.80",
                    "44.94"
                ],
                [
                    "[BOLD] en-fr",
                    "random + Soft-Att",
                    "12.99",
                    "92.44",
                    "10.62",
                    "28.34",
                    "12.12"
                ],
                [
                    "[BOLD] en-fr",
                    "Min-Grad + Soft-Att",
                    "[BOLD] 12.66",
                    "[BOLD] 93.87",
                    "[BOLD] 9.95",
                    "[BOLD] 27.21",
                    "[BOLD] 11.92"
                ]
            ]
        },
        "gold_description": "table vi shows the bleu scores for the original / adversarial sentence ( src ) and their respective translation by the four nmt models . in table vi , l1 denotes the blstm model under attack , l2 denotes the other blstm model , and ltrans are the transformer counterparts of l1 and l2 . , ltrans 1 2",
        "generated_description": "we can see that min - grad + soft - att obtains significant improvements over random and min - gradient + hotflip when using the same batch size for all the blstm layers . however , when using batches of size 100 , min - graduation fails to outperform random and soft - att on the majority of the tasks ."
    },
    {
        "table_id": "439",
        "table_info": {
            "table_caption": "Table 2: SacreBleu degradation as a function of the proportion of bitext data that is noised.",
            "table_column_names": [
                "% noised",
                "SacreBleu Newstest ’12",
                "SacreBleu Newstest ’17"
            ],
            "table_content_values": [
                [
                    "0%",
                    "22.4",
                    "28.1"
                ],
                [
                    "20%",
                    "22.4",
                    "27.9"
                ],
                [
                    "80%",
                    "21.5",
                    "27.0"
                ],
                [
                    "100%",
                    "21.2",
                    "25.6"
                ]
            ]
        },
        "gold_description": "we first show that noising ende bitext sources does not seriously impact the translation quality of the transformer - base baseline . table 2 shows results for various values of p . specifically , it presents the somewhat unexpected finding that even when noising 100 % of the source bitext ( so the model has never seen wellformed english ) , bleu on well - formed test data only drops by 2 . 5 .",
        "generated_description": "we observe that the degradation worsens as the proportion of bitext data is noised increases from 0 % to 20 % to 80 % , and then continues to increase as the amount of noised data is added to the training data increases . we see that the largest degradation occurs at about 80 % of the bitext size , suggesting that the model is already robust to a small amount of noise in the target domain . however , we do not observe a large degradation in performance between 80 % and 100 % , suggesting that there may be additional robustness in the model beyond a certain threshold ."
    },
    {
        "table_id": "440",
        "table_info": {
            "table_caption": "Table 4: Comparing SacreBleu scores for different flavors of BT for WMT16 EnRo. Previous works’ scores are reported in italics as they use detok.multi-bleu instead of SacreBleu, so are not guaranteed to be comparable. In this case, however, we do see identical Bleu on our systems when we score them with detok.multi-bleu, so we believe it to be a fair comparison.",
            "table_column_names": [
                "a. Forward models (EnRo) Model",
                "a. Forward models (EnRo) dev",
                "a. Forward models (EnRo) test"
            ],
            "table_content_values": [
                [
                    "Gehring et al. ( 2017 )",
                    "[EMPTY]",
                    "[ITALIC] 29.9"
                ],
                [
                    "Sennrich 2016 (BT)",
                    "[ITALIC] 29.3",
                    "[ITALIC] 28.1"
                ],
                [
                    "bitext",
                    "26.5",
                    "28.3"
                ],
                [
                    "BT",
                    "31.6",
                    "32.6"
                ],
                [
                    "NoisedBT",
                    "29.9",
                    "32.0"
                ],
                [
                    "TaggedBT",
                    "30.5",
                    "33.0"
                ],
                [
                    "It.-3 BT",
                    "31.3",
                    "32.8"
                ],
                [
                    "It.-3 NoisedBT",
                    "31.2",
                    "32.6"
                ],
                [
                    "It.-3 TaggedBT",
                    "31.4",
                    "[BOLD] 33.4"
                ],
                [
                    "b. Reverse models (RoEn)",
                    "b. Reverse models (RoEn)",
                    "b. Reverse models (RoEn)"
                ],
                [
                    "Model",
                    "dev",
                    "test"
                ],
                [
                    "bitext",
                    "32.9",
                    "31.9"
                ],
                [
                    "It.-2 BT",
                    "39.5",
                    "[BOLD] 37.3"
                ]
            ]
        },
        "gold_description": "we repeat these experiments for wmt enro ( table 4 ) . in this case , noisedbt is actually harmful , lagging standard bt by - 0 . 6 bleu . taggedbt closes this gap and passes standard bt by + 0 . 4 bleu , for a total gain of + 1 . 0 bleu over noisedbt . we further investigate the effects of taggedbt by performing one round of iterative backtranslation ( cotterell and kreutzer , 2018 ; vu cong duy hoang and cohn , 2018 ; niu et al . , 2018 ) , and find another difference between the different varieties of bt : noisedbt and taggedbt allow the model to bootstrap improvements from an improved reverse model , whereas standard bt does not . sacrebleu scores for all these models are displayed in table 4 . we find that the iteration - 3 bt models improve over their iteration - 1 counterparts only for noisedbt ( + 1 . 0 bleu , dev + test avg ) and taggedbt ( + 0 . 7 bleu , dev + test avg ) , whereas the iteration - 3 bt model shows no improvement over its iteration - 1 counterpart ( - 0 . 1 bleu , dev + test avg ) . iteration - 3 taggedbt ( table 4 ) , our best bleu score of 33 . 4 bleu , obtained using iterative taggedbt , shows a gain of + 3 . 5 bleu over the highest previously published result on this test - set that we are aware of . we furthermore match or out - perform the highest published results we are aware of on wmt ende that use only back - translation , with higher or equal bleu on five of seven test sets .",
        "generated_description": "sennrich et al . ( bt ) is a re - implementation of the bt approach of \\ newcitegeinrich - etal - 2017 - sennrich - corr : 2017 : wmt16 enro that leverages back - translated data to augment the bitext - based encoder - decoder model . the results are reported in tab . our it - 3 it - 2 bt outperforms the previous state - of - the - art bt model by 3 . 3 sacrebleu points ."
    },
    {
        "table_id": "441",
        "table_info": {
            "table_caption": "Table 5: Results on WMT15 EnFr, with bitext, BT, NoisedBT, and TaggedBT.",
            "table_column_names": [
                "Model",
                "Avg",
                "2008",
                "2009",
                "2010",
                "2011",
                "2012",
                "2013",
                "2014",
                "2015"
            ],
            "table_content_values": [
                [
                    "Bitext",
                    "32.8",
                    "26.3",
                    "28.8",
                    "32.0",
                    "32.9",
                    "30.1",
                    "33.5",
                    "40.6",
                    "38.4"
                ],
                [
                    "BT",
                    "29.2",
                    "22.2",
                    "27.3",
                    "28.8",
                    "29.3",
                    "27.9",
                    "30.7",
                    "32.6",
                    "34.8"
                ],
                [
                    "NoisedBT",
                    "33.8",
                    "26.8",
                    "29.9",
                    "33.4",
                    "[BOLD] 33.9",
                    "[BOLD] 31.3",
                    "34.3",
                    "42.3",
                    "38.8"
                ],
                [
                    "TaggedBT",
                    "[BOLD] 34.1",
                    "[BOLD] 27.0",
                    "[BOLD] 30.0",
                    "[BOLD] 33.6",
                    "[BOLD] 33.9",
                    "31.2",
                    "[BOLD] 34.4",
                    "[BOLD] 42.7",
                    "[BOLD] 39.8"
                ]
            ]
        },
        "gold_description": "we performed a minimal set of experiments on wmt enfr , which are summarized in table 5 . in this case , we see that bt alone hurts performance compared to the strong bitext baseline , but noisedbt indeed surpasses the bitext model . it is worth noting that our numbers are lower than those reported by edunov et al . ( 2018 ) on the years they report ( 36 . 1 , 43 . 8 , and 40 . 9 on 2013 , 2014 , and 2015 respectively ) . on wmt16 enro , taggedbt improves on vanilla bt by 0 . 4 bleu .",
        "generated_description": "tab . the bitext baseline is the base transformer model , and the noisedbt baseline is a combination of the bt and the bitext model . we can see that both noise and taggedbt significantly improve over the baseline bitext across all the intervals . the gains are more pronounced on the wmt15 enfr dataset , with taggingbt giving 1 . 8 bleu score absolute improvement over the base model and 1 . 7 bleu score absolute improvements over the noised bt baseline ."
    },
    {
        "table_id": "442",
        "table_info": {
            "table_caption": "Table 6: Attention sink ratio on the first and last token and entropy (at decoder layer 5) for the models in Table 3.a, averaged over all sentences in newstest14. For ASR, data is treated as if it were BT (noised and/or tagged, resp.), whereas for entropy the natural text is used. Outliers discussed in the text are bolded.",
            "table_column_names": [
                "Model",
                "ASR0",
                "ASR| [ITALIC] x|",
                "~H"
            ],
            "table_content_values": [
                [
                    "Bitext baseline",
                    "0.31",
                    "10.21",
                    "0.504"
                ],
                [
                    "BT",
                    "0.28",
                    "10.98",
                    "[BOLD] 0.455"
                ],
                [
                    "P3BT",
                    "0.37",
                    "7.66",
                    "0.558"
                ],
                [
                    "NoisedBT",
                    "1.01",
                    "3.96",
                    "0.619"
                ],
                [
                    "TaggedBT",
                    "[BOLD] 5.31",
                    "5.31",
                    "0.597"
                ],
                [
                    "TaggedNoisedBT",
                    "[BOLD] 7.33",
                    "7.33",
                    "0.491"
                ]
            ]
        },
        "gold_description": "to understand how the model treats the tag and what biases it learns from the data , we investigate the entropy of the attention probability distribution , as well as the attention captured by the tag . for the tagged variants , there is heavy attention on the tag when it is present ( table 6 ) , indicating that the model relies on the information signalled by the tag . table 6 reports the average length - normalized shannon entropy : the entropy of the attention probabilities from the model trained on bt data is the clear outlier .",
        "generated_description": "for asr , we treated the data as if it were bt ( noised and / or tagged , resp . ) , whereas for p3bt and taggedbt , the natural text was treated as bt . taggednoisedbt has the lowest attention sink ratio on the first token and the last token ( at decoder layer 5 ) for the bitext baseline . this indicates that the model tends to focus more on the more salient token at the end of the sequence , that is , the token that gives the highest attention weight to the final token . this is in line with the results of \\ newcitesennrich - haddow - birch : 2015 : naacl : 2014 : p16 - 1 who observed that a large number of attention scores for the decoder layers correlated negatively with the performance of the model , and that this is due to overfitting ."
    },
    {
        "table_id": "443",
        "table_info": {
            "table_caption": "Table 7: Comparing standard decoding with decoding as if the input were back-translated data, meaning that it is tagged (for the TaggedBT model) or noised (for the NoisedBT model) .",
            "table_column_names": [
                "Model",
                "Decode type",
                "AVG 13-17",
                "2010",
                "2011",
                "2012",
                "2013",
                "2014",
                "2015",
                "2016",
                "2017"
            ],
            "table_content_values": [
                [
                    "TaggedBT",
                    "standard",
                    "[BOLD] 33.24",
                    "26.5",
                    "[BOLD] 24.2",
                    "[BOLD] 25.2",
                    "[BOLD] 28.7",
                    "[BOLD] 32.8",
                    "[BOLD] 34.5",
                    "[BOLD] 38.1",
                    "[BOLD] 32.4"
                ],
                [
                    "[EMPTY]",
                    "as BT (tagged)",
                    "30.30",
                    "24.3",
                    "22.2",
                    "23.4",
                    "26.6",
                    "30.0",
                    "30.5",
                    "34.2",
                    "30.2"
                ],
                [
                    "NoisedBT",
                    "standard",
                    "33.06",
                    "[BOLD] 26.7",
                    "24.0",
                    "[BOLD] 25.2",
                    "28.6",
                    "32.6",
                    "33.9",
                    "38.0",
                    "32.2"
                ],
                [
                    "[EMPTY]",
                    "as BT (noised)",
                    "10.66",
                    "8.1",
                    "6.5",
                    "7.5",
                    "8.2",
                    "11.1",
                    "10.0",
                    "12.7",
                    "11.3"
                ]
            ]
        },
        "gold_description": "the bleu scores of each decoding method are presented in table 7 . the noised decode – decoding newstest sentences with the noisedbt model after noising the source – yields poor performance . the tagged decode , however , yields only somewhat lower performance than the standard decode on the same model ( - 2 . 9bleu on average ) .",
        "generated_description": "comparing the standard decoding and the as bt settings , we see that using the automatically tagged bt data as the decoder gives the best overall performance . however , the performance of the standard decoder is significantly worse than using the noisy bt data . the reason for this may be that the standard model is trained on the entire training data , while the noisy data is randomly sampled from different parts of the training data . in contrast , our proposed method ( as bt ) is trained only on the target domain data and does not suffer from overfitting to the source domain ."
    },
    {
        "table_id": "444",
        "table_info": {
            "table_caption": "Table 9: Source-target overlap for both back-translated data with decoding newstest as if it were bitext or BT data. Model decodes are averaged over newstest2010-newstest2017.",
            "table_column_names": [
                "Data",
                "src-tgt unigram overlap"
            ],
            "table_content_values": [
                [
                    "TaggedBT (standard decode)",
                    "8.9%"
                ],
                [
                    "TaggedBT (tagged decode)",
                    "10.7%"
                ],
                [
                    "Bitext",
                    "5.9%"
                ],
                [
                    "BT Data",
                    "11.4 %"
                ]
            ]
        },
        "gold_description": "we quantify the copy rate with the unigram overlap between source and target as a percentage of tokens in the target side , and compare those statistics to the bitext and the back - translated data ( table 9 ) .",
        "generated_description": "source - target overlap : we measure the percentage of target words in the target side of the bitext and bt data that do not appear in the original bitext training data . for example , the 11 . 4 % overlap for the back - translated bt data with the standard decode shows that the source - target overlaps are 8 . 9 % and 10 . 7 % for the tagged decode and standard decode , respectively . the overlap is lower for the taggedbt data , which we tagged as if it were bitext ."
    },
    {
        "table_id": "445",
        "table_info": {
            "table_caption": "TABLE I: Document distribution over the classes of the Reuters-8",
            "table_column_names": [
                "Class",
                "Number of samples"
            ],
            "table_content_values": [
                [
                    "acq",
                    "2292"
                ],
                [
                    "crude",
                    "374"
                ],
                [
                    "earn",
                    "3923"
                ],
                [
                    "grain",
                    "51"
                ],
                [
                    "interest",
                    "271"
                ],
                [
                    "money-fx",
                    "293"
                ],
                [
                    "ship",
                    "144"
                ],
                [
                    "trade",
                    "326"
                ]
            ]
        },
        "gold_description": "we used the reuters - 8 dataset without stop words from aiming at single - label classification , which is a preprocessed format of the reuters - 215782 . words in the texts were considered as they appeared , without performing stemming or typo correction . this database has eight different classes with the number of samples varying from 51 to over 3000 documents , as can be seen in table i .",
        "generated_description": "in order to perform document classification , we use the reuters - 8 dataset , which is rich in detail description of financial news documents . the dataset was originally created by the investment bank of china ( icc ) , and it contains 2292 news documents , which were extracted from the company ’ s website . the documents were categorized into 8 classes : acq , crude , earn , interest , profits , money - fx , and ship . the classes were chosen based on frequency of occurrence in the training data ."
    },
    {
        "table_id": "446",
        "table_info": {
            "table_caption": "TABLE II: Results for the text classification experiment on Reuters-8 database",
            "table_column_names": [
                "Method",
                "Feature",
                "Accuracy (%)",
                "Std. Deviation"
            ],
            "table_content_values": [
                [
                    "SA",
                    "w2v",
                    "78.73",
                    "1.56"
                ],
                [
                    "MSM",
                    "w2v",
                    "[BOLD] 90.62",
                    "0.42"
                ],
                [
                    "TF-MSM",
                    "w2v",
                    "[BOLD] 92.01",
                    "0.30"
                ],
                [
                    "MVB",
                    "binBOW",
                    "62.70",
                    "0.69"
                ],
                [
                    "MNB",
                    "tfBOW",
                    "[BOLD] 91.47",
                    "0.37"
                ],
                [
                    "LSA",
                    "w2v",
                    "34.58",
                    "0.40"
                ],
                [
                    "LSA",
                    "binBOW",
                    "86.92",
                    "0.74"
                ],
                [
                    "LSA",
                    "tfBOW",
                    "86.23",
                    "0.96"
                ],
                [
                    "LSA",
                    "tfidfBOW",
                    "86.35",
                    "1.03"
                ],
                [
                    "SVM",
                    "w2v",
                    "26.61",
                    "0.30"
                ],
                [
                    "SVM",
                    "binBOW",
                    "89.23",
                    "0.24"
                ],
                [
                    "SVM",
                    "tfBOW",
                    "89.10",
                    "0.29"
                ],
                [
                    "SVM",
                    "tfidfBOW",
                    "88.78",
                    "0.40"
                ]
            ]
        },
        "gold_description": "in this experiment , we performed text classification among the classes in the reuters - 8 database . the results can be seen in table ii . the simplest baseline , sa with w2v , achieved an accuracy rate of 78 . 73 % . this lsa with bow features was almost 10 % more accurate than sa , where the best results with binary weights were achieved with an approximation with 130 dimensions , with tf weights were achieved with 50 dimensions , and with tfidf weights were achieved with 30 dimensions . svm with bow features was about 3 % more accurate than lsa , with binary weights leading to a higher accuracy rate . among the baselines , the best method was mnb with tfbow features , with an accuracy of 91 . 47 % , msm with w2v had an accuracy rate of 90 . 62 % , with the best results achieved with word subspace dimensions for the training classes ranging from 150 to 181 , and for the query ranging from 3 to 217 . incorporating the frequency information in the subspace modeling resulted in higher accuracy , with tfmsm achieving 92 . 01 % , with dimensions of word subspaces",
        "generated_description": "the results show that the proposed tf - msm method outperforms all the other methods in terms of both accuracy and standard deviation . for sa and mnb , the best results are obtained using word2vec embeddings . for lsa , svm and cnn outperform tf - idfbow and binbow features . the standard deviation of lsa is much higher than that of svms and cnns . this is because the standard deviation for svms is very high ( close to 1 . 0 ) due to the large number of classes in the dataset . for cnns , the values of standard deviation are much smaller ( smaller than 0 . 2 ) , which means that the classifier can easily find the classes with high standard deviations ."
    },
    {
        "table_id": "447",
        "table_info": {
            "table_caption": "Table 2: Results of our Transformer-Language Model approach against similar approaches (Bryant and Briscoe, 2018) and state-of-the-art on Grammatical Error Correction. For each of the datasets, we use the corresponding test set, and we do not train our models on the corpora. As BERT, we report the best performing BERT model (12 layers, retaining uppercase characters). In the top part of each dataset, we report the scores of supervised methods and in the bottom the unsupervised ones. † denotes this system won the shared task competition.",
            "table_column_names": [
                "[BOLD] Dataset",
                "[BOLD] System",
                "[BOLD] ERRANT  [BOLD] P",
                "[BOLD] ERRANT  [BOLD] R",
                "[BOLD] ERRANT  [BOLD] F0.5",
                "[BOLD] M2  [BOLD] P",
                "[BOLD] M2  [BOLD] R",
                "[BOLD] M2  [BOLD] F0.5"
            ],
            "table_content_values": [
                [
                    "[BOLD] CoNLL-2014",
                    "Felice et al. ( 2014 ) †",
                    "-",
                    "-",
                    "-",
                    "39.71",
                    "30.10",
                    "37.33"
                ],
                [
                    "[BOLD] CoNLL-2014",
                    "Yannakoudakis et al. ( 2017 )",
                    "-",
                    "-",
                    "-",
                    "58.79",
                    "30.63",
                    "49.66"
                ],
                [
                    "[BOLD] CoNLL-2014",
                    "Chollampatt and Ng ( 2017 )",
                    "-",
                    "-",
                    "-",
                    "62.74",
                    "32.96",
                    "53.14"
                ],
                [
                    "[BOLD] CoNLL-2014",
                    "Chollampatt and Ng ( 2018 )",
                    "-",
                    "-",
                    "-",
                    "65.49",
                    "33.14",
                    "54.79"
                ],
                [
                    "[BOLD] CoNLL-2014",
                    "Ge et al. ( 2018 )",
                    "-",
                    "-",
                    "-",
                    "[BOLD] 74.12",
                    "[BOLD] 36.30",
                    "[BOLD] 61.34"
                ],
                [
                    "[BOLD] CoNLL-2014",
                    "Bryant and Briscoe ( 2018 )",
                    "36.62",
                    "19.93",
                    "31.37",
                    "40.56",
                    "20.81",
                    "34.09"
                ],
                [
                    "[BOLD] CoNLL-2014",
                    "BERT",
                    "33.27",
                    "[BOLD] 27.14",
                    "31.83",
                    "35.69",
                    "[BOLD] 27.99",
                    "33.83"
                ],
                [
                    "[BOLD] CoNLL-2014",
                    "GPT-1",
                    "49.58",
                    "27.06",
                    "42.5",
                    "51.08",
                    "27.45",
                    "43.57"
                ],
                [
                    "[BOLD] CoNLL-2014",
                    "GPT-2",
                    "[BOLD] 57.73",
                    "24.75",
                    "[BOLD] 45.58",
                    "[BOLD] 58.51",
                    "24.9",
                    "[BOLD] 46.08"
                ],
                [
                    "[BOLD] FCE",
                    "Yannakoudakis et al. ( 2017 )",
                    "-",
                    "-",
                    "-",
                    "[BOLD] 65.03",
                    "32.45",
                    "[BOLD] 54.15"
                ],
                [
                    "[BOLD] FCE",
                    "Bryant and Briscoe ( 2018 )",
                    "41.92",
                    "13.62",
                    "29.61",
                    "44.78",
                    "14.12",
                    "31.22"
                ],
                [
                    "[BOLD] FCE",
                    "BERT",
                    "29.56",
                    "[BOLD] 34.67",
                    "30.46",
                    "31.97",
                    "[BOLD] 35.01",
                    "32.53"
                ],
                [
                    "[BOLD] FCE",
                    "GPT-1",
                    "[BOLD] 62.75",
                    "32.19",
                    "52.74",
                    "[BOLD] 64.01",
                    "32.33",
                    "[BOLD] 53.52"
                ],
                [
                    "[BOLD] FCE",
                    "GPT-2",
                    "61.91",
                    "33.47",
                    "[BOLD] 52.92",
                    "62.64",
                    "33.74",
                    "53.48"
                ]
            ]
        },
        "gold_description": "table 2 presents the results of our method comparing them against recent state - of - the - art supervised models and the simple n - gram language model used by bryant and briscoe ( 2018 ) . a key result of table 2 is that transformer language models prove to be more than just a competitive baseline to legitimate grammatical error correction systems on their own . across the board , transformer models are able to outperform the simple n - gram model and even approach the performance of supervised gec systems . we see that their performance is nearly identical with gpt - 2 leading by a small margin in the conll14 dataset . bert surpasses the n - gram baseline overall , it achieves worse performance than the rest in terms of precision and f0 . 5 score .",
        "generated_description": "on the conll - 2014 and wsc - 2 test sets , our proposed model significantly outperforms the previous state - of - the - art by a large margin and achieves the best f0 . 5 score of 54 . 79 % and 53 . 14 % on the two test sets with respect to the previous best performance . on the other hand , the performance of our model on the wsc2 test set is slightly lower than that of the best result reported by ge et al ."
    },
    {
        "table_id": "448",
        "table_info": {
            "table_caption": "Table III: Reply classification labels and distribution per source",
            "table_column_names": [
                "[BOLD] Reply Type",
                "[BOLD] Description",
                "[BOLD] Antichat",
                "[BOLD] Hackforums"
            ],
            "table_content_values": [
                [
                    "Buy",
                    "Someone wants to buy or bought a product.",
                    "8%",
                    "12%"
                ],
                [
                    "Sell",
                    "Someone making a sale offer to the original poster of a thread.",
                    "8%",
                    "2%"
                ],
                [
                    "Other",
                    "Anything that didn’t fall into the previous categories.",
                    "84%",
                    "86%"
                ],
                [
                    "TOTAL",
                    "[EMPTY]",
                    "9,992",
                    "5,898"
                ]
            ]
        },
        "gold_description": "we label each reply into three categories : buy , sell and other . the distribution of reply types is highly dependant on the structure and rules of the forum as shown in table iii .",
        "generated_description": "antichat is the largest source of hate speech in terms of total number of messages , with a total of 9 , 992 messages and 5 , 898 unique messages . we classify the majority of the messages into three categories : buy , sell , and other , where we classify anything that didn ’ t fall into the other categories as ‘ other ’ . for example , in the second category ‘ sell ’ is a case of someone making a sale offer to the original poster of a thread , or ‘ want to buy or bought a product ’ , where the original post is asking for the user to display the buyer or seller side of the advertisement ."
    },
    {
        "table_id": "449",
        "table_info": {
            "table_caption": "Table IV: Weighted precision, recall and F1 scores of classifiers across datasets and tasks, with stratified k-fold cross-validation",
            "table_column_names": [
                "[EMPTY]",
                "[BOLD] Antichat  [ITALIC] Product",
                "[BOLD] Antichat  [ITALIC] Product",
                "[BOLD] Antichat  [ITALIC] Product",
                "[BOLD] Antichat  [ITALIC] Reply",
                "[BOLD] Antichat  [ITALIC] Reply",
                "[BOLD] Antichat  [ITALIC] Reply",
                "[BOLD] Hack Forums  [ITALIC] Product",
                "[BOLD] Hack Forums  [ITALIC] Product",
                "[BOLD] Hack Forums  [ITALIC] Product",
                "[BOLD] Hack Forums  [ITALIC] Reply",
                "[BOLD] Hack Forums  [ITALIC] Reply",
                "[BOLD] Hack Forums  [ITALIC] Reply"
            ],
            "table_content_values": [
                [
                    "[BOLD] Model",
                    "Prec",
                    "Recall",
                    "F1",
                    "Prec",
                    "Recall",
                    "F1",
                    "Prec",
                    "Recall",
                    "F1",
                    "Prec",
                    "Recall",
                    "F1"
                ],
                [
                    "[BOLD] FastText",
                    "0.824",
                    "0.734",
                    "0.764",
                    "0.823",
                    "0.318",
                    "0.450",
                    "0.722",
                    "0.582",
                    "0.627",
                    "0.800",
                    "0.427",
                    "0.539"
                ],
                [
                    "[BOLD] Logistic Regression",
                    "0.831",
                    "0.718",
                    "0.753",
                    "[BOLD] 0.874",
                    "0.245",
                    "0.381",
                    "0.617",
                    "0.564",
                    "0.617",
                    "[BOLD] 0.852",
                    "0.363",
                    "0.492"
                ],
                [
                    "[BOLD] SVM",
                    "0.817",
                    "0.748",
                    "0.767",
                    "0.654",
                    "0.213",
                    "0.301",
                    "0.716",
                    "0.578",
                    "0.614",
                    "0.812",
                    "0.332",
                    "0.440"
                ],
                [
                    "[BOLD] XGBoost",
                    "[BOLD] 0.824",
                    "0.677",
                    "0.729",
                    "0.713",
                    "0.227",
                    "0.328",
                    "[BOLD] 0.734",
                    "0.577",
                    "0.627",
                    "0.819",
                    "0.352",
                    "0.465"
                ]
            ]
        },
        "gold_description": "similarly a single classifier outperforms the rest in reply classification , as seen in table iv . by our weighted nonother precision metric , logistic regression performed the best across both datasets , providing 0 . 874 precision on antichat and 0 . 852 precision on hack forums .",
        "generated_description": "in this section , we evaluate the performance of the different models in terms of precision , recall and f1 - score on the antichat and hackforums datasets . we can see that the logistic regression model outperforms the fasttext model and the bilstm model by a large margin . the best recall is obtained by using fasttext as the feature extractor . however , the logistic regression model performs very poorly on the product and reply datasets . the reason is that these two datasets contain a large number of hate speech and offensive comments . therefore , it is difficult to train a good hate speech detection model with a small number of examples ."
    },
    {
        "table_id": "450",
        "table_info": {
            "table_caption": "Table 1: Comparison of pooling methods",
            "table_column_names": [
                "[BOLD] Task",
                "[BOLD] [CLS]",
                "[BOLD] Mean",
                "[BOLD] Max",
                "[BOLD] [SEP]"
            ],
            "table_content_values": [
                [
                    "Semantic Similarity",
                    "34.1",
                    "[BOLD] 84.5",
                    "80.7",
                    "13.0"
                ],
                [
                    "Text Classification",
                    "90.7",
                    "[BOLD] 95.4",
                    "89.7",
                    "88.9"
                ],
                [
                    "Entailment",
                    "72.4",
                    "[BOLD] 89.3",
                    "87.1",
                    "66.1"
                ],
                [
                    "Surface Information",
                    "45.6",
                    "[BOLD] 78.9",
                    "47.3",
                    "42.8"
                ],
                [
                    "Syntactic Information",
                    "78.2",
                    "[BOLD] 86.0",
                    "75.7",
                    "72.2"
                ],
                [
                    "Semantic Information",
                    "90.3",
                    "[BOLD] 93.7",
                    "89.5",
                    "86.7"
                ],
                [
                    "Average Score",
                    "68.6",
                    "[BOLD] 88.0",
                    "78.3",
                    "61.6"
                ]
            ]
        },
        "gold_description": "are summarized in table 1 , where the score for each task category is calculated by averaging the normalized values for the tasks within each category . although the activations of [ cls ] token hidden states are often used in fine - tuning bert for classification tasks , mean - pooling of hidden states performs the best in all task categories among all the pooling methods .",
        "generated_description": "we can see that the mean pooling method performs the best for most of the tasks . the maximum and minimum pooling methods perform the worst for the semantic similarity and the entailment tasks . for the text classification and the syntactic information tasks , average score is the best method . the results show that the average score of the classifier is much lower than the other methods ."
    },
    {
        "table_id": "451",
        "table_info": {
            "table_caption": "Table 3: Results of BERT passage-level embeddings on question-answering datasets",
            "table_column_names": [
                "[BOLD] Dataset / Model  [BOLD] WikiPassageQA",
                "[BOLD] Metrics  [BOLD] MAP",
                "[BOLD] Metrics  [BOLD] P@5",
                "[BOLD] Metrics  [BOLD] P@10"
            ],
            "table_content_values": [
                [
                    "BM25",
                    "53.7",
                    "19.5",
                    "11.5"
                ],
                [
                    "Memory-CNN-LSTM Cohen2018",
                    "56.1",
                    "20.8",
                    "12.3"
                ],
                [
                    "Pre-trained BERT Embedding",
                    "55.0",
                    "21.6",
                    "13.7"
                ],
                [
                    "SNLI Fine-tuned BERT Embedding",
                    "52.5",
                    "20.6",
                    "12.8"
                ],
                [
                    "In-domain Fine-tuned BERT",
                    "[BOLD] 74.9",
                    "[BOLD] 27.2",
                    "[BOLD] 15.2"
                ],
                [
                    "[BOLD] InsuranceQA",
                    "[BOLD] P@1",
                    "[BOLD] P@5",
                    "[BOLD] P@10"
                ],
                [
                    "BM25",
                    "60.2",
                    "19.5",
                    "10.9"
                ],
                [
                    "SUBMULT+NN Wang2016g",
                    "75.6",
                    "-",
                    "-"
                ],
                [
                    "DSSM Huang2013a",
                    "30.3",
                    "-",
                    "-"
                ],
                [
                    "Pre-trained BERT Embedding",
                    "44.9",
                    "17.6",
                    "10.6"
                ],
                [
                    "SNLI Fine-tuned BERT Embedding",
                    "48.0",
                    "18.5",
                    "11.0"
                ],
                [
                    "In-domain Fine-tuned BERT",
                    "[BOLD] 78.3",
                    "[BOLD] 25.4",
                    "[BOLD] 13.7"
                ],
                [
                    "[BOLD] Quasar-t",
                    "[BOLD] R@1",
                    "[BOLD] R@5",
                    "[BOLD] R@10"
                ],
                [
                    "BM25",
                    "38.7",
                    "59.2",
                    "66.0"
                ],
                [
                    "Pre-trained BERT Embedding",
                    "48.6",
                    "66.6",
                    "71.7"
                ],
                [
                    "SNLI Fine-tuned BERT Embedding",
                    "49.3",
                    "66.1",
                    "71.0"
                ],
                [
                    "In-domain Fine-tuned BERT",
                    "[BOLD] 59.5",
                    "[BOLD] 70.9",
                    "[BOLD] 74.6"
                ],
                [
                    "[BOLD] SearchQA",
                    "[BOLD] R@1",
                    "[BOLD] R@5",
                    "[BOLD] R@10"
                ],
                [
                    "BM25",
                    "50.5",
                    "83.3",
                    "90.9"
                ],
                [
                    "Pre-trained BERT Embedding",
                    "66.2",
                    "89.7",
                    "95.0"
                ],
                [
                    "SNLI Fine-tuned BERT Embedding",
                    "66.8",
                    "90.0",
                    "95.1"
                ],
                [
                    "In-domain Fine-tuned BERT",
                    "[BOLD] 76.3",
                    "[BOLD] 93.0",
                    "[BOLD] 96.7"
                ]
            ]
        },
        "gold_description": "the comparison between bert embeddings and other models is presented in table 3 . overall , in - domain fine - tuned bert delivers the best performance . we report new state - of - the - art results on wikipassageqa ( 33 % improvement in map ) and insuranceqa ( version 1 . 0 ) ( 3 . 6 % improvement in p @ 1 ) by supervised fine - tuning bert using pairwise rank hinge loss . when evaluated on non - factoid qa datasets , there is a big gap between bert embeddings and the fully fine - tuned bert , which suggests that deep interactions between questions and answers are critical to the task . however , the gap is much smaller for factoid qa datasets . since non - factoid qa depends more on content matching rather than vocabulary matching , the results are kind of expected . similar to bert for sentence embeddings , mean - pooling and combining the top and bottom layer embeddings lead to better performance , and ( u , v , u ∗ v , | u − v | ) shows the strongest results among other interaction schemes . different from sentence - level embeddings , fine - tuning bert on snli doesn ' t lead to significant improvement , which suggests possible domain mismatch between snli and the qa datasets . mlp layer usually provided a 1 - 2 percent boost in performance compared to the logistic regression layer . for wikipassageqa , bert embeddings perform comparably as bm25 baseline . for insuranceqa , bert embeddings outperform a strong representation - based matching model dssm ( huang et al . , 2013 ) , but still far behind the state - of - the - art interaction - based model submult + nn ( wang and jiang , 2016 ) and fully fine - tuned bert . on",
        "generated_description": "we can see that the in - domain fine - tuned bert model outperforms previous state - of - the - art models by a large margin , e . g . , outperforming the bm25 baseline by 7 . 3 % in terms of p @ 10 on the wikipassageqa dataset and 5 . 7 % on insuranceqa . the results demonstrate that the domain - specific bert embeddings are better than the general bert pre - trained models on these two datasets , which validates the effectiveness of domain - adaptive pre - training ."
    },
    {
        "table_id": "452",
        "table_info": {
            "table_caption": "Table 3: comparing results of our trained model with others",
            "table_column_names": [
                "[EMPTY]",
                "Arman word",
                "Arman phrase",
                "Peyma word",
                "Peyma phrase"
            ],
            "table_content_values": [
                [
                    "Bokaei and Mahmoudi Bokaei and Mahmoudi ( 2018 )",
                    "81.50",
                    "76.79",
                    "-",
                    "-"
                ],
                [
                    "Shahshahani et al.Shahshahani et al. ( 2018 )",
                    "-",
                    "-",
                    "80.0",
                    "-"
                ],
                [
                    "Beheshti-NER (Our Model)",
                    "[BOLD] 84.03",
                    "[BOLD] 79.93",
                    "[BOLD] 90.59",
                    "[BOLD] 87.62"
                ]
            ]
        },
        "gold_description": "these models are evaluated on two common datasets for ner : peyma and arman . bokaei and mahmoudi ( bokaei and mahmoudi , 2018 ) and shahshahani et al . ( shahshahani et al . , 2018 ) had reported the best results which you can see in table 3 as you see in table 3 in both word and phrase levels , our model outperform other ner approaches for the persian language . dataset . table 3 shows that our results are 10 percent better than shahshahani and colleagues on the same platform . on the other hand bokaei and mahmoudi ( bokaei and mahmoudi , 2018 ) reported their results on arman dataset which is lower than ours in both word and phrase levels according to table 3 .",
        "generated_description": "our model beheshti - ner significantly outperforms other models and achieves state - of - the - art results in both arman and peyma word embeddings . in addition , our model also outperforms shahshahani et al . ’ s model in the peyma phrase embedding . these results demonstrate the effectiveness of our model and highlight the importance of incorporating the structural information into neural networks ."
    },
    {
        "table_id": "453",
        "table_info": {
            "table_caption": "Table 4: Phrase-level evaluation for subtask A: 3-classes",
            "table_column_names": [
                "Team",
                "Team",
                "Test Data 1 In Domain",
                "Test Data 1 In Domain",
                "Test Data 1 In Domain",
                "Test Data 1 Out Domain",
                "Test Data 1 Out Domain",
                "Test Data 1 Out Domain",
                "Test Data 1 Total",
                "Test Data 1 Total",
                "Test Data 1 Total"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "[EMPTY]",
                    "P",
                    "R",
                    "F1",
                    "P",
                    "R",
                    "F1",
                    "P",
                    "R",
                    "F1"
                ],
                [
                    "1",
                    "MorphoBERT",
                    "88.7",
                    "85.5",
                    "87.1",
                    "86.3",
                    "83.8",
                    "85",
                    "87.3",
                    "84.5",
                    "85.9"
                ],
                [
                    "2",
                    "Beheshti-NER-1",
                    "85.3",
                    "84.4",
                    "84.8",
                    "84.4",
                    "82.6",
                    "83.5",
                    "84.8",
                    "83.3",
                    "84"
                ],
                [
                    "3",
                    "Team-3",
                    "87.4",
                    "77.2",
                    "82",
                    "87.4",
                    "73.4",
                    "79.8",
                    "87.4",
                    "75",
                    "80.7"
                ],
                [
                    "4",
                    "ICTRC-NLPGroup",
                    "87.5",
                    "76",
                    "81.3",
                    "86.2",
                    "69.6",
                    "77",
                    "86.8",
                    "72.3",
                    "78.9"
                ],
                [
                    "5",
                    "UT-NLP-IR",
                    "75.3",
                    "68.9",
                    "72",
                    "72.3",
                    "60.7",
                    "66",
                    "73.6",
                    "64.1",
                    "68.5"
                ],
                [
                    "6",
                    "SpeechTrans",
                    "41.5",
                    "39.5",
                    "40.5",
                    "43.1",
                    "38.7",
                    "40.8",
                    "42.4",
                    "39",
                    "40.6"
                ],
                [
                    "7",
                    "Baseline",
                    "32.2",
                    "45.8",
                    "37.8",
                    "32.8",
                    "39.1",
                    "35.7",
                    "32.5",
                    "41.9",
                    "36.6"
                ]
            ]
        },
        "gold_description": "tables 4 , 5 , 6 , 7 and 8 show the results of evaluation reported by competition for all teams which participated in the challenge . our method is mentioned as beheshti - ner - 1 table 4 and 5 show the results for subtask a . according to the tables , we reached to 84 . 0 % and 87 . 9 % f1 score respectively for phrase and word level evaluations .",
        "generated_description": "morphobert was the best performing model in the in - domain setting ( row 1 ) , and beheshti - ner - 1 the best out - of - domain model ( row 2 ) . the remaining four teams participated in the ood ner and nlpgroup tasks . we can see that the overall f1 score for the in - domain setting is much lower than the out - domain one , which shows that our model is better at dealing with out of domain data than the other models . in addition , our model outperforms most of the other teams in terms of the p , r , and f1 scores in both in and out domain settings ."
    },
    {
        "table_id": "454",
        "table_info": {
            "table_caption": "TABLE II: Ranked List of Target Words based on their Ambiguity Scores",
            "table_column_names": [
                "Medical Device Term",
                "Medical Device Score",
                "Medical Robot Term",
                "Medical Robot Score",
                "Sports Rehab Machine Term",
                "Sports Rehab Machine Score"
            ],
            "table_content_values": [
                [
                    "root",
                    "0.8802",
                    "stroke",
                    "0.919",
                    "kingdom",
                    "0.907"
                ],
                [
                    "mouse",
                    "0.8633",
                    "kingdom",
                    "0.893",
                    "stroke",
                    "0.8495"
                ],
                [
                    "kingdom",
                    "0.8383",
                    "vessel",
                    "0.8651",
                    "progressive",
                    "0.8414"
                ],
                [
                    "iron",
                    "0.8381",
                    "thread",
                    "0.8385",
                    "net",
                    "0.8334"
                ],
                [
                    "internal",
                    "0.8043",
                    "floating",
                    "0.8045",
                    "suspension",
                    "0.8322"
                ],
                [
                    "progressive",
                    "0.7957",
                    "strain",
                    "0.8018",
                    "induction",
                    "0.8244"
                ],
                [
                    "agent",
                    "0.7875",
                    "mouse",
                    "0.7997",
                    "thread",
                    "0.8236"
                ],
                [
                    "express",
                    "0.7733",
                    "progressive",
                    "0.7983",
                    "root",
                    "0.8093"
                ],
                [
                    "plasma",
                    "0.7685",
                    "die",
                    "0.787",
                    "transmission",
                    "0.7871"
                ],
                [
                    "net",
                    "0.7678",
                    "secondary",
                    "0.786",
                    "die",
                    "0.7821"
                ],
                [
                    "⋮",
                    "⋮",
                    "⋮",
                    "⋮",
                    "⋮",
                    "⋮"
                ],
                [
                    "argued",
                    "0.1631",
                    "corresponding",
                    "0.1695",
                    "corresponding",
                    "0.196"
                ],
                [
                    "richard",
                    "0.1608",
                    "corresponds",
                    "0.1693",
                    "told",
                    "0.1958"
                ],
                [
                    "authority",
                    "0.1606",
                    "feel",
                    "0.167",
                    "joseph",
                    "0.1956"
                ],
                [
                    "michael",
                    "0.1569",
                    "coating",
                    "0.1666",
                    "understanding",
                    "0.1953"
                ],
                [
                    "required",
                    "0.154",
                    "wife",
                    "0.1603",
                    "love",
                    "0.1902"
                ],
                [
                    "peter",
                    "0.1388",
                    "michael",
                    "0.159",
                    "economic",
                    "0.1902"
                ],
                [
                    "robert",
                    "0.1381",
                    "authority",
                    "0.156",
                    "coating",
                    "0.1892"
                ],
                [
                    "david",
                    "0.1201",
                    "peter",
                    "0.1437",
                    "pay",
                    "0.1856"
                ],
                [
                    "james",
                    "0.1188",
                    "david",
                    "0.1412",
                    "authority",
                    "0.1838"
                ],
                [
                    "charles",
                    "0.1157",
                    "required",
                    "0.1382",
                    "causing",
                    "0.1822"
                ]
            ]
        },
        "gold_description": "in order to study the cases of disagreement between the approaches proposed by this paper and ferrari et al . ( 2018 ) , the top - 5 words with the largest absolute differences between the assigned ranks have been reported for each scenario by table iii . the number of target words for each project scenario have also been mentioned in parenthesis . it can be observed that most of the cases of disagreement have a higher rank , i . e . relatively lower ambiguity score assigned by the linear transformation approach proposed by this paper . most of such cases are proper names such as robert , peter , and daniel .",
        "generated_description": "for example , the medical device term “ floating ” has a higher score than “ regime change ” and “ imbalance ” for both the medical device and the sports rehab machine terms . the latter term is used in two different contexts , in order to avoid confusion with “ hearing ” or “ simulating ” the robot terms . in the first case , “ hallucination ” is misclassified as stroke whereas “ stent ” misclassified in the second case . in both cases , the error is due to “ false positive ” when the robot is given incorrect medical device or sports rehab term . for instance , in the case of “ rogue ” , the robot was given the incorrect term ‘ remove ’ since “ remove ” was misclassified by the medical robot ."
    },
    {
        "table_id": "455",
        "table_info": {
            "table_caption": "Table 2: In-domain experiments, best values per column are highlighted. For each dataset (column head) we show two scores: Macro-F1 score (left-hand column) and F1 score for claims (right-hand column).",
            "table_column_names": [
                "Target → System ↓ <italic>neural network models</italic>",
                "<bold>MT</bold> <italic>neural network models</italic>",
                "<bold>MT</bold> <italic>neural network models</italic>",
                "<bold>OC</bold> <italic>neural network models</italic>",
                "<bold>OC</bold> <italic>neural network models</italic>",
                "<bold>PE</bold> <italic>neural network models</italic>",
                "<bold>PE</bold> <italic>neural network models</italic>",
                "<bold>VG</bold> <italic>neural network models</italic>",
                "<bold>VG</bold> <italic>neural network models</italic>",
                "<bold>WD</bold> <italic>neural network models</italic>",
                "<bold>WD</bold> <italic>neural network models</italic>",
                "<bold>WTP</bold> <italic>neural network models</italic>",
                "<bold>WTP</bold> <italic>neural network models</italic>",
                "<bold>Average</bold> <italic>neural network models</italic>",
                "<bold>Average</bold> <italic>neural network models</italic>"
            ],
            "table_content_values": [
                [
                    "BiLSTM",
                    "68.8",
                    "41.8",
                    "58.0",
                    "22.4",
                    "73.0",
                    "62.0",
                    "60.9",
                    "37.7",
                    "60.0",
                    "24.5",
                    "57.9",
                    "28.5",
                    "63.1",
                    "36.1"
                ],
                [
                    "CNN:rand",
                    "78.6",
                    "67.3",
                    "<bold>60.5</bold>",
                    "<bold>25.6</bold>",
                    "<bold>73.6</bold>",
                    "61.1",
                    "<bold>65.9</bold>",
                    "<bold>45.0</bold>",
                    "61.1",
                    "25.8",
                    "58.6",
                    "28.9",
                    "66.4",
                    "42.3"
                ],
                [
                    "CNN:w2vec",
                    "73.7",
                    "60.9",
                    "58.2",
                    "23.7",
                    "74.0",
                    "61.7",
                    "63.8",
                    "33.5",
                    "62.6",
                    "<bold>28.9</bold>",
                    "57.3",
                    "24.3",
                    "64.9",
                    "38.8"
                ],
                [
                    "LSTM",
                    "65.2",
                    "48.3",
                    "58.5",
                    "22.3",
                    "71.8",
                    "60.7",
                    "61.3",
                    "40.1",
                    "61.6",
                    "25.9",
                    "58.0",
                    "28.4",
                    "62.7",
                    "37.6"
                ],
                [
                    "LR",
                    "<italic>feature ablation and combination</italic>",
                    "<italic>feature ablation and combination</italic>",
                    "<italic>feature ablation and combination</italic>",
                    "<italic>feature ablation and combination</italic>",
                    "<italic>feature ablation and combination</italic>",
                    "<italic>feature ablation and combination</italic>",
                    "<italic>feature ablation and combination</italic>",
                    "<italic>feature ablation and combination</italic>",
                    "<italic>feature ablation and combination</italic>",
                    "<italic>feature ablation and combination</italic>",
                    "<italic>feature ablation and combination</italic>",
                    "<italic>feature ablation and combination</italic>",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "-Discourse",
                    "73.0",
                    "60.8",
                    "59.9",
                    "22.9",
                    "70.6",
                    "60.6",
                    "62.5",
                    "42.6",
                    "63.7",
                    "23.2",
                    "59.7",
                    "30.2",
                    "64.9",
                    "40.0"
                ],
                [
                    "-Embeddings",
                    "74.6",
                    "62.9",
                    "59.6",
                    "22.6",
                    "70.4",
                    "60.4",
                    "62.9",
                    "43.1",
                    "63.9",
                    "23.5",
                    "59.4",
                    "29.9",
                    "65.1",
                    "40.4"
                ],
                [
                    "-Lexical",
                    "72.1",
                    "59.5",
                    "59.6",
                    "22.5",
                    "65.9",
                    "55.1",
                    "60.8",
                    "40.5",
                    "60.1",
                    "18.5",
                    "57.7",
                    "27.8",
                    "62.7",
                    "37.3"
                ],
                [
                    "-Structure",
                    "74.4",
                    "62.6",
                    "60.0",
                    "23.0",
                    "70.4",
                    "60.4",
                    "62.0",
                    "41.8",
                    "64.2",
                    "23.4",
                    "59.5",
                    "30.0",
                    "65.1",
                    "40.2"
                ],
                [
                    "-Syntax",
                    "<bold>79.8</bold>",
                    "<bold>70.3</bold>",
                    "59.8",
                    "22.9",
                    "72.1",
                    "<bold>62.5</bold>",
                    "63.4",
                    "43.8",
                    "<bold>65.1</bold>",
                    "25.5",
                    "<bold>60.1</bold>",
                    "<bold>30.5</bold>",
                    "<bold>66.7</bold>",
                    "<bold>42.6</bold>"
                ],
                [
                    "All Features",
                    "74.4",
                    "62.7",
                    "59.9",
                    "22.9",
                    "70.6",
                    "60.6",
                    "62.5",
                    "42.6",
                    "63.8",
                    "23.3",
                    "59.7",
                    "30.2",
                    "65.1",
                    "40.4"
                ],
                [
                    "LR",
                    "<italic>single feature groups</italic>",
                    "<italic>single feature groups</italic>",
                    "<italic>single feature groups</italic>",
                    "<italic>single feature groups</italic>",
                    "<italic>single feature groups</italic>",
                    "<italic>single feature groups</italic>",
                    "<italic>single feature groups</italic>",
                    "<italic>single feature groups</italic>",
                    "<italic>single feature groups</italic>",
                    "<italic>single feature groups</italic>",
                    "<italic>single feature groups</italic>",
                    "<italic>single feature groups</italic>",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "+Discourse",
                    "70.0",
                    "56.7",
                    "49.4",
                    "13.8",
                    "50.1",
                    "41.7",
                    "49.6",
                    "30.6",
                    "57.6",
                    "14.9",
                    "49.5",
                    "18.4",
                    "54.4",
                    "29.3"
                ],
                [
                    "+Embeddings",
                    "72.4",
                    "59.8",
                    "58.8",
                    "20.8",
                    "68.2",
                    "57.7",
                    "59.7",
                    "39.3",
                    "64.2",
                    "23.8",
                    "59.0",
                    "28.9",
                    "63.7",
                    "38.4"
                ],
                [
                    "+Lexical",
                    "75.9",
                    "64.7",
                    "59.5",
                    "21.4",
                    "71.8",
                    "62.1",
                    "61.1",
                    "40.5",
                    "64.0",
                    "22.2",
                    "59.0",
                    "27.7",
                    "65.2",
                    "39.8"
                ],
                [
                    "+Structure",
                    "57.1",
                    "42.0",
                    "56.5",
                    "20.0",
                    "54.2",
                    "39.5",
                    "55.4",
                    "33.3",
                    "48.4",
                    "9.0",
                    "55.4",
                    "25.2",
                    "54.5",
                    "28.2"
                ],
                [
                    "+Syntax",
                    "66.7",
                    "52.5",
                    "58.1",
                    "21.0",
                    "64.1",
                    "52.9",
                    "60.7",
                    "40.4",
                    "57.6",
                    "15.5",
                    "57.0",
                    "27.0",
                    "60.7",
                    "34.9"
                ],
                [
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>"
                ],
                [
                    "Majority bsl",
                    "42.9",
                    "0.0",
                    "48.0",
                    "0.0",
                    "41.3",
                    "0.0",
                    "44.5",
                    "0.0",
                    "48.6",
                    "0.0",
                    "46.7",
                    "0.0",
                    "45.3",
                    "0.0"
                ],
                [
                    "Random bsl",
                    "50.7",
                    "33.2",
                    "49.9",
                    "13.5",
                    "50.8",
                    "38.0",
                    "50.4",
                    "28.8",
                    "51.6",
                    "10.8",
                    "48.9",
                    "18.8",
                    "50.4",
                    "23.9"
                ]
            ]
        },
        "gold_description": "the average performances of lr − syntax and cnn : rand are virtually identical , both for macro6described as fscorem in sokolova and lapalme ( 2009 ) . f1 and claim - f1 , with a slight advantage for the feature - based approach , but their difference is not statistically significant ( p ≤ 0 . 05 ) . altogether , these two systems exhibit significantly better average performances than all other models surveyed here , both those relying on and those not relying on hand - crafted features ( p ≤ 0 . 05 ) . the performance of the learners is quite divergent across datasets , with macro - f1 scores6 ranging from 60 % ( wtp ) to 80 % ( mt ) , average 67 % ( see table 2 ) . on all datasets , our best systems clearly outperform both baselines . in isolation , lexical , embedding , and syntax features are most helpful , whereas structural features did not help in most cases . discourse features only contribute significantly on mt . when looking at the performance of the feature - based approaches , the most striking finding is the importance of lexical ( in our setup , unigram ) information .",
        "generated_description": "we observe that mmnasnet outperforms all the other models on all the three target domains and the average performance across all the domains is over 10 % higher than that of the best performing model on the wtp and wd test sets ."
    },
    {
        "table_id": "456",
        "table_info": {
            "table_caption": "Table 3: Cross-domain experiments, best values per column are highlighted, in-domain results (for comparison) in italics; results only for selected systems. For each source/target combination we show two scores: Macro-F1 score (left-hand column) and F1 score for claims (right-hand column).",
            "table_column_names": [
                "Target → Source/Sys. ↓",
                "<bold>MT</bold> <italic>CNN:rand</italic>",
                "<bold>MT</bold> <italic>CNN:rand</italic>",
                "<bold>OC</bold> <italic>CNN:rand</italic>",
                "<bold>OC</bold> <italic>CNN:rand</italic>",
                "<bold>PE</bold> <italic>CNN:rand</italic>",
                "<bold>PE</bold> <italic>CNN:rand</italic>",
                "<bold>VG</bold> <italic>CNN:rand</italic>",
                "<bold>VG</bold> <italic>CNN:rand</italic>",
                "<bold>WD</bold> <italic>CNN:rand</italic>",
                "<bold>WD</bold> <italic>CNN:rand</italic>",
                "<bold>WTP</bold> <italic>CNN:rand</italic>",
                "<bold>WTP</bold> <italic>CNN:rand</italic>",
                "<bold>Average</bold>",
                "<bold>Average</bold>"
            ],
            "table_content_values": [
                [
                    "MT",
                    "<italic>78.6</italic>",
                    "<italic>67.3</italic>",
                    "51.0",
                    "7.4",
                    "56.9",
                    "22.1",
                    "57.2",
                    "15.7",
                    "52.4",
                    "9.4",
                    "49.4",
                    "10.9",
                    "53.4",
                    "13.1"
                ],
                [
                    "OC",
                    "57.1",
                    "39.7",
                    "<italic>60.5</italic>",
                    "<italic>25.6</italic>",
                    "56.4",
                    "42.8",
                    "58.9",
                    "37.3",
                    "54.6",
                    "13.2",
                    "<bold>58.4</bold>",
                    "<bold>28.9</bold>",
                    "57.1",
                    "32.4"
                ],
                [
                    "PE",
                    "59.8",
                    "18.0",
                    "54.2",
                    "9.5",
                    "<italic>73.6</italic>",
                    "<italic>61.1</italic>",
                    "57.5",
                    "18.7",
                    "<bold>55.5</bold>",
                    "<bold>15.9</bold>",
                    "54.7",
                    "16.0",
                    "56.3",
                    "15.6"
                ],
                [
                    "VG",
                    "<bold>68.7</bold>",
                    "<bold>51.5</bold>",
                    "55.8",
                    "19.2",
                    "<bold>57.0</bold>",
                    "32.0",
                    "<italic>65.9</italic>",
                    "<italic>45.0</italic>",
                    "51.7",
                    "10.5",
                    "54.7",
                    "22.0",
                    "57.6",
                    "27.0"
                ],
                [
                    "WD",
                    "64.4",
                    "3.5",
                    "51.3",
                    "1.3",
                    "41.3",
                    "0.0",
                    "44.5",
                    "0.0",
                    "<italic>61.1</italic>",
                    "<italic>25.8</italic>",
                    "46.7",
                    "0.0",
                    "49.6",
                    "1.0"
                ],
                [
                    "WTP",
                    "58.5",
                    "26.6",
                    "56.8",
                    "15.4",
                    "56.0",
                    "18.5",
                    "55.3",
                    "19.4",
                    "52.9",
                    "11.6",
                    "<italic>58.6</italic>",
                    "<italic>28.9</italic>",
                    "55.9",
                    "18.3"
                ],
                [
                    "<italic>Average</italic>",
                    "61.7",
                    "27.9",
                    "53.8",
                    "10.6",
                    "53.5",
                    "23.1",
                    "54.7",
                    "18.2",
                    "53.4",
                    "12.1",
                    "52.8",
                    "15.6",
                    "55.0",
                    "17.9"
                ],
                [
                    "[EMPTY]",
                    "<italic>LR All features</italic>",
                    "<italic>LR All features</italic>",
                    "<italic>LR All features</italic>",
                    "<italic>LR All features</italic>",
                    "<italic>LR All features</italic>",
                    "<italic>LR All features</italic>",
                    "<italic>LR All features</italic>",
                    "<italic>LR All features</italic>",
                    "<italic>LR All features</italic>",
                    "<italic>LR All features</italic>",
                    "<italic>LR All features</italic>",
                    "<italic>LR All features</italic>",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "MT",
                    "<italic>74.4</italic>",
                    "<italic>62.7</italic>",
                    "53.9",
                    "17.0",
                    "51.9",
                    "29.5",
                    "56.1",
                    "34.2",
                    "55.1",
                    "14.5",
                    "52.5",
                    "21.2",
                    "53.9",
                    "23.3"
                ],
                [
                    "OC",
                    "60.0",
                    "45.1",
                    "<italic>59.9</italic>",
                    "<italic>22.9</italic>",
                    "56.7",
                    "<bold>47.0</bold>",
                    "58.6",
                    "<bold>38.0</bold>",
                    "54.1",
                    "12.2",
                    "57.7",
                    "27.5",
                    "57.4",
                    "<bold>34.0</bold>"
                ],
                [
                    "PE",
                    "58.1",
                    "36.3",
                    "54.6",
                    "17.3",
                    "<italic>70.6</italic>",
                    "<italic>60.6</italic>",
                    "54.1",
                    "21.4",
                    "54.0",
                    "13.5",
                    "54.4",
                    "20.4",
                    "55.0",
                    "21.8"
                ],
                [
                    "VG",
                    "65.8",
                    "51.4",
                    "<bold>57.3</bold>",
                    "<bold>21.7</bold>",
                    "<bold>57.0</bold>",
                    "45.1",
                    "<italic>62.5</italic>",
                    "<italic>42.6</italic>",
                    "54.5",
                    "13.1",
                    "55.1",
                    "24.8",
                    "<bold>57.9</bold>",
                    "31.2"
                ],
                [
                    "WD",
                    "62.6",
                    "38.5",
                    "55.4",
                    "19.0",
                    "56.0",
                    "30.1",
                    "55.1",
                    "23.3",
                    "<italic>63.8</italic>",
                    "<italic>23.3</italic>",
                    "53.6",
                    "20.9",
                    "56.5",
                    "26.3"
                ],
                [
                    "WTP",
                    "58.0",
                    "41.7",
                    "56.1",
                    "20.3",
                    "56.8",
                    "42.6",
                    "<bold>59.1</bold>",
                    "<bold>38.0</bold>",
                    "52.2",
                    "11.2",
                    "<italic>59.7</italic>",
                    "<italic>30.2</italic>",
                    "56.5",
                    "30.8"
                ],
                [
                    "<italic>Average</italic>",
                    "60.9",
                    "42.6",
                    "55.5",
                    "19.1",
                    "55.7",
                    "38.9",
                    "56.6",
                    "31.0",
                    "54.0",
                    "12.9",
                    "54.7",
                    "23.0",
                    "56.2",
                    "27.9"
                ],
                [
                    "LR",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "<italic>single feature groups (averages across all source domains)</italic>",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "+Discourse",
                    "40.2",
                    "15.0",
                    "31.7",
                    "5.8",
                    "30.3",
                    "27.4",
                    "27.7",
                    "19.9",
                    "40.9",
                    "4.5",
                    "25.3",
                    "13.3",
                    "32.7",
                    "14.3"
                ],
                [
                    "+Embeddings",
                    "56.6",
                    "35.2",
                    "51.4",
                    "12.8",
                    "53.6",
                    "30.7",
                    "53.3",
                    "24.3",
                    "54.2",
                    "13.2",
                    "52.9",
                    "19.0",
                    "53.7",
                    "22.5"
                ],
                [
                    "+Lexical",
                    "61.0",
                    "42.2",
                    "55.2",
                    "18.3",
                    "56.2",
                    "38.6",
                    "54.7",
                    "29.1",
                    "53.1",
                    "11.9",
                    "54.9",
                    "23.4",
                    "55.9",
                    "27.2"
                ],
                [
                    "+Structure",
                    "44.2",
                    "22.9",
                    "53.6",
                    "18.5",
                    "52.5",
                    "38.4",
                    "53.6",
                    "32.1",
                    "49.1",
                    "9.0",
                    "53.4",
                    "23.3",
                    "51.1",
                    "24.0"
                ],
                [
                    "+Syntax",
                    "54.8",
                    "37.0",
                    "54.2",
                    "17.5",
                    "54.3",
                    "40.6",
                    "55.7",
                    "32.0",
                    "53.0",
                    "11.8",
                    "53.8",
                    "22.5",
                    "54.3",
                    "26.9"
                ],
                [
                    "[EMPTY]",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "<italic>baselines</italic>",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Majority bsl",
                    "42.9",
                    "0.0",
                    "48.0",
                    "0.0",
                    "41.3",
                    "0.0",
                    "44.5",
                    "0.0",
                    "48.6",
                    "0.0",
                    "46.7",
                    "0.0",
                    "45.3",
                    "0.0"
                ],
                [
                    "Random bsl",
                    "47.5",
                    "30.6",
                    "50.5",
                    "14.0",
                    "51.0",
                    "38.4",
                    "51.0",
                    "29.3",
                    "49.3",
                    "9.3",
                    "50.3",
                    "20.2",
                    "49.9",
                    "23.6"
                ]
            ]
        },
        "gold_description": "table 3 lists the results of the best feature - based ( lr all features ) and deep learning ( cnn : rand ) systems , as well as single feature groups ( averages over all source domains , for the lowest scoring datasets , oc and wtp , the differences are only marginal when trained on a suitable dataset the best feature - based approach outperforms the best deep learning approach in most scenarios . in particular , as opposed to the in - domain experiments , the difference of the claim - f1 measure between the feature - based approaches and the deep learning approaches is striking . in the feature - based approaches , on average , a combination of all features yields the best results for both macro - f1 and claim - f1 . when comparing single features , lexical ones do the best job .",
        "generated_description": "table 1 summarizes the results of the experiments conducted to compare our approach with the baseline methods . table 1 compares the accuracy of the two approaches on different target languages . we observe that our approach outperforms all the compared approaches on all the target languages , especially on resource - rich and resource - poor languages such as opensubtitles ( oc , pe , and wtp ) ."
    },
    {
        "table_id": "457",
        "table_info": {
            "table_caption": "Table 4: Leave-one-domain-out experiments, best values per column are highlighted. For each test dataset (column head) we show two scores: Macro-F1 score (left-hand column) and F1 score for claims (right-hand column).",
            "table_column_names": [
                "Target → System ↓ CNN:rand",
                "<bold>MT</bold> 62.8",
                "<bold>MT</bold> 41.4",
                "<bold>OC</bold> <bold>57.8</bold>",
                "<bold>OC</bold> <bold>22.4</bold>",
                "<bold>PE</bold> <bold>59.7</bold>",
                "<bold>PE</bold> 36.2",
                "<bold>VG</bold> <bold>58.6</bold>",
                "<bold>VG</bold> 28.1",
                "<bold>WD</bold> <bold>54.2</bold>",
                "<bold>WD</bold> <bold>14.1</bold>",
                "<bold>WTP</bold> <bold>56.8</bold>",
                "<bold>WTP</bold> 25.6",
                "<bold>Avg</bold> <bold>58.3</bold>",
                "<bold>Avg</bold> 28.0"
            ],
            "table_content_values": [
                [
                    "All features",
                    "<bold>64.7</bold>",
                    "<bold>49.5</bold>",
                    "56.4",
                    "20.6",
                    "57.8",
                    "<bold>45.8</bold>",
                    "58.2",
                    "<bold>36.4</bold>",
                    "52.3",
                    "11.3",
                    "56.0",
                    "<bold>26.0</bold>",
                    "57.6",
                    "<bold>31.6</bold>"
                ],
                [
                    "Majority bsl",
                    "42.9",
                    "0.0",
                    "48.0",
                    "0.0",
                    "41.3",
                    "0.0",
                    "44.5",
                    "0.0",
                    "48.6",
                    "0.0",
                    "46.7",
                    "0.0",
                    "45.3",
                    "0.0"
                ],
                [
                    "Random bsl",
                    "47.5",
                    "30.6",
                    "50.5",
                    "14.0",
                    "51.0",
                    "38.4",
                    "51.0",
                    "29.3",
                    "49.3",
                    "9.3",
                    "50.3",
                    "20.2",
                    "49.9",
                    "23.6"
                ]
            ]
        },
        "gold_description": "we also performed experiments with mixed sources , the results are shown in table 4 . in this scenario , the neural network systems seem to benefit from the increased amount of training data and thus gave the best results .",
        "generated_description": "table 3 presents the results for the mt , oc , pe , vg , and wd systems . the table shows that the overall accuracy of the mt system is 62 . 8 % , 41 . 4 % , 28 . 8 % and 28 . 0 % , respectively . table 3 shows the performance of the other systems when trained and tested on the same cnn : rand as the training set ."
    },
    {
        "table_id": "458",
        "table_info": {
            "table_caption": "Table 2: The accuracy(%) of the ML models for NLU.",
            "table_column_names": [
                "<bold>Model</bold>",
                "<bold>Intent Detection</bold>",
                "<bold>Slot Filling</bold>"
            ],
            "table_content_values": [
                [
                    "Random baseline",
                    "02.67",
                    "07.32"
                ],
                [
                    "Majority baseline",
                    "06.34",
                    "64.96"
                ],
                [
                    "HMM",
                    "-",
                    "87.20"
                ],
                [
                    "SVM",
                    "94.98",
                    "-"
                ],
                [
                    "GolVe-based",
                    "92.22",
                    "98.45"
                ]
            ]
        },
        "gold_description": "first , we represent utterances by their tf - idf representations as feature - vectors . we then supply the vector representation of each utterance to a support vector machine ( svm ) with a linear kernel for domain and intent identification , and to a hidden markov model ( hmm ) for slot filling . second , we encode words in a dialogue utterance by glove , as benchmark pre - trained word embeddings , to include the semantic relationships among words . we compute the average of word embeddings in an utterance to represent the utterance by a vector . table 2 shows the performance of the described models .",
        "generated_description": "we can see that the golve - based model achieves the best performance for intent detection and slot filling . for intent detection , the accuracy is 92 . 22 % and 98 . 45 % , which are significantly better than the random baseline and majority baseline , respectively . for slot filling , our model outperforms the majority baseline and the svm by a large margin , which demonstrates the effectiveness of our method ."
    },
    {
        "table_id": "459",
        "table_info": {
            "table_caption": "Table 4: The output of the general survey.",
            "table_column_names": [
                "[EMPTY]",
                "1 (strongly agree)",
                "2 (agree)",
                "3 (disagree)",
                "4 (strongly disagree)"
            ],
            "table_content_values": [
                [
                    "was able to “understand” my questions",
                    "16.7%",
                    "<bold>50.0%</bold>",
                    "33.3%",
                    "00.0%"
                ],
                [
                    "was able to provide answers to my questions",
                    "00.0%",
                    "<bold>50.0%</bold>",
                    "<bold>50.0%</bold>",
                    "00.0%"
                ],
                [
                    "I was satisfied with the informativeness of the answers provided by",
                    "<bold>33.3%</bold>",
                    "<bold>33.3%</bold>",
                    "00.0%",
                    "<bold>33.3%</bold>"
                ],
                [
                    "I was satisfied with the fluency of the answers provided by",
                    "16.7%",
                    "<bold>50.0%</bold>",
                    "16.7%",
                    "16.7%"
                ],
                [
                    "could respond in a reasonable time",
                    "<bold>50.0%</bold>",
                    "33.3%",
                    "16.7%",
                    "00.0%"
                ],
                [
                    "The GUI of was suitable for reading the provided answers",
                    "<bold>50.0%</bold>",
                    "00.0%",
                    "33.3%",
                    "16.7%"
                ],
                [
                    "reduces my need to google a specific information",
                    "16.7%",
                    "<bold>66.7%</bold>",
                    "00.0%",
                    "16.7%"
                ],
                [
                    "would help me save some time in my work",
                    "<bold>33.3%</bold>",
                    "<bold>33.3%</bold>",
                    "<bold>33.3%</bold>",
                    "00.0%"
                ],
                [
                    "I would like to use in the future on a daily basis",
                    "00.0%",
                    "<bold>66.7%</bold>",
                    "00.0%",
                    "33.3%"
                ],
                [
                    "I will use to plan for my next conference",
                    "16.7%",
                    "16.7%",
                    "<bold>33.3%</bold>",
                    "<bold>33.3%</bold>"
                ]
            ]
        },
        "gold_description": "the results of the survey shows a general sat , confirmisfaction feeling of interactions with ing our motivation that the existence of such an agent helps researchers ( see table 4 ) . 83 % of participants agree that reduces their needs to search through the web ( e . g . using search engines ) to obtain information related to their research ; and in the future . however , 66 % of hu66 % use man judges disagreed on using for planning their schedule for a conference . this observation could be because the current version of mainly retrieves information for users but planning for a conference needs some inferences on such information as well .",
        "generated_description": "fluency and informativeness of the answers provided by was found to be among the most important factors for the users to agree or disagree with the given answers . the percentage of strongly agree with the provided answers was in the range of 0 . 3 % to 1 . 0 % , indicating that the answers were of reasonable quality . among the users who were able to “ understand ” my questions , the percentage was able to provide answers to my questions in a reasonable time , which indicates that the answerers were familiar with the questions and could provide satisfactory answers . fluency was considered to be one of the factors for satisfaction in giving answers . a large percentage of the users indicated that they were satisfied with the answers given by the given answerers . for example , a large number of users ( 55 % ) selected the option of “ the time that was the fastest to respond ” as their main reason for answering the given questions . in addition , a small number ( 4 % ) selected a method that reduces the need to google a specific information about the given information , indicating that this method was a helpful means of obtaining a quick and accurate answer . finally , the preference of choosing the method that was suitable for reading the provided answer was also indicated in the results . in summary , the three factors that were considered helpful in providing answers to our questions were the ability to \" understand \" the questions , being able to \" agravely understand \" the corresponding questions , and having a concise method to provide a correct answer ."
    },
    {
        "table_id": "460",
        "table_info": {
            "table_caption": "Table 2: Accuracies of the different models on the cQA datasets and MAP/MRR on WikiPassageQA. Σ denotes the average accuracy over all cQA datasets.",
            "table_column_names": [
                "<bold>Model</bold>",
                "∑",
                "<bold>InsuranceQA</bold>",
                "<bold>Travel</bold>",
                "<bold>Cooking</bold>",
                "<bold>Academia</bold>",
                "<bold>Apple</bold>",
                "<bold>Aviation</bold>",
                "<bold>WikiPassageQA</bold>"
            ],
            "table_content_values": [
                [
                    "<bold>Unsupervised IR Baselines</bold>",
                    "<bold>Unsupervised IR Baselines</bold>",
                    "<bold>Unsupervised IR Baselines</bold>",
                    "<bold>Unsupervised IR Baselines</bold>",
                    "<bold>Unsupervised IR Baselines</bold>",
                    "<bold>Unsupervised IR Baselines</bold>",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "BM25",
                    "30.3",
                    "24.9",
                    "38.1",
                    "30.9",
                    "29.2",
                    "21.8",
                    "37.0",
                    "53.00 / 61.71"
                ],
                [
                    "TF*IDF",
                    "32.4",
                    "18.7",
                    "39.9",
                    "35.1",
                    "32.2",
                    "26.7",
                    "41.9",
                    "39.92 / 46.38"
                ],
                [
                    "<bold>Semantic Similarity Methods</bold>",
                    "<bold>Semantic Similarity Methods</bold>",
                    "<bold>Semantic Similarity Methods</bold>",
                    "<bold>Semantic Similarity Methods</bold>",
                    "<bold>Semantic Similarity Methods</bold>",
                    "<bold>Semantic Similarity Methods</bold>",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "InferSent",
                    "23.0",
                    "14.8",
                    "27.0",
                    "21.3",
                    "22.5",
                    "22.8",
                    "29.3",
                    "43.62 / 50.53"
                ],
                [
                    "p-mean Embeddings",
                    "25.7",
                    "17.0",
                    "32.1",
                    "29.3",
                    "24.3",
                    "19.6",
                    "31.7",
                    "42.82 / 50.44"
                ],
                [
                    "CNN",
                    "25.9",
                    "24.4",
                    "36.9",
                    "25.9",
                    "22.5",
                    "20.2",
                    "25.3",
                    "27.33 / 31.48"
                ],
                [
                    "BiLSTM",
                    "34.8",
                    "32.4",
                    "45.3",
                    "35.2",
                    "31.5",
                    "27.2",
                    "37.3",
                    "46.16 / 52.89"
                ],
                [
                    "Att.-BiLSTM",
                    "34.5",
                    "37.9",
                    "43.0",
                    "36.2",
                    "31.2",
                    "24.7",
                    "33.9",
                    "47.04 / 54.36"
                ],
                [
                    "AP-BiLSTM",
                    "31.3",
                    "31.9",
                    "38.8",
                    "32.2",
                    "27.3",
                    "22.9",
                    "34.5",
                    "46.98 / 55.20"
                ],
                [
                    "LW-BiLSTM",
                    "34.1",
                    "36.9",
                    "43.2",
                    "32.3",
                    "30.2",
                    "23.4",
                    "38.5",
                    "47.56 / 54.33"
                ],
                [
                    "<bold>Relevance Matching Methods</bold>",
                    "<bold>Relevance Matching Methods</bold>",
                    "<bold>Relevance Matching Methods</bold>",
                    "<bold>Relevance Matching Methods</bold>",
                    "<bold>Relevance Matching Methods</bold>",
                    "<bold>Relevance Matching Methods</bold>",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Bigrams",
                    "18.3",
                    "19.4",
                    "19.3",
                    "16.7",
                    "19.8",
                    "13.0",
                    "21.5",
                    "39.84 / 47.55"
                ],
                [
                    "CA-Wang",
                    "39.1",
                    "37.0",
                    "46.5",
                    "39.4",
                    "36.1",
                    "29.2",
                    "46.5",
                    "48.71 / 56.11"
                ],
                [
                    "COALA",
                    "43.6",
                    "38.0",
                    "53.8",
                    "47.3",
                    "42.2",
                    "32.0",
                    "48.4",
                    "<bold>60.58</bold> / <bold>69.40</bold>"
                ],
                [
                    "COALA p-means",
                    "<bold>45.2</bold>",
                    "<bold>39.9</bold>",
                    "53.4",
                    "46.5",
                    "<bold>44.2</bold>",
                    "<bold>34.5</bold>",
                    "<bold>52.9</bold>",
                    "59.29 / 68.48"
                ],
                [
                    "COALA syntax-aware",
                    "44.3",
                    "39.5",
                    "<bold>54.1</bold>",
                    "<bold>47.8</bold>",
                    "43.5",
                    "32.7",
                    "48.3",
                    "60.48 / 68.75"
                ]
            ]
        },
        "gold_description": "we compare all approaches across the insuranceqa and wikipassageqa benchmarks as well as the five stackexchange datasets in table 2 . for the cqa answer selection datasets we measure the accuracy , which is the ratio of correctly selected answers , and for the passage retrieval in wikipassageqa we report map / mrr . the results show that coala substantially outperforms all other relevance matching and semantic similarity approaches on all seven datasets . for instance , on the cqa datasets coala improves by 4 . 5pp over ca - wang and by 8 . 8pp over the best semantic similarity method on average . our extended approach coala p - means improves the performance of coala on these datasets by an additional 1 . 6pp . the proposed power mean aggregation achieves a strong improvement on four datasets and results in a small performance decrease in the remaining three cases . the results in table 2 show that our proposed syntax - aware extension coala syntax - aware , which incorporates syntactic roles of word sequences to learn syntax - aware aspect representations , improves the results in five out of seven cases . it thereby achieves an an average improvement of 0 . 7pp over coala in our cqa datasets .",
        "generated_description": "table 3 summarizes the performance of our semi - supervised and unsupervised ir baselines and the key observations from the ablation studies . acl - qa and eraser perform significantly better than both bm25 and tf * idf , which confirms the effectiveness of the document - level and document - query - document - term - document ( document - agnostic ) retrieval approaches . we observe that acl - ija outperforms both tf - idf and the original bm25 across all the four domains . with the help of pretrained word embeddings , our full model ( lexrank + bert ) significantly outperforms all the baselines across all four domains and outperforms the prior state - of - the - art method ( bert + att ) by a large margin ( > 6 % ) ."
    },
    {
        "table_id": "461",
        "table_info": {
            "table_caption": "Table 3: Thematic ranker evaluation, incl. random ranker (RND) and upper bound (UB); bold - best result over syntactic rankers, underlined - best result over thematic rankers",
            "table_column_names": [
                "[EMPTY]",
                "synt",
                "glob",
                "pair",
                "loc",
                "RND",
                "UB"
            ],
            "table_content_values": [
                [
                    "EN",
                    "SE1",
                    ".869",
                    ".887",
                    ".867",
                    ".509",
                    ".927"
                ],
                [
                    "EN",
                    "SE2",
                    "<bold>.930</bold>",
                    "<bold>.929</bold>",
                    "<bold>.913</bold>",
                    ".500",
                    "<bold>.932</bold>"
                ],
                [
                    "DE",
                    "SD1",
                    ".655",
                    ".726",
                    ".637",
                    ".471",
                    ".818"
                ],
                [
                    "DE",
                    "SD2",
                    "<bold>.790</bold>",
                    "<bold>.820</bold>",
                    "<bold>.820</bold>",
                    ".456",
                    "<bold>.920</bold>"
                ]
            ]
        },
        "gold_description": "we first compare the performance of thematic rankers with respect to syntactic rankers and available datasets . the results of this comparison are summarized in table 3 and show that syntactic rankers positioning the object second in the hierarchy ( se2 and sd2 ) lead to better alignment on both datasets and have a higher upper bound . for english the global hierarchy - based ranker approaches the upper bound , closely followed by the pairwise ranker . the accuracy on german data is lower and the pairwise and local rankers outperform the global hierarchy - based ranker .",
        "generated_description": "thematic rankers outperform the syntactic rankers by a wide margin in both languages , with the latter outperforming the random ranker and the upper bound . in general , the performance of the thematic ranker is slightly better than the random one , and the difference between the two is statistically significant ( p - value < 0 . 01 ) ."
    },
    {
        "table_id": "462",
        "table_info": {
            "table_caption": "Table 4: Induced hierarchies",
            "table_column_names": [
                "EN",
                "Agent ≺ Cause/Instrument/Experiencer ≺ Pivot ≺ Theme ≺ Patient ≺ Material/Source/Asset ≺ Product ≺ Recipient/Beneficiary/Destination/Location ≺ Value/Stimulus/Topic/Result/Predicate/Goal/InitialLocation/Attribute/Extent"
            ],
            "table_content_values": [
                [
                    "DE",
                    "Agent ≺ Experiencer ≺ Stimulus/Pivot ≺ Cause ≺ Theme ≺ Patient ≺ Topic ≺ Instrument ≺ Beneficiary/InitialLocation ≺ Result ≺ Product/Goal ≺ Destination/Attribute ≺ Recipient ≺ Value/Time/CoAgent/Locus/Manner/Source/Trajectory/Location/Duration/Path/Extent"
                ]
            ]
        },
        "gold_description": "table 4 shows full rankings extracted for english and german data . while some correspondence to the hierarchies proposed for english agent in literature is evident ( e . g . ≺ instrument ≺ theme , similar to ( fillmore , 1968 ) ) , a direct comparison is impossible due to the differences in role definitions and underlying syntactic formalisms . notice the high number of ties : some roles never co - occur ( either by chance or by design ) or occur on the same syntactic rank ( e . g . oblique ) so there is no evidence for preference even if we enforce transitivity .",
        "generated_description": "in order to determine the hierarchies of the newly created hierarchies , we perform a simple svm classifier on the gold edu segmentation treebank and extract the top - k hierarchies by applying the softmax function . the resulting hierarchies are created by summing up the k - many - many features of the original word embeddings . for example , the edu classifier for patient is composed of “ cause ” , “ experiencer ” and “ pivot ” . we can see that the overall distribution of the obtained hierarchies is similar to that in the original svm treebank . for instance , the distribution of patient and pivot is very similar in the en hierarchy , while in the de hierarchy , they are very different in the xl and xl - en hierarchical structure . this is due to the fact that the initial edu treebank is relatively small and it is difficult to extract the relevant features for different languages . therefore , we use the fuzzymax function to extract relevant features from the hyper - parameters of the initial word embedding tree . in the end , we obtain the following hierarchical structures : ( 1 ) patient : the set of words that are shared between the original and the newly induced hierarchies ; ( 2 ) event : the sets of words associated with events ; ( 3 ) message : the words related to the topic of the event ; ( 4 ) pivot : the word used in the initial theme ; ( 5 ) location : the location of the first event ; and ( 6 ) duration : the distance between the new event and the existing event using the newly introduced hierarchies . the hierarchical structures are created using the svm and softmax functions . we perform svm classification for both languages using svm ."
    },
    {
        "table_id": "463",
        "table_info": {
            "table_caption": "Table 5: Cross-lingual evaluation, global ranker",
            "table_column_names": [
                "[EMPTY]",
                "EN-test",
                "DE-test"
            ],
            "table_content_values": [
                [
                    "UB",
                    ".932",
                    ".920"
                ],
                [
                    "EN-train",
                    ".930",
                    ".787"
                ],
                [
                    "DE-train",
                    ".852",
                    ".790"
                ],
                [
                    "RND",
                    ".500",
                    ".456"
                ]
            ]
        },
        "gold_description": "table 5 contrasts the performance of ths induced from english and german training data , and evaluated on german and english test data respectively . while the cross - lingual performance is expectedly lower than the monolingual performance , it outperforms the random baseline by a large margin , suggesting the potential for crosslingual hierarchy induction .",
        "generated_description": "for the cross - lingual evaluation , we use the ub and de - train corpora as the training and test sets and the global ranker as the validation set . the ranker is trained on the training corpora and evaluated on the held - out test corpora . as expected , the performance of the global ranking is lower than that of the individual ranks . however , the difference in performance between the two evaluation sets is small ."
    },
    {
        "table_id": "464",
        "table_info": {
            "table_caption": "Table 3: Test Scores Document Ranking",
            "table_column_names": [
                "<bold>Test</bold>",
                "<bold>Metric</bold>",
                "<bold>tf-idf</bold>",
                "<bold>bm25</bold>",
                "<bold>emb</bold>",
                "<bold>feat</bold>",
                "<bold>auto/ cosine</bold>",
                "<bold>auto- rank</bold>",
                "<bold>auto- rank + bm25</bold>",
                "<bold>auto- rank + feat</bold>"
            ],
            "table_content_values": [
                [
                    "EGFR",
                    "MAP",
                    "0.289",
                    "0.632",
                    "0.310",
                    "0.575",
                    "0.054",
                    "0.545",
                    "0.588",
                    "<bold>0.699</bold>"
                ],
                [
                    "EGFR",
                    "nDCG",
                    "0.424",
                    "0.728",
                    "0.460",
                    "0.695",
                    "0.129",
                    "0.653",
                    "0.716",
                    "<bold>0.810</bold>"
                ],
                [
                    "KRAS",
                    "MAP",
                    "0.327",
                    "0.610",
                    "0.466",
                    "0.609",
                    "0.058",
                    "0.575",
                    "0.774",
                    "<bold>0.820</bold>"
                ],
                [
                    "KRAS",
                    "nDCG",
                    "0.456",
                    "0.723",
                    "0.592",
                    "0.712",
                    "0.145",
                    "0.688",
                    "0.867",
                    "<bold>0.914</bold>"
                ],
                [
                    "BRAF",
                    "MAP",
                    "0.342",
                    "0.656",
                    "0.427",
                    "0.704",
                    "0.063",
                    "0.563",
                    "0.702",
                    "<bold>0.812</bold>"
                ],
                [
                    "BRAF",
                    "nDCG",
                    "0.480",
                    "0.751",
                    "0.572",
                    "0.802",
                    "0.163.",
                    "0.671",
                    "0.820",
                    "<bold>0.901</bold>"
                ],
                [
                    "PIK3CA",
                    "MAP",
                    "0.341",
                    "0.633",
                    "0.486",
                    "0.625",
                    "0.079",
                    "0.541",
                    "0.779",
                    "<bold>0.810</bold>"
                ],
                [
                    "PIK3CA",
                    "nDCG",
                    "0.473",
                    "0.729",
                    "0.617",
                    "0.718",
                    "0.171",
                    "0.656",
                    "0.859",
                    "<bold>0.895</bold>"
                ]
            ]
        },
        "gold_description": "in table 3 we have listed the average map and ndcg scores of the test sets . the tf - idf model is outperformed by most of the other models . however , bm25 , which additionally takes the length of a document into account , performs very tf - idf and bm25 have the major benefit of well . fast computation . the feat model slightly outperforms the auto auto - rank + feat model is slightly better than the auto - rank + bm25 model , both of which have the overall best performance . this shows , that the auto - encoder learns something orthogonal to term frequency and document length . the best model with respect to document ranking is the auto - rank + feat model . in figure 3 we show the correlation between the different models . interestingly , the bm25 and the feat strongly correlate . however , the scores of bm25 do not correlate with the scores of the combination of auto - rank and bm25 . this indicates , that the model does not primarily learn to use the bm25 score but also focuses on the the auto - encoded representation . this underlines the hypothesis that the auto - encoder is able to represent latent features of the relationship of the query terms in the document . rank model . the distance features are a strong indicator for the semantic dependency between entities . these relationships need to be learned in the auto - rank model . the cosine similarity of a query and a document ( auto / cos ) does not yield a good result . this shows that the auto - encoder has learned many features , most of which do not correlate with our task . we also find that emb does not yield an equal performance to auto - rank . the combination of the",
        "generated_description": "for the egfr and kras metrics , we report the map and ndcg as well as the tf - idf and bm25 scores . the best overall performance is achieved by the “ auto - rank ” method , which outperforms all other methods in terms of all metrics on both datasets . this is due to the fact that there is a large number of top - ranked results when using the ranked results generated by this method to select the top response candidates . in addition , the overall performance of this method is boosted by the addition of the feature - based ranking and the machine - learning - based backpropagation ( emb ) , which leads to the best overall results ."
    },
    {
        "table_id": "465",
        "table_info": {
            "table_caption": "Table 1: Individual macro-F1 scores following Schulz et al. (2019a) for each of the epistemic activities. The BiLSTM uses FastText embeddings Bojanowski et al. (2017). This architecture is equal to Flair when only using FastText embeddings. UB reports the human upper bound (inter-annotator agreement) indicating room for improvement.",
            "table_column_names": [
                "<bold>Med</bold>",
                "BiLSTM",
                "EG 71.60",
                "EE 80.20",
                "HG 69.28",
                "DC 65.32"
            ],
            "table_content_values": [
                [
                    "<bold>Med</bold>",
                    "UB",
                    "85.61",
                    "90.25",
                    "86.37",
                    "85.58"
                ],
                [
                    "<bold>TEd</bold>",
                    "BiLSTM",
                    "78.53",
                    "78.87",
                    "57.16",
                    "61.77"
                ],
                [
                    "<bold>TEd</bold>",
                    "UB",
                    "93.29",
                    "90.71",
                    "81.77",
                    "82.11"
                ]
            ]
        },
        "gold_description": "in table 1 , we report the performance of the bilstm implementation for predicting epistemic activities in the med and ted data . as we can see , the difficulty of predicting the classes varies between different activities . despite some room for improvement with respect to the human upper bound ( ub ) based on inter - rater agreement , the interactive nature of famulus helps in succeeding in this attempt by continually improving the model when new data is available . we conduct similar experiments for the prediction of fine - grained diagnostic entities , but omit a comprehensive discussion due to space limitations .",
        "generated_description": "table 1 shows the individual macro - f1 scores following schulz et al . for each of the four tasks , we report the human upper bound ( inter - annotator agreement ) indicating room for room for further improvement . the bilstm obtains the highest score for each task and is equal to flair when only using fasttext embeddings ."
    },
    {
        "table_id": "466",
        "table_info": {
            "table_caption": "Table 2: SLC experiments on different feature sets",
            "table_column_names": [
                "<bold>Features</bold>",
                "<bold>Model</bold>",
                "<bold>Development</bold> P",
                "<bold>Development</bold> R",
                "<bold>Development</bold> F"
            ],
            "table_content_values": [
                [
                    "text",
                    "BERT",
                    "0.69",
                    "0.55",
                    "0.61"
                ],
                [
                    "<bold>text</bold>",
                    "<bold>BERT*</bold>",
                    "<bold>0.57</bold>",
                    "<bold>0.79</bold>",
                    "<bold>0.66</bold>"
                ],
                [
                    "context",
                    "BERT",
                    "0.70",
                    "0.53",
                    "0.60"
                ],
                [
                    "context",
                    "BERT*",
                    "0.63",
                    "0.67",
                    "0.65"
                ],
                [
                    "BERT logits + handcrafted**",
                    "LR",
                    "0.70",
                    "0.56",
                    "0.61"
                ],
                [
                    "BERT logits + handcrafted**",
                    "LR*",
                    "0.60",
                    "0.71",
                    "0.65"
                ],
                [
                    "BERT logits + tagged spans",
                    "LR",
                    "0.70",
                    "0.53",
                    "0.60"
                ],
                [
                    "BERT logits + tagged spans",
                    "LR*",
                    "0.61",
                    "0.71",
                    "0.66"
                ],
                [
                    "BERT logits + all",
                    "LR",
                    "0.71",
                    "0.52",
                    "0.60"
                ],
                [
                    "BERT logits + all",
                    "LR*",
                    "0.61",
                    "0.71",
                    "0.66"
                ]
            ]
        },
        "gold_description": "the final model used the finetuned bert model mentioned above with a condition to predict non - propaganda only if the prediction probability is above 0 . 70 for the nonpropaganda class . otherwise the prediction of the sentence will be propaganda even if the majority of the prediction probability mass was for the non - propaganda class . this was a way to handle the unbalance in the training data without having to discard part of the data . the 0 . 70 threshold was chosen after elaborate experiments on both the local and the shared - task ' s development sets . this condition consistently provided an improvement of around 5 points in f1 score of the propaganda class on all experiments using different sets of features as shown in table 2 . in slc , we ran multiple experiments using bert with and without additional features as shown in table 2 . the features include using the text passed as is to bert without any preprocessing . also , we experimented with adding the context which includes the two sentences that come before and after the target sentence . context sentences were concatenated and passed as the second bert input , while the target sentence was passed as the first bert input . in addition , we experimented with using bert logits ( i . e . , the probability predictions per class ) as features in a logistic regression ( lr ) classifier concatenated with handcrafted features ( e . g . , liwc , quotes , questions ) , and with predictions of our flc classifier ( tagged spans : whether the sentence has a propaganda fragment or not ) . however , none of these features added any statistically significant improvements . therefore , we used bert predictions for our final model with a condition to predict the majority class non - propaganda only if its prediction probability is more than 0 . 70 as shown in table 3 . this is a modified threshold as opposed to 0 . 80 in the experiments shown in table 2 to",
        "generated_description": "as can be seen from the table , using bert as the feature extractor results in the best precision , recall , and f1 scores for both development and test sets . however , the f1 score of bert for the development set is 0 . 79 , compared to 0 . 57 for the test set . this is due to the fact that the development data is significantly larger than that of the training data , which allows the model to learn more robust feature representations . as far as we know , this is the first time that this feature combination has been used in a ner setup ."
    },
    {
        "table_id": "467",
        "table_info": {
            "table_caption": "Table 5: Precision, recall and F1 scores of the FLC task on the development and test sets of the shared task.",
            "table_column_names": [
                "<bold>Propaganda</bold> <bold>Technique</bold>",
                "<bold>Development</bold> <bold>P</bold>",
                "<bold>Development</bold> <bold>R</bold>",
                "<bold>Development</bold> <bold>F</bold>",
                "<bold>Test</bold> <bold>F</bold>"
            ],
            "table_content_values": [
                [
                    "Appeal to Authority",
                    "0",
                    "0",
                    "0",
                    "0.212"
                ],
                [
                    "Appeal to Fear/Prejudice",
                    "0.285",
                    "0.006",
                    "0.011",
                    "0"
                ],
                [
                    "Bandwagon",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "Black-and-White Fallacy",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "Causal Oversimplification",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "Doubt",
                    "0.007",
                    "0.001",
                    "0.002",
                    "0"
                ],
                [
                    "Exaggeration,Minimisation",
                    "0.833",
                    "0.085",
                    "0.154",
                    "0"
                ],
                [
                    "Flag-Waving",
                    "0.534",
                    "0.102",
                    "0.171",
                    "0.195"
                ],
                [
                    "Loaded Language",
                    "0.471",
                    "0.160",
                    "0.237",
                    "0.130"
                ],
                [
                    "Name Calling,Labeling",
                    "0.270",
                    "0.112",
                    "0.158",
                    "0.150"
                ],
                [
                    "O,IV,C",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "Red Herring",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "Reductio ad hitlerum",
                    "0.318",
                    "0.069",
                    "0.113",
                    "0"
                ],
                [
                    "Repetition",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "Slogans",
                    "0.221",
                    "0.034",
                    "0.059",
                    "0.003"
                ],
                [
                    "Straw Men",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "Thought-terminating Cliches",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "Whataboutism",
                    "0",
                    "0",
                    "0",
                    "0"
                ],
                [
                    "<bold>Overall</bold>",
                    "0.365",
                    "0.073",
                    "0.122",
                    "0.131∗"
                ]
            ]
        },
        "gold_description": "we also experimented with stacking bert embeddings with all or some of the embeddings mentioned above . however , this resulted on lower 2https : / / www . urbandictionary . com / 3https : / / data . world / jaredfern / urban - dictionary embedding in flc , we only show the results of our best model in table 5 to focus more on the differences between propaganda techniques . a more elaborate study of performance of different models should follow in future work . the best model is a bilstm - crf with flair and urban glove embed as we can see in table 5 , we can divide the propaganda techniques into three groups according to the model ' s performance on the development and test sets . the first group includes techniques with non - zero f1 scores on both datasets : flag - waving , loaded language , name calling , labeling and slogans . this group has techniques that appear frequently in the data and / or techniques with strong lexical signals ( e . g . \" american people \" in flag - waving ) or punctuation signals ( e . g . quotes in slogans ) . the second group has the techniques with a nonzero f1 score on only one of the datasets but not the other , such as : appeal to authority , appeal to fear , doubt , reduction , and exaggeration , minimisation . two out of these five techniques ( appeal to fear and doubt ) have very small non - zero f1 on the development set which indicates that they are generally challenging on our model and were only tagged due to minor differences between the two datasets . however , the remaining three types show significant drops from development to test sets or vice - versa . this requires further analysis to understand why the model was able to do well on one dataset but get zero on the other dataset , which we leave for future work .",
        "generated_description": "for example , “ exaggeration , minimisation ” and “ causal oversimplification ” have very low f - measure on the development and test sets , which may be due to the fact that they are related to the concept of overimplification . however , they do not appear in the development or test sets . the f - measures for “ projected techniques ” are higher than those of “ propaganda ” , which suggests that the exploitation of propaganda techniques is more prevalent in the propaganda materials ."
    },
    {
        "table_id": "468",
        "table_info": {
            "table_caption": "Table 4: Ranked Entity Scores for KRAS Validation and PIK3CA Testing",
            "table_column_names": [
                "<bold>Metrics</bold>",
                "<bold>rocchio</bold>",
                "<bold>relevance model</bold>",
                "<bold>auto-ref</bold>",
                "<bold>rocchio + relevance</bold>",
                "<bold>auto-ref + rocchio + relevance</bold>"
            ],
            "table_content_values": [
                [
                    "nDCG@10",
                    "0.232",
                    "0.274",
                    "0.195",
                    "0.341",
                    "<bold>0.464</bold>"
                ],
                [
                    "nDCG@100",
                    "0.360",
                    "0.397",
                    "0.329",
                    "0.439",
                    "<bold>0.536</bold>"
                ],
                [
                    "MAP",
                    "0.182",
                    "0.223",
                    "0.156",
                    "0.270",
                    "<bold>0.386</bold>"
                ]
            ]
        },
        "gold_description": "to evaluate the ranking of entity terms we have computed ndcg @ 10 , ndcg @ 100 and map , see table 4 for the results . we also compute recall @ k of relevant documents for automatically refined queries using the 1st , 2nd and 3rd ranked entities . the scores can be found in table 5 . tables 4 and 5 show that the relevance model outperforms the rocchio algorithm in every aspect . both models outperform the auto - encoder approach ( auto - ref ) . we suspect that summing over the encodings distorts the individual features too much for a correct extraction of relevant entities to be possible . the combination of all three models ( auto - ref + rocchio + relevance ) outperforms the other models in most cases . especially the performance for ranking of entity terms is increased using the autoencoded features . however , it is interesting to see that the rocchio + relevance model outperforms the recall for second and third best terms . this indicates that for user - evaluated term suggestions , the inclusion of the auto - encoded features is advisable . for automatic query refinement however , in average , this is not the case .",
        "generated_description": "precision , recall and reciprocal rank : the evaluation metric for the pik3ca testing set is ndcg @ 100 , which measures the percentage of items for which the top 10 results are ranked in the top - 100 results . for the kras validation set , the evaluation metric is map , which ranks the items according to their map score . the results show that our model outperforms the state - of - the - art in terms of precision , recall , and roc auc ."
    },
    {
        "table_id": "469",
        "table_info": {
            "table_caption": "Table 1: Features of previous evaluation applications compared to Ours (Linspector Web). #Lang: Number of supported languages, #Task-Type: Number and type of the tasks, where WST: Word similarity tasks, DT: Downstream Tasks, PT: Probing Tasks. Static: Static word embeddings and Models: Pretrained downstream models.",
            "table_column_names": [
                "[EMPTY]",
                "#Lang",
                "#Task-Type",
                "Web",
                "Offline",
                "Static",
                "Models",
                "Layers",
                "Epochs"
            ],
            "table_content_values": [
                [
                    "Faruqui and Dyer (<ref id='bib-bib7'>2014a</ref>)",
                    "4",
                    "10-WST",
                    "×",
                    "×",
                    "×",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Nayak et al. (<ref id='bib-bib14'>2016</ref>)",
                    "1",
                    "7-DT",
                    "×",
                    "×",
                    "×",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Köhn (<ref id='bib-bib10'>2015</ref>)",
                    "7",
                    "7-PT",
                    "[EMPTY]",
                    "×",
                    "×",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Ours",
                    "28",
                    "16-PT",
                    "×",
                    "×",
                    "×",
                    "×",
                    "×",
                    "×"
                ]
            ]
        },
        "gold_description": "a now retired evaluation suite for word embeddings was wordvectors . org ( faruqui and dyer , 2014a ) . veceval ( nayak et al . , 2016 ) is another web based suite for static english word embeddings that perform evaluation on a set of downstream tasks which may take several hours . k ¨ ohn ( 2015 ) introduced an offline , multilingual probing suite for static embeddings limited in terms of the languages and the probing tasks . a comparison of the system features of previous studies is given in table 1 .",
        "generated_description": "for comparison , we also include results of previous evaluation applications on the test set of linspector web with the same number of languages , number of layers , and number of epochs . we compare our model with the following previous models : [ leftmargin = * ] faruqui and dyer [ referred to as 10 - wst , 10 - dt , 7 - pt ] , köhn et al . [ refuted id = ' bib - bib10 ' > 2015 ] for wst tasks , we use a static word embedding with pre - trained word embeddings , and for pt tasks we use pretrained word embedddings with additional parameters . our model outperforms these previous models by a large margin in terms of the number of trained layers and epochs , but underperforms on the downstream tasks . we attribute this to the nature of the tasks , where we focus on measuring the quality of the downstream results , which are more closely related to downstream tasks than word similarity tasks . for example , in the pt task , we compare the accuracy of measuring the word similarity between two sentences in a list of sentences , where the second sentence has to be a correct translation if the first sentence has the same word as the sentence in front of it . in contrast , our model is better at finding the sentence with the correct translation in the downstream tasks ."
    },
    {
        "table_id": "470",
        "table_info": {
            "table_caption": "Table 6: Refinement Terms for Query {PIK3CA}",
            "table_column_names": [
                "<bold>Variants</bold>",
                "<bold>Diseases</bold>",
                "<bold>Drugs</bold>"
            ],
            "table_content_values": [
                [
                    "H1047R",
                    "Color. Neop.",
                    "Lapatinib"
                ],
                [
                    "V600E",
                    "Liposarcoma",
                    "Mitomycin"
                ],
                [
                    "T790M",
                    "Adenocarcin.",
                    "Linsitinib"
                ],
                [
                    "E545K",
                    "Glioblastoma",
                    "Dactolisib"
                ],
                [
                    "E542K",
                    "Stomach Neop.",
                    "Pictrelisib"
                ]
            ]
        },
        "gold_description": "in table 6 we show the top ranked entities of type variants , diseases and drugs for the query { pik3ca } . while the diseases and the drugs are all relevant , v600e and t790m are in fact not variants of the gene pik3ca . however , when refining the query { pik3ca , v600e , braf , h1047r , dabrafenib } , the top ranked diseases are [ melanoma , neoplasms , carcinoma non small cell lung ( cnscl ) , thyroid neoplasms , colorectal neoplasms ] . using melanoma for refinement , retrieves the top ranked paper ( falchook et al . , 2013 ) which perfectly includes all these entities in a biomarker relationship .",
        "generated_description": "the first term for query { pik3ca } is the color . neop . the second term is used to denote the number of patients for which the ground truth number of positive and negative reviews is greater than or equal to 0 . 9 . the third term is called the specificity . for example , the term h1047r is used for the drug linsitinib and the term v600e for lapatinib . the terms t790m and e545k are used for various neologisms and duretics and the terms t545k and e542k for stomach neoprene and dactolisib are used in the neologism detection task ."
    },
    {
        "table_id": "471",
        "table_info": {
            "table_caption": "Table 1: Results of the difficulty prediction approaches. SVM (original) has been taken from Beinborn (2016)",
            "table_column_names": [
                "Model",
                "<bold>Original data</bold> <italic>ρ</italic>",
                "<bold>Original data</bold> rmse",
                "<bold>Original data</bold> qw<italic>κ</italic>",
                "<bold>New data</bold> <italic>ρ</italic>",
                "<bold>New data</bold> rmse",
                "<bold>New data</bold> qw<italic>κ</italic>"
            ],
            "table_content_values": [
                [
                    "SVM (original)",
                    ".50",
                    ".23",
                    ".44",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "SVM (reproduced)",
                    ".49",
                    ".24",
                    ".47",
                    ".50",
                    ".21",
                    ".39"
                ],
                [
                    "MLP",
                    ".42",
                    ".25",
                    ".31",
                    ".41",
                    ".22",
                    ".25"
                ],
                [
                    "BiLSTM",
                    ".49",
                    ".24",
                    ".35",
                    ".39",
                    ".24",
                    ".27"
                ]
            ]
        },
        "gold_description": "the right - hand side of table 1 shows the performance of our svm and the two neural methods . the results indicate that the svm setup is well suited for the difficulty prediction task and that it successfully generalizes to new data .",
        "generated_description": "on the original data , we see that the mlp and bilstm outperform the svm . on the new data , which contains the same number of instances as in the original evaluation , the mlm shows a slightly better performance , but still lags behind the lstm by a significant margin ( p ≤ 0 . 01 ) . the svm outperforms the original svm in terms of the rmse metric , but underperforms on the qw metric when evaluated with rmse . in this case , the performance of the difficulty approaches is very close to that obtained on the original dataset , showing that the difficulty model is able to exploit the data in the same way as it did on the dataset in the previous evaluation . in addition , in this new evaluation , there is also a significant performance gap between the original and rmse metrics , which suggests that the new dataset is harder than the original ."
    },
    {
        "table_id": "472",
        "table_info": {
            "table_caption": "Table 2: rmse for both strategies on each corpora with randomly sampled target difficulties τ",
            "table_column_names": [
                "Strategy",
                "Brown",
                "Reuters",
                "Gutenberg"
            ],
            "table_content_values": [
                [
                    "SEL",
                    ".11",
                    ".12",
                    ".10"
                ],
                [
                    "SIZE",
                    ".13",
                    ".15",
                    ".12"
                ]
            ]
        },
        "gold_description": "table 2 shows the results for our three corpora . throughout all three corpora , both manipulation strategies perform well . sel consistently outperforms size , which matches our observations from the previous experiment .",
        "generated_description": "we first compare the rmse for both strategies on each corpora with randomly sampled target difficulties τ . we observe that the sel strategy performs slightly better than the size strategy on the brown and reuters corpora , but worse on the gutenberg and gutenberg corpora . we hypothesize that this is because gutenberg and brown problems are more similar to each other than to sel and thus the target difficulties we are dealing with are more general and easier to tune ."
    },
    {
        "table_id": "473",
        "table_info": {
            "table_caption": "Table 3: Mean error rates e(T) per text and strategy. Results marked with ∗ deviate significantly from DEF",
            "table_column_names": [
                "[EMPTY]",
                "easy (dec) SEL",
                "easy (dec) SIZE",
                "default DEF",
                "hard (inc) SEL",
                "hard (inc) SIZE"
            ],
            "table_content_values": [
                [
                    "<italic>T</italic>1",
                    "–",
                    "–",
                    ".30",
                    "–",
                    "–"
                ],
                [
                    "<italic>T</italic>2",
                    ".17∗",
                    ".11∗",
                    ".34",
                    ".66∗",
                    ".44∗"
                ],
                [
                    "<italic>T</italic>3",
                    ".16∗",
                    ".10∗",
                    ".27",
                    ".52∗",
                    ".43∗"
                ],
                [
                    "<italic>T</italic>4",
                    ".28",
                    ".09∗",
                    ".30",
                    ".43∗",
                    ".45∗"
                ],
                [
                    "Average",
                    ".20∗",
                    ".10∗",
                    ".30",
                    ".53∗",
                    ".44∗"
                ]
            ]
        },
        "gold_description": "table 3 shows the error rates per c - test and strategy . both sel and size are overall able to significantly ( p < 0 . 025 ) increase and decrease the test ' s difficulty over def , and with the exception of sel , dec , the effect is also statistically significant 4 for all individual text and strategy pairs .",
        "generated_description": "table 3 shows the mean error rates e ( t ) per text and strategy for the easy and hard sel settings . as shown in the table , the error rates deviate significantly from def in both cases . the deviate is particularly pronounced in the hard setting , where the margin between the deviate from def is . 66 % for t = 2 and . 52 % for t = 3 . despite the small difference in error rates between the two settings ( only . 20 % vs . 30 % for the default def strategy ) , we see that the error reduction made by the small sel is more significant in the easy setting ( also shown with a high value of . 44 ) ."
    },
    {
        "table_id": "474",
        "table_info": {
            "table_caption": "Table 3: FrameId results (in %) on English (upper) and German (lower) with and without using the lexicon. Reported are accuracy and F1-macro, both also for ambiguous predicates (mean scores over ten runs). Models: (a) Data, Lexicon, and Data-Lexicon Baselines. (b) Previous models for English. (c) Ours: unimodal our-uni, multimodal on top of our-uni – our-mm – with Imagined embeddings (and synset visual embeddings for English). Best results highlighted in bold. The best run’s results for English were: our_uni: acc: 89.35 ; acc_amb: 76.45 ; F1-m: 76.95 ; F1-m_amb: 54.02 (with lexicon) our_mm (im, synsV): acc: 89.09 ; acc_amb: 75.86 ; F1-m: 78.17 ; F1-m_amb: 57.48 (with lexicon)",
            "table_column_names": [
                "[EMPTY]",
                "<bold>model</bold>",
                "<bold>with lexicon</bold> <bold>acc</bold>",
                "<bold>with lexicon</bold> <bold>acc_amb</bold>",
                "<bold>with lexicon</bold> <bold>F1-m</bold>",
                "<bold>with lexicon</bold> <bold>F1-m_amb</bold> 28.452756pt",
                "<bold>without lexicon</bold> <bold>acc</bold>",
                "<bold>without lexicon</bold> <bold>acc_amb</bold>",
                "<bold>without lexicon</bold> <bold>F1-m</bold>",
                "<bold>without lexicon</bold> <bold>F1-m_amb</bold>"
            ],
            "table_content_values": [
                [
                    "FrameNet",
                    "Data Baseline",
                    "79.06",
                    "69.73",
                    "33.00",
                    "37.42",
                    "79.06",
                    "69.73",
                    "33.00",
                    "37.42"
                ],
                [
                    "FrameNet",
                    "Lexicon Baseline",
                    "79.89",
                    "55.52",
                    "65.61",
                    "30.95",
                    "–",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "FrameNet",
                    "<bold>Data-Lexicon Baseline</bold>",
                    "86.32",
                    "69.73",
                    "64.54",
                    "37.42",
                    "–",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "FrameNet",
                    "hermann2014semantic",
                    "88.41",
                    "73.10",
                    "–",
                    "– 28.452756pt",
                    "–",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "FrameNet",
                    "Hartmann2017OOD",
                    "87.63",
                    "73.80",
                    "–",
                    "– 28.452756pt",
                    "77.49",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "FrameNet",
                    "<bold>our</bold>_uni",
                    "88.66",
                    "74.92",
                    "76.65",
                    "53.86",
                    "79.96",
                    "71.70",
                    "57.07",
                    "47.40"
                ],
                [
                    "FrameNet",
                    "<bold>our</bold>_mm (im, synsV)",
                    "88.82",
                    "75.28",
                    "76.77",
                    "54.80",
                    "81.21",
                    "72.51",
                    "57.81",
                    "49.38"
                ],
                [
                    "SALSA",
                    "Data Baseline",
                    "77.00",
                    "70.51",
                    "37.40",
                    "28.87",
                    "77.00",
                    "70.51",
                    "37.40",
                    "28.87"
                ],
                [
                    "SALSA",
                    "Lexicon Baseline",
                    "61.57",
                    "52.5",
                    "19.36",
                    "15.68",
                    "–",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "SALSA",
                    "<bold>Data-Lexicon Baseline</bold>",
                    "77.16",
                    "70.51",
                    "38.48",
                    "28.87",
                    "–",
                    "–",
                    "–",
                    "–"
                ],
                [
                    "SALSA",
                    "<bold>our</bold>_uni",
                    "80.76",
                    "75.59",
                    "48.42",
                    "41.38",
                    "80.59",
                    "75.52",
                    "47.64",
                    "41.17"
                ],
                [
                    "SALSA",
                    "<bold>our</bold>_mm (im)",
                    "80.71",
                    "75.58",
                    "48.29",
                    "41.19",
                    "80.51",
                    "75.51",
                    "47.36",
                    "40.93"
                ]
            ]
        },
        "gold_description": "first , we report our results on english data ( see table 3 , top ) we compare against german data ( see table 3 , bottom ) . our new strong data - lexicon baseline reaches a considerable accuracy of 86 . 32 % , which is hard to beat by trained models . even the most recent state of the art only beats it by about two points : 88 . 41 % ( hermann et al . , 2014 ) . however , the accuracy of the baseline drops for ambiguous predicates ( 69 . 73 % ) and the f1 - macro score reveals its weakness toward minority classes ( drop from 64 . 54 % to 37 . 42 % ) . our unimodal system trained and evaluated on english data slightly exceeds the accuracy of the previous state of the art ( 88 . 66 % on average versus 88 . 41 % for hermann et al . , 2014 ) ; our best run ' s accuracy is 89 . 35 % . table 3 : frameid results ( in % ) on english ( upper ) and german ( lower ) with and without using the lexicon . reported are accuracy and f1 - macro , both also for ambiguous predicates ( mean scores over ten runs ) . models : ( a ) data , lexicon , and data - lexicon baselines . ( b ) previous models for english . ( c ) ours : unimodal our - uni , multimodal on top of our - uni – our - mm – with imagined embeddings ( and synset visual embeddings for english ) . best results highlighted in bold . the best run ' s results for english were : our uni : acc : 89 . 35 ; acc amb : 76 . 45 ; f1 - m : 76 . 95 ; f1 - m amb : 54 . 02 ( with lexicon ) our mm ( im , synsv ) : acc : 89 . 09 ; acc amb",
        "generated_description": "for framenet and data - lexicon baseline , we use the data - lexicon baseline to compare the performance of our proposed lexicon - based and data - text - based methods with respect to the framenet baseline . the results show that with the lexicon , our framenet model achieves the best performance in terms of both accuracy and f1 - m when compared with the baseline . in addition , our method outperforms the data baseline when using framenet as the training data , which indicates that lexicon information is beneficial for framenet ."
    },
    {
        "table_id": "475",
        "table_info": {
            "table_caption": "Table 1. The performances of various models on MSCOCO Karpathy split",
            "table_column_names": [
                "Method",
                "<bold>BLEU-1</bold>",
                "<bold>BLEU-2</bold>",
                "<bold>BLEU-3</bold>",
                "<bold>BLEU-4</bold>",
                "<bold>METEOR</bold>",
                "<bold>ROUGE</bold>",
                "<bold>CIDEr</bold>"
            ],
            "table_content_values": [
                [
                    "Soft-Attention (xu2015show)",
                    "70.7",
                    "49.2",
                    "34.4",
                    "24.3",
                    "23.9",
                    "-",
                    "-"
                ],
                [
                    "CNN+Att (aneja2018convolutional)",
                    "71.1",
                    "53.8",
                    "39.4",
                    "28.7",
                    "24.4",
                    "52.2",
                    "91.2"
                ],
                [
                    "SCST (rennie2017self)",
                    "77.4",
                    "60.9",
                    "46.0",
                    "34.1",
                    "26.7",
                    "55.7",
                    "114.0"
                ],
                [
                    "Adaptive (lu2017knowing)",
                    "74.2",
                    "58.0",
                    "43.9",
                    "33.2",
                    "26.6",
                    "54.9",
                    "108.5"
                ],
                [
                    "GroupCap (chen2018groupcap)",
                    "74.4",
                    "58.1",
                    "44.3",
                    "33.8",
                    "26.2",
                    "-",
                    "-"
                ],
                [
                    "NBT (lu2018neural)",
                    "75.5",
                    "-",
                    "-",
                    "34.7",
                    "27.1",
                    "-",
                    "107.2"
                ],
                [
                    "StackCap (gu2018stack)",
                    "78.4",
                    "62.5",
                    "47.9",
                    "36.1",
                    "27.4",
                    "56.9",
                    "120.4"
                ],
                [
                    "Up-Down (anderson2018bottom)",
                    "79.8",
                    "63.4",
                    "48.4",
                    "36.3",
                    "27.7",
                    "56.9",
                    "120.1"
                ],
                [
                    "UGRIC",
                    "81.3",
                    "65.2",
                    "50.3",
                    "38.2",
                    "28.4",
                    "58.6",
                    "123.5"
                ],
                [
                    "UGRIC w/o copying",
                    "80.8",
                    "64.6",
                    "49.6",
                    "37.7",
                    "28.1",
                    "58.2",
                    "122.7"
                ],
                [
                    "UGRIC w/o discriminator",
                    "80.5",
                    "64.2",
                    "49.1",
                    "37.3",
                    "27.9",
                    "57.9",
                    "122.3"
                ]
            ]
        },
        "gold_description": "the automatic evaluation results on the test set of mscoco are shown in table 1 . our model outperforms all the compared approaches on all automatic evaluation metrics . in particular , by benefiting from the retrieved captions , our model gets the highest cider score of 123 . 5 , suggesting that the captions generated by our model are informative .",
        "generated_description": "from the table , we can see that our model outperforms previous state - of - the - art methods by a large margin , e . g . , outperforming scst by 1 . 8 in terms of bleu - 1 , 1 . 7 in terms on meteor , 2 . 3 on rouge and 1 . 6 on cider . our model also achieves the highest scores on all metrics except for cidr . we attribute this to the fact that the ground - truth captions for each video in the test set are automatically generated by the automatic metrics . in addition , our model also outperforms the neural network - based method lbt , which demonstrates the effectiveness of collective attentions for video captioning ."
    },
    {
        "table_id": "476",
        "table_info": {
            "table_caption": "Table 1: Fraction of incorrect summaries produced by recent summarization systems on the CNN-DM test set, evaluated on a subset of 100 summaries. ROUGE scores (on full test set) and average summary length for reference.",
            "table_column_names": [
                "<bold>Model</bold>",
                "<bold>Incorrect</bold>",
                "<bold>ROUGE-1</bold>",
                "<bold>ROUGE-2</bold>",
                "<bold>ROUGE-L</bold>",
                "<bold>Length</bold>"
            ],
            "table_content_values": [
                [
                    "PGC See17",
                    "8%",
                    "39.49%",
                    "17.24%",
                    "36.35%",
                    "59.7"
                ],
                [
                    "FAS Chen18",
                    "26%",
                    "40.88%",
                    "17.80%",
                    "38.53%",
                    "72.1"
                ],
                [
                    "BUS Gehrmann18",
                    "25%",
                    "41.52%",
                    "18.76%",
                    "38.60%",
                    "54.4"
                ]
            ]
        },
        "gold_description": "table 1 shows the evaluation results . in line with the findings for sentence summarization ( cao et al . , 2018 ; li et al . , 2018 ) , we observe that factual errors are also a frequent problem for document summarization . interestingly , the fraction of incorrect summaries is substantially higher for fas and bus compared to pgc . the length of the generated summaries appears to be unrelated to the number of errors . instead , the higher abstractiveness of summaries produced by fas and bus , as analyzed in their respective papers , seems to also increase the chance of introducing errors . in addition , we also observe that among the three systems correctness and rouge scores do not correlate , emphasizing one more time that a rouge based evaluation alone is far too limited to account for the full scope of the summarization task .",
        "generated_description": "table 1 shows the fraction of incorrect summaries produced by recent summarization systems on the cnn - dm test set , evaluated on a subset of 100 summaries . as shown in the table , the fas system has the highest fraction of correct summaries ( 26 % ) , followed by pgc see17 and bus gehrmann18 with 25 % and 18 . 76 % , and the pgc and fas systems have the lowest fraction ( 8 % ) and average length ( 59 . 7 ) ."
    },
    {
        "table_id": "477",
        "table_info": {
            "table_caption": "Table 2: Fraction of incorrect summaries at first position after reranking with different NLI models. ↑ and ↓ show the absolute number of improved (incorrect replaced by correct) and worsened (vice versa) instances.",
            "table_column_names": [
                "<bold>Split</bold>",
                "<bold>NLI Model</bold>",
                "<bold>Incor.</bold>",
                "Δ",
                "↑",
                "↓"
            ],
            "table_content_values": [
                [
                    "Val",
                    "<italic>Original</italic>",
                    "42.1%",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Val",
                    "Random",
                    "50.7%",
                    "+8.6",
                    "16",
                    "26"
                ],
                [
                    "Val",
                    "DA",
                    "51.4%",
                    "+9.3",
                    "13",
                    "23"
                ],
                [
                    "Val",
                    "SSE",
                    "45.8%",
                    "+3.7",
                    "18",
                    "22"
                ],
                [
                    "Val",
                    "ESIM",
                    "39.3%",
                    "-2.8",
                    "23",
                    "20"
                ],
                [
                    "Val",
                    "InferSent",
                    "38.3%",
                    "-3.8",
                    "24",
                    "20"
                ],
                [
                    "[EMPTY]",
                    "BERT",
                    "28.0%",
                    "-14.1",
                    "25",
                    "10"
                ],
                [
                    "Test",
                    "<italic>Original</italic>",
                    "26.0%",
                    "[EMPTY]",
                    "[EMPTY]",
                    "[EMPTY]"
                ],
                [
                    "Test",
                    "ESIM",
                    "29.0%",
                    "+3.0",
                    "11",
                    "14"
                ]
            ]
        },
        "gold_description": "for 107 out of the 200 documents , an incorrect and correct summary is among the 5 alternatives . table 2 shows that in this sample from the validation data , the fraction of incorrect summaries at first position , when the 5 alternatives are ranked as during beam search , is at 42 . 1 % . using entailment probabilities of esim and infersent , we can slightly improve upon that and reduce incorrect summaries . however , with da and sse , more incorrect summaries end up in the first position . note that these results are not in line with the model ' s nli accuracies , underlining that performance on nli does not directly transfer to our task . only for bert , which outperforms the other models on nli by a large margin , we also see substantially better reranking performance . but even for this powerful model , more than half of the errors still remain in the summaries . 5 interestingly , we also find that for esim and infersent , reranking hurts in many cases , leaving just a few cases of net improvement . given the validation results , we then applied reranking to the cnn - dm test data followed by a post - hoc correctness evaluation as in section 4 . we used the esim model and reranked all 10 beam hypotheses generated by fas . 6 in contrast to the validation sample , the fraction of incorrect summaries increases from 26 % to 29 % ( table 2 ) , demonstrating that the slight improvement on the validation data does not transfer to the test set .",
        "generated_description": "table 3 summarizes the results of the nli experiments . in general , we see that the neural models outperform the non - neural baselines by a large margin on both validation and test sets . da and sse perform better than random and esim on the snli dataset , but underperform on the infersent dataset . sse outperforms both da and random on both the nli and snli datasets . the results indicate that sse is able to learn better sentence - level sentence embeddings than the other two neural models . on the other hand , esim and bert exhibit the opposite trend , with bert outperforming the other models on both datasets , although the gain is larger on the nlli dataset ."
    },
    {
        "table_id": "478",
        "table_info": {
            "table_caption": "Table 2: French contraction rules.",
            "table_column_names": [
                "à le → au",
                "de lequel → duquel"
            ],
            "table_content_values": [
                [
                    "à les → aux",
                    "de lesquels → desquels"
                ],
                [
                    "à lequel → auquel",
                    "de lesquelles → desquelles"
                ],
                [
                    "à lesquels → auxquels",
                    "en les → ès"
                ],
                [
                    "à lesquelles → auxquelles",
                    "vois ci → voici"
                ],
                [
                    "de le → du",
                    "vois là → voilà"
                ],
                [
                    "de les → des",
                    "[EMPTY]"
                ]
            ]
        },
        "gold_description": "the two best - performing approaches in the task of generating sentences from dependency trees have been feature - based incremental text generation ( bohnet et al . , 2010 ; liu et al . , 2015 ; puduppully et al . , 2016 ; king and white , 2018 ) and 2http : / / universaldependencies . org / techniques performing more global input - output mapping ( castro ferreira et al . , 2018 ; elder and hokamp , 2018 ) . each of these approaches has their advantages and limitations ( table 2 ) .",
        "generated_description": "the rules of french contraction are very similar to those of german and italian contraction rules : à le → aux , à les quelle → auxquelles , de les quelles → desquels , à lequel → auquel , and en les qua → duquel . in addition , there are many more interesting cases where one of the conditions is averse to the other . for example , in italian , the contraction rules for de le → du and de les → des are “ de le que viene aux qua ” . in french , many of these rules are similar to the ones in german contraction rules , e . g . “ vois ci → voici ” and “ vois là → voilà ” , where the former is a contraction of the former and the latter is a variation of the latter . in some cases , the former may be a modification of “ la carrière aux quatrains d ’ hondt ” ( cf . the contraction rules can also be generalized to other languages . in order to have a complete picture of the rules in french contraction , we have prepared a french version of the constituent corpus bordes et al . due to lack of space , we do not show all the french contraction rules in the table , but we have selected the rules which follow the ones found in the french wikipedia ."
    },
    {
        "table_id": "479",
        "table_info": {
            "table_caption": "Table 1: Accuracy of postle models on *BLESS datasets, for two different sets of English distributional vectors: Skip-Gram (SG) and GloVe (GL). lear reports highest scores on *BLESS datasets in the literature.",
            "table_column_names": [
                "[EMPTY]",
                "Setup: Full DBLESS",
                "Setup: Full DBLESS",
                "Setup: Full WBLESS",
                "Setup: Full WBLESS",
                "Setup: Full BIBLESS",
                "Setup: Full BIBLESS",
                "Setup: Disjoint DBLESS",
                "Setup: Disjoint DBLESS",
                "Setup: Disjoint WBLESS",
                "Setup: Disjoint WBLESS",
                "Setup: Disjoint BIBLESS",
                "Setup: Disjoint BIBLESS"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "SG",
                    "GL",
                    "SG",
                    "GL",
                    "SG",
                    "GL",
                    "SG",
                    "GL",
                    "SG",
                    "GL",
                    "SG",
                    "GL"
                ],
                [
                    "lear [Vulic:2018naaclps]",
                    ".957",
                    ".955",
                    ".905",
                    ".910",
                    ".872",
                    ".875",
                    ".528",
                    ".531",
                    ".555",
                    ".529",
                    ".381",
                    ".389"
                ],
                [
                    "postle dffn",
                    ".957",
                    ".955",
                    ".905",
                    ".910",
                    ".872",
                    ".875",
                    ".898",
                    ".825",
                    ".754",
                    ".746",
                    ".696",
                    ".677"
                ],
                [
                    "postle adv",
                    ".957",
                    ".955",
                    ".905",
                    ".910",
                    ".872",
                    ".875",
                    "<bold>.942</bold>",
                    "<bold>.888</bold>",
                    "<bold>.832</bold>",
                    "<bold>.766</bold>",
                    "<bold>.757</bold>",
                    "<bold>.690</bold>"
                ]
            ]
        },
        "gold_description": "the accuracy scores on * bless test sets are provided in table 1 . 8 our postle models display exactly the same performance as lear in the full setting : this is simply because all words found in * bless datasets are covered by the lexical constraints , and postle does not generalize the initial lear transformation to unseen test words . in the disjoint setting , however , lear is left \" blind \" as it has not seen a single test word in the constraints : it leaves distributional vectors of * bless test words identical . in this setting , lear performance is equivalent to the original distributional space . in contrast , learning to generalize the le specialization function from lear - specializations of other words , postle models are able to successfully le - specialize vectors of test * bless words . as in the graded le , the adversarial postle architecture outperforms the simpler dffn model .",
        "generated_description": "in each of the three setups , our models outperform the baselines in all of the four tasks , with the exception of the bibless task in the full scenario , where our models perform slightly better than the neural models . in particular , we see that the performance of the sg models outperforms those in the lear [ vulic : 2018naaclps ] and postle adv [ postle adv ] in both full and disjoint scenarios ."
    },
    {
        "table_id": "480",
        "table_info": {
            "table_caption": "Table 2: Average precision (AP) of postle models in cross-lingual transfer. Results are shown for both postle models (dffn and adv), two target languages (Spanish and French) and three methods for inducing bilingual vector spaces: Ar [Artetxe:2018acl], Co [Conneau:2018iclr], and Sm [Smith:2017iclr].",
            "table_column_names": [
                "<bold>Random</bold>",
                "Target: Spanish .498",
                "Target: Spanish .498",
                "Target: Spanish .498",
                "Target: French .515",
                "Target: French .515",
                "Target: French .515"
            ],
            "table_content_values": [
                [
                    "<bold>Distributional</bold>",
                    ".362",
                    ".362",
                    ".362",
                    ".387",
                    ".387",
                    ".387"
                ],
                [
                    "[EMPTY]",
                    "Ar",
                    "Co",
                    "Sm",
                    "Ar",
                    "Co",
                    "Sm"
                ],
                [
                    "postle dffn",
                    "<bold>.798</bold>",
                    ".740",
                    ".728",
                    ".688",
                    ".735",
                    ".742"
                ],
                [
                    "postle adv",
                    ".768",
                    "<bold>.790</bold>",
                    "<bold>.782</bold>",
                    "<bold>.746</bold>",
                    "<bold>.770</bold>",
                    "<bold>.786</bold>"
                ]
            ]
        },
        "gold_description": "the average precision ( ap ) ranking scores achieved via cross - lingual transfer of postle are shown in table 2 . we report ap scores using three methods for cross - lingual word embedding induction , and compare their performance to two baselines : 1 ) random word pair scoring , and 2 ) the original ( fasttext ) vectors . the results uncover the inability of distributional vectors to capture le – they yield lower performance than the random baseline , which strongly emphasizes the need for the le - specialization . the transferred postle yields an immense improve",
        "generated_description": "distributional methods for inducing bilingual vector spaces artetxe : 2018acl , co and conneau : 2018iclr , and sm show promising results in cross - lingual transfer , with the exception of the random selection method for spanish and french . distributional methods , on the other hand , show poor results for the other two target languages . the distributional method , as expected , is not effective in this case as the number of target languages is small and the distributional embedding space is not sufficiently large ."
    },
    {
        "table_id": "481",
        "table_info": {
            "table_caption": "Table 4: Evaluations based on maximum span, MINA, and head spans on the CoNLL-2012 test set and WikiCoref. The ranking of corresponding scores is specified in parentheses. Rankings which are different based on maximum vs. MINA spans are highlighted.",
            "table_column_names": [
                "[EMPTY]",
                "CoNLL max",
                "CoNLL MINA",
                "CoNLL head",
                "LEA max",
                "LEA MINA",
                "LEA head"
            ],
            "table_content_values": [
                [
                    "[EMPTY]",
                    "CoNLL-2012 test set",
                    "CoNLL-2012 test set",
                    "CoNLL-2012 test set",
                    "CoNLL-2012 test set",
                    "CoNLL-2012 test set",
                    "CoNLL-2012 test set"
                ],
                [
                    "Stanford rule-based",
                    "55.60 (8)",
                    "57.55 (8)",
                    "57.38 (8)",
                    "47.31 (8)",
                    "49.65 (8)",
                    "49.44 (8)"
                ],
                [
                    "cort",
                    "63.03 (7)",
                    "64.60 (6)",
                    "64.51 (6)",
                    "56.10 (6)",
                    "58.05 (6)",
                    "57.93 (6)"
                ],
                [
                    "Peng et al.",
                    "63.05 (6)",
                    "63.50 (7)",
                    "63.54 (7)",
                    "55.22 (7)",
                    "55.76 (7)",
                    "55.80 (7)"
                ],
                [
                    "deep-coref ranking",
                    "65.59 (5)",
                    "67.29 (5)",
                    "67.09 (5)",
                    "59.58 (5)",
                    "61.70 (5)",
                    "61.43 (5)"
                ],
                [
                    "deep-coref RL",
                    "65.81 (4)",
                    "67.50 (4)",
                    "67.36 (4)",
                    "59.76 (4)",
                    "61.84 (4)",
                    "61.64 (4)"
                ],
                [
                    "Lee et al. 2017 single",
                    "67.23 (3)",
                    "68.55 (3)",
                    "68.53 (3)",
                    "61.24 (3)",
                    "62.87 (3)",
                    "62.82 (3)"
                ],
                [
                    "Lee et al. 2017 ensemble",
                    "68.87 (2)",
                    "70.12 (2)",
                    "70.05 (2)",
                    "63.19 (2)",
                    "64.76 (2)",
                    "64.64 (2)"
                ],
                [
                    "Lee et al. 2018",
                    "72.96 (1)",
                    "74.26 (1)",
                    "75.29 (1)",
                    "67.73 (1)",
                    "69.32 (1)",
                    "70.40 (1)"
                ],
                [
                    "[EMPTY]",
                    "WikiCoref",
                    "WikiCoref",
                    "WikiCoref",
                    "WikiCoref",
                    "WikiCoref",
                    "WikiCoref"
                ],
                [
                    "Stanford rule-based",
                    "51.78 (4)",
                    "53.79 (5)",
                    "57.10 (4)",
                    "43.28 (5)",
                    "45.48 (6)",
                    "49.28 (4)"
                ],
                [
                    "deep-coref ranking",
                    "52.90 (3)",
                    "55.16 (2)",
                    "57.13 (3)",
                    "44.40 (3)",
                    "46.98 (3)",
                    "49.05 (5)"
                ],
                [
                    "deep-coref RL",
                    "50.73 (5)",
                    "54.26 (4)",
                    "57.16 (2)",
                    "41.98 (6)",
                    "46.02 (4)",
                    "49.29 (3)"
                ],
                [
                    "Lee et al. 2017 single",
                    "50.38 (6)",
                    "52.16 (6)",
                    "54.02 (6)",
                    "43.86 (4)",
                    "45.75 (5)",
                    "47.69 (6)"
                ],
                [
                    "Lee et al. 2017 ensemble",
                    "53.63 (2)",
                    "55.03 (3)",
                    "56.80 (5)",
                    "47.50 (2)",
                    "48.98 (2)",
                    "50.87 (2)"
                ],
                [
                    "Lee et al. 2018",
                    "57.89 (1)",
                    "59.90 (1)",
                    "61.33 (1)",
                    "52.42 (1)",
                    "54.63 (1)",
                    "56.19 (1)"
                ]
            ]
        },
        "gold_description": "the reinforcement learning model of deep - coref , i . e . , deep - coref rl , has the most significant difference when it is evaluated based on maximum vs . minimum spans ( about 4 points ) . the ensemble model of e2e - coref , on the other hand , has the least difference between maximum and minimum span scores ( 1 . 4 points ) , which indicates it better recognizes maximum span boundaries in out - of - domain data . the ranking of systems is very different by using maximum vs . min table 4 shows the maximum vs . minimum span evaluations of several recent coreference resolvers on the conll - 2012 test set and the wikicoref dataset . the examined coreference resolvers are as follows : the stanford rule - based system ( lee et al . , 2013 ) , the coreference resolver of peng et al . ( 2015 ) , the ranking model of cort ( martschat and strube , 2015 ) , the ranking and reinforcement learning models of deep - coref ( clark and manning , 2016a , b ) , the single and ensemble models of lee et al . ( 2017 ) , and the current stateof - the - art system by lee et al . ( 2018 ) . the coreference resolver of peng et al . ( 2015 ) has the smallest difference between its maximum and minimum span evaluation scores . based on maximum spans , peng et al . ( 2015 ) performs on - par with cort while cort outperforms it by about one percent when they are evaluated based on minimum spans .",
        "generated_description": "the performance of the rule - based and deep - coref - ranking models on the conll - 2012 test sets are shown in rows 1 and 2 , respectively . we observe that peng et al . ’ s model achieves the best performance on max and mina , but their performance on lea is much worse than that on max . the reason may be that their model suffers from the vanishing gradient problem , which makes their scores highly correlated with human scores . on the other hand , we observe that the performance of deep models is much higher than that of the baselines , which shows the effectiveness of our model ."
    },
    {
        "table_id": "482",
        "table_info": {
            "table_caption": "Table 1: Quality of the regression model’s predictions on the test set.",
            "table_column_names": [
                "[EMPTY]",
                "MSE",
                "R2"
            ],
            "table_content_values": [
                [
                    "Experiment 1",
                    "0.03448",
                    "0.12238"
                ],
                [
                    "Experiment 2",
                    "0.03068",
                    "0.17576"
                ],
                [
                    "Experiment 3, random regression",
                    "0.17112",
                    "-3.39857"
                ]
            ]
        },
        "gold_description": "table 1 shows and compares mse and r2 reported from these experiments . the results show that in experiment 2 , the mean squared error is reduced and the r2 score is increased .",
        "generated_description": "in order to evaluate the quality of the regression model ’ s performance on the test set , three experiments were conducted : ( i ) experiment 1 , ( ii ) random regression , ( iii ) logistic regression , and ( iv ) unigram and log - likelihood regression . the results of these experiments are reported in tab . from the table , we can see that in all the three experiments , the proposed regression model achieved a lower mean squared error ( mse ) than the two baselines , and a lower root mean square error ( r2 ) than random regression . in addition , the results of random regression indicates that the misclassification of positive and negative instances is due to a very small number of instances ( 0 . 03 ) in the training set . on the other hand , in experiment 1 and 2 , the mse and r2 values of the proposed model were higher than those of the random regression model ."
    },
    {
        "table_id": "483",
        "table_info": {
            "table_caption": "TABLE III: The Accuracy of Different Syntactic Representations",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] CCAT10",
                "[BOLD] CCAT50",
                "[BOLD] BLOGS10",
                "[BOLD] BLOGS50"
            ],
            "table_content_values": [
                [
                    "ST-CNN",
                    "22.8",
                    "10.08",
                    "48.64",
                    "42.91"
                ],
                [
                    "POS-CNN",
                    "61.40",
                    "40.98",
                    "68.26",
                    "54.85"
                ],
                [
                    "POS-HAN",
                    "[BOLD] 63.14",
                    "[BOLD] 41.30",
                    "[BOLD] 69.32",
                    "[BOLD] 57.76"
                ]
            ]
        },
        "gold_description": "table iii reports the accuracy of different syntactic representations for all the benchmark datasets . in st encoding , the the corresponding paper . the experimental results demon strate that our proposed syntactic representation ( pos - cnn outperforms the previously proposed method ( st - cnn ) by the benchmark datasets ( 38 . 6 % in a large margin in all ccat10 , 30 . 80 % in ccat50 , 19 . 62 % in blogs10 , 11 . 94 % in blogs50 ) . this improvement in performance can be due syntactic representations are kept identical . according to table iii , pos - han outperforms pos - cnn model consistently across all the benchmark datasets ( 1 . 74 % in ccat10 , 0 . 32 % in ccat50 , 1 . 06 % in blogs10 , 2 . 91 % in blogs50 ) . this",
        "generated_description": "from the table , we can see that the pos - han outperforms the other two baselines by a large margin on both ccat10 and blogs10 . the pos - cnn and pos - st - cnn perform much worse than the st - cnn , which indicates that the simple pos tagger is not enough to capture the semantic information of words . in addition , the performance of pos - pos - cnn is also much lower than that of pos . the reason is that the sentence - level pos tags are usually not fine - grained enough to represent the meaning of a sentence , and the pos tags can be easily overwhelmed by the word - level information . on the other hand , the pos embeddings can capture the rich semantics of words , and thus can help to improve the performances of pos tagging ."
    },
    {
        "table_id": "484",
        "table_info": {
            "table_caption": "TABLE IV: The Accuracy of Syntactic (Syntactic-HAN), Lexical (Lexical-HAN),and combined (Style-HAN) Models",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] CCAT10",
                "[BOLD] CCAT50",
                "[BOLD] BLOGS10",
                "[BOLD] BLOGS50"
            ],
            "table_content_values": [
                [
                    "Syntactic-HAN",
                    "63.14",
                    "41.30",
                    "69.32",
                    "57.76"
                ],
                [
                    "Lexical-HAN",
                    "86.04",
                    "79.50",
                    "70.81",
                    "59.77"
                ],
                [
                    "Style-HAN",
                    "[BOLD] 90.58",
                    "[BOLD] 82.35",
                    "[BOLD] 72.83",
                    "[BOLD] 61.19"
                ]
            ]
        },
        "gold_description": "the table , lexical model consistently outperforms the syntactic model across all the benchmark datasets . moreover , combining the two representations further improves the performance results . based on the observation , we realize that even if syntactic - han achieves a comparable performance results combining it with lexical - han , slightly improves the overall performance ( style - han ) . as shown in table iv , the performance improvement in terms of accuracy is consistent across all the benchmark datasets ( 4 . 54 % in ccat10 , 2 . 85 % in ccat50 , 2 . 02 % in blogs10 , 1 . 42 % in blogs50 )",
        "generated_description": "in ccat10 and blogs10 , the accuracy of syntactic - han is 63 . 14 % and 41 . 30 % , respectively , and 57 . 76 % and 59 . 77 % for lexical - and style - enhanced versions of han respectively . in blogs50 , the accuracies of syntactic and lexical han are 86 . 04 % and 79 . 50 % respectively , which is much higher than that of the combined version ( 60 . 77 % ) . it indicates that the combined han is much better than the individual ones ."
    },
    {
        "table_id": "485",
        "table_info": {
            "table_caption": "TABLE V: The accuracy of Different Fusion Approaches",
            "table_column_names": [
                "[BOLD] Dataset",
                "[BOLD] Combined",
                "[BOLD] Parallel"
            ],
            "table_content_values": [
                [
                    "CCAT10",
                    "88.36",
                    "90.58"
                ],
                [
                    "CCAT50",
                    "81.21",
                    "82.35"
                ],
                [
                    "BLOG10",
                    "67.38",
                    "72.83"
                ],
                [
                    "BLOG50",
                    "58.81",
                    "61.19"
                ]
            ]
        },
        "gold_description": "table v reports the accuracy of the combined and the parallel fusion approaches . according to these results , training two parallel networks for lexical and syntax encoding achieves higher accuracy when compared to training the same network with combined embeddings .",
        "generated_description": "it can be observed that the combined approach performs better than the separate fusion approach for both ccat10 and blog50 datasets . the reason is that the ccat50 dataset is relatively simpler than blog10 and the number of sentences per document is much less than that of blog5 . therefore , it is easier for the model to find the commonalities among different sentences ."
    },
    {
        "table_id": "486",
        "table_info": {
            "table_caption": "TABLE VI: Test Accuracy of models for each dataset",
            "table_column_names": [
                "[BOLD] Model",
                "[BOLD] CCAT10",
                "[BOLD] CCAT50",
                "[BOLD] BLOGS10",
                "[BOLD] BLOGS50"
            ],
            "table_content_values": [
                [
                    "SVM-affix-punctuation 3-grams",
                    "78.8",
                    "69.3",
                    "#",
                    "#"
                ],
                [
                    "CNN-char",
                    "#",
                    "#",
                    "61.2",
                    "49.4"
                ],
                [
                    "Continuous n-gram",
                    "74.8",
                    "72.6",
                    "61.34",
                    "52.82"
                ],
                [
                    "N-gram CNN",
                    "86.8",
                    "76.5",
                    "63.74",
                    "53.09"
                ],
                [
                    "Syntax-CNN",
                    "88.20",
                    "81.00",
                    "64.10",
                    "56.73"
                ],
                [
                    "Style-HAN",
                    "[BOLD] 90.58",
                    "[BOLD] 82.35",
                    "[BOLD] 72.83",
                    "[BOLD] 61.19"
                ]
            ]
        },
        "gold_description": "table vi reports the accuracy of the models on the four benchmark datasets . all the results are bold . it shows that style - han outperforms the baselines by 2 . 38 % , 1 . 35 % , 8 . 73 % , and 4 . 46 % over the ccat10 , ccat50 , blogs10 , and blogs50 datasets , respectively .",
        "generated_description": "ccat10 and blogs10 . in ccat50 , han outperforms all the baselines by a large margin and achieves the best accuracy of 90 . 58 % , 82 . 35 % and 72 . 83 % in terms of test accuracy , respectively . for blogs10 , the performance of style - han is comparable to that of syntax - cnn and n - gram cnn , which are the most accurate neural network models for this task , and outperforms the more sophisticated continuous n - gram and syntax - based cnn models . the hyperparameters of the models are tuned on the validation set ."
    },
    {
        "table_id": "487",
        "table_info": {
            "table_caption": "Table 2: Experimental results for the discussion forum dataset (bold are best scores)",
            "table_column_names": [
                "Experiment",
                "[ITALIC] S P",
                "[ITALIC] S R",
                "[ITALIC] S F1",
                "[ITALIC] NS P",
                "[ITALIC] NS R",
                "[ITALIC] NS F1"
            ],
            "table_content_values": [
                [
                    "SVM [ITALIC] rbl",
                    "65.55",
                    "66.67",
                    "66.10",
                    "66.10",
                    "64.96",
                    "65.52"
                ],
                [
                    "SVM [ITALIC] c+ [ITALIC] rbl",
                    "63.32",
                    "61.97",
                    "62.63",
                    "62.77",
                    "64.10",
                    "63.5"
                ],
                [
                    "LSTM [ITALIC] r",
                    "67.90",
                    "66.23",
                    "67.1",
                    "67.08",
                    "[BOLD] 68.80",
                    "67.93"
                ],
                [
                    "LSTM [ITALIC] c+LSTM [ITALIC] r",
                    "66.19",
                    "79.49",
                    "72.23",
                    "74.33",
                    "59.40",
                    "66.03"
                ],
                [
                    "LSTM [ITALIC] conditional",
                    "[BOLD] 70.03",
                    "76.92",
                    "[BOLD] 73.32",
                    "74.41",
                    "67.10",
                    "[BOLD] 70.56"
                ],
                [
                    "LSTM [ITALIC] ras",
                    "69.45",
                    "70.94",
                    "70.19",
                    "70.30",
                    "68.80",
                    "69.45"
                ],
                [
                    "LSTM [ITALIC] cas+LSTM [ITALIC] ras",
                    "66.90",
                    "[BOLD] 82.05",
                    "[BOLD] 73.70",
                    "[BOLD] 76.80",
                    "59.40",
                    "66.99"
                ],
                [
                    "LSTM [ITALIC] caw+ [ITALIC] s+LSTM [ITALIC] raw+ [ITALIC] s",
                    "65.90",
                    "74.35",
                    "69.88",
                    "70.59",
                    "61.53",
                    "65.75"
                ]
            ]
        },
        "gold_description": "table 2 shows the classification results on the discussion forum dataset . although a vast majority of the context posts contain 3 - 4 sentences , around 100 context posts have more than ten sentences and thus we set a cutoff to a maximum of ten sentences for context modeling . for the reply r we considered the entire reply . the sv mbl models that are based on discrete features did not perform very well , and adding context actually hurt the performance . regarding the performance of the neural network models , we observe that modeling context improves the performance using all types of lstm architectures that read both context ( c ) and reply ( r ) the highest performance when considering both the s and s classes is achieved by the lstmconditional model ( 73 . 32 % f1 for s class and 70 . 56 % f1 for s , showing a 6 % and 3 % improvement over lstmr for s and s classes , respectively ) . the lstm model with sentence - level attentions on both context and reply ( lstmcas + lstmras ) gives the best f1 score of 73 . 7 % for the s class . for the s class , while we notice an improvement in precision we notice a drop in recall when compared to the lstm model with sentence level attention only on reply ( lstmras ) . we observe the performance ( 69 . 88 % f1 for s category ) deteriorates , probably due to the lack of enough training data .",
        "generated_description": "we can see that the conditional lstm performs the best in terms of s and ns , while the random classifier performs the worst . this is due to the nature of the data that we used in our experiments . in our experiments , we observed that the majority of the instances in the training set were labeled as offensive , which is also the reason why the performance of the conditional classifier was the worst in the original svm ."
    },
    {
        "table_id": "488",
        "table_info": {
            "table_caption": "Table 3: Experimental results for Twitter dataset (bold are best scores)",
            "table_column_names": [
                "Experiment",
                "[ITALIC] S P",
                "[ITALIC] S R",
                "[ITALIC] S F1",
                "[ITALIC] NS P",
                "[ITALIC] NS R",
                "[ITALIC] NS F1"
            ],
            "table_content_values": [
                [
                    "SVM [ITALIC] rbl",
                    "64.20",
                    "64.95",
                    "64.57",
                    "69.0",
                    "68.30",
                    "68.7"
                ],
                [
                    "SVM [ITALIC] c+ [ITALIC] rbl",
                    "65.64",
                    "65.86",
                    "65.75",
                    "70.11",
                    "69.91",
                    "70.0"
                ],
                [
                    "LSTM [ITALIC] r",
                    "73.25",
                    "58.72",
                    "65.19",
                    "61.47",
                    "75.44",
                    "67.74"
                ],
                [
                    "LSTM [ITALIC] c+LSTM [ITALIC] r",
                    "70.89",
                    "67.95",
                    "69.39",
                    "64.94",
                    "68.03",
                    "66.45"
                ],
                [
                    "LSTM [ITALIC] conditional",
                    "76.08",
                    "[BOLD] 76.53",
                    "[BOLD] 76.30",
                    "[BOLD] 72.93",
                    "72.44",
                    "[BOLD] 72.68"
                ],
                [
                    "LSTM [ITALIC] ras",
                    "76.00",
                    "73.18",
                    "74.56",
                    "70.52",
                    "73.52",
                    "71.9"
                ],
                [
                    "LSTM [ITALIC] cas+LSTM [ITALIC] ras",
                    "[BOLD] 77.25",
                    "75.51",
                    "[BOLD] 76.36",
                    "72.65",
                    "[BOLD] 74.52",
                    "[BOLD] 73.57"
                ],
                [
                    "LSTM [ITALIC] caw+LSTM [ITALIC] raw",
                    "76.74",
                    "69.77",
                    "73.09",
                    "68.63",
                    "75.77",
                    "72.02"
                ],
                [
                    "LSTM [ITALIC] caw+ [ITALIC] s+LSTM [ITALIC] raw+ [ITALIC] s",
                    "76.42",
                    "71.37",
                    "73.81",
                    "69.50",
                    "74.77",
                    "72.04"
                ]
            ]
        },
        "gold_description": "table 3 shows the results on the twitter dataset . as for discussion forums , adding context using the svm models does not show a statistically significant improvement . for the neural networks model , similar to the results on discussion forums , the lstm models that read both context and reply outperform the lstm model that reads only the reply ( lstmr ) . the best performing architectures are again the lstmconditional and lstm with sentence - level attentions ( lstmcas + lstmras ) . lstmconditional model shows an improvement of 11 % f1 on the s class and 4 - 5 % f1 on the s class , compared to lstmr . for the attentionbased models , the improvement using context is smaller ( ∼ 2 % f1 ) .",
        "generated_description": "we can observe that the conditional lstm performs the best in terms of s and ns , while the random classifier performs the worst for the other two classes . moreover , we can also see that the performance of the rbl classifier is close to that of the conditional classifier . this is due to the fact that this classifier has been trained on a large amount of unsupervised data , which may have contributed to its good performance ."
    },
    {
        "table_id": "489",
        "table_info": {
            "table_caption": "Table 2: Speaker-dependent setup. All results are averaged across five folds where each fold present weighted F-score across both sarcastic and non-sarcastic classes.",
            "table_column_names": [
                "Algorithm",
                "Modality",
                "Precision",
                "Recall",
                "F-Score"
            ],
            "table_content_values": [
                [
                    "Majority",
                    "-",
                    "25.0",
                    "50.0",
                    "33.3"
                ],
                [
                    "Random",
                    "-",
                    "49.5",
                    "49.5",
                    "49.8"
                ],
                [
                    "SVM",
                    "T",
                    "65.1",
                    "64.6",
                    "64.6"
                ],
                [
                    "SVM",
                    "A",
                    "65.9",
                    "64.6",
                    "64.6"
                ],
                [
                    "SVM",
                    "V",
                    "68.1",
                    "67.4",
                    "67.4"
                ],
                [
                    "SVM",
                    "T+A",
                    "66.6",
                    "66.2",
                    "66.2"
                ],
                [
                    "SVM",
                    "T+V",
                    "[BOLD] 72.0",
                    "[BOLD] 71.6",
                    "[BOLD] 71.6"
                ],
                [
                    "SVM",
                    "A+V",
                    "66.2",
                    "65.7",
                    "65.7"
                ],
                [
                    "SVM",
                    "T+A+V",
                    "71.9",
                    "71.4",
                    "71.5"
                ],
                [
                    "Δ [ITALIC] multi− [ITALIC] unimodal",
                    "[EMPTY]",
                    "↑3.9%",
                    "↑4.2%",
                    "↑4.2%"
                ],
                [
                    "Error rate reduction",
                    "[EMPTY]",
                    "↑12.2%",
                    "↑12.9%",
                    "↑12.9%"
                ]
            ]
        },
        "gold_description": "table 2 presents the classification results for sarcasm prediction in the speaker - dependent setup . the lowest performance is obtained with the majority baseline which achieves 33 . 3 % weighted fscore ( 66 . 7 % f - score for non - sarcastic class and 0 % for sarcastic ) . the pre - trained features for the visual modality provide the best performance among the unimodal variants . the addition of textual features through concatenation improves the unimodal baseline and achieves the best performance . the tri - modal variant is unable to achieve the best score due to a slightly sub - optimal performance from the audio modality . overall , the combination of visual and textual signals significantly improves over the unimodal variants , with a relative error rate reduction of up to 12 . 9 % . in multiple evaluations , the multimodal variants were shown to significantly outperform their unimodal counterparts , with relative error rate reductions of up to 12 . 9 % .",
        "generated_description": "the majority baseline is a random classifier with an f - score of 49 . 8 % . all the proposed approaches outperform the majority baseline by a large margin . the addition of visual features ( t , a , v ) and textual features ( a , t + a ) to the svm classifier improves the classification performance by 1 . 4 % and 1 . 6 % in terms of f - score , respectively . the best performance is achieved by the proposed multi - modal representation ( multi − multi − immodal ) , where the accuracy of multimodal representation is increased by 3 . 9 % and 4 . 2 % compared to the unimodal one ( multimodal + visual ) ."
    },
    {
        "table_id": "490",
        "table_info": {
            "table_caption": "Table 3: Multimodal sarcasm classification. Evaluated using an speaker-independent setup. Note: T=text, A=audio, V=video.",
            "table_column_names": [
                "Algorithm",
                "Modality",
                "Precision",
                "Recall",
                "F-Score"
            ],
            "table_content_values": [
                [
                    "Majority",
                    "-",
                    "32.8",
                    "57.3",
                    "41.7"
                ],
                [
                    "Random",
                    "-",
                    "51.1",
                    "50.2",
                    "50.4"
                ],
                [
                    "SVM",
                    "T",
                    "60.9",
                    "59.6",
                    "59.8"
                ],
                [
                    "SVM",
                    "A",
                    "[BOLD] 65.1",
                    "62.6",
                    "62.7"
                ],
                [
                    "SVM",
                    "V",
                    "54.9",
                    "53.4",
                    "53.6"
                ],
                [
                    "SVM",
                    "T+A",
                    "64.7",
                    "[BOLD] 62.9",
                    "[BOLD] 63.1"
                ],
                [
                    "SVM",
                    "T+V",
                    "62.2",
                    "61.5",
                    "61.7"
                ],
                [
                    "SVM",
                    "A+V",
                    "64.1",
                    "61.8",
                    "61.9"
                ],
                [
                    "SVM",
                    "T+A+V",
                    "64.3",
                    "62.6",
                    "62.8"
                ],
                [
                    "Δ [ITALIC] multi− [ITALIC] unimodal",
                    "[EMPTY]",
                    "↓0.4%",
                    "↑0.3%",
                    "↑0.4%"
                ],
                [
                    "Error rate reduction",
                    "[EMPTY]",
                    "↓1.1%",
                    "↑0.8%",
                    "↑1.1%"
                ]
            ]
        },
        "gold_description": "table 3 presents the performance of our baselines in the speaker - independent setup . in this case , the multimodal variants do not greatly outperform the unimodal counterparts . unlike table 2 , the audio channel plays a more important role , and it is slightly improved by adding text . in multiple evaluations , the multimodal variants were shown to significantly outperform their unimodal counterparts , with relative error rate reductions of up to 12 . 9 % .",
        "generated_description": "the majority baseline is a logistic regression classifier with the tf - idf kernel . we use the same hyperparameter settings as in the majority baseline . all the neural models outperform the random baseline . the best f - score is achieved using the unigram features ( 61 . 9 % and 63 . 1 % ) . the addition of the augmented visual features ( a ) improves the performance of the svm classifier by 1 . 3 % in terms of f - score and the error rate is reduced from 0 . 4 % to 0 . 3 % . the best accuracy is achieved when the multi - modal representation is concatenated with the unimodal representation ( t + a + v ) . the improvement is 1 . 4 % . the improvement can be explained by the fact that multimodal representations are more robust to the modality of the text . for example , in the case of a video , the visual features a and v encode the same information about the image , but the representation of the image is different from the one encoded by t ."
    },
    {
        "table_id": "491",
        "table_info": {
            "table_caption": "Table 4: Role of context and utterance’s speaker. Note: T=text, A=audio, V=video.",
            "table_column_names": [
                "Setup",
                "Features",
                "Precision",
                "Recall",
                "F-Score"
            ],
            "table_content_values": [
                [
                    "Speaker Dependent",
                    "T",
                    "65.1",
                    "64.6",
                    "64.6"
                ],
                [
                    "Speaker Dependent",
                    "+ context",
                    "65.5",
                    "65.1",
                    "65.0"
                ],
                [
                    "Speaker Dependent",
                    "+ speaker",
                    "[BOLD] 67.7",
                    "[BOLD] 67.2",
                    "[BOLD] 67.3"
                ],
                [
                    "Speaker Dependent",
                    "Best (T + V)",
                    "72.0",
                    "71.6",
                    "[BOLD] 71.8"
                ],
                [
                    "Speaker Dependent",
                    "+ context",
                    "71.9",
                    "71.4",
                    "71.5"
                ],
                [
                    "Speaker Dependent",
                    "+ speaker",
                    "[BOLD] 72.1",
                    "[BOLD] 71.7",
                    "[BOLD] 71.8"
                ],
                [
                    "Speaker Independent",
                    "T",
                    "[BOLD] 60.9",
                    "59.6",
                    "59.8"
                ],
                [
                    "Speaker Independent",
                    "+ context",
                    "57.9",
                    "54.5",
                    "54.1"
                ],
                [
                    "Speaker Independent",
                    "+ speaker",
                    "60.7",
                    "[BOLD] 60.7",
                    "[BOLD] 60.7"
                ],
                [
                    "Speaker Independent",
                    "Best (T + A)",
                    "64.7",
                    "[BOLD] 62.9",
                    "[BOLD] 63.1"
                ],
                [
                    "Speaker Independent",
                    "+ context",
                    "[BOLD] 65.2",
                    "[BOLD] 62.9",
                    "63.0"
                ],
                [
                    "Speaker Independent",
                    "+ speaker",
                    "64.7",
                    "[BOLD] 62.9",
                    "[BOLD] 63.1"
                ]
            ]
        },
        "gold_description": "table 4 shows the results for both evaluation settings for the textual baseline and the best multimodal variant . for the context features , we see a slight improvement in the best variant of the speaker independent setup ( text plus audio ) ; however , in other models , there is no improvement .",
        "generated_description": "in the dependency and speaker independent setups , the best results are achieved when both speaker independent features and context features are used . however , the results for the best ( t + v ) and best ( a ) setups are slightly worse than the results in the speaker dependency setup . this may be due to the fact that these two models were trained with the same training data , and therefore do not share the same hyper - parameters . in the speaker independent setup , since each speaker contributes independently to the model , the effect of context features is not as clear as the one from the speaker dependent feature set . the dependency feature set achieves better results than the best feature set in both speaker dependent and independent setups ."
    }
]